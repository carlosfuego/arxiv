[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v1",
                "updated": "2024-11-18T11:12:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v2",
                "updated": "2024-11-14T15:40:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    59,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v4",
                "updated": "2024-11-13T16:33:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    33,
                    33,
                    2,
                    318,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v1",
                "updated": "2024-11-18T18:59:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate.In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate.In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11832v1",
                "updated": "2024-11-18T18:51:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    51,
                    46,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:51:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    51,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Effects of Metallicity on Graphite, TiC, and SiC Condensation in Carbon\n  Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effects of Metallicity on Graphite, TiC, and SiC Condensation in Carbon\n  Stars"
                },
                "summary": "From transmission electron microscopy and other laboratory studies of\npresolar grains, the implicit condensation sequence of carbon-bearing\ncondensates in circumstellar envelopes of carbon stars is (from first to last)\nTiC-graphite-SiC. We use thermochemical equilibrium condensation calculations\nand show that the condensation sequence of TiC, graphite, and SiC depends on\nmetallicity in addition to C/O ratio and total pressure. Calculations were\nperformed for a characteristic carbon star ratio of C/O = 1.2 from 1E-10 to\n1E-4 bars total pressure and for uniform metallicity variations ranging from\n0.01 to 100 times solar elemental abundances. TiC always condenses before SiC,\nand the carbide condensation temperatures increase with increasing metallicity\nand total pressure. Graphite, however, can condense in a cooling circumstellar\nenvelope before TiC, between TiC and SiC, or after SiC, depending on the\ncarbon-bearing gas chemistry, which is dependent on metallicity and total\npressure. Analytical expressions for the graphite, TiC, and SiC condensation\ntemperatures as functions of metallicity and total pressure are presented. The\ninferred sequence from laboratory presolar grain studies, TiC-graphite-SiC, is\nfavored under equilibrium conditions at solar and subsolar metallicities\nbetween ~1E-5 to 1E-8 bar total pressure within circumstellar envelopes of\ncarbon stars with nominal C/O = 1.2. We also explored the dependence of the\nsequence at C/O ratios of 1.1 and 3.0 and found that as the C/O ratio\nincreases, the TiC-graphite-SiC region shifts towards higher total pressures\nand lower metallicities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From transmission electron microscopy and other laboratory studies of\npresolar grains, the implicit condensation sequence of carbon-bearing\ncondensates in circumstellar envelopes of carbon stars is (from first to last)\nTiC-graphite-SiC. We use thermochemical equilibrium condensation calculations\nand show that the condensation sequence of TiC, graphite, and SiC depends on\nmetallicity in addition to C/O ratio and total pressure. Calculations were\nperformed for a characteristic carbon star ratio of C/O = 1.2 from 1E-10 to\n1E-4 bars total pressure and for uniform metallicity variations ranging from\n0.01 to 100 times solar elemental abundances. TiC always condenses before SiC,\nand the carbide condensation temperatures increase with increasing metallicity\nand total pressure. Graphite, however, can condense in a cooling circumstellar\nenvelope before TiC, between TiC and SiC, or after SiC, depending on the\ncarbon-bearing gas chemistry, which is dependent on metallicity and total\npressure. Analytical expressions for the graphite, TiC, and SiC condensation\ntemperatures as functions of metallicity and total pressure are presented. The\ninferred sequence from laboratory presolar grain studies, TiC-graphite-SiC, is\nfavored under equilibrium conditions at solar and subsolar metallicities\nbetween ~1E-5 to 1E-8 bar total pressure within circumstellar envelopes of\ncarbon stars with nominal C/O = 1.2. We also explored the dependence of the\nsequence at C/O ratios of 1.1 and 3.0 and found that as the C/O ratio\nincreases, the TiC-graphite-SiC region shifts towards higher total pressures\nand lower metallicities."
                },
                "authors": [
                    {
                        "name": "Gabrielle Adams"
                    },
                    {
                        "name": "Katharina Lodders"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Lodders"
                },
                "author": "Katharina Lodders",
                "arxiv_comment": "33 pages, 9 figures, 2 tables. Preprint submitted to The\n  Astrophysical Journal, November 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07681v2",
                "updated": "2024-11-18T18:49:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    49,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-12T09:52:40Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    52,
                    40,
                    1,
                    317,
                    0
                ],
                "title": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?"
                },
                "summary": "Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques."
                },
                "authors": [
                    {
                        "name": "Katie Kang"
                    },
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Dibya Ghosh"
                    },
                    {
                        "name": "Jacob Steinhardt"
                    },
                    {
                        "name": "Claire Tomlin"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11830v1",
                "updated": "2024-11-18T18:48:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    45,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:48:45Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    45,
                    0,
                    323,
                    0
                ],
                "title": "Investigating the galaxy-halo connection of DESI Emission-Line Galaxies\n  with SHAMe-SF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the galaxy-halo connection of DESI Emission-Line Galaxies\n  with SHAMe-SF"
                },
                "summary": "The Dark Energy Spectroscopic Instrument (DESI) survey is mapping the\nlarge-scale distribution of millions of Emission Line Galaxies (ELGs) over vast\ncosmic volumes to measure the growth history of the Universe. However, compared\nto Luminous Red Galaxies (LRGs), very little is known about the connection of\nELGs with the underlying matter field. In this paper, we employ a novel\ntheoretical model, SHAMe-SF, to infer the connection between ELGs and their\nhost dark matter subhaloes. SHAMe-SF is a version of subhalo abundance matching\nthat incorporates prescriptions for multiple processes, including star\nformation, tidal stripping, environmental correlations, and quenching. We\nanalyse the public measurements of the projected and redshift-space ELGs\ncorrelation functions at $z=1.0$ and $z=1.3$ from DESI One Percent data\nrelease, which we fit over a broad range of scales $r \\in [0.1, 30]/h^{-1}$Mpc\nto within the statistical uncertainties of the data. We also validate the\ninference pipeline using two mock DESI ELG catalogues built from hydrodynamical\n(TNG300) and semi-analytical galaxy formation models (\\texttt{L-Galaxies}).\nSHAMe-SF is able to reproduce the clustering of DESI-ELGs and the mock DESI\nsamples within statistical uncertainties. We infer that DESI ELGs typically\nreside in haloes of $\\sim 10^{11.8}h^{-1}$M$_{\\odot}$ when they are central,\nand $\\sim 10^{12.5}h^{-1}$M$_{\\odot}$ when they are a satellite, which occurs\nin $\\sim$30 \\% of the cases. In addition, compared to the distribution of dark\nmatter within halos, satellite ELGs preferentially reside both in the outskirts\nand inside haloes, and have a net infall velocity towards the centre. Finally,\nour results show evidence of assembly bias and conformity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Energy Spectroscopic Instrument (DESI) survey is mapping the\nlarge-scale distribution of millions of Emission Line Galaxies (ELGs) over vast\ncosmic volumes to measure the growth history of the Universe. However, compared\nto Luminous Red Galaxies (LRGs), very little is known about the connection of\nELGs with the underlying matter field. In this paper, we employ a novel\ntheoretical model, SHAMe-SF, to infer the connection between ELGs and their\nhost dark matter subhaloes. SHAMe-SF is a version of subhalo abundance matching\nthat incorporates prescriptions for multiple processes, including star\nformation, tidal stripping, environmental correlations, and quenching. We\nanalyse the public measurements of the projected and redshift-space ELGs\ncorrelation functions at $z=1.0$ and $z=1.3$ from DESI One Percent data\nrelease, which we fit over a broad range of scales $r \\in [0.1, 30]/h^{-1}$Mpc\nto within the statistical uncertainties of the data. We also validate the\ninference pipeline using two mock DESI ELG catalogues built from hydrodynamical\n(TNG300) and semi-analytical galaxy formation models (\\texttt{L-Galaxies}).\nSHAMe-SF is able to reproduce the clustering of DESI-ELGs and the mock DESI\nsamples within statistical uncertainties. We infer that DESI ELGs typically\nreside in haloes of $\\sim 10^{11.8}h^{-1}$M$_{\\odot}$ when they are central,\nand $\\sim 10^{12.5}h^{-1}$M$_{\\odot}$ when they are a satellite, which occurs\nin $\\sim$30 \\% of the cases. In addition, compared to the distribution of dark\nmatter within halos, satellite ELGs preferentially reside both in the outskirts\nand inside haloes, and have a net infall velocity towards the centre. Finally,\nour results show evidence of assembly bias and conformity."
                },
                "authors": [
                    {
                        "name": "Sara Ortega-Martinez"
                    },
                    {
                        "name": "Sergio Contreras"
                    },
                    {
                        "name": "Raul E. Angulo"
                    },
                    {
                        "name": "Jonas Chaves-Montero"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Chaves-Montero"
                },
                "author": "Jonas Chaves-Montero",
                "arxiv_comment": "23 pages, 17 figures. To be submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11829v1",
                "updated": "2024-11-18T18:48:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:48:13Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    13,
                    0,
                    323,
                    0
                ],
                "title": "Tackling prediction tasks in relational databases with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tackling prediction tasks in relational databases with LLMs"
                },
                "summary": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction."
                },
                "authors": [
                    {
                        "name": "Marek Wydmuch"
                    },
                    {
                        "name": "ukasz Borchmann"
                    },
                    {
                        "name": "Filip Graliski"
                    }
                ],
                "author_detail": {
                    "name": "Filip Graliski"
                },
                "author": "Filip Graliski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11824v1",
                "updated": "2024-11-18T18:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    44,
                    0,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:44:00Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    44,
                    0,
                    0,
                    323,
                    0
                ],
                "title": "Theoretical Foundations of Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Foundations of Conformal Prediction"
                },
                "summary": "This book is about conformal prediction and related inferential techniques\nthat build on permutation tests and exchangeability. These techniques are\nuseful in a diverse array of tasks, including hypothesis testing and providing\nuncertainty quantification guarantees for machine learning systems. Much of the\ncurrent interest in conformal prediction is due to its ability to integrate\ninto complex machine learning workflows, solving the problem of forming\nprediction sets without any assumptions on the form of the data generating\ndistribution. Since contemporary machine learning algorithms have generally\nproven difficult to analyze directly, conformal prediction's main appeal is its\nability to provide formal, finite-sample guarantees when paired with such\nmethods.\n  The goal of this book is to teach the reader about the fundamental technical\narguments that arise when researching conformal prediction and related\nquestions in distribution-free inference. Many of these proof strategies,\nespecially the more recent ones, are scattered among research papers, making it\ndifficult for researchers to understand where to look, which results are\nimportant, and how exactly the proofs work. We hope to bridge this gap by\ncurating what we believe to be some of the most important results in the\nliterature and presenting their proofs in a unified language, with\nillustrations, and with an eye towards pedagogy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This book is about conformal prediction and related inferential techniques\nthat build on permutation tests and exchangeability. These techniques are\nuseful in a diverse array of tasks, including hypothesis testing and providing\nuncertainty quantification guarantees for machine learning systems. Much of the\ncurrent interest in conformal prediction is due to its ability to integrate\ninto complex machine learning workflows, solving the problem of forming\nprediction sets without any assumptions on the form of the data generating\ndistribution. Since contemporary machine learning algorithms have generally\nproven difficult to analyze directly, conformal prediction's main appeal is its\nability to provide formal, finite-sample guarantees when paired with such\nmethods.\n  The goal of this book is to teach the reader about the fundamental technical\narguments that arise when researching conformal prediction and related\nquestions in distribution-free inference. Many of these proof strategies,\nespecially the more recent ones, are scattered among research papers, making it\ndifficult for researchers to understand where to look, which results are\nimportant, and how exactly the proofs work. We hope to bridge this gap by\ncurating what we believe to be some of the most important results in the\nliterature and presenting their proofs in a unified language, with\nillustrations, and with an eye towards pedagogy."
                },
                "authors": [
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Rina Foygel Barber"
                    },
                    {
                        "name": "Stephen Bates"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Bates"
                },
                "author": "Stephen Bates",
                "arxiv_comment": "This material will be published by Cambridge University Press as\n  Theoretical Foundations of Conformal Prediction by Anastasios N.\n  Angelopoulos, Rina Foygel Barber and Stephen Bates. This prepublication\n  version is free to view/download for personal use only. Not for\n  redistribution/resale/use in derivative works. Copyright Anastasios N.\n  Angelopoulos, Rina Foygel Barber and Stephen Bates, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00024v2",
                "updated": "2024-11-18T18:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    41,
                    8,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-28T22:30:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    22,
                    30,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges"
                },
                "summary": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements."
                },
                "authors": [
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Benjamin Danek"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Christina Mack"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Yajuan Wang"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05478v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05478v5",
                "updated": "2024-11-18T18:23:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    23,
                    40,
                    0,
                    323,
                    0
                ],
                "published": "2024-07-07T19:33:30Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    19,
                    33,
                    30,
                    6,
                    189,
                    0
                ],
                "title": "Sequential Gaussian Variational Inference for Nonlinear State Estimation\n  and Its Application in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Gaussian Variational Inference for Nonlinear State Estimation\n  and Its Application in Robot Navigation"
                },
                "summary": "Probabilistic state estimation is essential for robots navigating uncertain\nenvironments. Accurately and efficiently managing uncertainty in estimated\nstates is key to robust robotic operation. However, nonlinearities in robotic\nplatforms pose significant challenges that require advanced estimation\ntechniques. Gaussian variational inference (GVI) offers an optimization\nperspective on the estimation problem, providing analytically tractable\nsolutions and efficiencies derived from the geometry of Gaussian space. We\npropose a Sequential Gaussian Variational Inference (S-GVI) method to address\nnonlinearity and provide efficient sequential inference processes. Our approach\nintegrates sequential Bayesian principles into the GVI framework, which are\naddressed using statistical approximations and gradient updates on the\ninformation geometry. Validations through simulations and real-world\nexperiments demonstrate significant improvements in state estimation over the\nMaximum A Posteriori (MAP) estimation method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic state estimation is essential for robots navigating uncertain\nenvironments. Accurately and efficiently managing uncertainty in estimated\nstates is key to robust robotic operation. However, nonlinearities in robotic\nplatforms pose significant challenges that require advanced estimation\ntechniques. Gaussian variational inference (GVI) offers an optimization\nperspective on the estimation problem, providing analytically tractable\nsolutions and efficiencies derived from the geometry of Gaussian space. We\npropose a Sequential Gaussian Variational Inference (S-GVI) method to address\nnonlinearity and provide efficient sequential inference processes. Our approach\nintegrates sequential Bayesian principles into the GVI framework, which are\naddressed using statistical approximations and gradient updates on the\ninformation geometry. Validations through simulations and real-world\nexperiments demonstrate significant improvements in state estimation over the\nMaximum A Posteriori (MAP) estimation method."
                },
                "authors": [
                    {
                        "name": "Min-Won Seo"
                    },
                    {
                        "name": "Solmaz S. Kia"
                    }
                ],
                "author_detail": {
                    "name": "Solmaz S. Kia"
                },
                "author": "Solmaz S. Kia",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05478v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05478v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11799v1",
                "updated": "2024-11-18T18:11:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    11,
                    53,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:11:53Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    11,
                    53,
                    0,
                    323,
                    0
                ],
                "title": "Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical\n  Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical\n  Image Fusion"
                },
                "summary": "Multimodal medical image fusion is a crucial task that combines complementary\ninformation from different imaging modalities into a unified representation,\nthereby enhancing diagnostic accuracy and treatment planning. While deep\nlearning methods, particularly Convolutional Neural Networks (CNNs) and\nTransformers, have significantly advanced fusion performance, some of the\nexisting CNN-based methods fall short in capturing fine-grained multiscale and\nedge features, leading to suboptimal feature integration. Transformer-based\nmodels, on the other hand, are computationally intensive in both the training\nand fusion stages, making them impractical for real-time clinical use.\nMoreover, the clinical application of fused images remains unexplored. In this\npaper, we propose a novel CNN-based architecture that addresses these\nlimitations by introducing a Dilated Residual Attention Network Module for\neffective multiscale feature extraction, coupled with a gradient operator to\nenhance edge detail learning. To ensure fast and efficient fusion, we present a\nparameter-free fusion strategy based on the weighted nuclear norm of softmax,\nwhich requires no additional computations during training or inference.\nExtensive experiments, including a downstream brain tumor classification task,\ndemonstrate that our approach outperforms various baseline methods in terms of\nvisual quality, texture preservation, and fusion speed, making it a possible\npractical solution for real-world clinical applications. The code will be\nreleased at https://github.com/simonZhou86/en_dran.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal medical image fusion is a crucial task that combines complementary\ninformation from different imaging modalities into a unified representation,\nthereby enhancing diagnostic accuracy and treatment planning. While deep\nlearning methods, particularly Convolutional Neural Networks (CNNs) and\nTransformers, have significantly advanced fusion performance, some of the\nexisting CNN-based methods fall short in capturing fine-grained multiscale and\nedge features, leading to suboptimal feature integration. Transformer-based\nmodels, on the other hand, are computationally intensive in both the training\nand fusion stages, making them impractical for real-time clinical use.\nMoreover, the clinical application of fused images remains unexplored. In this\npaper, we propose a novel CNN-based architecture that addresses these\nlimitations by introducing a Dilated Residual Attention Network Module for\neffective multiscale feature extraction, coupled with a gradient operator to\nenhance edge detail learning. To ensure fast and efficient fusion, we present a\nparameter-free fusion strategy based on the weighted nuclear norm of softmax,\nwhich requires no additional computations during training or inference.\nExtensive experiments, including a downstream brain tumor classification task,\ndemonstrate that our approach outperforms various baseline methods in terms of\nvisual quality, texture preservation, and fusion speed, making it a possible\npractical solution for real-world clinical applications. The code will be\nreleased at https://github.com/simonZhou86/en_dran."
                },
                "authors": [
                    {
                        "name": "Meng Zhou"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Xiaolan Xu"
                    },
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Farzad Khalvati"
                    }
                ],
                "author_detail": {
                    "name": "Farzad Khalvati"
                },
                "author": "Farzad Khalvati",
                "arxiv_comment": "An extended version of the paper accepted at IEEE BIBM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11795v1",
                "updated": "2024-11-18T18:08:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    8,
                    52,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:08:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    8,
                    52,
                    0,
                    323,
                    0
                ],
                "title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods"
                },
                "summary": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review)."
                },
                "authors": [
                    {
                        "name": "Egor Kovalev"
                    },
                    {
                        "name": "Georgii Bychkov"
                    },
                    {
                        "name": "Khaled Abud"
                    },
                    {
                        "name": "Aleksandr Gushchin"
                    },
                    {
                        "name": "Anna Chistyakova"
                    },
                    {
                        "name": "Sergey Lavrushkin"
                    },
                    {
                        "name": "Dmitriy Vatolin"
                    },
                    {
                        "name": "Anastasia Antsiferova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Antsiferova"
                },
                "author": "Anastasia Antsiferova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14846v2",
                "updated": "2024-11-18T18:03:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    3,
                    34,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-18T19:50:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    50,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Early Bright Galaxies from Helium Enhancements in High-Redshift Star\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Bright Galaxies from Helium Enhancements in High-Redshift Star\n  Clusters"
                },
                "summary": "The first few cycles of JWST have identified an overabundance of UV-bright\ngalaxies and a general excess of UV luminosity density at $z\\gtrsim10$ compared\nto expectations from most (pre-JWST) theoretical models. Moreover, some of the\nbrightest high-redshift spectroscopically confirmed galaxies exhibit peculiar\nchemical abundance patterns, most notably extremely high N/O ratios. Since N/O\nhas been empirically shown to scale strongly with He/H, as expected for hot\nhydrogen burning, these same bright high-redshift galaxies are likely also\nhelium-enhanced. Under simplistic assumptions for stellar evolution, the\nbolometric luminosity of a star scales as $L\\propto\n(4-\\frac{9}{2}Y+\\frac{5}{4}Y^{2})^{-1}$ -- hence a higher He/H leads to\nbrighter stars. In this Letter, we evolve a series of MESA models to the\nzero-age main-sequence and highlight that the helium enhancements at the levels\nmeasured and inferred for high-redshift galaxies can boost the 1500\n$\\mathring{\\rm A}$ UV luminosity by up to $\\sim50\\%$, while simultaneously\nincreasing the stellar effective temperature. The combination of helium\nenhancements with nebular continuum emission expected for intense bursts of\nstar formation have the potential to help reduce the tension between JWST\nobservations and certain galaxy formation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first few cycles of JWST have identified an overabundance of UV-bright\ngalaxies and a general excess of UV luminosity density at $z\\gtrsim10$ compared\nto expectations from most (pre-JWST) theoretical models. Moreover, some of the\nbrightest high-redshift spectroscopically confirmed galaxies exhibit peculiar\nchemical abundance patterns, most notably extremely high N/O ratios. Since N/O\nhas been empirically shown to scale strongly with He/H, as expected for hot\nhydrogen burning, these same bright high-redshift galaxies are likely also\nhelium-enhanced. Under simplistic assumptions for stellar evolution, the\nbolometric luminosity of a star scales as $L\\propto\n(4-\\frac{9}{2}Y+\\frac{5}{4}Y^{2})^{-1}$ -- hence a higher He/H leads to\nbrighter stars. In this Letter, we evolve a series of MESA models to the\nzero-age main-sequence and highlight that the helium enhancements at the levels\nmeasured and inferred for high-redshift galaxies can boost the 1500\n$\\mathring{\\rm A}$ UV luminosity by up to $\\sim50\\%$, while simultaneously\nincreasing the stellar effective temperature. The combination of helium\nenhancements with nebular continuum emission expected for intense bursts of\nstar formation have the potential to help reduce the tension between JWST\nobservations and certain galaxy formation models."
                },
                "authors": [
                    {
                        "name": "Harley Katz"
                    },
                    {
                        "name": "Alexander P. Ji"
                    },
                    {
                        "name": "O. Grace Telford"
                    },
                    {
                        "name": "Peter Senchyna"
                    }
                ],
                "author_detail": {
                    "name": "Peter Senchyna"
                },
                "author": "Peter Senchyna",
                "arxiv_comment": "4 pages, 3 figures, Accepted by The Open Journal of Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11779v1",
                "updated": "2024-11-18T17:56:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    56,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:56:13Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    56,
                    13,
                    0,
                    323,
                    0
                ],
                "title": "LLM-IE: A Python Package for Generative Information Extraction with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-IE: A Python Package for Generative Information Extraction with\n  Large Language Models"
                },
                "summary": "Objectives: Despite the recent adoption of large language models (LLMs) for\nbiomedical information extraction, challenges in prompt engineering and\nalgorithms persist, with no dedicated software available. To address this, we\ndeveloped LLM-IE: a Python package for building complete information extraction\npipelines. Our key innovation is an interactive LLM agent to support schema\ndefinition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity\nattribute extraction, and relation extraction tasks. We benchmarked on the i2b2\ndatasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best\nperformance while requiring a longer inference time. System evaluation provided\nintuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare\nand has been adopted in internal projects. It should hold great value to the\nbiomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building\nblocks for robust information extraction pipeline construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Despite the recent adoption of large language models (LLMs) for\nbiomedical information extraction, challenges in prompt engineering and\nalgorithms persist, with no dedicated software available. To address this, we\ndeveloped LLM-IE: a Python package for building complete information extraction\npipelines. Our key innovation is an interactive LLM agent to support schema\ndefinition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity\nattribute extraction, and relation extraction tasks. We benchmarked on the i2b2\ndatasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best\nperformance while requiring a longer inference time. System evaluation provided\nintuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare\nand has been adopted in internal projects. It should hold great value to the\nbiomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building\nblocks for robust information extraction pipeline construction."
                },
                "authors": [
                    {
                        "name": "Enshuo Hsu"
                    },
                    {
                        "name": "Kirk Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Kirk Roberts"
                },
                "author": "Kirk Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11767v1",
                "updated": "2024-11-18T17:46:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    46,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:46:32Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    46,
                    32,
                    0,
                    323,
                    0
                ],
                "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drowning in Documents: Consequences of Scaling Reranker Inference"
                },
                "summary": "Rerankers, typically cross-encoders, are often used to re-score the documents\nretrieved by cheaper initial IR systems. This is because, though expensive,\nrerankers are assumed to be more effective. We challenge this assumption by\nmeasuring reranker performance for full retrieval, not just re-scoring\nfirst-stage retrieval. Our experiments reveal a surprising trend: the best\nexisting rerankers provide diminishing returns when scoring progressively more\ndocuments and actually degrade quality beyond a certain limit. In fact, in this\nsetting, rerankers can frequently assign high scores to documents with no\nlexical or semantic overlap with the query. We hope that our findings will spur\nfuture research to improve reranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rerankers, typically cross-encoders, are often used to re-score the documents\nretrieved by cheaper initial IR systems. This is because, though expensive,\nrerankers are assumed to be more effective. We challenge this assumption by\nmeasuring reranker performance for full retrieval, not just re-scoring\nfirst-stage retrieval. Our experiments reveal a surprising trend: the best\nexisting rerankers provide diminishing returns when scoring progressively more\ndocuments and actually degrade quality beyond a certain limit. In fact, in this\nsetting, rerankers can frequently assign high scores to documents with no\nlexical or semantic overlap with the query. We hope that our findings will spur\nfuture research to improve reranking."
                },
                "authors": [
                    {
                        "name": "Mathew Jacob"
                    },
                    {
                        "name": "Erik Lindgren"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Michael Carbin"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Andrew Drozdov"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Drozdov"
                },
                "author": "Andrew Drozdov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22339v2",
                "updated": "2024-11-18T17:30:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    30,
                    47,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-11T18:47:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    18,
                    47,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "DAWN: Designing Distributed Agents in a Worldwide Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAWN: Designing Distributed Agents in a Worldwide Network"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has transformed them from\nbasic conversational tools into sophisticated entities capable of complex\nreasoning and decision-making. These advancements have led to the development\nof specialized LLM-based agents designed for diverse tasks such as coding and\nweb browsing. As these agents become more capable, the need for a robust\nframework that facilitates global communication and collaboration among them\ntowards advanced objectives has become increasingly critical. Distributed\nAgents in a Worldwide Network (DAWN) addresses this need by offering a\nversatile framework that integrates LLM-based agents with traditional software\nsystems, enabling the creation of agentic applications suited for a wide range\nof use cases. DAWN enables distributed agents worldwide to register and be\neasily discovered through Gateway Agents. Collaborations among these agents are\ncoordinated by a Principal Agent equipped with reasoning strategies. DAWN\noffers three operational modes: No-LLM Mode for deterministic tasks, Copilot\nfor augmented decision-making, and LLM Agent for autonomous operations.\nAdditionally, DAWN ensures the safety and security of agent collaborations\nglobally through a dedicated safety, security, and compliance layer, protecting\nthe network against attackers and adhering to stringent security and compliance\nstandards. These features make DAWN a robust network for deploying agent-based\napplications across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has transformed them from\nbasic conversational tools into sophisticated entities capable of complex\nreasoning and decision-making. These advancements have led to the development\nof specialized LLM-based agents designed for diverse tasks such as coding and\nweb browsing. As these agents become more capable, the need for a robust\nframework that facilitates global communication and collaboration among them\ntowards advanced objectives has become increasingly critical. Distributed\nAgents in a Worldwide Network (DAWN) addresses this need by offering a\nversatile framework that integrates LLM-based agents with traditional software\nsystems, enabling the creation of agentic applications suited for a wide range\nof use cases. DAWN enables distributed agents worldwide to register and be\neasily discovered through Gateway Agents. Collaborations among these agents are\ncoordinated by a Principal Agent equipped with reasoning strategies. DAWN\noffers three operational modes: No-LLM Mode for deterministic tasks, Copilot\nfor augmented decision-making, and LLM Agent for autonomous operations.\nAdditionally, DAWN ensures the safety and security of agent collaborations\nglobally through a dedicated safety, security, and compliance layer, protecting\nthe network against attackers and adhering to stringent security and compliance\nstandards. These features make DAWN a robust network for deploying agent-based\napplications across various industries."
                },
                "authors": [
                    {
                        "name": "Zahra Aminiranjbar"
                    },
                    {
                        "name": "Jianan Tang"
                    },
                    {
                        "name": "Qiudan Wang"
                    },
                    {
                        "name": "Shubha Pant"
                    },
                    {
                        "name": "Mahesh Viswanathan"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Viswanathan"
                },
                "author": "Mahesh Viswanathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11752v1",
                "updated": "2024-11-18T17:27:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    27,
                    56,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:27:56Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    27,
                    56,
                    0,
                    323,
                    0
                ],
                "title": "sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality\n  Spaces with LLMs and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality\n  Spaces with LLMs and Generative AI"
                },
                "summary": "In mixed reality (MR) environments, understanding space and creating virtual\nobjects is crucial to providing an intuitive and rich user experience. This\npaper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an\nMR application that combines Generative AI (GenAI) with large language models\n(LLMs) to assist users in creating, placing, and managing virtual objects\nwithin physical spaces. sMoRe allows users to use voice or typed text commands\nto create and place virtual objects using GenAI while specifying spatial\nconstraints. The system leverages LLMs to interpret users' commands, analyze\nthe current scene, and identify optimal locations. Additionally, sMoRe\nintegrates text-to-3D generative AI to dynamically create 3D objects based on\nusers' descriptions. Our user study demonstrates the effectiveness of sMoRe in\nenhancing user comprehension, interaction, and organization of the MR\nenvironment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In mixed reality (MR) environments, understanding space and creating virtual\nobjects is crucial to providing an intuitive and rich user experience. This\npaper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an\nMR application that combines Generative AI (GenAI) with large language models\n(LLMs) to assist users in creating, placing, and managing virtual objects\nwithin physical spaces. sMoRe allows users to use voice or typed text commands\nto create and place virtual objects using GenAI while specifying spatial\nconstraints. The system leverages LLMs to interpret users' commands, analyze\nthe current scene, and identify optimal locations. Additionally, sMoRe\nintegrates text-to-3D generative AI to dynamically create 3D objects based on\nusers' descriptions. Our user study demonstrates the effectiveness of sMoRe in\nenhancing user comprehension, interaction, and organization of the MR\nenvironment."
                },
                "authors": [
                    {
                        "name": "Yunhao Xing"
                    },
                    {
                        "name": "Que Liu"
                    },
                    {
                        "name": "Jingwu Wang"
                    },
                    {
                        "name": "Diego Gomez-Zara"
                    }
                ],
                "author_detail": {
                    "name": "Diego Gomez-Zara"
                },
                "author": "Diego Gomez-Zara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11749v1",
                "updated": "2024-11-18T17:25:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    25,
                    22,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:25:22Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    25,
                    22,
                    0,
                    323,
                    0
                ],
                "title": "Bounds on new neutrino interactions from the first CE$$NS data at\n  direct detection experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounds on new neutrino interactions from the first CE$$NS data at\n  direct detection experiments"
                },
                "summary": "Recently, two dark matter direct detection experiments have announced the\nfirst indications of nuclear recoils from solar $^8$B neutrinos via coherent\nelastic neutrino-nucleus scattering (CE$\\nu$NS) with xenon nuclei. These\nresults constitute a turning point, not only for dark matter searches that are\nnow entering the \\textit{neutrino fog}, but they also bring out new\nopportunities to exploit dark matter facilities as neutrino detectors. We\ninvestigate the implications of recent data from the PandaX-4T and XENONnT\nexperiments on both Standard Model physics and new neutrino interactions. We\nfirst extract information on the weak mixing angle at low momentum transfer.\nThen, following a phenomenological approach, we consider Lorentz-invariant\ninteractions (scalar, vector, axial-vector, and tensor) between neutrinos,\nquarks and charged leptons. Furthermore, we study the $U(1)_\\mathrm{B-L}$\nscenario as a concrete example of a new anomaly-free vector interaction. We\nfind that despite the low statistics of these first experimental results, the\ninferred bounds are in some cases already competitive. For the scope of this\nwork we also compute new bounds on some of the interactions using CE$\\nu$NS\ndata from COHERENT and electron recoil data from XENONnT, LUX-ZEPLIN,\nPandaX-4T, and TEXONO. It seems clear that while direct detection experiments\ncontinue to take data, more precise measurements will be available, thus\nallowing to test new neutrino interactions at the same level or even improving\nover dedicated neutrino facilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, two dark matter direct detection experiments have announced the\nfirst indications of nuclear recoils from solar $^8$B neutrinos via coherent\nelastic neutrino-nucleus scattering (CE$\\nu$NS) with xenon nuclei. These\nresults constitute a turning point, not only for dark matter searches that are\nnow entering the \\textit{neutrino fog}, but they also bring out new\nopportunities to exploit dark matter facilities as neutrino detectors. We\ninvestigate the implications of recent data from the PandaX-4T and XENONnT\nexperiments on both Standard Model physics and new neutrino interactions. We\nfirst extract information on the weak mixing angle at low momentum transfer.\nThen, following a phenomenological approach, we consider Lorentz-invariant\ninteractions (scalar, vector, axial-vector, and tensor) between neutrinos,\nquarks and charged leptons. Furthermore, we study the $U(1)_\\mathrm{B-L}$\nscenario as a concrete example of a new anomaly-free vector interaction. We\nfind that despite the low statistics of these first experimental results, the\ninferred bounds are in some cases already competitive. For the scope of this\nwork we also compute new bounds on some of the interactions using CE$\\nu$NS\ndata from COHERENT and electron recoil data from XENONnT, LUX-ZEPLIN,\nPandaX-4T, and TEXONO. It seems clear that while direct detection experiments\ncontinue to take data, more precise measurements will be available, thus\nallowing to test new neutrino interactions at the same level or even improving\nover dedicated neutrino facilities."
                },
                "authors": [
                    {
                        "name": "Valentina De Romeri"
                    },
                    {
                        "name": "Dimitrios K. Papoulias"
                    },
                    {
                        "name": "Christoph A. Ternes"
                    }
                ],
                "author_detail": {
                    "name": "Christoph A. Ternes"
                },
                "author": "Christoph A. Ternes",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06153v2",
                "updated": "2024-11-18T17:25:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    25,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-08T15:52:42Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    52,
                    42,
                    1,
                    282,
                    0
                ],
                "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Likai Ma"
                    },
                    {
                        "name": "Jiahe Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11745v1",
                "updated": "2024-11-18T17:16:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:16:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Ahmed F. AbouElhamayed"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Marta Andronic"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11737v1",
                "updated": "2024-11-18T17:04:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    4,
                    49,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:04:49Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    4,
                    49,
                    0,
                    323,
                    0
                ],
                "title": "Randomization-based Z-estimation for evaluating average and individual\n  treatment effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization-based Z-estimation for evaluating average and individual\n  treatment effects"
                },
                "summary": "Randomized experiments have been the gold standard for drawing causal\ninference. The conventional model-based approach has been one of the most\npopular ways for analyzing treatment effects from randomized experiments, which\nis often carried through inference for certain model parameters. In this paper,\nwe provide a systematic investigation of model-based analyses for treatment\neffects under the randomization-based inference framework. This framework does\nnot impose any distributional assumptions on the outcomes, covariates and their\ndependence, and utilizes only randomization as the \"reasoned basis\". We first\nderive the asymptotic theory for Z-estimation in completely randomized\nexperiments, and propose sandwich-type conservative covariance estimation. We\nthen apply the developed theory to analyze both average and individual\ntreatment effects in randomized experiments. For the average treatment effect,\nwe consider three estimation strategies: model-based, model-imputed, and\nmodel-assisted, where the first two can be sensitive to model misspecification\nor require specific ways for parameter estimation. The model-assisted approach\nis robust to arbitrary model misspecification and always provides consistent\naverage treatment effect estimation. We propose optimal ways to conduct\nmodel-assisted estimation using generally nonlinear least squares for parameter\nestimation. For the individual treatment effects, we propose to directly model\nthe relationship between individual effects and covariates, and discuss the\nmodel's identifiability, inference and interpretation allowing model\nmisspecification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments have been the gold standard for drawing causal\ninference. The conventional model-based approach has been one of the most\npopular ways for analyzing treatment effects from randomized experiments, which\nis often carried through inference for certain model parameters. In this paper,\nwe provide a systematic investigation of model-based analyses for treatment\neffects under the randomization-based inference framework. This framework does\nnot impose any distributional assumptions on the outcomes, covariates and their\ndependence, and utilizes only randomization as the \"reasoned basis\". We first\nderive the asymptotic theory for Z-estimation in completely randomized\nexperiments, and propose sandwich-type conservative covariance estimation. We\nthen apply the developed theory to analyze both average and individual\ntreatment effects in randomized experiments. For the average treatment effect,\nwe consider three estimation strategies: model-based, model-imputed, and\nmodel-assisted, where the first two can be sensitive to model misspecification\nor require specific ways for parameter estimation. The model-assisted approach\nis robust to arbitrary model misspecification and always provides consistent\naverage treatment effect estimation. We propose optimal ways to conduct\nmodel-assisted estimation using generally nonlinear least squares for parameter\nestimation. For the individual treatment effects, we propose to directly model\nthe relationship between individual effects and covariates, and discuss the\nmodel's identifiability, inference and interpretation allowing model\nmisspecification."
                },
                "authors": [
                    {
                        "name": "Tianyi Qu"
                    },
                    {
                        "name": "Jiangchuan Du"
                    },
                    {
                        "name": "Xinran Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Li"
                },
                "author": "Xinran Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15367v2",
                "updated": "2024-11-18T17:00:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    0,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-18T18:36:18Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    18,
                    36,
                    18,
                    2,
                    262,
                    0
                ],
                "title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss"
                },
                "summary": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Chernov"
                },
                "author": "Andrei Chernov",
                "arxiv_comment": "4 main pages; 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11731v1",
                "updated": "2024-11-18T16:59:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:59:59Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    59,
                    0,
                    323,
                    0
                ],
                "title": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment"
                },
                "summary": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion."
                },
                "authors": [
                    {
                        "name": "Allison Huang"
                    },
                    {
                        "name": "Yulu Niki Pi"
                    },
                    {
                        "name": "Carlos Mougan"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Mougan"
                },
                "author": "Carlos Mougan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11730v1",
                "updated": "2024-11-18T16:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    44,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    44,
                    0,
                    323,
                    0
                ],
                "title": "Lifted Model Construction without Normalisation: A Vectorised Approach\n  to Exploit Symmetries in Factor Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifted Model Construction without Normalisation: A Vectorised Approach\n  to Exploit Symmetries in Factor Graphs"
                },
                "summary": "Lifted probabilistic inference exploits symmetries in a probabilistic model\nto allow for tractable probabilistic inference with respect to domain sizes of\nlogical variables. We found that the current state-of-the-art algorithm to\nconstruct a lifted representation in form of a parametric factor graph misses\nsymmetries between factors that are exchangeable but scaled differently,\nthereby leading to a less compact representation. In this paper, we propose a\ngeneralisation of the advanced colour passing (ACP) algorithm, which is the\nstate of the art to construct a parametric factor graph. Our proposed algorithm\nallows for potentials of factors to be scaled arbitrarily and efficiently\ndetects more symmetries than the original ACP algorithm. By detecting strictly\nmore symmetries than ACP, our algorithm significantly reduces online query\ntimes for probabilistic inference when the resulting model is applied, which we\nalso confirm in our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifted probabilistic inference exploits symmetries in a probabilistic model\nto allow for tractable probabilistic inference with respect to domain sizes of\nlogical variables. We found that the current state-of-the-art algorithm to\nconstruct a lifted representation in form of a parametric factor graph misses\nsymmetries between factors that are exchangeable but scaled differently,\nthereby leading to a less compact representation. In this paper, we propose a\ngeneralisation of the advanced colour passing (ACP) algorithm, which is the\nstate of the art to construct a parametric factor graph. Our proposed algorithm\nallows for potentials of factors to be scaled arbitrarily and efficiently\ndetects more symmetries than the original ACP algorithm. By detecting strictly\nmore symmetries than ACP, our algorithm significantly reduces online query\ntimes for probabilistic inference when the resulting model is applied, which we\nalso confirm in our experiments."
                },
                "authors": [
                    {
                        "name": "Malte Luttermann"
                    },
                    {
                        "name": "Ralf Mller"
                    },
                    {
                        "name": "Marcel Gehrke"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Gehrke"
                },
                "author": "Marcel Gehrke",
                "arxiv_comment": "Accepted to the Proceedings of the 3rd Learning on Graphs Conference\n  (LoG 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11714v1",
                "updated": "2024-11-18T16:42:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    42,
                    7,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:42:07Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    42,
                    7,
                    0,
                    323,
                    0
                ],
                "title": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via\n  Skill Library and Tactile Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via\n  Skill Library and Tactile Representation"
                },
                "summary": "Deploying robots in open-world environments involves complex tasks\ncharacterized by long sequences and rich interactions, necessitating efficient\ntransfer of robotic skills across diverse and complex scenarios. To address\nthis challenge, we propose a skill library framework based on knowledge graphs,\nwhich endows robots with high-level skill awareness and spatial semantic\nunderstanding. The framework hierarchically organizes operational knowledge by\nconstructing a \"task graph\" and a \"scene graph\" to represent task and scene\nsemantic information, respectively. We introduce a \"state graph\" to facilitate\ninteraction between high-level task planning and low-level scene information.\nFurthermore, we propose a hierarchical transfer framework for operational\nskills. At the task level, the framework integrates contextual learning and\nchain-of-thought prompting within a four-stage prompt paradigm, leveraging\nlarge language models' (LLMs) reasoning and generalization capabilities to\nachieve task-level subtask sequence transfer. At the motion level, an adaptive\ntrajectory transfer method is developed using the A* algorithm and the skill\nlibrary, enabling motion-level adaptive trajectory transfer. At the physical\nlevel, we introduce an adaptive contour extraction and posture perception\nmethod based on tactile perception. This method dynamically obtains\nhigh-precision contour and posture information from visual-tactile texture data\nand adjusts transferred skills, such as contact positions and postures, to\nensure effectiveness in new environments. Experimental results validate the\neffectiveness of the proposed methods. Project\nwebsite:https://github.com/MingchaoQi/skill_transfer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying robots in open-world environments involves complex tasks\ncharacterized by long sequences and rich interactions, necessitating efficient\ntransfer of robotic skills across diverse and complex scenarios. To address\nthis challenge, we propose a skill library framework based on knowledge graphs,\nwhich endows robots with high-level skill awareness and spatial semantic\nunderstanding. The framework hierarchically organizes operational knowledge by\nconstructing a \"task graph\" and a \"scene graph\" to represent task and scene\nsemantic information, respectively. We introduce a \"state graph\" to facilitate\ninteraction between high-level task planning and low-level scene information.\nFurthermore, we propose a hierarchical transfer framework for operational\nskills. At the task level, the framework integrates contextual learning and\nchain-of-thought prompting within a four-stage prompt paradigm, leveraging\nlarge language models' (LLMs) reasoning and generalization capabilities to\nachieve task-level subtask sequence transfer. At the motion level, an adaptive\ntrajectory transfer method is developed using the A* algorithm and the skill\nlibrary, enabling motion-level adaptive trajectory transfer. At the physical\nlevel, we introduce an adaptive contour extraction and posture perception\nmethod based on tactile perception. This method dynamically obtains\nhigh-precision contour and posture information from visual-tactile texture data\nand adjusts transferred skills, such as contact positions and postures, to\nensure effectiveness in new environments. Experimental results validate the\neffectiveness of the proposed methods. Project\nwebsite:https://github.com/MingchaoQi/skill_transfer"
                },
                "authors": [
                    {
                        "name": "Mingchao Qi"
                    },
                    {
                        "name": "Yuanjin Li"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Zhengxiong Liu"
                    },
                    {
                        "name": "Panfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panfeng Huang"
                },
                "author": "Panfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11707v1",
                "updated": "2024-11-18T16:34:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    34,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:34:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    34,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models"
                },
                "summary": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data."
                },
                "authors": [
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Guoqiang Ma"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11699v1",
                "updated": "2024-11-18T16:23:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    23,
                    34,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:23:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    23,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "LiTformer: Efficient Modeling and Analysis of High-Speed Link\n  Transmitters Using Non-Autoregressive Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiTformer: Efficient Modeling and Analysis of High-Speed Link\n  Transmitters Using Non-Autoregressive Transformer"
                },
                "summary": "High-speed serial links are fundamental to energy-efficient and\nhigh-performance computing systems such as artificial intelligence, 5G mobile\nand automotive, enabling low-latency and high-bandwidth communication.\nTransmitters (TXs) within these links are key to signal quality, while their\nmodeling presents challenges due to nonlinear behavior and dynamic interactions\nwith links. In this paper, we propose LiTformer: a Transformer-based model for\nhigh-speed link TXs, with a non-sequential encoder and a Transformer decoder to\nincorporate link parameters and capture long-range dependencies of output\nsignals. We employ a non-autoregressive mechanism in model training and\ninference for parallel prediction of the signal sequence. LiTformer achieves\nprecise TX modeling considering link impacts including crosstalk from multiple\nlinks, and provides fast prediction for various long-sequence signals with high\ndata rates. Experimental results show that LiTformer achieves 148-456$\\times$\nspeedup for 2-link TXs and 404-944$\\times$ speedup for 16-link with mean\nrelative errors of 0.68-1.25%, supporting 4-bit signals at Gbps data rates of\nsingle-ended and differential TXs, as well as PAM4 TXs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-speed serial links are fundamental to energy-efficient and\nhigh-performance computing systems such as artificial intelligence, 5G mobile\nand automotive, enabling low-latency and high-bandwidth communication.\nTransmitters (TXs) within these links are key to signal quality, while their\nmodeling presents challenges due to nonlinear behavior and dynamic interactions\nwith links. In this paper, we propose LiTformer: a Transformer-based model for\nhigh-speed link TXs, with a non-sequential encoder and a Transformer decoder to\nincorporate link parameters and capture long-range dependencies of output\nsignals. We employ a non-autoregressive mechanism in model training and\ninference for parallel prediction of the signal sequence. LiTformer achieves\nprecise TX modeling considering link impacts including crosstalk from multiple\nlinks, and provides fast prediction for various long-sequence signals with high\ndata rates. Experimental results show that LiTformer achieves 148-456$\\times$\nspeedup for 2-link TXs and 404-944$\\times$ speedup for 16-link with mean\nrelative errors of 0.68-1.25%, supporting 4-bit signals at Gbps data rates of\nsingle-ended and differential TXs, as well as PAM4 TXs."
                },
                "authors": [
                    {
                        "name": "Songyu Sun"
                    },
                    {
                        "name": "Xiao Dong"
                    },
                    {
                        "name": "Yanliang Sha"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Cheng Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhuo"
                },
                "author": "Cheng Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02365v2",
                "updated": "2024-11-18T16:22:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    22,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-08-05T10:22:31Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    10,
                    22,
                    31,
                    0,
                    218,
                    0
                ],
                "title": "Non-parametric late-time expansion history reconstruction and\n  implications for the Hubble tension in light of recent DESI and Type Ia\n  supernovae data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-parametric late-time expansion history reconstruction and\n  implications for the Hubble tension in light of recent DESI and Type Ia\n  supernovae data"
                },
                "summary": "We non-parametrically reconstruct the late-time expansion history in light of\nthe latest Baryon Acoustic Oscillation (BAO) measurements from DESI combined\nwith various Type Ia Supernovae (SNeIa) catalogs, using interpolation through\npiece-wise natural cubic splines, and a reconstruction procedure based on\nGaussian Processes (GPs). Applied to DESI BAO and PantheonPlus SNeIa data, both\nmethods indicate that deviations from a reference $\\Lambda$CDM model in the $z\n\\lesssim 2$ unnormalized expansion rate $E(z)$ are constrained to be $\\lesssim\n10\\%$, but also consistently identify two features in $E(z)$: a bump at $z \\sim\n0.5$, and a depression at $z \\sim 0.9$, which cannot be simultaneously captured\nby a $w_0w_a$CDM fit. These features, which are stable against assumptions\nregarding spatial curvature, interpolation knots, and GP kernel, disappear if\none adopts the older SDSS BAO measurements in place of DESI, and decrease in\nsignificance when replacing the PantheonPlus catalog with the Union3 and DESY5\nones. We infer $c/(r_dH_0)=29.90 \\pm 0.33$, with $r_d$ the sound horizon at\nbaryon drag and $H_0$ the Hubble constant. Breaking the $r_d$-$H_0$ degeneracy\nwith the SH0ES prior on $H_0$, the significance of the tension between our\nnon-parametric determination of $r_d=136.20^{+2.20}_{-2.40}\\,{\\text{Mpc}}$ and\nthe \\textit{Planck} $\\Lambda$CDM-based determination is at the $5\\sigma$ level,\nslightly lower than the $6\\sigma$ obtained when adopting the older SDSS dataset\nin place of DESI. This indicates the persistence at very high significance of\nthe ``sound horizon tension'', reinforcing the need for pre-recombination new\nphysics. If substantiated in forthcoming data releases, our results tentatively\npoint to oscillatory/non-monotonic features in the shape of the expansion rate\nat $z \\lesssim 2$, of potential interest for dark energy model-building.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We non-parametrically reconstruct the late-time expansion history in light of\nthe latest Baryon Acoustic Oscillation (BAO) measurements from DESI combined\nwith various Type Ia Supernovae (SNeIa) catalogs, using interpolation through\npiece-wise natural cubic splines, and a reconstruction procedure based on\nGaussian Processes (GPs). Applied to DESI BAO and PantheonPlus SNeIa data, both\nmethods indicate that deviations from a reference $\\Lambda$CDM model in the $z\n\\lesssim 2$ unnormalized expansion rate $E(z)$ are constrained to be $\\lesssim\n10\\%$, but also consistently identify two features in $E(z)$: a bump at $z \\sim\n0.5$, and a depression at $z \\sim 0.9$, which cannot be simultaneously captured\nby a $w_0w_a$CDM fit. These features, which are stable against assumptions\nregarding spatial curvature, interpolation knots, and GP kernel, disappear if\none adopts the older SDSS BAO measurements in place of DESI, and decrease in\nsignificance when replacing the PantheonPlus catalog with the Union3 and DESY5\nones. We infer $c/(r_dH_0)=29.90 \\pm 0.33$, with $r_d$ the sound horizon at\nbaryon drag and $H_0$ the Hubble constant. Breaking the $r_d$-$H_0$ degeneracy\nwith the SH0ES prior on $H_0$, the significance of the tension between our\nnon-parametric determination of $r_d=136.20^{+2.20}_{-2.40}\\,{\\text{Mpc}}$ and\nthe \\textit{Planck} $\\Lambda$CDM-based determination is at the $5\\sigma$ level,\nslightly lower than the $6\\sigma$ obtained when adopting the older SDSS dataset\nin place of DESI. This indicates the persistence at very high significance of\nthe ``sound horizon tension'', reinforcing the need for pre-recombination new\nphysics. If substantiated in forthcoming data releases, our results tentatively\npoint to oscillatory/non-monotonic features in the shape of the expansion rate\nat $z \\lesssim 2$, of potential interest for dark energy model-building."
                },
                "authors": [
                    {
                        "name": "Jun-Qian Jiang"
                    },
                    {
                        "name": "Davide Pedrotti"
                    },
                    {
                        "name": "Simony Santos da Costa"
                    },
                    {
                        "name": "Sunny Vagnozzi"
                    }
                ],
                "author_detail": {
                    "name": "Sunny Vagnozzi"
                },
                "author": "Sunny Vagnozzi",
                "arxiv_comment": "19 pages, 8 sub-figures arranged into 5 figures. v2: additional\n  references added, minor clarifications to analysis, slight changes to title,\n  abstract, and figures. Version accepted for publication in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11694v1",
                "updated": "2024-11-18T16:15:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:15:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search"
                },
                "summary": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "LLM;Complex Reasoning;Math",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v1",
                "updated": "2024-11-18T16:09:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the\n  Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the\n  Physical World"
                },
                "summary": "Robotic manipulation refers to the autonomous handling and interaction of\nrobots with objects using advanced techniques in robotics and artificial\nintelligence. The advent of powerful tools such as large language models (LLMs)\nand large vision-language models (LVLMs) has significantly enhanced the\ncapabilities of these robots in environmental perception and decision-making.\nHowever, the introduction of these intelligent agents has led to security\nthreats such as jailbreak attacks and adversarial attacks.\n  In this research, we take a further step by proposing a backdoor attack\nspecifically targeting robotic manipulation and, for the first time,\nimplementing backdoor attack in the physical world. By embedding a backdoor\nvisual language model into the visual perception module within the robotic\nsystem, we successfully mislead the robotic arm's operation in the physical\nworld, given the presence of common items as triggers. Experimental evaluations\nin the physical world demonstrate the effectiveness of the proposed backdoor\nattack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation refers to the autonomous handling and interaction of\nrobots with objects using advanced techniques in robotics and artificial\nintelligence. The advent of powerful tools such as large language models (LLMs)\nand large vision-language models (LVLMs) has significantly enhanced the\ncapabilities of these robots in environmental perception and decision-making.\nHowever, the introduction of these intelligent agents has led to security\nthreats such as jailbreak attacks and adversarial attacks.\n  In this research, we take a further step by proposing a backdoor attack\nspecifically targeting robotic manipulation and, for the first time,\nimplementing backdoor attack in the physical world. By embedding a backdoor\nvisual language model into the visual perception module within the robotic\nsystem, we successfully mislead the robotic arm's operation in the physical\nworld, given the presence of common items as triggers. Experimental evaluations\nin the physical world demonstrate the effectiveness of the proposed backdoor\nattack."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "Initial version with preliminary results. We welcome any feedback or\n  suggestions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06954v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06954v3",
                "updated": "2024-11-18T16:06:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    6,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-09T14:51:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    51,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "How Unique is Whose Web Browser? The role of demographics in browser\n  fingerprinting among US users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Unique is Whose Web Browser? The role of demographics in browser\n  fingerprinting among US users"
                },
                "summary": "Browser fingerprinting can be used to identify and track users across the\nWeb, even without cookies, by collecting attributes from users' devices to\ncreate unique \"fingerprints\". This technique and resulting privacy risks have\nbeen studied for over a decade. Yet further research is limited because prior\nstudies used data not publicly available. Additionally, data in prior studies\nlacked user demographics. Here we provide a first-of-its-kind dataset to enable\nfurther research. It includes browser attributes with users' demographics and\nsurvey responses, collected with informed consent from 8,400 US study\nparticipants. We use this dataset to demonstrate how fingerprinting risks\ndiffer across demographic groups. For example, we find lower income users are\nmore at risk, and find that as users' age increases, they are both more likely\nto be concerned about fingerprinting and at real risk of fingerprinting.\nFurthermore, we demonstrate an overlooked risk: user demographics, such as\ngender, age, income level and race, can be inferred from browser attributes\ncommonly used for fingerprinting, and we identify which browser attributes most\ncontribute to this risk. Our data collection process also conducted an\nexperiment to study what impacts users' likelihood to share browser data for\nopen research, in order to inform future data collection efforts, with\nresponses from 12,461 total participants. Female participants were\nsignificantly less likely to share their browser data, as were participants who\nwere shown the browser data we asked to collect. Overall, we show the important\nrole of user demographics in the ongoing work that intends to assess\nfingerprinting risks and improve user privacy, with findings to inform future\nprivacy enhancing browser developments. The dataset and data collection tool we\nprovide can be used to further study research questions not addressed in this\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Browser fingerprinting can be used to identify and track users across the\nWeb, even without cookies, by collecting attributes from users' devices to\ncreate unique \"fingerprints\". This technique and resulting privacy risks have\nbeen studied for over a decade. Yet further research is limited because prior\nstudies used data not publicly available. Additionally, data in prior studies\nlacked user demographics. Here we provide a first-of-its-kind dataset to enable\nfurther research. It includes browser attributes with users' demographics and\nsurvey responses, collected with informed consent from 8,400 US study\nparticipants. We use this dataset to demonstrate how fingerprinting risks\ndiffer across demographic groups. For example, we find lower income users are\nmore at risk, and find that as users' age increases, they are both more likely\nto be concerned about fingerprinting and at real risk of fingerprinting.\nFurthermore, we demonstrate an overlooked risk: user demographics, such as\ngender, age, income level and race, can be inferred from browser attributes\ncommonly used for fingerprinting, and we identify which browser attributes most\ncontribute to this risk. Our data collection process also conducted an\nexperiment to study what impacts users' likelihood to share browser data for\nopen research, in order to inform future data collection efforts, with\nresponses from 12,461 total participants. Female participants were\nsignificantly less likely to share their browser data, as were participants who\nwere shown the browser data we asked to collect. Overall, we show the important\nrole of user demographics in the ongoing work that intends to assess\nfingerprinting risks and improve user privacy, with findings to inform future\nprivacy enhancing browser developments. The dataset and data collection tool we\nprovide can be used to further study research questions not addressed in this\nwork."
                },
                "authors": [
                    {
                        "name": "Alex Berke"
                    },
                    {
                        "name": "Enrico Bacis"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Robin Lassonde"
                    },
                    {
                        "name": "Pasin Manurangsi"
                    },
                    {
                        "name": "Umar Syed"
                    }
                ],
                "author_detail": {
                    "name": "Umar Syed"
                },
                "author": "Umar Syed",
                "arxiv_doi": "10.56553/popets-2025-0038",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.56553/popets-2025-0038",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.06954v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06954v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings on Privacy Enhancing Technologies 2025(1)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11679v1",
                "updated": "2024-11-18T15:59:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    50,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T15:59:50Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    50,
                    0,
                    323,
                    0
                ],
                "title": "Strong nanophotonic quantum squeezing exceeding 3.5 dB in a\n  foundry-compatible Kerr microresonator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong nanophotonic quantum squeezing exceeding 3.5 dB in a\n  foundry-compatible Kerr microresonator"
                },
                "summary": "Squeezed light, with its quantum noise reduction capabilities, has emerged as\na powerful resource in quantum information processing and precision metrology.\nTo reach noise reduction levels such that a quantum advantage is achieved,\noff-chip squeezers are typically used. The development of on-chip squeezed\nlight sources, particularly in nanophotonic platforms, has been challenging. We\nreport 3.7 $\\pm$ 0.2 dB of directly detected nanophotonic quantum squeezing\nusing foundry-fabricated silicon nitride (Si$_3$N$_4$) microrings with an\ninferred squeezing level of 10.7 dB on-chip. The squeezing level is robust\nacross multiple devices and pump detunings, and is consistent with the\novercoupling degree without noticeable degradation from excess classical noise.\nWe also offer insights to mitigate thermally-induced excess noise, that\ntypically degrades squeezing, by using small-radius rings with a larger free\nspectral range (450 GHz) and consequently lower parametric oscillation\nthresholds. Our results demonstrate that Si$_3$N$_4$ is a viable platform for\nstrong quantum noise reduction in a CMOS-compatible, scalable architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed light, with its quantum noise reduction capabilities, has emerged as\na powerful resource in quantum information processing and precision metrology.\nTo reach noise reduction levels such that a quantum advantage is achieved,\noff-chip squeezers are typically used. The development of on-chip squeezed\nlight sources, particularly in nanophotonic platforms, has been challenging. We\nreport 3.7 $\\pm$ 0.2 dB of directly detected nanophotonic quantum squeezing\nusing foundry-fabricated silicon nitride (Si$_3$N$_4$) microrings with an\ninferred squeezing level of 10.7 dB on-chip. The squeezing level is robust\nacross multiple devices and pump detunings, and is consistent with the\novercoupling degree without noticeable degradation from excess classical noise.\nWe also offer insights to mitigate thermally-induced excess noise, that\ntypically degrades squeezing, by using small-radius rings with a larger free\nspectral range (450 GHz) and consequently lower parametric oscillation\nthresholds. Our results demonstrate that Si$_3$N$_4$ is a viable platform for\nstrong quantum noise reduction in a CMOS-compatible, scalable architecture."
                },
                "authors": [
                    {
                        "name": "Yichen Shen"
                    },
                    {
                        "name": "Ping-Yen Hsieh"
                    },
                    {
                        "name": "Sashank Kaushik Sridhar"
                    },
                    {
                        "name": "Samantha Feldman"
                    },
                    {
                        "name": "You-Chia Chang"
                    },
                    {
                        "name": "Thomas A. Smith"
                    },
                    {
                        "name": "Avik Dutt"
                    }
                ],
                "author_detail": {
                    "name": "Avik Dutt"
                },
                "author": "Avik Dutt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11678v1",
                "updated": "2024-11-18T15:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    30,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T15:59:30Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    30,
                    0,
                    323,
                    0
                ],
                "title": "Analysis of Hardware Synthesis Strategies for Machine Learning in\n  Collider Trigger and Data Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Hardware Synthesis Strategies for Machine Learning in\n  Collider Trigger and Data Acquisition"
                },
                "summary": "To fully exploit the physics potential of current and future high energy\nparticle colliders, machine learning (ML) can be implemented in detector\nelectronics for intelligent data processing and acquisition. The implementation\nof ML in real-time at colliders requires very low latencies that are\nunachievable with a software-based approach, requiring optimization and\nsynthesis of ML algorithms for deployment on hardware. An analysis of neural\nnetwork inference efficiency is presented, focusing on the application of\ncollider trigger algorithms in field programmable gate arrays (FPGAs).\nTrade-offs are evaluated between two frameworks, the SLAC Neural Network\nLibrary (SNL) and hls4ml, in terms of resources and latency for different model\nsizes. Results highlight the strengths and limitations of each approach,\noffering valuable insights for optimizing real-time neural network deployments\nat colliders. This work aims to guide researchers and engineers in selecting\nthe most suitable hardware and software configurations for real-time,\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To fully exploit the physics potential of current and future high energy\nparticle colliders, machine learning (ML) can be implemented in detector\nelectronics for intelligent data processing and acquisition. The implementation\nof ML in real-time at colliders requires very low latencies that are\nunachievable with a software-based approach, requiring optimization and\nsynthesis of ML algorithms for deployment on hardware. An analysis of neural\nnetwork inference efficiency is presented, focusing on the application of\ncollider trigger algorithms in field programmable gate arrays (FPGAs).\nTrade-offs are evaluated between two frameworks, the SLAC Neural Network\nLibrary (SNL) and hls4ml, in terms of resources and latency for different model\nsizes. Results highlight the strengths and limitations of each approach,\noffering valuable insights for optimizing real-time neural network deployments\nat colliders. This work aims to guide researchers and engineers in selecting\nthe most suitable hardware and software configurations for real-time,\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Haoyi Jia"
                    },
                    {
                        "name": "Abhilasha Dave"
                    },
                    {
                        "name": "Julia Gonski"
                    },
                    {
                        "name": "Ryan Herbst"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Herbst"
                },
                "author": "Ryan Herbst",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11672v1",
                "updated": "2024-11-18T15:51:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    51,
                    45,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T15:51:45Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    51,
                    45,
                    0,
                    323,
                    0
                ],
                "title": "Artificial Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Scientific Discovery"
                },
                "summary": "Rooted in the explosion of deep learning over the past decade, this thesis\nspans from AlphaGo to ChatGPT to empirically examine the fundamental concepts\nneeded to realize the vision of an artificial scientist: a machine with the\ncapacity to autonomously generate original research and contribute to the\nexpansion of human knowledge. The investigation begins with {\\sc Olivaw}, an\nAlphaGo Zero-like agent that discovers Othello knowledge from scratch but is\nunable to communicate it. This realization leads to the development of the\nExplanatory Learning (EL) framework, a formalization of the problem faced by a\nscientist when trying to explain a new phenomenon to their peers. The effective\nEL prescriptions allow us to crack Zendo, a board game simulating the\nscientific endeavor. This success comes with a fundamental insight: an\nartificial scientist must develop its own interpretation of the language used\nto explain its findings. This perspective then leads us to see modern\nmultimodal models as interpreters, and to devise a new way to build\ninterpretable and cost-effective CLIP-like models: by coupling two unimodal\nmodels using little multimodal data and no further training. Finally, we\ndiscuss what ChatGPT and its siblings are still missing to become artificial\nscientists, and introduce Odeen, a benchmark about interpreting explanations\nthat sees LLMs going no further than random chance while being instead fully\nsolved by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rooted in the explosion of deep learning over the past decade, this thesis\nspans from AlphaGo to ChatGPT to empirically examine the fundamental concepts\nneeded to realize the vision of an artificial scientist: a machine with the\ncapacity to autonomously generate original research and contribute to the\nexpansion of human knowledge. The investigation begins with {\\sc Olivaw}, an\nAlphaGo Zero-like agent that discovers Othello knowledge from scratch but is\nunable to communicate it. This realization leads to the development of the\nExplanatory Learning (EL) framework, a formalization of the problem faced by a\nscientist when trying to explain a new phenomenon to their peers. The effective\nEL prescriptions allow us to crack Zendo, a board game simulating the\nscientific endeavor. This success comes with a fundamental insight: an\nartificial scientist must develop its own interpretation of the language used\nto explain its findings. This perspective then leads us to see modern\nmultimodal models as interpreters, and to devise a new way to build\ninterpretable and cost-effective CLIP-like models: by coupling two unimodal\nmodels using little multimodal data and no further training. Finally, we\ndiscuss what ChatGPT and its siblings are still missing to become artificial\nscientists, and introduce Odeen, a benchmark about interpreting explanations\nthat sees LLMs going no further than random chance while being instead fully\nsolved by humans."
                },
                "authors": [
                    {
                        "name": "Antonio Norelli"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Norelli"
                },
                "author": "Antonio Norelli",
                "arxiv_comment": "PhD thesis, 123 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12804v2",
                "updated": "2024-11-18T15:41:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    41,
                    24,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-24T16:31:11Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    16,
                    31,
                    11,
                    0,
                    176,
                    0
                ],
                "title": "Modulating Language Model Experiences through Frictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modulating Language Model Experiences through Frictions"
                },
                "summary": "Language models are transforming the ways that their users engage with the\nworld. Despite impressive capabilities, over-consumption of language model\noutputs risks propagating unchecked errors in the short-term and damaging human\ncapabilities for critical thinking in the long-term. How can we develop\nscaffolding around language models to curate more appropriate use? We propose\nselective frictions for language model experiences, inspired by behavioral\nscience interventions, to dampen misuse. Frictions involve small modifications\nto a user's experience, e.g., the addition of a button impeding model access\nand reminding a user of their expertise relative to the model. Through a user\nstudy with real humans, we observe shifts in user behavior from the imposition\nof a friction over LLMs in the context of a multi-topic question-answering task\nas a representative task that people may use LLMs for, e.g., in education and\ninformation retrieval. We find that frictions modulate over-reliance by driving\ndown users' click rates while minimally affecting accuracy for those topics.\nYet, frictions may have unintended effects. We find marked differences in\nusers' click behaviors even on topics where frictions were not provisioned. Our\ncontributions motivate further study of human-AI behavioral interaction to\ninform more effective and appropriate LLM use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are transforming the ways that their users engage with the\nworld. Despite impressive capabilities, over-consumption of language model\noutputs risks propagating unchecked errors in the short-term and damaging human\ncapabilities for critical thinking in the long-term. How can we develop\nscaffolding around language models to curate more appropriate use? We propose\nselective frictions for language model experiences, inspired by behavioral\nscience interventions, to dampen misuse. Frictions involve small modifications\nto a user's experience, e.g., the addition of a button impeding model access\nand reminding a user of their expertise relative to the model. Through a user\nstudy with real humans, we observe shifts in user behavior from the imposition\nof a friction over LLMs in the context of a multi-topic question-answering task\nas a representative task that people may use LLMs for, e.g., in education and\ninformation retrieval. We find that frictions modulate over-reliance by driving\ndown users' click rates while minimally affecting accuracy for those topics.\nYet, frictions may have unintended effects. We find marked differences in\nusers' click behaviors even on topics where frictions were not provisioned. Our\ncontributions motivate further study of human-AI behavioral interaction to\ninform more effective and appropriate LLM use."
                },
                "authors": [
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Malak Sadek"
                    },
                    {
                        "name": "Holli Sargeant"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Umang Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Umang Bhatt"
                },
                "author": "Umang Bhatt",
                "arxiv_comment": "NeurIPS Workshop on Behavioral ML; non-archival",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v6",
                "updated": "2024-11-18T15:41:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    41,
                    1,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing them shows limited performance. In this work, we facilitate utilizing\nLLMs in an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to guide the LLM to refine the modified molecule. We\nconduct experiments across both single- and multi-property objectives with 2\nthresholds, where $\\text{Re}^3$DF shows significant improvements. Particularly,\nfor 20 single-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 16.95%\nand 20.76% under loose (\\texttt{l}) and strict (\\texttt{s}) thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing them shows limited performance. In this work, we facilitate utilizing\nLLMs in an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to guide the LLM to refine the modified molecule. We\nconduct experiments across both single- and multi-property objectives with 2\nthresholds, where $\\text{Re}^3$DF shows significant improvements. Particularly,\nfor 20 single-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 16.95%\nand 20.76% under loose (\\texttt{l}) and strict (\\texttt{s}) thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13747v2",
                "updated": "2024-11-18T15:13:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    13,
                    19,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-17T16:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    45,
                    26,
                    3,
                    291,
                    0
                ],
                "title": "BayeSN and SALT: A Comparison of Dust Inference Across SN Ia Light-curve\n  Models with DES5YR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayeSN and SALT: A Comparison of Dust Inference Across SN Ia Light-curve\n  Models with DES5YR"
                },
                "summary": "In recent years there has been significant debate around the impact of dust\non SNe Ia, a major source of uncertainty in cosmological analyses. We perform\nthe first cross-comparison of the probabilistic hierarchical SN Ia SED model\nBayeSN with the conventional SALT model, an important test given the history of\nconflicting conclusions regarding the distributions of host galaxy dust\nproperties between the two. Applying BayeSN to SALT-based simulations, we find\nthat BayeSN is able to accurately recover our simulated inputs, establishing\nexcellent consistency between the two models. When inferring dust parameters\nwith simulated samples including non-Ia contamination, we find that our choice\nof photometric classifier causes a bias in the inferred dust distribution; this\narises because SNe Ia heavily impacted by dust are misclassified as\ncontaminants and excluded. We then apply BayeSN to the sample of SNe from\nDES5YR to jointly infer host galaxy dust distributions and intrinsic\ndifferences on either side of the 'mass step' at $10^{10}$ M$\\odot$. We find\nevidence in favour of an intrinsic contribution to the mass step and differing\n$R_V$ distributions. We also build on recent results supporting an\nenvironmental-dependence on the secondary maximum of SNe Ia in $i$-band. Twenty\ndays post-peak, we find an offset in intrinsic $i$-band light curve between\neach mass bin at a significance in excess of $3\\sigma$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years there has been significant debate around the impact of dust\non SNe Ia, a major source of uncertainty in cosmological analyses. We perform\nthe first cross-comparison of the probabilistic hierarchical SN Ia SED model\nBayeSN with the conventional SALT model, an important test given the history of\nconflicting conclusions regarding the distributions of host galaxy dust\nproperties between the two. Applying BayeSN to SALT-based simulations, we find\nthat BayeSN is able to accurately recover our simulated inputs, establishing\nexcellent consistency between the two models. When inferring dust parameters\nwith simulated samples including non-Ia contamination, we find that our choice\nof photometric classifier causes a bias in the inferred dust distribution; this\narises because SNe Ia heavily impacted by dust are misclassified as\ncontaminants and excluded. We then apply BayeSN to the sample of SNe from\nDES5YR to jointly infer host galaxy dust distributions and intrinsic\ndifferences on either side of the 'mass step' at $10^{10}$ M$\\odot$. We find\nevidence in favour of an intrinsic contribution to the mass step and differing\n$R_V$ distributions. We also build on recent results supporting an\nenvironmental-dependence on the secondary maximum of SNe Ia in $i$-band. Twenty\ndays post-peak, we find an offset in intrinsic $i$-band light curve between\neach mass bin at a significance in excess of $3\\sigma$."
                },
                "authors": [
                    {
                        "name": "Matthew Grayling"
                    },
                    {
                        "name": "Brodie Popovic"
                    }
                ],
                "author_detail": {
                    "name": "Brodie Popovic"
                },
                "author": "Brodie Popovic",
                "arxiv_comment": "13 pages, 2 figures, 9 tables. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07781v2",
                "updated": "2024-11-18T15:07:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    7,
                    8,
                    0,
                    323,
                    0
                ],
                "published": "2024-07-10T15:56:30Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    15,
                    56,
                    30,
                    2,
                    192,
                    0
                ],
                "title": "Sequential Kalman Tuning of the $t$-preconditioned Crank-Nicolson\n  algorithm: efficient, adaptive and gradient-free inference for Bayesian\n  inverse problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Kalman Tuning of the $t$-preconditioned Crank-Nicolson\n  algorithm: efficient, adaptive and gradient-free inference for Bayesian\n  inverse problems"
                },
                "summary": "Ensemble Kalman Inversion (EKI) has been proposed as an efficient method for\nthe approximate solution of Bayesian inverse problems with expensive forward\nmodels. However, when applied to the Bayesian inverse problem EKI is only exact\nin the regime of Gaussian target measures and linear forward models. In this\nwork we propose embedding EKI and Flow Annealed Kalman Inversion (FAKI), its\nnormalizing flow (NF) preconditioned variant, within a Bayesian annealing\nscheme as part of an adaptive implementation of the $t$-preconditioned\nCrank-Nicolson (tpCN) sampler. The tpCN sampler differs from standard pCN in\nthat its proposal is reversible with respect to the multivariate\n$t$-distribution. The more flexible tail behaviour allows for better adaptation\nto sampling from non-Gaussian targets. Within our Sequential Kalman Tuning\n(SKT) adaptation scheme, EKI is used to initialize and precondition the tpCN\nsampler for each annealed target. The subsequent tpCN iterations ensure\nparticles are correctly distributed according to each annealed target, avoiding\nthe accumulation of errors that would otherwise impact EKI. We demonstrate the\nperformance of SKT for tpCN on three challenging numerical benchmarks, showing\nsignificant improvements in the rate of convergence compared to adaptation\nwithin standard SMC with importance weighted resampling at each temperature\nlevel, and compared to similar adaptive implementations of standard pCN. The\nSKT scheme applied to tpCN offers an efficient, practical solution for solving\nthe Bayesian inverse problem when gradients of the forward model are not\navailable. Code implementing the SKT schemes for tpCN is available at\n\\url{https://github.com/RichardGrumitt/KalmanMC}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Kalman Inversion (EKI) has been proposed as an efficient method for\nthe approximate solution of Bayesian inverse problems with expensive forward\nmodels. However, when applied to the Bayesian inverse problem EKI is only exact\nin the regime of Gaussian target measures and linear forward models. In this\nwork we propose embedding EKI and Flow Annealed Kalman Inversion (FAKI), its\nnormalizing flow (NF) preconditioned variant, within a Bayesian annealing\nscheme as part of an adaptive implementation of the $t$-preconditioned\nCrank-Nicolson (tpCN) sampler. The tpCN sampler differs from standard pCN in\nthat its proposal is reversible with respect to the multivariate\n$t$-distribution. The more flexible tail behaviour allows for better adaptation\nto sampling from non-Gaussian targets. Within our Sequential Kalman Tuning\n(SKT) adaptation scheme, EKI is used to initialize and precondition the tpCN\nsampler for each annealed target. The subsequent tpCN iterations ensure\nparticles are correctly distributed according to each annealed target, avoiding\nthe accumulation of errors that would otherwise impact EKI. We demonstrate the\nperformance of SKT for tpCN on three challenging numerical benchmarks, showing\nsignificant improvements in the rate of convergence compared to adaptation\nwithin standard SMC with importance weighted resampling at each temperature\nlevel, and compared to similar adaptive implementations of standard pCN. The\nSKT scheme applied to tpCN offers an efficient, practical solution for solving\nthe Bayesian inverse problem when gradients of the forward model are not\navailable. Code implementing the SKT schemes for tpCN is available at\n\\url{https://github.com/RichardGrumitt/KalmanMC}."
                },
                "authors": [
                    {
                        "name": "Richard D. P. Grumitt"
                    },
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_doi": "10.1088/1361-6420/ad934b",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6420/ad934b",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "58 pages, 22 figures, accepted in Inverse Problems",
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11211v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11211v3",
                "updated": "2024-11-18T14:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    43,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-07-15T19:53:02Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    19,
                    53,
                    2,
                    0,
                    197,
                    0
                ],
                "title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion"
                },
                "summary": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues."
                },
                "authors": [
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Kyra Ahrens"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Published at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11211v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10261v2",
                "updated": "2024-11-18T14:43:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    43,
                    25,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-15T15:08:04Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    8,
                    4,
                    4,
                    320,
                    0
                ],
                "title": "Partial Scene Text Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial Scene Text Retrieval"
                },
                "summary": "The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Minghui Liao"
                    },
                    {
                        "name": "Zhouyi Xie"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Accepted on TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08745v2",
                "updated": "2024-11-18T14:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    41,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-13T16:26:19Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    26,
                    19,
                    2,
                    318,
                    0
                ],
                "title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers"
                },
                "summary": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models."
                },
                "authors": [
                    {
                        "name": "Clment Dumas"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "12 pages, 10 figures, previous version published under the title \"How\n  Do Llamas Process Multilingual Text? A Latent Exploration through Activation\n  Patching\" at the ICML 2024 mechanistic interpretability workshop at\n  https://openreview.net/forum?id=0ku2hIm4BS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07302v2",
                "updated": "2024-11-18T14:40:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    40,
                    54,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-11T14:30:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    14,
                    30,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BertaQA: How Much Do Language Models Know About Local Culture?"
                },
                "summary": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA."
                },
                "authors": [
                    {
                        "name": "Julen Etxaniz"
                    },
                    {
                        "name": "Gorka Azkune"
                    },
                    {
                        "name": "Aitor Soroa"
                    },
                    {
                        "name": "Oier Lopez de Lacalle"
                    },
                    {
                        "name": "Mikel Artetxe"
                    }
                ],
                "author_detail": {
                    "name": "Mikel Artetxe"
                },
                "author": "Mikel Artetxe",
                "arxiv_comment": "NEURIPS Datasets & Benchmarks 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18971v2",
                "updated": "2024-11-18T14:08:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    8,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-24T17:59:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "Detection of Undeclared EV Charging Events in a Green Energy\n  Certification Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Undeclared EV Charging Events in a Green Energy\n  Certification Scheme"
                },
                "summary": "The green potential of electric vehicles (EVs) can be fully realized only if\ntheir batteries are charged using energy generated from renewable (i.e. green)\nsources. For logistic or economic reasons, however, EV drivers may be tempted\nto avoid charging stations certified as providing green energy, instead opting\nfor conventional ones, where only a fraction of the available energy is green.\nThis behaviour may slow down the achievement of decarbonisation targets of the\nroad transport sector. In this paper, we use GPS data to infer whether an\nundeclared charging event has occurred. Specifically, we construct a Bayesian\nhypothesis test for the charging behaviour of the EV. Extensive simulations are\ncarried out for an area of London, using the mobility simulator, SUMO, and\nexploring various operating conditions. Excellent detection rates for\nundeclared charging events are reported. We explain how the algorithm can serve\nas the basis for an incentivization scheme, encouraging compliance by drivers\nwith green charging policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The green potential of electric vehicles (EVs) can be fully realized only if\ntheir batteries are charged using energy generated from renewable (i.e. green)\nsources. For logistic or economic reasons, however, EV drivers may be tempted\nto avoid charging stations certified as providing green energy, instead opting\nfor conventional ones, where only a fraction of the available energy is green.\nThis behaviour may slow down the achievement of decarbonisation targets of the\nroad transport sector. In this paper, we use GPS data to infer whether an\nundeclared charging event has occurred. Specifically, we construct a Bayesian\nhypothesis test for the charging behaviour of the EV. Extensive simulations are\ncarried out for an area of London, using the mobility simulator, SUMO, and\nexploring various operating conditions. Excellent detection rates for\nundeclared charging events are reported. We explain how the algorithm can serve\nas the basis for an incentivization scheme, encouraging compliance by drivers\nwith green charging policies."
                },
                "authors": [
                    {
                        "name": "Luca Domenico Loiacono"
                    },
                    {
                        "name": "Anthony Quinn"
                    },
                    {
                        "name": "Emanuele Crisostomi"
                    },
                    {
                        "name": "Robert Shorten"
                    }
                ],
                "author_detail": {
                    "name": "Robert Shorten"
                },
                "author": "Robert Shorten",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05724v3",
                "updated": "2024-11-18T14:07:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    7,
                    33,
                    0,
                    323,
                    0
                ],
                "published": "2024-04-08T17:59:02Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    17,
                    59,
                    2,
                    0,
                    99,
                    0
                ],
                "title": "JADES: Primaeval Lyman-$\\mathrm$ emitting galaxies reveal early\n  sites of reionisation out to redshift $z \\sim 9$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES: Primaeval Lyman-$\\mathrm$ emitting galaxies reveal early\n  sites of reionisation out to redshift $z \\sim 9$"
                },
                "summary": "$\\require{mediawiki-texvc}$Given the sensitivity of the resonant\nLyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by\nneutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs)\nhave been widely used to probe the ionising capabilities of reionisation-era\ngalaxies and their impact on the intergalactic medium (IGM). However, prior to\nJWST our understanding of the contribution of fainter sources and of ionised\n`bubbles' at earlier stages of reionisation remained uncertain. Here, we\npresent the characterisation of three exceptionally distant LAEs at $z>8$,\nnewly discovered by JWST/NIRSpec in the JADES survey. These three similarly\nbright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small\nLy$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta\nv_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of\nLy$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).\nThe former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions\n($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes\nremarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 72\\%$),\nwhich moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).\nWe find these LAEs are low-mass galaxies dominated by very recent, vigorous\nbursts of star formation accompanied by strong nebular emission from metal-poor\ngas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which\nreveals evidence for ionisation by an active galactic nucleus, may have\nreasonably produced small ionised bubbles preventing complete IGM absorption of\nLy$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical\nMpc}$ bubble, indicating faint galaxies have contributed significantly. The\nmost distant LAEs thus continue to be powerful observational probes into the\nearlier stages of reionisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\require{mediawiki-texvc}$Given the sensitivity of the resonant\nLyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by\nneutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs)\nhave been widely used to probe the ionising capabilities of reionisation-era\ngalaxies and their impact on the intergalactic medium (IGM). However, prior to\nJWST our understanding of the contribution of fainter sources and of ionised\n`bubbles' at earlier stages of reionisation remained uncertain. Here, we\npresent the characterisation of three exceptionally distant LAEs at $z>8$,\nnewly discovered by JWST/NIRSpec in the JADES survey. These three similarly\nbright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small\nLy$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta\nv_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of\nLy$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).\nThe former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions\n($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes\nremarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 72\\%$),\nwhich moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).\nWe find these LAEs are low-mass galaxies dominated by very recent, vigorous\nbursts of star formation accompanied by strong nebular emission from metal-poor\ngas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which\nreveals evidence for ionisation by an active galactic nucleus, may have\nreasonably produced small ionised bubbles preventing complete IGM absorption of\nLy$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical\nMpc}$ bubble, indicating faint galaxies have contributed significantly. The\nmost distant LAEs thus continue to be powerful observational probes into the\nearlier stages of reionisation."
                },
                "authors": [
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Kristan Boyett"
                    },
                    {
                        "name": "Alex J. Cameron"
                    },
                    {
                        "name": "Phillip A. Cargile"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stphane Charlot"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Ryan Hausen"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Isaac Laseter"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Chris Willott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Willott"
                },
                "author": "Chris Willott",
                "arxiv_doi": "10.1093/mnras/stae2535",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2535",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.05724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 13 figures, accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11582v1",
                "updated": "2024-11-18T13:59:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    59,
                    29,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    59,
                    29,
                    0,
                    323,
                    0
                ],
                "title": "Exploring LLMs for Verifying Technical System Specifications Against\n  Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Verifying Technical System Specifications Against\n  Requirements"
                },
                "summary": "Requirements engineering is a knowledge intensive process and crucial for the\nsuccess of engineering projects. The field of knowledge-based requirements\nengineering (KBRE) aims to support engineers by providing knowledge to assist\nin the elicitation, validation, and management of system requirements. The\nadvent of large language models (LLMs) opens new opportunities in the field of\nKBRE. This work experimentally investigates the potential of LLMs in\nrequirements verification. Therein, LLMs are provided with a set of\nrequirements and a textual system specification and are prompted to assess\nwhich requirements are fulfilled by the system specification. Different\nexperimental variables such as system specification complexity, the number of\nrequirements, and prompting strategies were analyzed. Formal rule-based systems\nserve as a benchmark to compare LLM performance to. Requirements and system\nspecifications are derived from the smart-grid domain. Results show that\nadvanced LLMs, like GPT-4o and Claude 3.5 Sonnet, achieved f1-scores between 79\n% and 94 % in identifying non-fulfilled requirements, indicating potential for\nLLMs to be leveraged for requirements verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements engineering is a knowledge intensive process and crucial for the\nsuccess of engineering projects. The field of knowledge-based requirements\nengineering (KBRE) aims to support engineers by providing knowledge to assist\nin the elicitation, validation, and management of system requirements. The\nadvent of large language models (LLMs) opens new opportunities in the field of\nKBRE. This work experimentally investigates the potential of LLMs in\nrequirements verification. Therein, LLMs are provided with a set of\nrequirements and a textual system specification and are prompted to assess\nwhich requirements are fulfilled by the system specification. Different\nexperimental variables such as system specification complexity, the number of\nrequirements, and prompting strategies were analyzed. Formal rule-based systems\nserve as a benchmark to compare LLM performance to. Requirements and system\nspecifications are derived from the smart-grid domain. Results show that\nadvanced LLMs, like GPT-4o and Claude 3.5 Sonnet, achieved f1-scores between 79\n% and 94 % in identifying non-fulfilled requirements, indicating potential for\nLLMs to be leveraged for requirements verification."
                },
                "authors": [
                    {
                        "name": "Lasse M. Reinpold"
                    },
                    {
                        "name": "Marvin Schieseck"
                    },
                    {
                        "name": "Lukas P. Wagner"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "arxiv_comment": "Submitted to 3rd IEEE Industrial Electronics Society Annual Online\n  Conference (ONCON)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v1",
                "updated": "2024-11-18T13:57:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agents Social Interaction Simulations on One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agents Social Interaction Simulations on One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (\\emph{i.e.}, X, Reddit) with more realistic\nlarge language model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (\\emph{i.e.},\ndynamic social networks and post information), diverse action spaces\n(\\emph{i.e.}, following, commenting), and recommendation systems (\\emph{i.e.},\ninterest-based and hot-score-based). Additionally, OASIS supports large-scale\nuser simulations, capable of modeling up to one million users. With these\nfeatures, OASIS can be easily extended to different social media platforms to\nstudy large-scale group phenomena and behaviors. We replicate various social\nphenomena, including information spreading, group polarization, and herd\neffects across X and Reddit platforms. Moreover, we provide observations of\nsocial phenomena at different agent group scales. We observe that the larger\nagent group scale leads to more enhanced group dynamics and more diverse and\nhelpful agents' opinions. These findings demonstrate OASIS's potential as a\npowerful tool for studying complex systems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (\\emph{i.e.}, X, Reddit) with more realistic\nlarge language model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (\\emph{i.e.},\ndynamic social networks and post information), diverse action spaces\n(\\emph{i.e.}, following, commenting), and recommendation systems (\\emph{i.e.},\ninterest-based and hot-score-based). Additionally, OASIS supports large-scale\nuser simulations, capable of modeling up to one million users. With these\nfeatures, OASIS can be easily extended to different social media platforms to\nstudy large-scale group phenomena and behaviors. We replicate various social\nphenomena, including information spreading, group polarization, and herd\neffects across X and Reddit platforms. Moreover, we provide observations of\nsocial phenomena at different agent group scales. We observe that the larger\nagent group scale leads to more enhanced group dynamics and more diverse and\nhelpful agents' opinions. These findings demonstrate OASIS's potential as a\npowerful tool for studying complex systems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11560v1",
                "updated": "2024-11-18T13:26:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    26,
                    9,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:26:09Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    26,
                    9,
                    0,
                    323,
                    0
                ],
                "title": "Topology-aware Preemptive Scheduling for Co-located LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology-aware Preemptive Scheduling for Co-located LLM Workloads"
                },
                "summary": "Hosting diverse large language model workloads in a unified resource pool\nthrough co-location is cost-effective. For example, long-running chat services\ngenerally follow diurnal traffic patterns, which inspire co-location of batch\njobs to fulfill resource valleys between successive peaks, and thus to saturate\nresource allocation in cluster-wide scope. These heterogeneous workloads often\nhave different business priorities, and therefore preemption can be leveraged\nfor resource elasticity. However, workloads often have distinct topology\npreferences as well. The resources released by lower-priority instances may\nfail to meet the requirements of high-priority online services which are\nusually latency-sensitive. The root cause behind such mis-match is a lack of\ntopology awareness of resource scheduler, especially during preemption. To\nbridge this gap, we develop a fine-grained topology-aware method for preemptive\nscheduling of hybrid workloads. The method ensures that the resources freed by\npreempted tasks adhere to the topological affinity needs of high-priority\npreemptors in a guaranteed or best-effort manner. This dynamic alignment\nsignificantly increases the efficiency of preemption and improves overall\nscheduled performance for LLM workloads by $55\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hosting diverse large language model workloads in a unified resource pool\nthrough co-location is cost-effective. For example, long-running chat services\ngenerally follow diurnal traffic patterns, which inspire co-location of batch\njobs to fulfill resource valleys between successive peaks, and thus to saturate\nresource allocation in cluster-wide scope. These heterogeneous workloads often\nhave different business priorities, and therefore preemption can be leveraged\nfor resource elasticity. However, workloads often have distinct topology\npreferences as well. The resources released by lower-priority instances may\nfail to meet the requirements of high-priority online services which are\nusually latency-sensitive. The root cause behind such mis-match is a lack of\ntopology awareness of resource scheduler, especially during preemption. To\nbridge this gap, we develop a fine-grained topology-aware method for preemptive\nscheduling of hybrid workloads. The method ensures that the resources freed by\npreempted tasks adhere to the topological affinity needs of high-priority\npreemptors in a guaranteed or best-effort manner. This dynamic alignment\nsignificantly increases the efficiency of preemption and improves overall\nscheduled performance for LLM workloads by $55\\%$."
                },
                "authors": [
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Jinjie Yang"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_comment": "17 Pages, 11 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06913v2",
                "updated": "2024-11-18T13:15:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    15,
                    41,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-09T14:12:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning"
                },
                "summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict, on the other hand, emerges as the LLM's\nknowledge evolves during SFT, allowing it to answer questions that were\npreviously unanswerable. Yet, these now-answerable training samples still\nretain the original \"I don't know\" supervision signals based on the initial LLM\nstate, resulting in inconsistencies. These conflicts cause the trained LLM to\nmisclassify known questions as unknown, resulting in over-refusal. To address\nthis issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware\nInstructions Tuning (CRaFT). CRaFT centers on two main contributions: First, we\nadditionally incorporate response certainty to selectively filter and modify\ndata, reducing static conflicts. Second, we implement preliminary rehearsal\ntraining to characterize changes in the LLM's knowledge state, which helps\nmitigate dynamic conflicts during the fine-tuning process. We conducted\nextensive experiments on open-ended question answering and multiple-choice\nquestion task. Experiment results show that CRaFT can improve LLM's overall\nperformance during the RAIT process. Source code and training data will be\nreleased at Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict, on the other hand, emerges as the LLM's\nknowledge evolves during SFT, allowing it to answer questions that were\npreviously unanswerable. Yet, these now-answerable training samples still\nretain the original \"I don't know\" supervision signals based on the initial LLM\nstate, resulting in inconsistencies. These conflicts cause the trained LLM to\nmisclassify known questions as unknown, resulting in over-refusal. To address\nthis issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware\nInstructions Tuning (CRaFT). CRaFT centers on two main contributions: First, we\nadditionally incorporate response certainty to selectively filter and modify\ndata, reducing static conflicts. Second, we implement preliminary rehearsal\ntraining to characterize changes in the LLM's knowledge state, which helps\nmitigate dynamic conflicts during the fine-tuning process. We conducted\nextensive experiments on open-ended question answering and multiple-choice\nquestion task. Experiment results show that CRaFT can improve LLM's overall\nperformance during the RAIT process. Source code and training data will be\nreleased at Github."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08738v3",
                "updated": "2024-11-18T13:09:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    9,
                    52,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-14T16:24:56Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    16,
                    24,
                    56,
                    1,
                    135,
                    0
                ],
                "title": "Calibrated sensitivity models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated sensitivity models"
                },
                "summary": "In causal inference, sensitivity models assess how unmeasured confounders\ncould alter causal analyses, but the sensitivity parameter -- which quantifies\nthe degree of unmeasured confounding -- is often difficult to interpret. For\nthis reason, researchers sometimes compare the sensitivity parameter to an\nestimate of measured confounding. This is known as calibration, or\nbenchmarking. Although it can aid interpretation, calibration is typically\nconducted post hoc, and uncertainty in the estimate for unmeasured confounding\nis rarely accounted for. To address these limitations, we propose calibrated\nsensitivity models, which directly bound the degree of unmeasured confounding\nby a multiple of measured confounding. The calibrated sensitivity parameter is\ninterpretable as a ratio of unmeasured to measured confounding, and uncertainty\ndue to estimating measured confounding can be incorporated. Incorporating this\nuncertainty shows causal analyses can be less or more robust to unmeasured\nconfounding than suggested by standard approaches. We develop efficient\nestimators and inferential methods for bounds on the average treatment effect\nwith three calibrated sensitivity models, establishing parametric efficiency\nand asymptotic normality under doubly robust style nonparametric conditions. We\nillustrate our methods with an analysis of the effect of mothers' smoking on\ninfant birthweight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, sensitivity models assess how unmeasured confounders\ncould alter causal analyses, but the sensitivity parameter -- which quantifies\nthe degree of unmeasured confounding -- is often difficult to interpret. For\nthis reason, researchers sometimes compare the sensitivity parameter to an\nestimate of measured confounding. This is known as calibration, or\nbenchmarking. Although it can aid interpretation, calibration is typically\nconducted post hoc, and uncertainty in the estimate for unmeasured confounding\nis rarely accounted for. To address these limitations, we propose calibrated\nsensitivity models, which directly bound the degree of unmeasured confounding\nby a multiple of measured confounding. The calibrated sensitivity parameter is\ninterpretable as a ratio of unmeasured to measured confounding, and uncertainty\ndue to estimating measured confounding can be incorporated. Incorporating this\nuncertainty shows causal analyses can be less or more robust to unmeasured\nconfounding than suggested by standard approaches. We develop efficient\nestimators and inferential methods for bounds on the average treatment effect\nwith three calibrated sensitivity models, establishing parametric efficiency\nand asymptotic normality under doubly robust style nonparametric conditions. We\nillustrate our methods with an analysis of the effect of mothers' smoking on\ninfant birthweight."
                },
                "authors": [
                    {
                        "name": "Alec McClean"
                    },
                    {
                        "name": "Zach Branson"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    }
                ],
                "author_detail": {
                    "name": "Edward H. Kennedy"
                },
                "author": "Edward H. Kennedy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v1",
                "updated": "2024-11-18T13:01:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11539v1",
                "updated": "2024-11-18T12:52:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    52,
                    4,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:52:04Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    52,
                    4,
                    0,
                    323,
                    0
                ],
                "title": "Channel Capacity-Aware Distributed Encoding for Multi-View Sensing and\n  Edge Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Capacity-Aware Distributed Encoding for Multi-View Sensing and\n  Edge Inference"
                },
                "summary": "Integrated sensing and communication (ISAC) unifies wireless communication\nand sensing by sharing spectrum and hardware, which often incurs trade-offs\nbetween two functions due to limited resources. However, this paper shifts\nfocus to exploring the synergy between communication and sensing, using WiFi\nsensing as an exemplary scenario where communication signals are repurposed to\nprobe the environment without dedicated sensing waveforms, followed by data\nuploading to the edge server for inference. While increased device\nparticipation enhances multi-view sensing data, it also imposes significant\ncommunication overhead between devices and the edge server. To address this\nchallenge, we aim to maximize the sensing task performance, measured by mutual\ninformation, under the channel capacity constraint. The information-theoretic\noptimization problem is solved by the proposed ADE-MI, a novel framework that\nemploys a two-stage optimization two-stage optimization approach: (1) adaptive\ndistributed encoding (ADE) at the device, which ensures transmitted bits are\nmost relevant to sensing tasks, and (2) multi-view Inference (MI) at the edge\nserver, which orchestrates multi-view data from distributed devices. Our\nexperimental results highlight the synergy between communication and sensing,\nshowing that more frequent communication from WiFi access points to edge\ndevices improves sensing inference accuracy. The proposed ADE-MI achieves 92\\%\nrecognition accuracy with over $10^4$-fold reduction in latency compared to\nschemes with raw data communication, achieving both high sensing inference\naccuracy and low communication latency simultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated sensing and communication (ISAC) unifies wireless communication\nand sensing by sharing spectrum and hardware, which often incurs trade-offs\nbetween two functions due to limited resources. However, this paper shifts\nfocus to exploring the synergy between communication and sensing, using WiFi\nsensing as an exemplary scenario where communication signals are repurposed to\nprobe the environment without dedicated sensing waveforms, followed by data\nuploading to the edge server for inference. While increased device\nparticipation enhances multi-view sensing data, it also imposes significant\ncommunication overhead between devices and the edge server. To address this\nchallenge, we aim to maximize the sensing task performance, measured by mutual\ninformation, under the channel capacity constraint. The information-theoretic\noptimization problem is solved by the proposed ADE-MI, a novel framework that\nemploys a two-stage optimization two-stage optimization approach: (1) adaptive\ndistributed encoding (ADE) at the device, which ensures transmitted bits are\nmost relevant to sensing tasks, and (2) multi-view Inference (MI) at the edge\nserver, which orchestrates multi-view data from distributed devices. Our\nexperimental results highlight the synergy between communication and sensing,\nshowing that more frequent communication from WiFi access points to edge\ndevices improves sensing inference accuracy. The proposed ADE-MI achieves 92\\%\nrecognition accuracy with over $10^4$-fold reduction in latency compared to\nschemes with raw data communication, achieving both high sensing inference\naccuracy and low communication latency simultaneously."
                },
                "authors": [
                    {
                        "name": "Mingjie Yang"
                    },
                    {
                        "name": "Guangming Liang"
                    },
                    {
                        "name": "Dongzhu Liu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11536v1",
                "updated": "2024-11-18T12:48:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    48,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:48:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    48,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Hierarchical-Graph-Structured Edge Partition Models for Learning\n  Evolving Community Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical-Graph-Structured Edge Partition Models for Learning\n  Evolving Community Structure"
                },
                "summary": "We propose a novel dynamic network model to capture evolving latent\ncommunities within temporal networks. To achieve this, we decompose each\nobserved dynamic edge between vertices using a Poisson-gamma edge partition\nmodel, assigning each vertex to one or more latent communities through\n\\emph{nonnegative} vertex-community memberships. Specifically, hierarchical\ntransition kernels are employed to model the interactions between these latent\ncommunities in the observed temporal network. A hierarchical graph prior is\nplaced on the transition structure of the latent communities, allowing us to\nmodel how they evolve and interact over time. Consequently, our dynamic network\nenables the inferred community structure to merge, split, and interact with one\nanother, providing a comprehensive understanding of complex network dynamics.\nExperiments on various real-world network datasets demonstrate that the\nproposed model not only effectively uncovers interpretable latent structures\nbut also surpasses other state-of-the art dynamic network models in the tasks\nof link prediction and community detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel dynamic network model to capture evolving latent\ncommunities within temporal networks. To achieve this, we decompose each\nobserved dynamic edge between vertices using a Poisson-gamma edge partition\nmodel, assigning each vertex to one or more latent communities through\n\\emph{nonnegative} vertex-community memberships. Specifically, hierarchical\ntransition kernels are employed to model the interactions between these latent\ncommunities in the observed temporal network. A hierarchical graph prior is\nplaced on the transition structure of the latent communities, allowing us to\nmodel how they evolve and interact over time. Consequently, our dynamic network\nenables the inferred community structure to merge, split, and interact with one\nanother, providing a comprehensive understanding of complex network dynamics.\nExperiments on various real-world network datasets demonstrate that the\nproposed model not only effectively uncovers interpretable latent structures\nbut also surpasses other state-of-the art dynamic network models in the tasks\nof link prediction and community detection."
                },
                "authors": [
                    {
                        "name": "Xincan Yu"
                    },
                    {
                        "name": "Sikun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sikun Yang"
                },
                "author": "Sikun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11534v1",
                "updated": "2024-11-18T12:46:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    46,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:46:28Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    46,
                    28,
                    0,
                    323,
                    0
                ],
                "title": "A Broad-line, Low-luminosity Active Galactic Nucleus at ${z=7.3}$\n  Anchoring a Large Galaxy Overdensity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Broad-line, Low-luminosity Active Galactic Nucleus at ${z=7.3}$\n  Anchoring a Large Galaxy Overdensity"
                },
                "summary": "The James Webb Space Telescope has uncovered a puzzling population of\nUV-faint broad-line active galactic nuclei (AGN), nicknamed ``Little Red Dots''\n(LRD) owing to their compact morphology and red rest-frame optical colours.\nInterpreted as dust attenuated AGN, their inferred intrinsic luminosities and\nsupermassive black hole (SMBH) masses rival those of UV-luminous quasars,\nalthough they are $>100$ times more abundant. If LRDs and quasars are members\nof the same underlying population, they should inhabit comparable mass dark\nmatter halos, traced by similar overdensities of galaxies. Otherwise, they\nrepresent distinct populations with different physical properties and formation\nhistories. Characterizing LRD environments thus provides a critical test of\ntheir nature. Here, we report the discovery of a LRD at $z=7.3$, attenuated by\nmoderate amounts of dust, $A_V = {3.26}\\,\\rm{mag}$, with an intrinsic\nbolometric luminosity of $10^{46.7}\\,\\rm{erg}\\,\\rm{s}^{-1}$ and a SMBH mass of\n$7\\times10^8\\,\\rm{M}_\\odot$. Most notably, this object is embedded in an\noverdensity of eight nearby galaxies, allowing us to calculate the first\nspectroscopic estimate of the clustering of galaxies around LRDs. We find a\nLRD-galaxy cross-correlation length of $r_0\\!=\\!9\\pm2\\,\\rm{h}^{-1}\\,\\rm{cMpc}$,\ncomparable to that of $z\\!\\sim\\!6$ UV-luminous quasars. The resulting estimate\nof their minimum dark matter halo mass of $\\log_{10}(M_{\\rm{halo,\nmin}}/\\rm{M}_{\\odot})= 12.3_{-0.8}^{+0.7}$ indicates that nearly all halos\nabove this mass must host actively accreting SMBHs at $z\\approx7$, in strong\ncontrast with the far smaller duty cycle of luminous quasars ($<1\\%$). Our\nresults, taken at face value, motivate a picture in which LRDs are the obscured\ncounterparts of UV-luminous quasars, which provides a natural explanation for\nthe short UV-luminous lifetimes inferred from both quasar clustering and quasar\nproximity zones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope has uncovered a puzzling population of\nUV-faint broad-line active galactic nuclei (AGN), nicknamed ``Little Red Dots''\n(LRD) owing to their compact morphology and red rest-frame optical colours.\nInterpreted as dust attenuated AGN, their inferred intrinsic luminosities and\nsupermassive black hole (SMBH) masses rival those of UV-luminous quasars,\nalthough they are $>100$ times more abundant. If LRDs and quasars are members\nof the same underlying population, they should inhabit comparable mass dark\nmatter halos, traced by similar overdensities of galaxies. Otherwise, they\nrepresent distinct populations with different physical properties and formation\nhistories. Characterizing LRD environments thus provides a critical test of\ntheir nature. Here, we report the discovery of a LRD at $z=7.3$, attenuated by\nmoderate amounts of dust, $A_V = {3.26}\\,\\rm{mag}$, with an intrinsic\nbolometric luminosity of $10^{46.7}\\,\\rm{erg}\\,\\rm{s}^{-1}$ and a SMBH mass of\n$7\\times10^8\\,\\rm{M}_\\odot$. Most notably, this object is embedded in an\noverdensity of eight nearby galaxies, allowing us to calculate the first\nspectroscopic estimate of the clustering of galaxies around LRDs. We find a\nLRD-galaxy cross-correlation length of $r_0\\!=\\!9\\pm2\\,\\rm{h}^{-1}\\,\\rm{cMpc}$,\ncomparable to that of $z\\!\\sim\\!6$ UV-luminous quasars. The resulting estimate\nof their minimum dark matter halo mass of $\\log_{10}(M_{\\rm{halo,\nmin}}/\\rm{M}_{\\odot})= 12.3_{-0.8}^{+0.7}$ indicates that nearly all halos\nabove this mass must host actively accreting SMBHs at $z\\approx7$, in strong\ncontrast with the far smaller duty cycle of luminous quasars ($<1\\%$). Our\nresults, taken at face value, motivate a picture in which LRDs are the obscured\ncounterparts of UV-luminous quasars, which provides a natural explanation for\nthe short UV-luminous lifetimes inferred from both quasar clustering and quasar\nproximity zones."
                },
                "authors": [
                    {
                        "name": "Jan-Torge Schindler"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    },
                    {
                        "name": "Frederick B. Davies"
                    },
                    {
                        "name": "Sarah E. I. Bosman"
                    },
                    {
                        "name": "Ryan Endsley"
                    },
                    {
                        "name": "Feige Wang"
                    },
                    {
                        "name": "Jinyi Yang"
                    },
                    {
                        "name": "Aaron J. Barth"
                    },
                    {
                        "name": "Anna-Christina Eilers"
                    },
                    {
                        "name": "Xiaohui Fan"
                    },
                    {
                        "name": "Koki Kakiichi"
                    },
                    {
                        "name": "Michael Maseda"
                    },
                    {
                        "name": "Elia Pizzati"
                    },
                    {
                        "name": "Riccardo Nanni"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Nanni"
                },
                "author": "Riccardo Nanni",
                "arxiv_comment": "Submitted. Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11532v1",
                "updated": "2024-11-18T12:41:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "title": "A Code Knowledge Graph-Enhanced System for LLM-Based Fuzz Driver\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Code Knowledge Graph-Enhanced System for LLM-Based Fuzz Driver\n  Generation"
                },
                "summary": "The rapid development of large language models (LLMs) with advanced\nprogramming capabilities has paved the way for innovative approaches in\nsoftware testing. Fuzz testing, a cornerstone for improving software\nreliability and detecting vulnerabilities, often relies on manually written\nfuzz drivers, limiting scalability and efficiency. To address this challenge,\nwe propose CodeGraphGPT, a novel system that integrates code knowledge graphs\nwith an LLM-powered intelligent agent to automate the fuzz driver generation\nprocess. By framing fuzz driver creation as a code generation task,\nCodeGraphGPT leverages program analysis to construct a knowledge graph of code\nrepositories, where nodes represent code entities, such as functions or files,\nand edges capture their relationships. This enables the system to generate\ntailored fuzz drivers and input seeds, resolve compilation errors, and analyze\ncrash reports, all while adapting to specific API usage scenarios.\nAdditionally, querying the knowledge graph helps identify precise testing\ntargets and contextualize the purpose of each fuzz driver within the fuzzing\nloop. We evaluated CodeGraphGPT on eight open-source software projects,\nachieving an average improvement of 8.73\\% in code coverage compared to\nstate-of-the-art methods. Moreover, it reduced the manual workload in crash\ncase analysis by 84.4\\% and identified 11 real-world bugs, including nine\npreviously unreported ones. This work highlights how integrating LLMs with code\nknowledge graphs enhances fuzz driver generation, offering an efficient\nsolution for vulnerability detection and software quality improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) with advanced\nprogramming capabilities has paved the way for innovative approaches in\nsoftware testing. Fuzz testing, a cornerstone for improving software\nreliability and detecting vulnerabilities, often relies on manually written\nfuzz drivers, limiting scalability and efficiency. To address this challenge,\nwe propose CodeGraphGPT, a novel system that integrates code knowledge graphs\nwith an LLM-powered intelligent agent to automate the fuzz driver generation\nprocess. By framing fuzz driver creation as a code generation task,\nCodeGraphGPT leverages program analysis to construct a knowledge graph of code\nrepositories, where nodes represent code entities, such as functions or files,\nand edges capture their relationships. This enables the system to generate\ntailored fuzz drivers and input seeds, resolve compilation errors, and analyze\ncrash reports, all while adapting to specific API usage scenarios.\nAdditionally, querying the knowledge graph helps identify precise testing\ntargets and contextualize the purpose of each fuzz driver within the fuzzing\nloop. We evaluated CodeGraphGPT on eight open-source software projects,\nachieving an average improvement of 8.73\\% in code coverage compared to\nstate-of-the-art methods. Moreover, it reduced the manual workload in crash\ncase analysis by 84.4\\% and identified 11 real-world bugs, including nine\npreviously unreported ones. This work highlights how integrating LLMs with code\nknowledge graphs enhances fuzz driver generation, offering an efficient\nsolution for vulnerability detection and software quality improvement."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Ting Zhou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11531v1",
                "updated": "2024-11-18T12:40:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    40,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:40:51Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    40,
                    51,
                    0,
                    323,
                    0
                ],
                "title": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality"
                },
                "summary": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval."
                },
                "authors": [
                    {
                        "name": "Viktoriia Chekalina"
                    },
                    {
                        "name": "Anton Razzigaev"
                    },
                    {
                        "name": "Elizaveta Goncharova"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Kuznetsov"
                },
                "author": "Andrey Kuznetsov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16937v2",
                "updated": "2024-11-18T12:36:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    36,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-17T09:39:34Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    39,
                    34,
                    0,
                    169,
                    0
                ],
                "title": "A Complete Survey on LLM-based AI Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Complete Survey on LLM-based AI Chatbots"
                },
                "summary": "The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI."
                },
                "authors": [
                    {
                        "name": "Sumit Kumar Dam"
                    },
                    {
                        "name": "Choong Seon Hong"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Chaoning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chaoning Zhang"
                },
                "author": "Chaoning Zhang",
                "arxiv_comment": "23 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11521v1",
                "updated": "2024-11-18T12:31:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    31,
                    22,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:31:22Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    31,
                    22,
                    0,
                    323,
                    0
                ],
                "title": "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions"
                },
                "summary": "Individuals have been increasingly interacting with online Large Language\nModels (LLMs), both in their work and personal lives. These interactions raise\nprivacy issues as the LLMs are typically hosted by third-parties who can gather\na variety of sensitive information about users and their companies. Text\nSanitization techniques have been proposed in the literature and can be used to\nsanitize user prompts before sending them to the LLM. However, sanitization has\nan impact on the downstream task performed by the LLM, and often to such an\nextent that it leads to unacceptable results for the user. This is not just a\nminor annoyance, with clear monetary consequences as LLM services charge on a\nper use basis as well as great amount of computing resources wasted. We propose\nan architecture leveraging a Small Language Model (SLM) at the user-side to\nhelp estimate the impact of sanitization on a prompt before it is sent to the\nLLM, thus preventing resource losses.\n  Our evaluation of this architecture revealed a significant problem with text\nsanitization based on Differential Privacy, on which we want to draw the\nattention of the community for further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individuals have been increasingly interacting with online Large Language\nModels (LLMs), both in their work and personal lives. These interactions raise\nprivacy issues as the LLMs are typically hosted by third-parties who can gather\na variety of sensitive information about users and their companies. Text\nSanitization techniques have been proposed in the literature and can be used to\nsanitize user prompts before sending them to the LLM. However, sanitization has\nan impact on the downstream task performed by the LLM, and often to such an\nextent that it leads to unacceptable results for the user. This is not just a\nminor annoyance, with clear monetary consequences as LLM services charge on a\nper use basis as well as great amount of computing resources wasted. We propose\nan architecture leveraging a Small Language Model (SLM) at the user-side to\nhelp estimate the impact of sanitization on a prompt before it is sent to the\nLLM, thus preventing resource losses.\n  Our evaluation of this architecture revealed a significant problem with text\nsanitization based on Differential Privacy, on which we want to draw the\nattention of the community for further investigation."
                },
                "authors": [
                    {
                        "name": "Robin Carpentier"
                    },
                    {
                        "name": "Benjamin Zi Hao Zhao"
                    },
                    {
                        "name": "Hassan Jameel Asghar"
                    },
                    {
                        "name": "Dali Kaafar"
                    }
                ],
                "author_detail": {
                    "name": "Dali Kaafar"
                },
                "author": "Dali Kaafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11514v1",
                "updated": "2024-11-18T12:22:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    22,
                    29,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:22:29Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    22,
                    29,
                    0,
                    323,
                    0
                ],
                "title": "Learning a Neural Association Network for Self-supervised Multi-Object\n  Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a Neural Association Network for Self-supervised Multi-Object\n  Tracking"
                },
                "summary": "This paper introduces a novel framework to learn data association for\nmulti-object tracking in a self-supervised manner. Fully-supervised learning\nmethods are known to achieve excellent tracking performances, but acquiring\nidentity-level annotations is tedious and time-consuming. Motivated by the fact\nthat in real-world scenarios object motion can be usually represented by a\nMarkov process, we present a novel expectation maximization (EM) algorithm that\ntrains a neural network to associate detections for tracking, without requiring\nprior knowledge of their temporal correspondences. At the core of our method\nlies a neural Kalman filter, with an observation model conditioned on\nassociations of detections parameterized by a neural network. Given a batch of\nframes as input, data associations between detections from adjacent frames are\npredicted by a neural network followed by a Sinkhorn normalization that\ndetermines the assignment probabilities of detections to states. Kalman\nsmoothing is then used to obtain the marginal probability of observations given\nthe inferred states, producing a training objective to maximize this marginal\nprobability using gradient descent. The proposed framework is fully\ndifferentiable, allowing the underlying neural model to be trained end-to-end.\nWe evaluate our approach on the challenging MOT17 and MOT20 datasets and\nachieve state-of-the-art results in comparison to self-supervised trackers\nusing public detections. We furthermore demonstrate the capability of the\nlearned model to generalize across datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework to learn data association for\nmulti-object tracking in a self-supervised manner. Fully-supervised learning\nmethods are known to achieve excellent tracking performances, but acquiring\nidentity-level annotations is tedious and time-consuming. Motivated by the fact\nthat in real-world scenarios object motion can be usually represented by a\nMarkov process, we present a novel expectation maximization (EM) algorithm that\ntrains a neural network to associate detections for tracking, without requiring\nprior knowledge of their temporal correspondences. At the core of our method\nlies a neural Kalman filter, with an observation model conditioned on\nassociations of detections parameterized by a neural network. Given a batch of\nframes as input, data associations between detections from adjacent frames are\npredicted by a neural network followed by a Sinkhorn normalization that\ndetermines the assignment probabilities of detections to states. Kalman\nsmoothing is then used to obtain the marginal probability of observations given\nthe inferred states, producing a training objective to maximize this marginal\nprobability using gradient descent. The proposed framework is fully\ndifferentiable, allowing the underlying neural model to be trained end-to-end.\nWe evaluate our approach on the challenging MOT17 and MOT20 datasets and\nachieve state-of-the-art results in comparison to self-supervised trackers\nusing public detections. We furthermore demonstrate the capability of the\nlearned model to generalize across datasets."
                },
                "authors": [
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Michael Burke"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    },
                    {
                        "name": "Juergen Gall"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Gall"
                },
                "author": "Juergen Gall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11511v1",
                "updated": "2024-11-18T12:16:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    16,
                    3,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:16:03Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    16,
                    3,
                    0,
                    323,
                    0
                ],
                "title": "Structure learning with Temporal Gaussian Mixture for model-based\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure learning with Temporal Gaussian Mixture for model-based\n  Reinforcement Learning"
                },
                "summary": "Model-based reinforcement learning refers to a set of approaches capable of\nsample-efficient decision making, which create an explicit model of the\nenvironment. This model can subsequently be used for learning optimal policies.\nIn this paper, we propose a temporal Gaussian Mixture Model composed of a\nperception model and a transition model. The perception model extracts discrete\n(latent) states from continuous observations using a variational Gaussian\nmixture likelihood. Importantly, our model constantly monitors the collected\ndata searching for new Gaussian components, i.e., the perception model performs\na form of structure learning (Smith et al., 2020; Friston et al., 2018; Neacsu\net al., 2022) as it learns the number of Gaussian components in the mixture.\nAdditionally, the transition model learns the temporal transition between\nconsecutive time steps by taking advantage of the Dirichlet-categorical\nconjugacy. Both the perception and transition models are able to forget part of\nthe data points, while integrating the information they provide within the\nprior, which ensure fast variational inference. Finally, decision making is\nperformed with a variant of Q-learning which is able to learn Q-values from\nbeliefs over states. Empirically, we have demonstrated the model's ability to\nlearn the structure of several mazes: the model discovered the number of states\nand the transition probabilities between these states. Moreover, using its\nlearned Q-values, the agent was able to successfully navigate from the starting\nposition to the maze's exit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-based reinforcement learning refers to a set of approaches capable of\nsample-efficient decision making, which create an explicit model of the\nenvironment. This model can subsequently be used for learning optimal policies.\nIn this paper, we propose a temporal Gaussian Mixture Model composed of a\nperception model and a transition model. The perception model extracts discrete\n(latent) states from continuous observations using a variational Gaussian\nmixture likelihood. Importantly, our model constantly monitors the collected\ndata searching for new Gaussian components, i.e., the perception model performs\na form of structure learning (Smith et al., 2020; Friston et al., 2018; Neacsu\net al., 2022) as it learns the number of Gaussian components in the mixture.\nAdditionally, the transition model learns the temporal transition between\nconsecutive time steps by taking advantage of the Dirichlet-categorical\nconjugacy. Both the perception and transition models are able to forget part of\nthe data points, while integrating the information they provide within the\nprior, which ensure fast variational inference. Finally, decision making is\nperformed with a variant of Q-learning which is able to learn Q-values from\nbeliefs over states. Empirically, we have demonstrated the model's ability to\nlearn the structure of several mazes: the model discovered the number of states\nand the transition probabilities between these states. Moreover, using its\nlearned Q-values, the agent was able to successfully navigate from the starting\nposition to the maze's exit."
                },
                "authors": [
                    {
                        "name": "Thophile Champion"
                    },
                    {
                        "name": "Marek Grze"
                    },
                    {
                        "name": "Howard Bowman"
                    }
                ],
                "author_detail": {
                    "name": "Howard Bowman"
                },
                "author": "Howard Bowman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11505v1",
                "updated": "2024-11-18T12:05:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    5,
                    27,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:05:27Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    5,
                    27,
                    0,
                    323,
                    0
                ],
                "title": "LaVin-DiT: Large Vision Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaVin-DiT: Large Vision Diffusion Transformer"
                },
                "summary": "This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a\nscalable and unified foundation model designed to tackle over 20 computer\nvision tasks in a generative framework. Unlike existing large vision models\ndirectly adapted from natural language processing architectures, which rely on\nless efficient autoregressive techniques and disrupt spatial relationships\nessential for vision data, LaVin-DiT introduces key innovations to optimize\ngenerative performance for vision tasks. First, to address the high\ndimensionality of visual data, we incorporate a spatial-temporal variational\nautoencoder that encodes data into a continuous latent space. Second, for\ngenerative modeling, we develop a joint diffusion transformer that\nprogressively produces vision outputs. Third, for unified multi-task training,\nin-context learning is implemented. Input-target pairs serve as task context,\nwhich guides the diffusion transformer to align outputs with specific tasks\nwithin the latent space. During inference, a task-specific context set and test\ndata as queries allow LaVin-DiT to generalize across tasks without fine-tuning.\nTrained on extensive vision datasets, the model is scaled from 0.1B to 3.4B\nparameters, demonstrating substantial scalability and state-of-the-art\nperformance across diverse vision tasks. This work introduces a novel pathway\nfor large vision foundation models, underscoring the promising potential of\ndiffusion transformers. The code and models will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a\nscalable and unified foundation model designed to tackle over 20 computer\nvision tasks in a generative framework. Unlike existing large vision models\ndirectly adapted from natural language processing architectures, which rely on\nless efficient autoregressive techniques and disrupt spatial relationships\nessential for vision data, LaVin-DiT introduces key innovations to optimize\ngenerative performance for vision tasks. First, to address the high\ndimensionality of visual data, we incorporate a spatial-temporal variational\nautoencoder that encodes data into a continuous latent space. Second, for\ngenerative modeling, we develop a joint diffusion transformer that\nprogressively produces vision outputs. Third, for unified multi-task training,\nin-context learning is implemented. Input-target pairs serve as task context,\nwhich guides the diffusion transformer to align outputs with specific tasks\nwithin the latent space. During inference, a task-specific context set and test\ndata as queries allow LaVin-DiT to generalize across tasks without fine-tuning.\nTrained on extensive vision datasets, the model is scaled from 0.1B to 3.4B\nparameters, demonstrating substantial scalability and state-of-the-art\nperformance across diverse vision tasks. This work introduces a novel pathway\nfor large vision foundation models, underscoring the promising potential of\ndiffusion transformers. The code and models will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Xiaobo Xia"
                    },
                    {
                        "name": "Runnan Chen"
                    },
                    {
                        "name": "Dongdong Yu"
                    },
                    {
                        "name": "Changhu Wang"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Tongliang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tongliang Liu"
                },
                "author": "Tongliang Liu",
                "arxiv_comment": "11 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11500v1",
                "updated": "2024-11-18T12:01:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    1,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:01:59Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    1,
                    59,
                    0,
                    323,
                    0
                ],
                "title": "Timescale-agnostic characterisation for collective attention events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timescale-agnostic characterisation for collective attention events"
                },
                "summary": "Online communications, and in particular social media, are a key component of\nhow society interacts with and promotes content online. Collective attention on\nsuch content can vary wildly. The majority of breaking topics quickly fade into\nobscurity after only a handful of interactions, while the possibility exists\nfor content to ``go viral'', seeing sustained interaction by large audiences\nover long periods. In this paper we investigate the mechanisms behind such\nevents and introduce a new representation that enables direct comparison of\nevents over diverse time and volume scales. We find four characteristic\nbehaviours in the usage of hashtags on Twitter that are indicative of different\npatterns of attention to topics. We go on to develop an agent-based model for\ngenerating collective attention events to test the factors affecting emergence\nof these phenomena. This model can reproduce the characteristic behaviours seen\nin the Twitter dataset using a small set of parameters, and reveal that three\nof these behaviours instead represent a continuum determined by model\nparameters rather than discrete categories. These insights suggest that\ncollective attention in social systems develops in line with a set of universal\nprinciples independent of effects inherent to system scale, and the techniques\nwe introduce here present a valuable opportunity to infer the possible\nmechanisms of attention flow in online communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online communications, and in particular social media, are a key component of\nhow society interacts with and promotes content online. Collective attention on\nsuch content can vary wildly. The majority of breaking topics quickly fade into\nobscurity after only a handful of interactions, while the possibility exists\nfor content to ``go viral'', seeing sustained interaction by large audiences\nover long periods. In this paper we investigate the mechanisms behind such\nevents and introduce a new representation that enables direct comparison of\nevents over diverse time and volume scales. We find four characteristic\nbehaviours in the usage of hashtags on Twitter that are indicative of different\npatterns of attention to topics. We go on to develop an agent-based model for\ngenerating collective attention events to test the factors affecting emergence\nof these phenomena. This model can reproduce the characteristic behaviours seen\nin the Twitter dataset using a small set of parameters, and reveal that three\nof these behaviours instead represent a continuum determined by model\nparameters rather than discrete categories. These insights suggest that\ncollective attention in social systems develops in line with a set of universal\nprinciples independent of effects inherent to system scale, and the techniques\nwe introduce here present a valuable opportunity to infer the possible\nmechanisms of attention flow in online communications."
                },
                "authors": [
                    {
                        "name": "Tristan J. B. Cann"
                    },
                    {
                        "name": "Iain S. Weaver"
                    },
                    {
                        "name": "Hywel T. P. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Hywel T. P. Williams"
                },
                "author": "Hywel T. P. Williams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11498v1",
                "updated": "2024-11-18T11:58:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    58,
                    52,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:58:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    58,
                    52,
                    0,
                    323,
                    0
                ],
                "title": "Efficient smoothness selection for nonparametric Markov-switching models\n  via quasi restricted maximum likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient smoothness selection for nonparametric Markov-switching models\n  via quasi restricted maximum likelihood"
                },
                "summary": "Markov-switching models are powerful tools that allow capturing complex\npatterns from time series data driven by latent states. Recent work has\nhighlighted the benefits of estimating components of these models\nnonparametrically, enhancing their flexibility and reducing biases, which in\nturn can improve state decoding, forecasting, and overall inference.\nFormulating such models using penalized splines is straightforward, but\npractically feasible methods for a data-driven smoothness selection in these\nmodels are still lacking. Traditional techniques, such as cross-validation and\ninformation criteria-based selection suffer from major drawbacks, most\nimportantly their reliance on computationally expensive grid search methods,\nhampering practical usability for Markov-switching models. Michelot (2022)\nsuggested treating spline coefficients as random effects with a multivariate\nnormal distribution and using the R package TMB (Kristensen et al., 2016) for\nmarginal likelihood maximization. While this method avoids grid search and\ntypically results in adequate smoothness selection, it entails a nested\noptimization problem, thus being computationally demanding. We propose to\nexploit the simple structure of penalized splines treated as random effects,\nthereby greatly reducing the computational burden while potentially improving\nfixed effects parameter estimation accuracy. Our proposed method offers a\nreliable and efficient mechanism for smoothness selection, rendering the\nestimation of Markov-switching models involving penalized splines feasible for\ncomplex data structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov-switching models are powerful tools that allow capturing complex\npatterns from time series data driven by latent states. Recent work has\nhighlighted the benefits of estimating components of these models\nnonparametrically, enhancing their flexibility and reducing biases, which in\nturn can improve state decoding, forecasting, and overall inference.\nFormulating such models using penalized splines is straightforward, but\npractically feasible methods for a data-driven smoothness selection in these\nmodels are still lacking. Traditional techniques, such as cross-validation and\ninformation criteria-based selection suffer from major drawbacks, most\nimportantly their reliance on computationally expensive grid search methods,\nhampering practical usability for Markov-switching models. Michelot (2022)\nsuggested treating spline coefficients as random effects with a multivariate\nnormal distribution and using the R package TMB (Kristensen et al., 2016) for\nmarginal likelihood maximization. While this method avoids grid search and\ntypically results in adequate smoothness selection, it entails a nested\noptimization problem, thus being computationally demanding. We propose to\nexploit the simple structure of penalized splines treated as random effects,\nthereby greatly reducing the computational burden while potentially improving\nfixed effects parameter estimation accuracy. Our proposed method offers a\nreliable and efficient mechanism for smoothness selection, rendering the\nestimation of Markov-switching models involving penalized splines feasible for\ncomplex data structures."
                },
                "authors": [
                    {
                        "name": "Jan-Ole Koslik"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Ole Koslik"
                },
                "author": "Jan-Ole Koslik",
                "arxiv_comment": "32 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14148v2",
                "updated": "2024-11-18T11:58:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    58,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-18T03:34:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    34,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment"
                },
                "summary": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models."
                },
                "authors": [
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12060v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12060v3",
                "updated": "2024-11-18T11:51:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    51,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-17T20:00:04Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    20,
                    0,
                    4,
                    0,
                    169,
                    0
                ],
                "title": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to\n  Address Shortcut Shifts in Natural Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to\n  Address Shortcut Shifts in Natural Language Understanding"
                },
                "summary": "Recent models for natural language understanding are inclined to exploit\nsimple patterns in datasets, commonly known as shortcuts. These shortcuts hinge\non spurious correlations between labels and latent features existing in the\ntraining data. At inference time, shortcut-dependent models are likely to\ngenerate erroneous predictions under distribution shifts, particularly when\nsome latent features are no longer correlated with the labels. To avoid this,\nprevious studies have trained models to eliminate the reliance on shortcuts. In\nthis study, we explore a different direction: pessimistically aggregating the\npredictions of a mixture-of-experts, assuming each expert captures relatively\ndifferent latent features. The experimental results demonstrate that our\npost-hoc control over the experts significantly enhances the model's robustness\nto the distribution shift in shortcuts. Besides, we show that our approach has\nsome practical advantages. We also analyze our model and provide results to\nsupport the assumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent models for natural language understanding are inclined to exploit\nsimple patterns in datasets, commonly known as shortcuts. These shortcuts hinge\non spurious correlations between labels and latent features existing in the\ntraining data. At inference time, shortcut-dependent models are likely to\ngenerate erroneous predictions under distribution shifts, particularly when\nsome latent features are no longer correlated with the labels. To avoid this,\nprevious studies have trained models to eliminate the reliance on shortcuts. In\nthis study, we explore a different direction: pessimistically aggregating the\npredictions of a mixture-of-experts, assuming each expert captures relatively\ndifferent latent features. The experimental results demonstrate that our\npost-hoc control over the experts significantly enhances the model's robustness\nto the distribution shift in shortcuts. Besides, we show that our approach has\nsome practical advantages. We also analyze our model and provide results to\nsupport the assumption."
                },
                "authors": [
                    {
                        "name": "Ukyo Honda"
                    },
                    {
                        "name": "Tatsushi Oka"
                    },
                    {
                        "name": "Peinan Zhang"
                    },
                    {
                        "name": "Masato Mita"
                    }
                ],
                "author_detail": {
                    "name": "Masato Mita"
                },
                "author": "Masato Mita",
                "arxiv_doi": "10.1162/tacl_a_00701",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/tacl_a_00701",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.12060v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12060v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 5 figures (the layout differs from the MIT Press\n  publication version)",
                "arxiv_journal_ref": "Transactions of the Association for Computational Linguistics\n  (TACL), Vol 12 (2024), pages 1268-1289",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11483v1",
                "updated": "2024-11-18T11:42:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    42,
                    20,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:42:20Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    42,
                    20,
                    0,
                    323,
                    0
                ],
                "title": "Robust State Estimation for Legged Robots with Dual Beta Kalman Filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust State Estimation for Legged Robots with Dual Beta Kalman Filter"
                },
                "summary": "Existing state estimation algorithms for legged robots that rely on\nproprioceptive sensors often overlook foot slippage and leg deformation in the\nphysical world, leading to large estimation errors. To address this limitation,\nwe propose a comprehensive measurement model that accounts for both foot\nslippage and variable leg length by analyzing the relative motion between foot\ncontact points and the robot's body center. We show that leg length is an\nobservable quantity, meaning that its value can be explicitly inferred by\ndesigning an auxiliary filter. To this end, we introduce a dual estimation\nframework that iteratively employs a parameter filter to estimate the leg\nlength parameters and a state filter to estimate the robot's state. To prevent\nerror accumulation in this iterative framework, we construct a partial\nmeasurement model for the parameter filter using the leg static equation. This\napproach ensures that leg length estimation relies solely on joint torques and\nfoot contact forces, avoiding the influence of state estimation errors on the\nparameter estimation. Unlike leg length which can be directly estimated, foot\nslippage cannot be measured directly with the current sensor configuration.\nHowever, since foot slippage occurs at a low frequency, it can be treated as\noutliers in the measurement data. To mitigate the impact of these outliers, we\npropose the beta Kalman filter (beta KF), which redefines the estimation loss\nin canonical Kalman filtering using beta divergence. This divergence can assign\nlow weights to outliers in an adaptive manner, thereby enhancing the robustness\nof the estimation algorithm. These techniques together form the dual\nbeta-Kalman filter (Dual beta KF), a novel algorithm for robust state\nestimation in legged robots. Experimental results on the Unitree GO2 robot\ndemonstrate that the Dual beta KF significantly outperforms state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing state estimation algorithms for legged robots that rely on\nproprioceptive sensors often overlook foot slippage and leg deformation in the\nphysical world, leading to large estimation errors. To address this limitation,\nwe propose a comprehensive measurement model that accounts for both foot\nslippage and variable leg length by analyzing the relative motion between foot\ncontact points and the robot's body center. We show that leg length is an\nobservable quantity, meaning that its value can be explicitly inferred by\ndesigning an auxiliary filter. To this end, we introduce a dual estimation\nframework that iteratively employs a parameter filter to estimate the leg\nlength parameters and a state filter to estimate the robot's state. To prevent\nerror accumulation in this iterative framework, we construct a partial\nmeasurement model for the parameter filter using the leg static equation. This\napproach ensures that leg length estimation relies solely on joint torques and\nfoot contact forces, avoiding the influence of state estimation errors on the\nparameter estimation. Unlike leg length which can be directly estimated, foot\nslippage cannot be measured directly with the current sensor configuration.\nHowever, since foot slippage occurs at a low frequency, it can be treated as\noutliers in the measurement data. To mitigate the impact of these outliers, we\npropose the beta Kalman filter (beta KF), which redefines the estimation loss\nin canonical Kalman filtering using beta divergence. This divergence can assign\nlow weights to outliers in an adaptive manner, thereby enhancing the robustness\nof the estimation algorithm. These techniques together form the dual\nbeta-Kalman filter (Dual beta KF), a novel algorithm for robust state\nestimation in legged robots. Experimental results on the Unitree GO2 robot\ndemonstrate that the Dual beta KF significantly outperforms state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Wenhan Cao"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Jiangtao Li"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengbo Eben Li"
                },
                "author": "Shengbo Eben Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12138v2",
                "updated": "2024-11-18T11:29:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    29,
                    47,
                    0,
                    323,
                    0
                ],
                "published": "2024-04-18T12:40:59Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    12,
                    40,
                    59,
                    3,
                    109,
                    0
                ],
                "title": "Character is Destiny: Can Role-Playing Language Agents Make\n  Persona-Driven Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Character is Destiny: Can Role-Playing Language Agents Make\n  Persona-Driven Decisions?"
                },
                "summary": "Can Large Language Models (LLMs) simulate humans in making important\ndecisions? Recent research has unveiled the potential of using LLMs to develop\nrole-playing language agents (RPLAs), mimicking mainly the knowledge and tones\nof various characters. However, imitative decision-making necessitates a more\nnuanced understanding of personas. In this paper, we benchmark the ability of\nLLMs in persona-driven decision-making. Specifically, we investigate whether\nLLMs can predict characters' decisions provided by the preceding stories in\nhigh-quality novels. Leveraging character analyses written by literary experts,\nwe construct a dataset LIFECHOICE comprising 1,462 characters' decision points\nfrom 388 books. Then, we conduct comprehensive experiments on LIFECHOICE, with\nvarious LLMs and RPLA methodologies. The results demonstrate that\nstate-of-the-art LLMs exhibit promising capabilities in this task, yet\nsubstantial room for improvement remains. Hence, we further propose the CHARMAP\nmethod, which adopts persona-based memory retrieval and significantly advances\nRPLAs on this task, achieving 5.03% increase in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models (LLMs) simulate humans in making important\ndecisions? Recent research has unveiled the potential of using LLMs to develop\nrole-playing language agents (RPLAs), mimicking mainly the knowledge and tones\nof various characters. However, imitative decision-making necessitates a more\nnuanced understanding of personas. In this paper, we benchmark the ability of\nLLMs in persona-driven decision-making. Specifically, we investigate whether\nLLMs can predict characters' decisions provided by the preceding stories in\nhigh-quality novels. Leveraging character analyses written by literary experts,\nwe construct a dataset LIFECHOICE comprising 1,462 characters' decision points\nfrom 388 books. Then, we conduct comprehensive experiments on LIFECHOICE, with\nvarious LLMs and RPLA methodologies. The results demonstrate that\nstate-of-the-art LLMs exhibit promising capabilities in this task, yet\nsubstantial room for improvement remains. Hence, we further propose the CHARMAP\nmethod, which adopts persona-based memory retrieval and significantly advances\nRPLAs on this task, achieving 5.03% increase in accuracy."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Xiaoqing Dong"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12737v2",
                "updated": "2024-11-18T11:21:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    21,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-04-19T09:30:07Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    9,
                    30,
                    7,
                    4,
                    110,
                    0
                ],
                "title": "LLM App Store Analysis: A Vision and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM App Store Analysis: A Vision and Roadmap"
                },
                "summary": "The rapid growth and popularity of large language model (LLM) app stores have\ncreated new opportunities and challenges for researchers, developers, users,\nand app store managers. As the LLM app ecosystem continues to evolve, it is\ncrucial to understand the current landscape and identify potential areas for\nfuture research and development. This paper presents a forward-looking analysis\nof LLM app stores, focusing on key aspects such as data mining, security risk\nidentification, development assistance, and market dynamics. Our comprehensive\nexamination extends to the intricate relationships between various stakeholders\nand the technological advancements driving the ecosystem's growth. We explore\nthe ethical considerations and potential societal impacts of widespread LLM app\nadoption, highlighting the need for responsible innovation and governance\nframeworks. By examining these aspects, we aim to provide a vision for future\nresearch directions and highlight the importance of collaboration among\nstakeholders to address the challenges and opportunities within the LLM app\necosystem. The insights and recommendations provided in this paper serve as a\nfoundation for driving innovation, ensuring responsible development, and\ncreating a thriving, user-centric LLM app landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth and popularity of large language model (LLM) app stores have\ncreated new opportunities and challenges for researchers, developers, users,\nand app store managers. As the LLM app ecosystem continues to evolve, it is\ncrucial to understand the current landscape and identify potential areas for\nfuture research and development. This paper presents a forward-looking analysis\nof LLM app stores, focusing on key aspects such as data mining, security risk\nidentification, development assistance, and market dynamics. Our comprehensive\nexamination extends to the intricate relationships between various stakeholders\nand the technological advancements driving the ecosystem's growth. We explore\nthe ethical considerations and potential societal impacts of widespread LLM app\nadoption, highlighting the need for responsible innovation and governance\nframeworks. By examining these aspects, we aim to provide a vision for future\nresearch directions and highlight the importance of collaboration among\nstakeholders to address the challenges and opportunities within the LLM app\necosystem. The insights and recommendations provided in this paper serve as a\nfoundation for driving innovation, ensuring responsible development, and\ncreating a thriving, user-centric LLM app landscape."
                },
                "authors": [
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18009v2",
                "updated": "2024-11-18T11:15:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    15,
                    56,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-28T09:50:46Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    9,
                    50,
                    46,
                    1,
                    149,
                    0
                ],
                "title": "Exploring Context Window of Large Language Models via Decomposed\n  Positional Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Context Window of Large Language Models via Decomposed\n  Positional Vectors"
                },
                "summary": "Transformer-based large language models (LLMs) typically have a limited\ncontext window, resulting in significant performance degradation when\nprocessing text beyond the length of the context window. Extensive studies have\nbeen proposed to extend the context window and achieve length extrapolation of\nLLMs, but there is still a lack of in-depth interpretation of these approaches.\nIn this study, we explore the positional information within and beyond the\ncontext window for deciphering the underlying mechanism of LLMs. By using a\nmean-based decomposition method, we disentangle positional vectors from hidden\nstates of LLMs and analyze their formation and effect on attention.\nFurthermore, when texts exceed the context window, we analyze the change of\npositional vectors in two settings, i.e., direct extrapolation and context\nwindow extension. Based on our findings, we design two training-free context\nwindow extension methods, positional vector replacement and attention window\nextension. Experimental results show that our methods can effectively extend\nthe context window length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) typically have a limited\ncontext window, resulting in significant performance degradation when\nprocessing text beyond the length of the context window. Extensive studies have\nbeen proposed to extend the context window and achieve length extrapolation of\nLLMs, but there is still a lack of in-depth interpretation of these approaches.\nIn this study, we explore the positional information within and beyond the\ncontext window for deciphering the underlying mechanism of LLMs. By using a\nmean-based decomposition method, we disentangle positional vectors from hidden\nstates of LLMs and analyze their formation and effect on attention.\nFurthermore, when texts exceed the context window, we analyze the change of\npositional vectors in two settings, i.e., direct extrapolation and context\nwindow extension. Based on our findings, we design two training-free context\nwindow extension methods, positional vector replacement and attention window\nextension. Experimental results show that our methods can effectively extend\nthe context window length."
                },
                "authors": [
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingbing Wang"
                    },
                    {
                        "name": "Zhen Tian"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted by Neurips 2024 as a spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11464v1",
                "updated": "2024-11-18T10:58:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    58,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T10:58:16Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    58,
                    16,
                    0,
                    323,
                    0
                ],
                "title": "PALMS: Parallel Adaptive Lasso with Multi-directional Signals for Latent\n  Networks Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PALMS: Parallel Adaptive Lasso with Multi-directional Signals for Latent\n  Networks Reconstruction"
                },
                "summary": "Large-scale networks exist in many field and play an important role in\nreal-world dynamics. However, the networks are usually latent and expensive to\ndetect, which becomes the main challenging for many applications and empirical\nanalysis. Several statistical methods were proposed to infer the edges, but the\ncomplexity of algorithms make them hard to be applied for large-scale networks.\nIn this paper, we proposed a general distributed and parallel computing\nframework for network reconstruction methods via compressive sensing technical,\nto make them feasible for inferring the super large networks in practice.\nCombining with the CALMS, we proposed for those estimators enjoy additional\ntheoretical properties, such as the consistency and asymptotic normality, we\nprove that the approximate estimation utilizing the distributed algorithm can\nkeep the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale networks exist in many field and play an important role in\nreal-world dynamics. However, the networks are usually latent and expensive to\ndetect, which becomes the main challenging for many applications and empirical\nanalysis. Several statistical methods were proposed to infer the edges, but the\ncomplexity of algorithms make them hard to be applied for large-scale networks.\nIn this paper, we proposed a general distributed and parallel computing\nframework for network reconstruction methods via compressive sensing technical,\nto make them feasible for inferring the super large networks in practice.\nCombining with the CALMS, we proposed for those estimators enjoy additional\ntheoretical properties, such as the consistency and asymptotic normality, we\nprove that the approximate estimation utilizing the distributed algorithm can\nkeep the theoretical results."
                },
                "authors": [
                    {
                        "name": "Zhaoyu Xing"
                    },
                    {
                        "name": "Wei Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhong"
                },
                "author": "Wei Zhong",
                "arxiv_comment": "48 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v4",
                "updated": "2024-11-18T10:32:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    32,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "arxiv_comment": "Presented at the Scientific Methods for Understanding Deep Learning\n  (SciForDL) workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10153v2",
                "updated": "2024-11-18T10:16:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    16,
                    14,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-15T12:52:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    52,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "BONE: a unifying framework for Bayesian online learning in\n  non-stationary environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BONE: a unifying framework for Bayesian online learning in\n  non-stationary environments"
                },
                "summary": "We propose a unifying framework for methods that perform Bayesian online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for (B)ayesian (O)nline learning in (N)on-stationary (E)nvironments.\nBONE provides a common structure to tackle a variety of problems, including\nonline continual learning, prequential forecasting, and contextual bandits. The\nframework requires specifying three modelling choices: (i) a model for\nmeasurements (e.g., a neural network), (ii) an auxiliary process to model\nnon-stationarity (e.g., the time since the last changepoint), and (iii) a\nconditional prior over model parameters (e.g., a multivariate Gaussian). The\nframework also requires two algorithmic choices, which we use to carry out\napproximate inference under this framework: (i) an algorithm to estimate\nbeliefs (posterior distribution) about the model parameters given the auxiliary\nvariable, and (ii) an algorithm to estimate beliefs about the auxiliary\nvariable. We show how this modularity allows us to write many different\nexisting methods as instances of BONE; we also use this framework to propose a\nnew method. We then experimentally compare existing methods with our proposed\nnew method on several datasets; we provide insights into the situations that\nmake one method more suitable than another for a given task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unifying framework for methods that perform Bayesian online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for (B)ayesian (O)nline learning in (N)on-stationary (E)nvironments.\nBONE provides a common structure to tackle a variety of problems, including\nonline continual learning, prequential forecasting, and contextual bandits. The\nframework requires specifying three modelling choices: (i) a model for\nmeasurements (e.g., a neural network), (ii) an auxiliary process to model\nnon-stationarity (e.g., the time since the last changepoint), and (iii) a\nconditional prior over model parameters (e.g., a multivariate Gaussian). The\nframework also requires two algorithmic choices, which we use to carry out\napproximate inference under this framework: (i) an algorithm to estimate\nbeliefs (posterior distribution) about the model parameters given the auxiliary\nvariable, and (ii) an algorithm to estimate beliefs about the auxiliary\nvariable. We show how this modularity allows us to write many different\nexisting methods as instances of BONE; we also use this framework to propose a\nnew method. We then experimentally compare existing methods with our proposed\nnew method on several datasets; we provide insights into the situations that\nmake one method more suitable than another for a given task."
                },
                "authors": [
                    {
                        "name": "Gerardo Duran-Martin"
                    },
                    {
                        "name": "Leandro Snchez-Betancourt"
                    },
                    {
                        "name": "Alexander Y. Shestopaloff"
                    },
                    {
                        "name": "Kevin Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Murphy"
                },
                "author": "Kevin Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11435v1",
                "updated": "2024-11-18T10:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    4,
                    10,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T10:04:10Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    4,
                    10,
                    0,
                    323,
                    0
                ],
                "title": "GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced\n  Aesthetic Text Glyph Layouts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced\n  Aesthetic Text Glyph Layouts"
                },
                "summary": "Text logo design heavily relies on the creativity and expertise of\nprofessional designers, in which arranging element layouts is one of the most\nimportant procedures. However, few attention has been paid to this specific\ntask which needs to take precise textural details and user constraints into\nconsideration, but only on the broader tasks such as document/poster layout\ngeneration. In this paper, we propose a VLM-based framework that generates\ncontent-aware text logo layouts by integrating multi-modal inputs with user\nconstraints, supporting a more flexible and stable layout design in real-world\napplications. We introduce two model techniques to reduce the computation for\nprocessing multiple glyph images simultaneously, while does not face\nperformance degradation. To support instruction-tuning of out model, we\nconstruct two extensive text logo datasets, which are 5x more larger than the\nexisting public dataset. Except for the geometric annotations (e.g. text masks\nand character recognition), we also compliment with comprehensive layout\ndescriptions in natural language format, for more effective training to have\nreasoning ability when dealing with complex layouts and custom user\nconstraints. Experimental studies demonstrate the effectiveness of our proposed\nmodel and datasets, when comparing with previous methods in various benchmarks\nto evaluate geometric aesthetics and human preferences. The code and datasets\nwill be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text logo design heavily relies on the creativity and expertise of\nprofessional designers, in which arranging element layouts is one of the most\nimportant procedures. However, few attention has been paid to this specific\ntask which needs to take precise textural details and user constraints into\nconsideration, but only on the broader tasks such as document/poster layout\ngeneration. In this paper, we propose a VLM-based framework that generates\ncontent-aware text logo layouts by integrating multi-modal inputs with user\nconstraints, supporting a more flexible and stable layout design in real-world\napplications. We introduce two model techniques to reduce the computation for\nprocessing multiple glyph images simultaneously, while does not face\nperformance degradation. To support instruction-tuning of out model, we\nconstruct two extensive text logo datasets, which are 5x more larger than the\nexisting public dataset. Except for the geometric annotations (e.g. text masks\nand character recognition), we also compliment with comprehensive layout\ndescriptions in natural language format, for more effective training to have\nreasoning ability when dealing with complex layouts and custom user\nconstraints. Experimental studies demonstrate the effectiveness of our proposed\nmodel and datasets, when comparing with previous methods in various benchmarks\nto evaluate geometric aesthetics and human preferences. The code and datasets\nwill be publicly available."
                },
                "authors": [
                    {
                        "name": "Junwen He"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Jun-Yan He"
                    },
                    {
                        "name": "Chenyang Li"
                    },
                    {
                        "name": "Hanyuan Chen"
                    },
                    {
                        "name": "Jin-Peng Lan"
                    },
                    {
                        "name": "Bin Luo"
                    },
                    {
                        "name": "Yifeng Geng"
                    }
                ],
                "author_detail": {
                    "name": "Yifeng Geng"
                },
                "author": "Yifeng Geng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15978v2",
                "updated": "2024-11-18T09:57:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    57,
                    24,
                    0,
                    323,
                    0
                ],
                "published": "2024-01-29T09:08:01Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    9,
                    8,
                    1,
                    0,
                    29,
                    0
                ],
                "title": "Multilevel Markov Chain Monte Carlo with likelihood scaling for Bayesian\n  inversion with high-resolution observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel Markov Chain Monte Carlo with likelihood scaling for Bayesian\n  inversion with high-resolution observations"
                },
                "summary": "We propose a multilevel Markov chain Monte Carlo (MCMC) method for the\nBayesian inference of random field parameters in PDEs using high-resolution\ndata. Compared to existing multilevel MCMC methods, we additionally consider\nlevel-dependent data resolution and introduce a suitable likelihood scaling to\nenable consistent cross-level comparisons. We theoretically show that this\napproach attains the same convergence rates as when using level-independent\ntreatment of data, but at significantly reduced computational cost. The\nconvergence analysis focuses on Lipschitz continuous transformations of\nGaussian random fields with Mat\\'ern covariance structure. These results are\nillustrated using numerical experiments for a 2D plane stress problem, where\nthe Young's modulus is estimated from discretisations of the displacement\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a multilevel Markov chain Monte Carlo (MCMC) method for the\nBayesian inference of random field parameters in PDEs using high-resolution\ndata. Compared to existing multilevel MCMC methods, we additionally consider\nlevel-dependent data resolution and introduce a suitable likelihood scaling to\nenable consistent cross-level comparisons. We theoretically show that this\napproach attains the same convergence rates as when using level-independent\ntreatment of data, but at significantly reduced computational cost. The\nconvergence analysis focuses on Lipschitz continuous transformations of\nGaussian random fields with Mat\\'ern covariance structure. These results are\nillustrated using numerical experiments for a 2D plane stress problem, where\nthe Young's modulus is estimated from discretisations of the displacement\nfield."
                },
                "authors": [
                    {
                        "name": "Pieter Vanmechelen"
                    },
                    {
                        "name": "Geert Lombaert"
                    },
                    {
                        "name": "Giovanni Samaey"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Samaey"
                },
                "author": "Giovanni Samaey",
                "arxiv_comment": "25 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 35R60, 65C40 (Primary), 62M05, 65C05, 65N30 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08449v2",
                "updated": "2024-11-18T09:57:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    57,
                    4,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-13T09:11:56Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    9,
                    11,
                    56,
                    2,
                    318,
                    0
                ],
                "title": "Towards Evaluating Large Language Models for Graph Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating Large Language Models for Graph Query Generation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the landscape of Generative\nArtificial Intelligence (GenAI), with innovative LLM-backed solutions emerging\nrapidly. However, when applied to database technologies, specifically query\ngeneration for graph databases and Knowledge Graphs (KGs), LLMs still face\nsignificant challenges. While research on LLM-driven query generation for\nStructured Query Language (SQL) exists, similar systems for graph databases\nremain underdeveloped. This paper presents a comparative study addressing the\nchallenge of generating Cypher queries a powerful language for interacting with\ngraph databases using open-access LLMs. We rigorously evaluate several LLM\nagents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a\nlocally deployed Llama 3.1 8B) using a designed few-shot learning prompt and\nRetrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)\nreasoning. Our empirical analysis of query generation accuracy reveals that\nClaude Sonnet 3.5 outperforms its counterparts in this specific domain.\nFurther, we highlight promising future research directions to address the\nidentified limitations and advance LLM-driven query generation for graph\ndatabases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the landscape of Generative\nArtificial Intelligence (GenAI), with innovative LLM-backed solutions emerging\nrapidly. However, when applied to database technologies, specifically query\ngeneration for graph databases and Knowledge Graphs (KGs), LLMs still face\nsignificant challenges. While research on LLM-driven query generation for\nStructured Query Language (SQL) exists, similar systems for graph databases\nremain underdeveloped. This paper presents a comparative study addressing the\nchallenge of generating Cypher queries a powerful language for interacting with\ngraph databases using open-access LLMs. We rigorously evaluate several LLM\nagents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a\nlocally deployed Llama 3.1 8B) using a designed few-shot learning prompt and\nRetrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)\nreasoning. Our empirical analysis of query generation accuracy reveals that\nClaude Sonnet 3.5 outperforms its counterparts in this specific domain.\nFurther, we highlight promising future research directions to address the\nidentified limitations and advance LLM-driven query generation for graph\ndatabases."
                },
                "authors": [
                    {
                        "name": "Siraj Munir"
                    },
                    {
                        "name": "Alessandro Aldini"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Aldini"
                },
                "author": "Alessandro Aldini",
                "arxiv_comment": "Paper accepted and will be presented at CSCI2024 in December 2024,\n  Later will be published at Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11424v1",
                "updated": "2024-11-18T09:50:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    50,
                    54,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:50:54Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    50,
                    54,
                    0,
                    323,
                    0
                ],
                "title": "Membership Inference Attack against Long-Context Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attack against Long-Context Large Language Models"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled them to overcome\ntheir context window limitations, and demonstrate exceptional retrieval and\nreasoning capacities on longer context. Quesion-answering systems augmented\nwith Long-Context Language Models (LCLMs) can automatically search massive\nexternal data and incorporate it into their contexts, enabling faithful\npredictions and reducing issues such as hallucinations and knowledge staleness.\nExisting studies targeting LCLMs mainly concentrate on addressing the so-called\nlost-in-the-middle problem or improving the inference effiencicy, leaving their\nprivacy risks largely unexplored. In this paper, we aim to bridge this gap and\nargue that integrating all information into the long context makes it a\nrepository of sensitive information, which often contains private data such as\nmedical records or personal identities. We further investigate the membership\nprivacy within LCLMs external context, with the aim of determining whether a\ngiven document or sequence is included in the LCLMs context. Our basic idea is\nthat if a document lies in the context, it will exhibit a low generation loss\nor a high degree of semantic similarity to the contents generated by LCLMs. We\nfor the first time propose six membership inference attack (MIA) strategies\ntailored for LCLMs and conduct extensive experiments on various popular models.\nEmpirical results demonstrate that our attacks can accurately infer membership\nstatus in most cases, e.g., 90.66% attack F1-score on Multi-document QA\ndatasets with LongChat-7b-v1.5-32k, highlighting significant risks of\nmembership leakage within LCLMs input contexts. Furthermore, we examine the\nunderlying reasons why LCLMs are susceptible to revealing such membership\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled them to overcome\ntheir context window limitations, and demonstrate exceptional retrieval and\nreasoning capacities on longer context. Quesion-answering systems augmented\nwith Long-Context Language Models (LCLMs) can automatically search massive\nexternal data and incorporate it into their contexts, enabling faithful\npredictions and reducing issues such as hallucinations and knowledge staleness.\nExisting studies targeting LCLMs mainly concentrate on addressing the so-called\nlost-in-the-middle problem or improving the inference effiencicy, leaving their\nprivacy risks largely unexplored. In this paper, we aim to bridge this gap and\nargue that integrating all information into the long context makes it a\nrepository of sensitive information, which often contains private data such as\nmedical records or personal identities. We further investigate the membership\nprivacy within LCLMs external context, with the aim of determining whether a\ngiven document or sequence is included in the LCLMs context. Our basic idea is\nthat if a document lies in the context, it will exhibit a low generation loss\nor a high degree of semantic similarity to the contents generated by LCLMs. We\nfor the first time propose six membership inference attack (MIA) strategies\ntailored for LCLMs and conduct extensive experiments on various popular models.\nEmpirical results demonstrate that our attacks can accurately infer membership\nstatus in most cases, e.g., 90.66% attack F1-score on Multi-document QA\ndatasets with LongChat-7b-v1.5-32k, highlighting significant risks of\nmembership leakage within LCLMs input contexts. Furthermore, we examine the\nunderlying reasons why LCLMs are susceptible to revealing such membership\ninformation."
                },
                "authors": [
                    {
                        "name": "Zixiong Wang"
                    },
                    {
                        "name": "Gaoyang Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Chen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Wang"
                },
                "author": "Chen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18492v3",
                "updated": "2024-11-18T09:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    44,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-28T18:01:52Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    18,
                    1,
                    52,
                    1,
                    149,
                    0
                ],
                "title": "LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance"
                },
                "summary": "Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code can\nbe found at https://github.com/felixbmuller/llms-memorization-copyright.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code can\nbe found at https://github.com/felixbmuller/llms-memorization-copyright."
                },
                "authors": [
                    {
                        "name": "Felix B Mueller"
                    },
                    {
                        "name": "Rebekka Grge"
                    },
                    {
                        "name": "Anna K Bernzen"
                    },
                    {
                        "name": "Janna C Pirk"
                    },
                    {
                        "name": "Maximilian Poretschkin"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Poretschkin"
                },
                "author": "Maximilian Poretschkin",
                "arxiv_comment": "10 pages, 3 figures, AIES 2024 conference",
                "arxiv_journal_ref": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,\n  7(1), 984-996, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11410v1",
                "updated": "2024-11-18T09:30:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    30,
                    14,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:30:14Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    30,
                    14,
                    0,
                    323,
                    0
                ],
                "title": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries"
                },
                "summary": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose an MPDetector for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detect\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose an MPDetector for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detect\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing."
                },
                "authors": [
                    {
                        "name": "Xiufeng Xu"
                    },
                    {
                        "name": "Fuman Xie"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Guangdong Bai"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11407v1",
                "updated": "2024-11-18T09:28:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    28,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:28:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    28,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on\n  Large Language Models"
                },
                "summary": "The widespread deployment of large language models (LLMs) across various\ndomains has showcased their immense potential while exposing significant safety\nvulnerabilities. A major concern is ensuring that LLM-generated content aligns\nwith human values. Existing jailbreak techniques reveal how this alignment can\nbe compromised through specific prompts or adversarial suffixes. In this study,\nwe introduce a new threat: LLMs' bias toward authority. While this inherent\nbias can improve the quality of outputs generated by LLMs, it also introduces a\npotential vulnerability, increasing the risk of producing harmful content.\nNotably, the biases in LLMs is the varying levels of trust given to different\ntypes of authoritative information in harmful queries. For example, malware\ndevelopment often favors trust GitHub. To better reveal the risks with LLM, we\npropose DarkCite, an adaptive authority citation matcher and generator designed\nfor a black-box setting. DarkCite matches optimal citation types to specific\nrisk types and generates authoritative citations relevant to harmful\ninstructions, enabling more effective jailbreak attacks on aligned LLMs.Our\nexperiments show that DarkCite achieves a higher attack success rate (e.g.,\nLLama-2 at 76% versus 68%) than previous methods. To counter this risk, we\npropose an authenticity and harm verification defense strategy, raising the\naverage defense pass rate (DPR) from 11% to 74%. More importantly, the ability\nto link citations to the content they encompass has become a foundational\nfunction in LLMs, amplifying the influence of LLMs' bias toward authority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large language models (LLMs) across various\ndomains has showcased their immense potential while exposing significant safety\nvulnerabilities. A major concern is ensuring that LLM-generated content aligns\nwith human values. Existing jailbreak techniques reveal how this alignment can\nbe compromised through specific prompts or adversarial suffixes. In this study,\nwe introduce a new threat: LLMs' bias toward authority. While this inherent\nbias can improve the quality of outputs generated by LLMs, it also introduces a\npotential vulnerability, increasing the risk of producing harmful content.\nNotably, the biases in LLMs is the varying levels of trust given to different\ntypes of authoritative information in harmful queries. For example, malware\ndevelopment often favors trust GitHub. To better reveal the risks with LLM, we\npropose DarkCite, an adaptive authority citation matcher and generator designed\nfor a black-box setting. DarkCite matches optimal citation types to specific\nrisk types and generates authoritative citations relevant to harmful\ninstructions, enabling more effective jailbreak attacks on aligned LLMs.Our\nexperiments show that DarkCite achieves a higher attack success rate (e.g.,\nLLama-2 at 76% versus 68%) than previous methods. To counter this risk, we\npropose an authenticity and harm verification defense strategy, raising the\naverage defense pass rate (DPR) from 11% to 74%. More importantly, the ability\nto link citations to the content they encompass has become a foundational\nfunction in LLMs, amplifying the influence of LLMs' bias toward authority."
                },
                "authors": [
                    {
                        "name": "Xikang Yang"
                    },
                    {
                        "name": "Xuehai Tang"
                    },
                    {
                        "name": "Jizhong Han"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00476v2",
                "updated": "2024-11-18T09:25:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    25,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-01T08:06:45Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    6,
                    45,
                    1,
                    275,
                    0
                ],
                "title": "Importance sampling-based gradient method for dimension reduction in\n  Poisson log-normal model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance sampling-based gradient method for dimension reduction in\n  Poisson log-normal model"
                },
                "summary": "High-dimensional count data poses significant challenges for statistical\nanalysis, necessitating effective methods that also preserve explainability. We\nfocus on a low rank constrained variant of the Poisson log-normal model, which\nrelates the observed data to a latent low-dimensional multivariate Gaussian\nvariable via a Poisson distribution. Variational inference methods have become\na golden standard solution to infer such a model. While computationally\nefficient, they usually lack theoretical statistical properties with respect to\nthe model. To address this issue we propose a projected stochastic gradient\nscheme that directly maximizes the log-likelihood. We prove the convergence of\nthe proposed method when using importance sampling for estimating the gradient.\nSpecifically, we obtain a rate of convergence of $O(T^{\\nicefrac{-1}{2}} +\nN^{-1})$ with $T$ the number of iterations and $N$ the number of Monte Carlo\ndraws. The latter follows from a novel descent lemma for non convex $L$-smooth\nobjective functions, and random biased gradient estimate. We also demonstrate\nnumerically the efficiency of our solution compared to its variational\ncompetitor. Our method not only scales with respect to the number of observed\nsamples but also provides access to the desirable properties of the maximum\nlikelihood estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional count data poses significant challenges for statistical\nanalysis, necessitating effective methods that also preserve explainability. We\nfocus on a low rank constrained variant of the Poisson log-normal model, which\nrelates the observed data to a latent low-dimensional multivariate Gaussian\nvariable via a Poisson distribution. Variational inference methods have become\na golden standard solution to infer such a model. While computationally\nefficient, they usually lack theoretical statistical properties with respect to\nthe model. To address this issue we propose a projected stochastic gradient\nscheme that directly maximizes the log-likelihood. We prove the convergence of\nthe proposed method when using importance sampling for estimating the gradient.\nSpecifically, we obtain a rate of convergence of $O(T^{\\nicefrac{-1}{2}} +\nN^{-1})$ with $T$ the number of iterations and $N$ the number of Monte Carlo\ndraws. The latter follows from a novel descent lemma for non convex $L$-smooth\nobjective functions, and random biased gradient estimate. We also demonstrate\nnumerically the efficiency of our solution compared to its variational\ncompetitor. Our method not only scales with respect to the number of observed\nsamples but also provides access to the desirable properties of the maximum\nlikelihood estimator."
                },
                "authors": [
                    {
                        "name": "Bastien Batardire"
                    },
                    {
                        "name": "Julien Chiquet"
                    },
                    {
                        "name": "Joon Kwon"
                    },
                    {
                        "name": "Julien Stoehr"
                    }
                ],
                "author_detail": {
                    "name": "Julien Stoehr"
                },
                "arxiv_affiliation": "CEREMADE",
                "author": "Julien Stoehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11401v1",
                "updated": "2024-11-18T09:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?"
                },
                "summary": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence."
                },
                "authors": [
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Ahmad Tayeb"
                    },
                    {
                        "name": "Ozren Dabi"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14259v2",
                "updated": "2024-11-18T09:19:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    19,
                    25,
                    0,
                    323,
                    0
                ],
                "published": "2024-02-22T03:46:08Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    3,
                    46,
                    8,
                    3,
                    53,
                    0
                ],
                "title": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form\n  Medical Question Answering Applications and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form\n  Medical Question Answering Applications and Beyond"
                },
                "summary": "Uncertainty estimation is crucial for the reliability of safety-critical\nhuman and artificial intelligence (AI) interaction systems, particularly in the\ndomain of healthcare engineering. However, a robust and general uncertainty\nmeasure for free-form answers has not been well-established in open-ended\nmedical question-answering (QA) tasks, where generative inequality introduces a\nlarge number of irrelevant words and sequences within the generated set for\nuncertainty quantification (UQ), which can lead to biases. This paper\nintroduces Word-Sequence Entropy (WSE), a method that calibrates uncertainty at\nboth the word and sequence levels, considering semantic relevance. WSE\nquantifies uncertainty in a way that is more closely aligned with the\nreliability of LLMs during uncertainty quantification (UQ). We compare WSE with\nsix baseline methods on five free-form medical QA datasets, utilizing seven\npopular large language models (LLMs). Experimental results demonstrate that WSE\nexhibits superior performance in UQ under two standard criteria for correctness\nevaluation. Additionally, in terms of real-world medical QA applications, the\nperformance of LLMs is significantly enhanced (e.g., a 6.36% improvement in\nmodel accuracy on the COVID-QA dataset) by employing responses with lower\nuncertainty that are identified by WSE as final answers, without any additional\ntask-specific fine-tuning or architectural modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is crucial for the reliability of safety-critical\nhuman and artificial intelligence (AI) interaction systems, particularly in the\ndomain of healthcare engineering. However, a robust and general uncertainty\nmeasure for free-form answers has not been well-established in open-ended\nmedical question-answering (QA) tasks, where generative inequality introduces a\nlarge number of irrelevant words and sequences within the generated set for\nuncertainty quantification (UQ), which can lead to biases. This paper\nintroduces Word-Sequence Entropy (WSE), a method that calibrates uncertainty at\nboth the word and sequence levels, considering semantic relevance. WSE\nquantifies uncertainty in a way that is more closely aligned with the\nreliability of LLMs during uncertainty quantification (UQ). We compare WSE with\nsix baseline methods on five free-form medical QA datasets, utilizing seven\npopular large language models (LLMs). Experimental results demonstrate that WSE\nexhibits superior performance in UQ under two standard criteria for correctness\nevaluation. Additionally, in terms of real-world medical QA applications, the\nperformance of LLMs is significantly enhanced (e.g., a 6.36% improvement in\nmodel accuracy on the COVID-QA dataset) by employing responses with lower\nuncertainty that are identified by WSE as final answers, without any additional\ntask-specific fine-tuning or architectural modifications."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Chenxi Yuan"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ren Wang"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "Accepted by Engineering Applications of Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20911v2",
                "updated": "2024-11-18T09:15:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    15,
                    46,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-28T10:43:34Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    43,
                    34,
                    0,
                    302,
                    0
                ],
                "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks"
                },
                "summary": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis"
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Ateniese"
                },
                "author": "Giuseppe Ateniese",
                "arxiv_comment": "v0.2 (evaluated on more agents)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11389v1",
                "updated": "2024-11-18T09:03:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    3,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:03:51Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    3,
                    51,
                    0,
                    323,
                    0
                ],
                "title": "Adapting to Cyber Threats: A Phishing Evolution Network (PEN) Framework\n  for Phishing Generation and Analyzing Evolution Patterns using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting to Cyber Threats: A Phishing Evolution Network (PEN) Framework\n  for Phishing Generation and Analyzing Evolution Patterns using Large Language\n  Models"
                },
                "summary": "Phishing remains a pervasive cyber threat, as attackers craft deceptive\nemails to lure victims into revealing sensitive information. While Artificial\nIntelligence (AI), particularly deep learning, has become a key component in\ndefending against phishing attacks, these approaches face critical limitations.\nThe scarcity of publicly available, diverse, and updated data, largely due to\nprivacy concerns, constrains their effectiveness. As phishing tactics evolve\nrapidly, models trained on limited, outdated data struggle to detect new,\nsophisticated deception strategies, leaving systems vulnerable to an\never-growing array of attacks. Addressing this gap is essential to\nstrengthening defenses in an increasingly hostile cyber landscape. To address\nthis gap, we propose the Phishing Evolution Network (PEN), a framework\nleveraging large language models (LLMs) and adversarial training mechanisms to\ncontinuously generate high quality and realistic diverse phishing samples, and\nanalyze features of LLM-provided phishing to understand evolving phishing\npatterns. We evaluate the quality and diversity of phishing samples generated\nby PEN and find that it produces over 80% realistic phishing samples,\neffectively expanding phishing datasets across seven dominant types. These\nPEN-generated samples enhance the performance of current phishing detectors,\nleading to a 40% improvement in detection accuracy. Additionally, the use of\nPEN significantly boosts model robustness, reducing detectors' sensitivity to\nperturbations by up to 60%, thereby decreasing attack success rates under\nadversarial conditions. When we analyze the phishing patterns that are used in\nLLM-generated phishing, the cognitive complexity and the tone of time\nlimitation are detected with statistically significant differences compared\nwith existing phishing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing remains a pervasive cyber threat, as attackers craft deceptive\nemails to lure victims into revealing sensitive information. While Artificial\nIntelligence (AI), particularly deep learning, has become a key component in\ndefending against phishing attacks, these approaches face critical limitations.\nThe scarcity of publicly available, diverse, and updated data, largely due to\nprivacy concerns, constrains their effectiveness. As phishing tactics evolve\nrapidly, models trained on limited, outdated data struggle to detect new,\nsophisticated deception strategies, leaving systems vulnerable to an\never-growing array of attacks. Addressing this gap is essential to\nstrengthening defenses in an increasingly hostile cyber landscape. To address\nthis gap, we propose the Phishing Evolution Network (PEN), a framework\nleveraging large language models (LLMs) and adversarial training mechanisms to\ncontinuously generate high quality and realistic diverse phishing samples, and\nanalyze features of LLM-provided phishing to understand evolving phishing\npatterns. We evaluate the quality and diversity of phishing samples generated\nby PEN and find that it produces over 80% realistic phishing samples,\neffectively expanding phishing datasets across seven dominant types. These\nPEN-generated samples enhance the performance of current phishing detectors,\nleading to a 40% improvement in detection accuracy. Additionally, the use of\nPEN significantly boosts model robustness, reducing detectors' sensitivity to\nperturbations by up to 60%, thereby decreasing attack success rates under\nadversarial conditions. When we analyze the phishing patterns that are used in\nLLM-generated phishing, the cognitive complexity and the tone of time\nlimitation are detected with statistically significant differences compared\nwith existing phishing."
                },
                "authors": [
                    {
                        "name": "Fengchao Chen"
                    },
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Van Nguyen"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Carsten Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rudolph"
                },
                "author": "Carsten Rudolph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11371v1",
                "updated": "2024-11-18T08:34:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    34,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T08:34:38Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    34,
                    38,
                    0,
                    323,
                    0
                ],
                "title": "Rethinking Thinking Tokens: Understanding Why They Underperform in\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Thinking Tokens: Understanding Why They Underperform in\n  Practice"
                },
                "summary": "Thinking Tokens (TT) have been proposed as an unsupervised method to\nfacilitate reasoning in language models. However, despite their conceptual\nappeal, our findings show that TTs marginally improves performance and\nconsistently underperforms compared to Chain-of-Thought (CoT) reasoning across\nmultiple benchmarks. We hypothesize that this underperformance stems from the\nreliance on a single embedding for TTs, which results in inconsistent learning\nsignals and introduces noisy gradients. This paper provides a comprehensive\nempirical analysis to validate this hypothesis and discusses the implications\nfor future research on unsupervised reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Tokens (TT) have been proposed as an unsupervised method to\nfacilitate reasoning in language models. However, despite their conceptual\nappeal, our findings show that TTs marginally improves performance and\nconsistently underperforms compared to Chain-of-Thought (CoT) reasoning across\nmultiple benchmarks. We hypothesize that this underperformance stems from the\nreliance on a single embedding for TTs, which results in inconsistent learning\nsignals and introduces noisy gradients. This paper provides a comprehensive\nempirical analysis to validate this hypothesis and discusses the implications\nfor future research on unsupervised reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Sreeram Vennam"
                    },
                    {
                        "name": "David Valente"
                    },
                    {
                        "name": "David Herel"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00499v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00499v3",
                "updated": "2024-11-18T08:33:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    33,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-29T17:33:07Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    17,
                    33,
                    7,
                    5,
                    181,
                    0
                ],
                "title": "ConU: Conformal Uncertainty in Large Language Models with Correctness\n  Coverage Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConU: Conformal Uncertainty in Large Language Models with Correctness\n  Coverage Guarantees"
                },
                "summary": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks\nremains an open challenge, exacerbated by the closed-source nature of the\nlatest large language models (LLMs). This study investigates applying conformal\nprediction (CP), which can transform any heuristic uncertainty notion into\nrigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We\nintroduce a novel uncertainty measure based on self-consistency theory, and\nthen develop a conformal uncertainty criterion by integrating the uncertainty\ncondition aligned with correctness into the CP algorithm. Empirical evaluations\nindicate that our uncertainty measure outperforms prior state-of-the-art\nmethods. Furthermore, we achieve strict control over the correctness coverage\nrate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning\ngeneral-purpose and medical scenarios. Additionally, the calibrated prediction\nsets with small size further highlights the efficiency of our method in\nproviding trustworthy guarantees for practical open-ended NLG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks\nremains an open challenge, exacerbated by the closed-source nature of the\nlatest large language models (LLMs). This study investigates applying conformal\nprediction (CP), which can transform any heuristic uncertainty notion into\nrigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We\nintroduce a novel uncertainty measure based on self-consistency theory, and\nthen develop a conformal uncertainty criterion by integrating the uncertainty\ncondition aligned with correctness into the CP algorithm. Empirical evaluations\nindicate that our uncertainty measure outperforms prior state-of-the-art\nmethods. Furthermore, we achieve strict control over the correctness coverage\nrate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning\ngeneral-purpose and medical scenarios. Additionally, the calibrated prediction\nsets with small size further highlights the efficiency of our method in\nproviding trustworthy guarantees for practical open-ended NLG applications."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Hengtao Shen"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Zhu"
                },
                "author": "Xiaofeng Zhu",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00499v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00499v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10370v2",
                "updated": "2024-11-18T08:29:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    29,
                    8,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-16T18:03:41Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    18,
                    3,
                    41,
                    3,
                    137,
                    0
                ],
                "title": "Grounded 3D-LLM with Referent Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded 3D-LLM with Referent Tokens"
                },
                "summary": "Prior studies on 3D scene understanding have primarily developed specialized\nmodels for specific tasks or required task-specific fine-tuning. In this study,\nwe propose Grounded 3D-LLM, which explores the potential of 3D large\nmulti-modal models (3D LMMs) to consolidate various 3D vision tasks within a\nunified generative framework. The model uses scene referent tokens as special\nnoun phrases to reference 3D scenes, enabling it to handle sequences that\ninterleave 3D and textual data. Per-task instruction-following templates are\nemployed to ensure natural and diversity in translating 3D vision tasks into\nlanguage formats. To facilitate the use of referent tokens in subsequent\nlanguage modeling, we provide a large-scale, automatically curated grounded\nscene-text dataset with over 1 million phrase-to-region correspondences and\nintroduce Contrastive Language-Scene Pre-training (CLASP) to perform\nphrase-level scene-text alignment using this data. Our comprehensive evaluation\ncovers open-ended tasks like dense captioning and 3D question answering,\nalongside close-ended tasks such as object detection and language grounding.\nExperiments across multiple 3D benchmarks reveal the leading performance and\nthe broad applicability of Grounded 3D-LLM. Code and datasets are available at\nthe https://groundedscenellm.github.io/grounded_3d-llm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies on 3D scene understanding have primarily developed specialized\nmodels for specific tasks or required task-specific fine-tuning. In this study,\nwe propose Grounded 3D-LLM, which explores the potential of 3D large\nmulti-modal models (3D LMMs) to consolidate various 3D vision tasks within a\nunified generative framework. The model uses scene referent tokens as special\nnoun phrases to reference 3D scenes, enabling it to handle sequences that\ninterleave 3D and textual data. Per-task instruction-following templates are\nemployed to ensure natural and diversity in translating 3D vision tasks into\nlanguage formats. To facilitate the use of referent tokens in subsequent\nlanguage modeling, we provide a large-scale, automatically curated grounded\nscene-text dataset with over 1 million phrase-to-region correspondences and\nintroduce Contrastive Language-Scene Pre-training (CLASP) to perform\nphrase-level scene-text alignment using this data. Our comprehensive evaluation\ncovers open-ended tasks like dense captioning and 3D question answering,\nalongside close-ended tasks such as object detection and language grounding.\nExperiments across multiple 3D benchmarks reveal the leading performance and\nthe broad applicability of Grounded 3D-LLM. Code and datasets are available at\nthe https://groundedscenellm.github.io/grounded_3d-llm.github.io."
                },
                "authors": [
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Ruiyuan Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02506v2",
                "updated": "2024-11-18T07:59:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    59,
                    55,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-04T17:24:19Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    24,
                    19,
                    1,
                    156,
                    0
                ],
                "title": "An Open-Source Tool for Mapping War Destruction at Scale in Ukraine\n  using Sentinel-1 Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Open-Source Tool for Mapping War Destruction at Scale in Ukraine\n  using Sentinel-1 Time Series"
                },
                "summary": "Access to detailed war impact assessments is crucial for humanitarian\norganizations to effectively assist populations most affected by armed\nconflicts. However, maintaining a comprehensive understanding of the situation\non the ground is challenging, especially in conflicts that cover vast\nterritories and extend over long periods. This study presents a scalable and\ntransferable method for estimating war-induced damage to buildings. We first\ntrain a machine learning model to output pixel-wise probability of destruction\nfrom Synthetic Aperture Radar (SAR) satellite image time series, leveraging\nexisting, manual damage assessments as ground truth and cloud-based geospatial\nanalysis tools for large-scale inference. We further post-process these\nassessments using open building footprints to obtain a final damage estimate\nper building. We introduce an accessible, open-source tool that allows users to\nadjust the confidence interval based on their specific requirements and use\ncases. Our approach enables humanitarian organizations and other actors to\nrapidly screen large geographic regions for war impacts. We provide two\npublicly accessible dashboards: a Ukraine Damage Explorer to dynamically view\nour pre-computed estimates, and a Rapid Damage Mapping Tool to easily run our\nmethod and produce custom maps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to detailed war impact assessments is crucial for humanitarian\norganizations to effectively assist populations most affected by armed\nconflicts. However, maintaining a comprehensive understanding of the situation\non the ground is challenging, especially in conflicts that cover vast\nterritories and extend over long periods. This study presents a scalable and\ntransferable method for estimating war-induced damage to buildings. We first\ntrain a machine learning model to output pixel-wise probability of destruction\nfrom Synthetic Aperture Radar (SAR) satellite image time series, leveraging\nexisting, manual damage assessments as ground truth and cloud-based geospatial\nanalysis tools for large-scale inference. We further post-process these\nassessments using open building footprints to obtain a final damage estimate\nper building. We introduce an accessible, open-source tool that allows users to\nadjust the confidence interval based on their specific requirements and use\ncases. Our approach enables humanitarian organizations and other actors to\nrapidly screen large geographic regions for war impacts. We provide two\npublicly accessible dashboards: a Ukraine Damage Explorer to dynamically view\nour pre-computed estimates, and a Rapid Damage Mapping Tool to easily run our\nmethod and produce custom maps."
                },
                "authors": [
                    {
                        "name": "Olivier Dietrich"
                    },
                    {
                        "name": "Torben Peters"
                    },
                    {
                        "name": "Vivien Sainte Fare Garnot"
                    },
                    {
                        "name": "Valerie Sticher"
                    },
                    {
                        "name": "Thao Ton-That Whelan"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Jan Dirk Wegner"
                    }
                ],
                "author_detail": {
                    "name": "Jan Dirk Wegner"
                },
                "author": "Jan Dirk Wegner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11350v1",
                "updated": "2024-11-18T07:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    39,
                    46,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T07:39:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    39,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Zero-Shot Load Forecasting with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Load Forecasting with Large Language Models"
                },
                "summary": "Deep learning models have shown strong performance in load forecasting, but\nthey generally require large amounts of data for model training before being\napplied to new scenarios, which limits their effectiveness in data-scarce\nscenarios. Inspired by the great success of pre-trained language models (LLMs)\nin natural language processing, this paper proposes a zero-shot load\nforecasting approach using an advanced LLM framework denoted as the Chronos\nmodel. By utilizing its extensive pre-trained knowledge, the Chronos model\nenables accurate load forecasting in data-scarce scenarios without the need for\nextensive data-specific training. Simulation results across five real-world\ndatasets demonstrate that the Chronos model significantly outperforms nine\npopular baseline models for both deterministic and probabilistic load\nforecasting with various forecast horizons (e.g., 1 to 48 hours), even though\nthe Chronos model is neither tailored nor fine-tuned to these specific load\ndatasets. Notably, Chronos reduces root mean squared error (RMSE), continuous\nranked probability score (CRPS), and quantile score (QS) by approximately\n7.34%-84.30%, 19.63%-60.06%, and 22.83%-54.49%, respectively, compared to\nbaseline models. These results highlight the superiority and flexibility of the\nChronos model, positioning it as an effective solution in data-scarce\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have shown strong performance in load forecasting, but\nthey generally require large amounts of data for model training before being\napplied to new scenarios, which limits their effectiveness in data-scarce\nscenarios. Inspired by the great success of pre-trained language models (LLMs)\nin natural language processing, this paper proposes a zero-shot load\nforecasting approach using an advanced LLM framework denoted as the Chronos\nmodel. By utilizing its extensive pre-trained knowledge, the Chronos model\nenables accurate load forecasting in data-scarce scenarios without the need for\nextensive data-specific training. Simulation results across five real-world\ndatasets demonstrate that the Chronos model significantly outperforms nine\npopular baseline models for both deterministic and probabilistic load\nforecasting with various forecast horizons (e.g., 1 to 48 hours), even though\nthe Chronos model is neither tailored nor fine-tuned to these specific load\ndatasets. Notably, Chronos reduces root mean squared error (RMSE), continuous\nranked probability score (CRPS), and quantile score (QS) by approximately\n7.34%-84.30%, 19.63%-60.06%, and 22.83%-54.49%, respectively, compared to\nbaseline models. These results highlight the superiority and flexibility of the\nChronos model, positioning it as an effective solution in data-scarce\nscenarios."
                },
                "authors": [
                    {
                        "name": "Wenlong Liao"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Mengshuo Jia"
                    },
                    {
                        "name": "Christian Rehtanz"
                    },
                    {
                        "name": "Jiannong Fang"
                    },
                    {
                        "name": "Fernando Port-Agel"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Port-Agel"
                },
                "author": "Fernando Port-Agel",
                "arxiv_comment": "21 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02156v2",
                "updated": "2024-11-18T07:36:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    36,
                    36,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-03T02:36:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    2,
                    36,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "The why, what, and how of AI-based coding in scientific research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The why, what, and how of AI-based coding in scientific research"
                },
                "summary": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress."
                },
                "authors": [
                    {
                        "name": "Tonghe Zhuang"
                    },
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_comment": "23 pages, 7 figure, 3 boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11344v1",
                "updated": "2024-11-18T07:33:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    33,
                    10,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T07:33:10Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    33,
                    10,
                    0,
                    323,
                    0
                ],
                "title": "Mitigating Knowledge Conflicts in Language Model-Driven Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Knowledge Conflicts in Language Model-Driven Question\n  Answering"
                },
                "summary": "Knowledge-aware sequence to sequence generation tasks such as document\nquestion answering and abstract summarization typically requires two types of\nknowledge: encoded parametric knowledge and retrieved contextual information.\nPrevious work show improper correlation between parametric knowledge and\nanswers in the training set could cause the model ignore input information at\ntest time, resulting in un-desirable model behaviour such as over-stability and\nhallucination. In this work, we argue that hallucination could be mitigated via\nexplicit correlation between input source and generated content. We focus on a\ntypical example of hallucination, entity-based knowledge conflicts in question\nanswering, where correlation of entities and their description at training time\nhinders model behaviour during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-aware sequence to sequence generation tasks such as document\nquestion answering and abstract summarization typically requires two types of\nknowledge: encoded parametric knowledge and retrieved contextual information.\nPrevious work show improper correlation between parametric knowledge and\nanswers in the training set could cause the model ignore input information at\ntest time, resulting in un-desirable model behaviour such as over-stability and\nhallucination. In this work, we argue that hallucination could be mitigated via\nexplicit correlation between input source and generated content. We focus on a\ntypical example of hallucination, entity-based knowledge conflicts in question\nanswering, where correlation of entities and their description at training time\nhinders model behaviour during inference."
                },
                "authors": [
                    {
                        "name": "Han Cao"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Xiangtian Li"
                    },
                    {
                        "name": "Chufan Wu"
                    },
                    {
                        "name": "Hansong Zhang"
                    },
                    {
                        "name": "Wenqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqing Zhang"
                },
                "author": "Wenqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08484v2",
                "updated": "2024-11-18T07:32:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    32,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-03-13T12:50:23Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    12,
                    50,
                    23,
                    2,
                    73,
                    0
                ],
                "title": "Targeted Efficient Fine-tuning: Optimizing Parameter Updates with\n  Data-Driven Sample Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Efficient Fine-tuning: Optimizing Parameter Updates with\n  Data-Driven Sample Selection"
                },
                "summary": "Fine-tuning all parameters of Large Language Models (LLMs) is computationally\nexpensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nselectively fine-tuning specific parameters. Most of the parameter efficient\nfine-tuning (PEFT) methods center on selecting or introducing a set of\nparameters to be fine-tuned. However, there are few methods that consider the\nimpact of data samples on parameter selecting. Representative data driven\nmethods include FISH Mask based method, which randomly selects a portion of\ndata samples as a basis when selecting parameters. However, this random data\nsample selection method cannot select optimal parameters for unstable data\ndistribution. In this work, we introduce a data-centric approach and propose\nthe Iterative Range Decreasing (IRD) algorithm to optimize the sample-parameter\npair selection in FISH Mask. IRD iteratively refines the selection by\nidentifying subsets of samples and parameters exhibiting higher Fisher\ninformation. We demonstrate the effectiveness and rationality of proposed\nstrategy by conducting experiments on GLUE benchmark. Experimental results show\nour strategy optimizes the parameter selection and achieves preferable\nperformance over some typical baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning all parameters of Large Language Models (LLMs) is computationally\nexpensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nselectively fine-tuning specific parameters. Most of the parameter efficient\nfine-tuning (PEFT) methods center on selecting or introducing a set of\nparameters to be fine-tuned. However, there are few methods that consider the\nimpact of data samples on parameter selecting. Representative data driven\nmethods include FISH Mask based method, which randomly selects a portion of\ndata samples as a basis when selecting parameters. However, this random data\nsample selection method cannot select optimal parameters for unstable data\ndistribution. In this work, we introduce a data-centric approach and propose\nthe Iterative Range Decreasing (IRD) algorithm to optimize the sample-parameter\npair selection in FISH Mask. IRD iteratively refines the selection by\nidentifying subsets of samples and parameters exhibiting higher Fisher\ninformation. We demonstrate the effectiveness and rationality of proposed\nstrategy by conducting experiments on GLUE benchmark. Experimental results show\nour strategy optimizes the parameter selection and achieves preferable\nperformance over some typical baseline methods."
                },
                "authors": [
                    {
                        "name": "Ming Dong"
                    },
                    {
                        "name": "Kang Xue"
                    },
                    {
                        "name": "Bolong Zheng"
                    },
                    {
                        "name": "Tingting He"
                    }
                ],
                "author_detail": {
                    "name": "Tingting He"
                },
                "author": "Tingting He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08807v3",
                "updated": "2024-11-18T07:30:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    30,
                    6,
                    0,
                    323,
                    0
                ],
                "published": "2024-01-16T20:13:50Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    20,
                    13,
                    50,
                    1,
                    16,
                    0
                ],
                "title": "SpecGen: Automated Generation of Formal Program Specifications via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecGen: Automated Generation of Formal Program Specifications via Large\n  Language Models"
                },
                "summary": "Formal program specifications play a crucial role in various stages of\nsoftware development. However, manually crafting formal program specifications\nis rather difficult, making the job time-consuming and labor-intensive. It is\neven more challenging to write specifications that correctly and\ncomprehensively describe the semantics of complex programs. To reduce the\nburden on software developers, automated specification generation methods have\nemerged. However, existing methods usually rely on predefined templates or\ngrammar, making them struggle to accurately describe the behavior and\nfunctionality of complex real-world programs. To tackle this challenge, we\nintroduce SpecGen, a novel technique for formal program specification\ngeneration based on Large Language Models. Our key insight is to overcome the\nlimitations of existing methods by leveraging the code comprehension capability\nof LLMs. The process of SpecGen consists of two phases. The first phase employs\na conversational approach that guides the LLM to generate appropriate\nspecifications for a given program. The second phase, designed for where the\nLLM fails to generate correct specifications, applies four mutation operators\nto the model-generated specifications and selects verifiable specifications\nfrom the mutated ones through a novel heuristic selection strategy. We evaluate\nSpecGen on two datasets, including the SV-COMP Java category benchmark and a\nmanually constructed dataset. Experimental results demonstrate that SpecGen\nsucceeds in generating verifiable specifications for 279 out of 385 programs,\noutperforming the existing purely LLM-based approaches and conventional\nspecification generation tools like Houdini and Daikon. Further investigations\non the quality of generated specifications indicate that SpecGen can\ncomprehensively articulate the behaviors of the input program.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal program specifications play a crucial role in various stages of\nsoftware development. However, manually crafting formal program specifications\nis rather difficult, making the job time-consuming and labor-intensive. It is\neven more challenging to write specifications that correctly and\ncomprehensively describe the semantics of complex programs. To reduce the\nburden on software developers, automated specification generation methods have\nemerged. However, existing methods usually rely on predefined templates or\ngrammar, making them struggle to accurately describe the behavior and\nfunctionality of complex real-world programs. To tackle this challenge, we\nintroduce SpecGen, a novel technique for formal program specification\ngeneration based on Large Language Models. Our key insight is to overcome the\nlimitations of existing methods by leveraging the code comprehension capability\nof LLMs. The process of SpecGen consists of two phases. The first phase employs\na conversational approach that guides the LLM to generate appropriate\nspecifications for a given program. The second phase, designed for where the\nLLM fails to generate correct specifications, applies four mutation operators\nto the model-generated specifications and selects verifiable specifications\nfrom the mutated ones through a novel heuristic selection strategy. We evaluate\nSpecGen on two datasets, including the SV-COMP Java category benchmark and a\nmanually constructed dataset. Experimental results demonstrate that SpecGen\nsucceeds in generating verifiable specifications for 279 out of 385 programs,\noutperforming the existing purely LLM-based approaches and conventional\nspecification generation tools like Houdini and Daikon. Further investigations\non the quality of generated specifications indicate that SpecGen can\ncomprehensively articulate the behaviors of the input program."
                },
                "authors": [
                    {
                        "name": "Lezhi Ma"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Lei Bu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bu"
                },
                "author": "Lei Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11343v1",
                "updated": "2024-11-18T07:26:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    26,
                    9,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T07:26:09Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    26,
                    9,
                    0,
                    323,
                    0
                ],
                "title": "Teaching Video Diffusion Model with Latent Physical Phenomenon Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Video Diffusion Model with Latent Physical Phenomenon Knowledge"
                },
                "summary": "Video diffusion models have exhibited tremendous progress in various video\ngeneration tasks. However, existing models struggle to capture latent physical\nknowledge, failing to infer physical phenomena that are challenging to\narticulate with natural language. Generating videos following the fundamental\nphysical laws is still an opening challenge. To address this challenge, we\npropose a novel method to teach video diffusion models with latent physical\nphenomenon knowledge, enabling the accurate generation of physically informed\nphenomena. Specifically, we first pretrain Masked Autoencoders (MAE) to\nreconstruct the physical phenomena, resulting in output embeddings that\nencapsulate latent physical phenomenon knowledge. Leveraging these embeddings,\nwe could generate the pseudo-language prompt features based on the aligned\nspatial relationships between CLIP vision and language encoders. Particularly,\ngiven that diffusion models typically use CLIP's language encoder for text\nprompt embeddings, our approach integrates the CLIP visual features informed by\nlatent physical knowledge into a quaternion hidden space. This enables the\nmodeling of spatial relationships to produce physical knowledge-informed\npseudo-language prompts. By incorporating these prompt features and fine-tuning\nthe video diffusion model in a parameter-efficient manner, the physical\nknowledge-informed videos are successfully generated. We validate our method\nextensively through both numerical simulations and real-world observations of\nphysical phenomena, demonstrating its remarkable performance across diverse\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion models have exhibited tremendous progress in various video\ngeneration tasks. However, existing models struggle to capture latent physical\nknowledge, failing to infer physical phenomena that are challenging to\narticulate with natural language. Generating videos following the fundamental\nphysical laws is still an opening challenge. To address this challenge, we\npropose a novel method to teach video diffusion models with latent physical\nphenomenon knowledge, enabling the accurate generation of physically informed\nphenomena. Specifically, we first pretrain Masked Autoencoders (MAE) to\nreconstruct the physical phenomena, resulting in output embeddings that\nencapsulate latent physical phenomenon knowledge. Leveraging these embeddings,\nwe could generate the pseudo-language prompt features based on the aligned\nspatial relationships between CLIP vision and language encoders. Particularly,\ngiven that diffusion models typically use CLIP's language encoder for text\nprompt embeddings, our approach integrates the CLIP visual features informed by\nlatent physical knowledge into a quaternion hidden space. This enables the\nmodeling of spatial relationships to produce physical knowledge-informed\npseudo-language prompts. By incorporating these prompt features and fine-tuning\nthe video diffusion model in a parameter-efficient manner, the physical\nknowledge-informed videos are successfully generated. We validate our method\nextensively through both numerical simulations and real-world observations of\nphysical phenomena, demonstrating its remarkable performance across diverse\nscenarios."
                },
                "authors": [
                    {
                        "name": "Qinglong Cao"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Yuntian Chen"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "7 figures, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03911v3",
                "updated": "2024-11-18T07:17:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    17,
                    56,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-07T00:08:15Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    0,
                    8,
                    15,
                    1,
                    128,
                    0
                ],
                "title": "Federated Graph Condensation with Information Bottleneck Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Graph Condensation with Information Bottleneck Principles"
                },
                "summary": "Graph condensation, which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has immediately\nbenefited various graph learning tasks. However, existing graph condensation\nmethods rely on centralized data storage, which is unfeasible for real-world\ndecentralized data distribution, and overlook data holders' privacy-preserving\nrequirements. To bridge the gap, we propose and study the novel problem of\nfederated graph condensation for graph neural networks (GNNs). Specifically, we\nfirst propose a general framework for federated graph condensation, in which we\ndecouple the typical gradient matching process for graph condensation into\nclient-side gradient calculation and server-side gradient matching. In this\nway, the burdensome computation cost in client-side is largely alleviated.\nBesides, our empirical studies show that under the federated setting, the\ncondensed graph will consistently leak data membership privacy, i.e., the\ncondensed graph during the federated training can be utilized to steal the\ntraining data under the membership inference attacks (MIA). To tackle this\nissue, we innovatively incorporate information bottleneck principles into the\nfederated graph condensation, which only needs to extract partial node features\nin one local pre-training step and utilize the features during federated\ntraining. Extensive experiments on real-world datasets demonstrate that our\nframework can consistently protect membership privacy during training.\nMeanwhile, it also achieves comparable and even superior performance against\nexisting centralized graph condensation and federated graph learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph condensation, which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has immediately\nbenefited various graph learning tasks. However, existing graph condensation\nmethods rely on centralized data storage, which is unfeasible for real-world\ndecentralized data distribution, and overlook data holders' privacy-preserving\nrequirements. To bridge the gap, we propose and study the novel problem of\nfederated graph condensation for graph neural networks (GNNs). Specifically, we\nfirst propose a general framework for federated graph condensation, in which we\ndecouple the typical gradient matching process for graph condensation into\nclient-side gradient calculation and server-side gradient matching. In this\nway, the burdensome computation cost in client-side is largely alleviated.\nBesides, our empirical studies show that under the federated setting, the\ncondensed graph will consistently leak data membership privacy, i.e., the\ncondensed graph during the federated training can be utilized to steal the\ntraining data under the membership inference attacks (MIA). To tackle this\nissue, we innovatively incorporate information bottleneck principles into the\nfederated graph condensation, which only needs to extract partial node features\nin one local pre-training step and utilize the features during federated\ntraining. Extensive experiments on real-world datasets demonstrate that our\nframework can consistently protect membership privacy during training.\nMeanwhile, it also achieves comparable and even superior performance against\nexisting centralized graph condensation and federated graph learning methods."
                },
                "authors": [
                    {
                        "name": "Bo Yan"
                    },
                    {
                        "name": "Sihao He"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11339v1",
                "updated": "2024-11-18T07:13:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    13,
                    52,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T07:13:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    13,
                    52,
                    0,
                    323,
                    0
                ],
                "title": "Characterizing Superflares in HR 1099 using Temporal and Spectral\n  Analysis of XMM-Newton Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Superflares in HR 1099 using Temporal and Spectral\n  Analysis of XMM-Newton Observations"
                },
                "summary": "In the present paper, we analyze three energetic X-ray flares from the active\nRS CVn binary HR 1099 using data obtained from XMM-Newton. The flare duration\nranges from 2.8 to 4.1 h, with e-folding rise and decay times in the range of\n27 to 38 minutes and 1.3 to 2.4 h, respectively, indicating rapid rise and\nslower decay phases. The flare frequency for HR 1099 is one flare per rotation\nperiod. Time-resolved spectroscopy reveals peak flare temperatures of 39.44,\n35.96, and 32.48 MK, emission measures of $7 \\times 10^{53}$ to $8 \\times\n10^{54}$ cm$^{-3}$, global abundances of 0.250, 0.299, and 0.362 $Z_\\odot$, and\npeak X-ray luminosities of $ 10^{31.21-32.29}$ erg s$^{-1}$. The quiescent\nstate is modeled with a three-temperature plasma maintained at 3.02, 6.96, and\n12.53 MK. Elemental abundances during quiescent and flaring states exhibit the\ninverse-FIP effect. We have conducted a comparative analysis of coronal\nabundances with previous studies and found evidence supporting the i-FIP\neffect. The derived flare semi-loop lengths of 6 to 8.9 $\\times 10^{10}$ cm\nwere found to be comparable to the other flares detected on HR 1099; however,\nthey are significantly larger than typical solar flare loops. The estimated\nflare energies, ranging from $10^{35.83-37.03}$ erg, classify these flares as\nsuper-flares. The magnetic field strengths of the loops are found to be in the\nrange of 350 to 450 G. We diagnose the physical conditions of the flaring\ncorona in HR 1099 through the observations of superflares and provide inference\non the plasma processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the present paper, we analyze three energetic X-ray flares from the active\nRS CVn binary HR 1099 using data obtained from XMM-Newton. The flare duration\nranges from 2.8 to 4.1 h, with e-folding rise and decay times in the range of\n27 to 38 minutes and 1.3 to 2.4 h, respectively, indicating rapid rise and\nslower decay phases. The flare frequency for HR 1099 is one flare per rotation\nperiod. Time-resolved spectroscopy reveals peak flare temperatures of 39.44,\n35.96, and 32.48 MK, emission measures of $7 \\times 10^{53}$ to $8 \\times\n10^{54}$ cm$^{-3}$, global abundances of 0.250, 0.299, and 0.362 $Z_\\odot$, and\npeak X-ray luminosities of $ 10^{31.21-32.29}$ erg s$^{-1}$. The quiescent\nstate is modeled with a three-temperature plasma maintained at 3.02, 6.96, and\n12.53 MK. Elemental abundances during quiescent and flaring states exhibit the\ninverse-FIP effect. We have conducted a comparative analysis of coronal\nabundances with previous studies and found evidence supporting the i-FIP\neffect. The derived flare semi-loop lengths of 6 to 8.9 $\\times 10^{10}$ cm\nwere found to be comparable to the other flares detected on HR 1099; however,\nthey are significantly larger than typical solar flare loops. The estimated\nflare energies, ranging from $10^{35.83-37.03}$ erg, classify these flares as\nsuper-flares. The magnetic field strengths of the loops are found to be in the\nrange of 350 to 450 G. We diagnose the physical conditions of the flaring\ncorona in HR 1099 through the observations of superflares and provide inference\non the plasma processes."
                },
                "authors": [
                    {
                        "name": "Shweta Didel"
                    },
                    {
                        "name": "Jeewan C Pandey"
                    },
                    {
                        "name": "A. K. Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "A. K. Srivastava"
                },
                "author": "A. K. Srivastava",
                "arxiv_comment": "12 pages, 7 figures, 7 tables, Accepted for publication in\n  Astronomical Journal (AJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09822v2",
                "updated": "2024-11-18T07:05:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    5,
                    33,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-16T05:39:08Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    5,
                    39,
                    8,
                    3,
                    137,
                    0
                ],
                "title": "SEEK: Semantic Reasoning for Object Goal Navigation in Real World\n  Inspection Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEEK: Semantic Reasoning for Object Goal Navigation in Real World\n  Inspection Tasks"
                },
                "summary": "This paper addresses the problem of object-goal navigation in autonomous\ninspections in real-world environments. Object-goal navigation is crucial to\nenable effective inspections in various settings, often requiring the robot to\nidentify the target object within a large search space. Current object\ninspection methods fall short of human efficiency because they typically cannot\nbootstrap prior and common sense knowledge as humans do. In this paper, we\nintroduce a framework that enables robots to use semantic knowledge from prior\nspatial configurations of the environment and semantic common sense knowledge.\nWe propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines\nsemantic prior knowledge with the robot's observations to search for and\nnavigate toward target objects more efficiently. SEEK maintains two\nrepresentations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network\n(RSN). The RSN is a compact and practical model that estimates the probability\nof finding the target object across spatial elements in the DSG. We propose a\nnovel probabilistic planning framework to search for the object using\nrelational semantic knowledge. Our simulation analyses demonstrate that SEEK\noutperforms the classical planning and Large Language Models (LLMs)-based\nmethods that are examined in this study in terms of efficiency for object-goal\ninspection tasks. We validated our approach on a physical legged robot in urban\nenvironments, showcasing its practicality and effectiveness in real-world\ninspection scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of object-goal navigation in autonomous\ninspections in real-world environments. Object-goal navigation is crucial to\nenable effective inspections in various settings, often requiring the robot to\nidentify the target object within a large search space. Current object\ninspection methods fall short of human efficiency because they typically cannot\nbootstrap prior and common sense knowledge as humans do. In this paper, we\nintroduce a framework that enables robots to use semantic knowledge from prior\nspatial configurations of the environment and semantic common sense knowledge.\nWe propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines\nsemantic prior knowledge with the robot's observations to search for and\nnavigate toward target objects more efficiently. SEEK maintains two\nrepresentations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network\n(RSN). The RSN is a compact and practical model that estimates the probability\nof finding the target object across spatial elements in the DSG. We propose a\nnovel probabilistic planning framework to search for the object using\nrelational semantic knowledge. Our simulation analyses demonstrate that SEEK\noutperforms the classical planning and Large Language Models (LLMs)-based\nmethods that are examined in this study in terms of efficiency for object-goal\ninspection tasks. We validated our approach on a physical legged robot in urban\nenvironments, showcasing its practicality and effectiveness in real-world\ninspection scenarios."
                },
                "authors": [
                    {
                        "name": "Muhammad Fadhil Ginting"
                    },
                    {
                        "name": "Sung-Kyun Kim"
                    },
                    {
                        "name": "David D. Fan"
                    },
                    {
                        "name": "Matteo Palieri"
                    },
                    {
                        "name": "Mykel J. Kochenderfer"
                    },
                    {
                        "name": "Ali-akbar Agha-Mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Ali-akbar Agha-Mohammadi"
                },
                "author": "Ali-akbar Agha-Mohammadi",
                "arxiv_journal_ref": "Proc. of Robotics: Science and Systems 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09261v2",
                "updated": "2024-11-18T06:41:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    41,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-14T07:58:44Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    58,
                    44,
                    3,
                    319,
                    0
                ],
                "title": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming"
                },
                "summary": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design."
                },
                "authors": [
                    {
                        "name": "Umar Alkafaween"
                    },
                    {
                        "name": "Ibrahim Albluwi"
                    },
                    {
                        "name": "Paul Denny"
                    }
                ],
                "author_detail": {
                    "name": "Paul Denny"
                },
                "author": "Paul Denny",
                "arxiv_comment": "Submitted to Journal of Computer Assisted Learning; updated table\n  refs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11325v1",
                "updated": "2024-11-18T06:35:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    35,
                    2,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T06:35:02Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    35,
                    2,
                    0,
                    323,
                    0
                ],
                "title": "Lorentz: Learned SKU Recommendation Using Profile Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lorentz: Learned SKU Recommendation Using Profile Data"
                },
                "summary": "Cloud operators have expanded their service offerings, known as Stock Keeping\nUnits (SKUs), to accommodate diverse demands, resulting in increased complexity\nfor customers to select appropriate configurations. In a studied system, only\n43% of the resource capacity was correctly chosen. Automated solutions\naddressing this issue often require enriched data, such as workload traces,\nwhich are unavailable for new services. However, telemetry from existing users\nand customer satisfaction feedback provide valuable insights for understanding\ncustomer needs and improving provisioning recommendations.\n  This paper introduces Lorentz, an intelligent SKU recommender for\nprovisioning compute resources without relying on workload traces. Lorentz uses\ncustomer profile data to forecast resource capacities for new users by\nprofiling existing ones. It also incorporates a continuous feedback loop to\nrefine recommendations based on customer performance versus cost preferences\ninferred from satisfaction signals. Validated with production data from Azure\nPostgreSQL DB, Lorentz achieves over 60% slack reduction without increasing\nthrottling compared to user selections and existing defaults. Evaluations with\nsynthetic data demonstrate Lorentz's ability to iteratively learn user\npreferences with high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud operators have expanded their service offerings, known as Stock Keeping\nUnits (SKUs), to accommodate diverse demands, resulting in increased complexity\nfor customers to select appropriate configurations. In a studied system, only\n43% of the resource capacity was correctly chosen. Automated solutions\naddressing this issue often require enriched data, such as workload traces,\nwhich are unavailable for new services. However, telemetry from existing users\nand customer satisfaction feedback provide valuable insights for understanding\ncustomer needs and improving provisioning recommendations.\n  This paper introduces Lorentz, an intelligent SKU recommender for\nprovisioning compute resources without relying on workload traces. Lorentz uses\ncustomer profile data to forecast resource capacities for new users by\nprofiling existing ones. It also incorporates a continuous feedback loop to\nrefine recommendations based on customer performance versus cost preferences\ninferred from satisfaction signals. Validated with production data from Azure\nPostgreSQL DB, Lorentz achieves over 60% slack reduction without increasing\nthrottling compared to user selections and existing defaults. Evaluations with\nsynthetic data demonstrate Lorentz's ability to iteratively learn user\npreferences with high accuracy."
                },
                "authors": [
                    {
                        "name": "Nicholas Glaze"
                    },
                    {
                        "name": "Tria McNeely"
                    },
                    {
                        "name": "Yiwen Zhu"
                    },
                    {
                        "name": "Matthew Gleeson"
                    },
                    {
                        "name": "Helen Serr"
                    },
                    {
                        "name": "Rajeev Bhopi"
                    },
                    {
                        "name": "Subru Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Subru Krishnan"
                },
                "author": "Subru Krishnan",
                "arxiv_doi": "10.1145/3654952",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3654952",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 149.\n  Publication date: June 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11323v1",
                "updated": "2024-11-18T06:33:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    33,
                    5,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T06:33:05Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    33,
                    5,
                    0,
                    323,
                    0
                ],
                "title": "SayComply: Grounding Field Robotic Tasks in Operational Compliance\n  through Retrieval-Based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SayComply: Grounding Field Robotic Tasks in Operational Compliance\n  through Retrieval-Based Language Models"
                },
                "summary": "This paper addresses the problem of task planning for robots that must comply\nwith operational manuals in real-world settings. Task planning under these\nconstraints is essential for enabling autonomous robot operation in domains\nthat require adherence to domain-specific knowledge. Current methods for\ngenerating robot goals and plans rely on common sense knowledge encoded in\nlarge language models. However, these models lack grounding of robot plans to\ndomain-specific knowledge and are not easily transferable between multiple\nsites or customers with different compliance needs. In this work, we present\nSayComply, which enables grounding robotic task planning with operational\ncompliance using retrieval-based language models. We design a hierarchical\ndatabase of operational, environment, and robot embodiment manuals and\nprocedures to enable efficient retrieval of the relevant context under the\nlimited context length of the LLMs. We then design a task planner using a\ntree-based retrieval augmented generation (RAG) technique to generate robot\ntasks that follow user instructions while simultaneously complying with the\ndomain knowledge in the database. We demonstrate the benefits of our approach\nthrough simulations and hardware experiments in real-world scenarios that\nrequire precise context retrieval across various types of context,\noutperforming the standard RAG method. Our approach bridges the gap in\ndeploying robots that consistently adhere to operational protocols, offering a\nscalable and edge-deployable solution for ensuring compliance across varied and\ncomplex real-world environments. Project website: saycomply.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of task planning for robots that must comply\nwith operational manuals in real-world settings. Task planning under these\nconstraints is essential for enabling autonomous robot operation in domains\nthat require adherence to domain-specific knowledge. Current methods for\ngenerating robot goals and plans rely on common sense knowledge encoded in\nlarge language models. However, these models lack grounding of robot plans to\ndomain-specific knowledge and are not easily transferable between multiple\nsites or customers with different compliance needs. In this work, we present\nSayComply, which enables grounding robotic task planning with operational\ncompliance using retrieval-based language models. We design a hierarchical\ndatabase of operational, environment, and robot embodiment manuals and\nprocedures to enable efficient retrieval of the relevant context under the\nlimited context length of the LLMs. We then design a task planner using a\ntree-based retrieval augmented generation (RAG) technique to generate robot\ntasks that follow user instructions while simultaneously complying with the\ndomain knowledge in the database. We demonstrate the benefits of our approach\nthrough simulations and hardware experiments in real-world scenarios that\nrequire precise context retrieval across various types of context,\noutperforming the standard RAG method. Our approach bridges the gap in\ndeploying robots that consistently adhere to operational protocols, offering a\nscalable and edge-deployable solution for ensuring compliance across varied and\ncomplex real-world environments. Project website: saycomply.github.io."
                },
                "authors": [
                    {
                        "name": "Muhammad Fadhil Ginting"
                    },
                    {
                        "name": "Dong-Ki Kim"
                    },
                    {
                        "name": "Sung-Kyun Kim"
                    },
                    {
                        "name": "Bandi Jai Krishna"
                    },
                    {
                        "name": "Mykel J. Kochenderfer"
                    },
                    {
                        "name": "Shayegan Omidshafiei"
                    },
                    {
                        "name": "Ali-akbar Agha-mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Ali-akbar Agha-mohammadi"
                },
                "author": "Ali-akbar Agha-mohammadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19979v3",
                "updated": "2024-11-18T06:28:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    28,
                    1,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-30T06:07:12Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    7,
                    12,
                    0,
                    274,
                    0
                ],
                "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model"
                },
                "summary": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimi Suzuki"
                },
                "author": "Yoshimi Suzuki",
                "arxiv_comment": "Long paper accepted to EMNLP 2024 Main. 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v2",
                "updated": "2024-11-18T06:14:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    14,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v1",
                "updated": "2024-11-18T18:59:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate.In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate.In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07681v2",
                "updated": "2024-11-18T18:49:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    49,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-12T09:52:40Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    52,
                    40,
                    1,
                    317,
                    0
                ],
                "title": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?"
                },
                "summary": "Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques."
                },
                "authors": [
                    {
                        "name": "Katie Kang"
                    },
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Dibya Ghosh"
                    },
                    {
                        "name": "Jacob Steinhardt"
                    },
                    {
                        "name": "Claire Tomlin"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11829v1",
                "updated": "2024-11-18T18:48:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:48:13Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    48,
                    13,
                    0,
                    323,
                    0
                ],
                "title": "Tackling prediction tasks in relational databases with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tackling prediction tasks in relational databases with LLMs"
                },
                "summary": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction."
                },
                "authors": [
                    {
                        "name": "Marek Wydmuch"
                    },
                    {
                        "name": "ukasz Borchmann"
                    },
                    {
                        "name": "Filip Graliski"
                    }
                ],
                "author_detail": {
                    "name": "Filip Graliski"
                },
                "author": "Filip Graliski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00024v2",
                "updated": "2024-11-18T18:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    41,
                    8,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-28T22:30:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    22,
                    30,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges"
                },
                "summary": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements."
                },
                "authors": [
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Benjamin Danek"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Christina Mack"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Yajuan Wang"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11795v1",
                "updated": "2024-11-18T18:08:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    8,
                    52,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:08:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    8,
                    52,
                    0,
                    323,
                    0
                ],
                "title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods"
                },
                "summary": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review)."
                },
                "authors": [
                    {
                        "name": "Egor Kovalev"
                    },
                    {
                        "name": "Georgii Bychkov"
                    },
                    {
                        "name": "Khaled Abud"
                    },
                    {
                        "name": "Aleksandr Gushchin"
                    },
                    {
                        "name": "Anna Chistyakova"
                    },
                    {
                        "name": "Sergey Lavrushkin"
                    },
                    {
                        "name": "Dmitriy Vatolin"
                    },
                    {
                        "name": "Anastasia Antsiferova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Antsiferova"
                },
                "author": "Anastasia Antsiferova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11779v1",
                "updated": "2024-11-18T17:56:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    56,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:56:13Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    56,
                    13,
                    0,
                    323,
                    0
                ],
                "title": "LLM-IE: A Python Package for Generative Information Extraction with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-IE: A Python Package for Generative Information Extraction with\n  Large Language Models"
                },
                "summary": "Objectives: Despite the recent adoption of large language models (LLMs) for\nbiomedical information extraction, challenges in prompt engineering and\nalgorithms persist, with no dedicated software available. To address this, we\ndeveloped LLM-IE: a Python package for building complete information extraction\npipelines. Our key innovation is an interactive LLM agent to support schema\ndefinition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity\nattribute extraction, and relation extraction tasks. We benchmarked on the i2b2\ndatasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best\nperformance while requiring a longer inference time. System evaluation provided\nintuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare\nand has been adopted in internal projects. It should hold great value to the\nbiomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building\nblocks for robust information extraction pipeline construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Despite the recent adoption of large language models (LLMs) for\nbiomedical information extraction, challenges in prompt engineering and\nalgorithms persist, with no dedicated software available. To address this, we\ndeveloped LLM-IE: a Python package for building complete information extraction\npipelines. Our key innovation is an interactive LLM agent to support schema\ndefinition and prompt design.\n  Materials and Methods: The LLM-IE supports named entity recognition, entity\nattribute extraction, and relation extraction tasks. We benchmarked on the i2b2\ndatasets and conducted a system evaluation.\n  Results: The sentence-based prompting algorithm resulted in the best\nperformance while requiring a longer inference time. System evaluation provided\nintuitive visualization.\n  Discussion: LLM-IE was designed from practical NLP experience in healthcare\nand has been adopted in internal projects. It should hold great value to the\nbiomedical NLP community.\n  Conclusion: We developed a Python package, LLM-IE, that provides building\nblocks for robust information extraction pipeline construction."
                },
                "authors": [
                    {
                        "name": "Enshuo Hsu"
                    },
                    {
                        "name": "Kirk Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Kirk Roberts"
                },
                "author": "Kirk Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11778v1",
                "updated": "2024-11-18T17:55:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    55,
                    2,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:55:02Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    55,
                    2,
                    0,
                    323,
                    0
                ],
                "title": "Design And Optimization Of Multi-rendezvous Manoeuvres Based On\n  Reinforcement Learning And Convex Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design And Optimization Of Multi-rendezvous Manoeuvres Based On\n  Reinforcement Learning And Convex Optimization"
                },
                "summary": "Optimizing space vehicle routing is crucial for critical applications such as\non-orbit servicing, constellation deployment, and space debris de-orbiting.\nMulti-target Rendezvous presents a significant challenge in this domain. This\nproblem involves determining the optimal sequence in which to visit a set of\ntargets, and the corresponding optimal trajectories: this results in a\ndemanding NP-hard problem. We introduce a framework for the design and\nrefinement of multi-rendezvous trajectories based on heuristic combinatorial\noptimization and Sequential Convex Programming. Our framework is both highly\nmodular and capable of leveraging candidate solutions obtained with advanced\napproaches and handcrafted heuristics. We demonstrate this flexibility by\nintegrating an Attention-based routing policy trained with Reinforcement\nLearning to improve the performance of the combinatorial optimization process.\nWe show that Reinforcement Learning approaches for combinatorial optimization\ncan be effectively applied to spacecraft routing problems. We apply the\nproposed framework to the UARX Space OSSIE mission: we are able to thoroughly\nexplore the mission design space, finding optimal tours and trajectories for a\nwide variety of mission scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing space vehicle routing is crucial for critical applications such as\non-orbit servicing, constellation deployment, and space debris de-orbiting.\nMulti-target Rendezvous presents a significant challenge in this domain. This\nproblem involves determining the optimal sequence in which to visit a set of\ntargets, and the corresponding optimal trajectories: this results in a\ndemanding NP-hard problem. We introduce a framework for the design and\nrefinement of multi-rendezvous trajectories based on heuristic combinatorial\noptimization and Sequential Convex Programming. Our framework is both highly\nmodular and capable of leveraging candidate solutions obtained with advanced\napproaches and handcrafted heuristics. We demonstrate this flexibility by\nintegrating an Attention-based routing policy trained with Reinforcement\nLearning to improve the performance of the combinatorial optimization process.\nWe show that Reinforcement Learning approaches for combinatorial optimization\ncan be effectively applied to spacecraft routing problems. We apply the\nproposed framework to the UARX Space OSSIE mission: we are able to thoroughly\nexplore the mission design space, finding optimal tours and trajectories for a\nwide variety of mission scenarios."
                },
                "authors": [
                    {
                        "name": "Antonio Lpez Rivera"
                    },
                    {
                        "name": "Lucrezia Marcovaldi"
                    },
                    {
                        "name": "Jess Ramrez"
                    },
                    {
                        "name": "Alex Cuenca"
                    },
                    {
                        "name": "David Bermejo"
                    }
                ],
                "author_detail": {
                    "name": "David Bermejo"
                },
                "author": "David Bermejo",
                "arxiv_comment": "18 pages, 12 figures, 5 tables",
                "arxiv_journal_ref": "Proceedings of the International Astronautical Congress, 75, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11762v1",
                "updated": "2024-11-18T17:40:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    40,
                    43,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:40:43Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    40,
                    43,
                    0,
                    323,
                    0
                ],
                "title": "High-Speed Cornering Control and Real-Vehicle Deployment for Autonomous\n  Electric Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Speed Cornering Control and Real-Vehicle Deployment for Autonomous\n  Electric Vehicles"
                },
                "summary": "Executing drift maneuvers during high-speed cornering presents significant\nchallenges for autonomous vehicles, yet offers the potential to minimize\nturning time and enhance driving dynamics. While reinforcement learning (RL)\nhas shown promising results in simulated environments, discrepancies between\nsimulations and real-world conditions have limited its practical deployment.\nThis study introduces an innovative control framework that integrates\ntrajectory optimization with drift maneuvers, aiming to improve the algorithm's\nadaptability for real-vehicle implementation. We leveraged Bezier-based\npre-trajectory optimization to enhance rewards and optimize the controller\nthrough Twin Delayed Deep Deterministic Policy Gradient (TD3) in a simulated\nenvironment. For real-world deployment, we implement a hybrid RL-MPC fusion\nmechanism, , where TD3-derived maneuvers serve as primary inputs for a Model\nPredictive Controller (MPC). This integration enables precise real-time\ntracking of the optimal trajectory, with MPC providing corrective inputs to\nbridge the gap between simulation and reality. The efficacy of this method is\nvalidated through real-vehicle tests on consumer-grade electric vehicles,\nfocusing on drift U-turns and drift right-angle turns. The control outcomes of\nthese real-vehicle tests are thoroughly documented in the paper, supported by\nsupplementary video evidence (https://youtu.be/5wp67FcpfL8). Notably, this\nstudy is the first to deploy and apply an RL-based transient drift cornering\nalgorithm on consumer-grade electric vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing drift maneuvers during high-speed cornering presents significant\nchallenges for autonomous vehicles, yet offers the potential to minimize\nturning time and enhance driving dynamics. While reinforcement learning (RL)\nhas shown promising results in simulated environments, discrepancies between\nsimulations and real-world conditions have limited its practical deployment.\nThis study introduces an innovative control framework that integrates\ntrajectory optimization with drift maneuvers, aiming to improve the algorithm's\nadaptability for real-vehicle implementation. We leveraged Bezier-based\npre-trajectory optimization to enhance rewards and optimize the controller\nthrough Twin Delayed Deep Deterministic Policy Gradient (TD3) in a simulated\nenvironment. For real-world deployment, we implement a hybrid RL-MPC fusion\nmechanism, , where TD3-derived maneuvers serve as primary inputs for a Model\nPredictive Controller (MPC). This integration enables precise real-time\ntracking of the optimal trajectory, with MPC providing corrective inputs to\nbridge the gap between simulation and reality. The efficacy of this method is\nvalidated through real-vehicle tests on consumer-grade electric vehicles,\nfocusing on drift U-turns and drift right-angle turns. The control outcomes of\nthese real-vehicle tests are thoroughly documented in the paper, supported by\nsupplementary video evidence (https://youtu.be/5wp67FcpfL8). Notably, this\nstudy is the first to deploy and apply an RL-based transient drift cornering\nalgorithm on consumer-grade electric vehicles."
                },
                "authors": [
                    {
                        "name": "Shiyue Zhao"
                    },
                    {
                        "name": "Junzhi Zhang"
                    },
                    {
                        "name": "Neda Masoud"
                    },
                    {
                        "name": "Yuhong Jiang"
                    },
                    {
                        "name": "Heye Huang"
                    },
                    {
                        "name": "Tao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Liu"
                },
                "author": "Tao Liu",
                "arxiv_comment": "In the process of being submitted to the Journal of IEEE Transactions\n  on Industrial Electronics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22339v2",
                "updated": "2024-11-18T17:30:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    30,
                    47,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-11T18:47:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    18,
                    47,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "DAWN: Designing Distributed Agents in a Worldwide Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAWN: Designing Distributed Agents in a Worldwide Network"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has transformed them from\nbasic conversational tools into sophisticated entities capable of complex\nreasoning and decision-making. These advancements have led to the development\nof specialized LLM-based agents designed for diverse tasks such as coding and\nweb browsing. As these agents become more capable, the need for a robust\nframework that facilitates global communication and collaboration among them\ntowards advanced objectives has become increasingly critical. Distributed\nAgents in a Worldwide Network (DAWN) addresses this need by offering a\nversatile framework that integrates LLM-based agents with traditional software\nsystems, enabling the creation of agentic applications suited for a wide range\nof use cases. DAWN enables distributed agents worldwide to register and be\neasily discovered through Gateway Agents. Collaborations among these agents are\ncoordinated by a Principal Agent equipped with reasoning strategies. DAWN\noffers three operational modes: No-LLM Mode for deterministic tasks, Copilot\nfor augmented decision-making, and LLM Agent for autonomous operations.\nAdditionally, DAWN ensures the safety and security of agent collaborations\nglobally through a dedicated safety, security, and compliance layer, protecting\nthe network against attackers and adhering to stringent security and compliance\nstandards. These features make DAWN a robust network for deploying agent-based\napplications across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has transformed them from\nbasic conversational tools into sophisticated entities capable of complex\nreasoning and decision-making. These advancements have led to the development\nof specialized LLM-based agents designed for diverse tasks such as coding and\nweb browsing. As these agents become more capable, the need for a robust\nframework that facilitates global communication and collaboration among them\ntowards advanced objectives has become increasingly critical. Distributed\nAgents in a Worldwide Network (DAWN) addresses this need by offering a\nversatile framework that integrates LLM-based agents with traditional software\nsystems, enabling the creation of agentic applications suited for a wide range\nof use cases. DAWN enables distributed agents worldwide to register and be\neasily discovered through Gateway Agents. Collaborations among these agents are\ncoordinated by a Principal Agent equipped with reasoning strategies. DAWN\noffers three operational modes: No-LLM Mode for deterministic tasks, Copilot\nfor augmented decision-making, and LLM Agent for autonomous operations.\nAdditionally, DAWN ensures the safety and security of agent collaborations\nglobally through a dedicated safety, security, and compliance layer, protecting\nthe network against attackers and adhering to stringent security and compliance\nstandards. These features make DAWN a robust network for deploying agent-based\napplications across various industries."
                },
                "authors": [
                    {
                        "name": "Zahra Aminiranjbar"
                    },
                    {
                        "name": "Jianan Tang"
                    },
                    {
                        "name": "Qiudan Wang"
                    },
                    {
                        "name": "Shubha Pant"
                    },
                    {
                        "name": "Mahesh Viswanathan"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Viswanathan"
                },
                "author": "Mahesh Viswanathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11752v1",
                "updated": "2024-11-18T17:27:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    27,
                    56,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:27:56Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    27,
                    56,
                    0,
                    323,
                    0
                ],
                "title": "sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality\n  Spaces with LLMs and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality\n  Spaces with LLMs and Generative AI"
                },
                "summary": "In mixed reality (MR) environments, understanding space and creating virtual\nobjects is crucial to providing an intuitive and rich user experience. This\npaper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an\nMR application that combines Generative AI (GenAI) with large language models\n(LLMs) to assist users in creating, placing, and managing virtual objects\nwithin physical spaces. sMoRe allows users to use voice or typed text commands\nto create and place virtual objects using GenAI while specifying spatial\nconstraints. The system leverages LLMs to interpret users' commands, analyze\nthe current scene, and identify optimal locations. Additionally, sMoRe\nintegrates text-to-3D generative AI to dynamically create 3D objects based on\nusers' descriptions. Our user study demonstrates the effectiveness of sMoRe in\nenhancing user comprehension, interaction, and organization of the MR\nenvironment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In mixed reality (MR) environments, understanding space and creating virtual\nobjects is crucial to providing an intuitive and rich user experience. This\npaper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an\nMR application that combines Generative AI (GenAI) with large language models\n(LLMs) to assist users in creating, placing, and managing virtual objects\nwithin physical spaces. sMoRe allows users to use voice or typed text commands\nto create and place virtual objects using GenAI while specifying spatial\nconstraints. The system leverages LLMs to interpret users' commands, analyze\nthe current scene, and identify optimal locations. Additionally, sMoRe\nintegrates text-to-3D generative AI to dynamically create 3D objects based on\nusers' descriptions. Our user study demonstrates the effectiveness of sMoRe in\nenhancing user comprehension, interaction, and organization of the MR\nenvironment."
                },
                "authors": [
                    {
                        "name": "Yunhao Xing"
                    },
                    {
                        "name": "Que Liu"
                    },
                    {
                        "name": "Jingwu Wang"
                    },
                    {
                        "name": "Diego Gomez-Zara"
                    }
                ],
                "author_detail": {
                    "name": "Diego Gomez-Zara"
                },
                "author": "Diego Gomez-Zara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06153v2",
                "updated": "2024-11-18T17:25:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    25,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-08T15:52:42Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    52,
                    42,
                    1,
                    282,
                    0
                ],
                "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSquare: Automatic LLM Agent Search in Modular Design Space"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Likai Ma"
                    },
                    {
                        "name": "Jiahe Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11745v1",
                "updated": "2024-11-18T17:16:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:16:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Ahmed F. AbouElhamayed"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Marta Andronic"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02270v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02270v4",
                "updated": "2024-11-18T17:16:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    16,
                    34,
                    0,
                    323,
                    0
                ],
                "published": "2023-06-04T06:27:17Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    6,
                    27,
                    17,
                    6,
                    155,
                    0
                ],
                "title": "Crypto-Ransomware and Their Defenses: In-depth Behavioral\n  Characterization, Discussion of Deployability, and New Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crypto-Ransomware and Their Defenses: In-depth Behavioral\n  Characterization, Discussion of Deployability, and New Insights"
                },
                "summary": "Crypto-ransomware has caused an unprecedented scope of impact in recent years\nwith an evolving level of sophistication. An extensive range of studies have\nbeen on defending against ransomware and reviewing the efficacy of various\nprotections. However, for practical defenses, deployability holds equal\nsignificance as detection accuracy. Therefore, in this study, we review 117\npublished ransomware defense works, categorize them by the level they are\nimplemented at, and discuss the deployability. API-based solutions are easy to\ndeploy and most existing works focus on machine learning-based classification.\nTo provide more insights, we quantitively characterize the runtime behaviors of\nreal-world ransomware samples. Based on our experimental findings, we present a\npossible future detection direction with our consistency analysis and\nAPI-contrast-based refinement. Moreover, we experimentally evaluate various\ncommercial defenses and identify the security gaps. Our findings help the field\nunderstand the deployability of ransomware defenses and create more effective,\npractical solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crypto-ransomware has caused an unprecedented scope of impact in recent years\nwith an evolving level of sophistication. An extensive range of studies have\nbeen on defending against ransomware and reviewing the efficacy of various\nprotections. However, for practical defenses, deployability holds equal\nsignificance as detection accuracy. Therefore, in this study, we review 117\npublished ransomware defense works, categorize them by the level they are\nimplemented at, and discuss the deployability. API-based solutions are easy to\ndeploy and most existing works focus on machine learning-based classification.\nTo provide more insights, we quantitively characterize the runtime behaviors of\nreal-world ransomware samples. Based on our experimental findings, we present a\npossible future detection direction with our consistency analysis and\nAPI-contrast-based refinement. Moreover, we experimentally evaluate various\ncommercial defenses and identify the security gaps. Our findings help the field\nunderstand the deployability of ransomware defenses and create more effective,\npractical solutions."
                },
                "authors": [
                    {
                        "name": "Wenjia Song"
                    },
                    {
                        "name": "Sanjula Karanam"
                    },
                    {
                        "name": "Ya Xiao"
                    },
                    {
                        "name": "Jingyuan Qi"
                    },
                    {
                        "name": "Nathan Dautenhahn"
                    },
                    {
                        "name": "Na Meng"
                    },
                    {
                        "name": "Elena Ferrari"
                    },
                    {
                        "name": "Danfeng"
                    },
                    {
                        "name": "Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yao"
                },
                "arxiv_affiliation": "Daphne",
                "author": "Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02270v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02270v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15367v2",
                "updated": "2024-11-18T17:00:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    0,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-18T18:36:18Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    18,
                    36,
                    18,
                    2,
                    262,
                    0
                ],
                "title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss"
                },
                "summary": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Chernov"
                },
                "author": "Andrei Chernov",
                "arxiv_comment": "4 main pages; 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11731v1",
                "updated": "2024-11-18T16:59:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    59,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:59:59Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    59,
                    0,
                    323,
                    0
                ],
                "title": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment"
                },
                "summary": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion."
                },
                "authors": [
                    {
                        "name": "Allison Huang"
                    },
                    {
                        "name": "Yulu Niki Pi"
                    },
                    {
                        "name": "Carlos Mougan"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Mougan"
                },
                "author": "Carlos Mougan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11717v1",
                "updated": "2024-11-18T16:45:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    45,
                    44,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:45:44Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    45,
                    44,
                    0,
                    323,
                    0
                ],
                "title": "RAWMamba: Unified sRGB-to-RAW De-rendering With State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAWMamba: Unified sRGB-to-RAW De-rendering With State Space Model"
                },
                "summary": "Recent advancements in sRGB-to-RAW de-rendering have increasingly emphasized\nmetadata-driven approaches to reconstruct RAW data from sRGB images,\nsupplemented by partial RAW information. In image-based de-rendering, metadata\nis commonly obtained through sampling, whereas in video tasks, it is typically\nderived from the initial frame. The distinct metadata requirements necessitate\nspecialized network architectures, leading to architectural incompatibilities\nthat increase deployment complexity. In this paper, we propose RAWMamba, a\nMamba-based unified framework developed for sRGB-to-RAW de-rendering across\nboth image and video domains. The core of RAWMamba is the Unified Metadata\nEmbedding (UME) module, which harmonizes diverse metadata types into a unified\nrepresentation. In detail, a multi-perspective affinity modeling method is\nproposed to promote the extraction of reference information. In addition, we\nintroduce the Local Tone-Aware Mamba (LTA-Mamba) module, which captures\nlong-range dependencies to enable effective global propagation of metadata.\nExperimental results demonstrate that the proposed RAWMamba achieves\nstate-of-the-art performance, yielding high-quality RAW data reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in sRGB-to-RAW de-rendering have increasingly emphasized\nmetadata-driven approaches to reconstruct RAW data from sRGB images,\nsupplemented by partial RAW information. In image-based de-rendering, metadata\nis commonly obtained through sampling, whereas in video tasks, it is typically\nderived from the initial frame. The distinct metadata requirements necessitate\nspecialized network architectures, leading to architectural incompatibilities\nthat increase deployment complexity. In this paper, we propose RAWMamba, a\nMamba-based unified framework developed for sRGB-to-RAW de-rendering across\nboth image and video domains. The core of RAWMamba is the Unified Metadata\nEmbedding (UME) module, which harmonizes diverse metadata types into a unified\nrepresentation. In detail, a multi-perspective affinity modeling method is\nproposed to promote the extraction of reference information. In addition, we\nintroduce the Local Tone-Aware Mamba (LTA-Mamba) module, which captures\nlong-range dependencies to enable effective global propagation of metadata.\nExperimental results demonstrate that the proposed RAWMamba achieves\nstate-of-the-art performance, yielding high-quality RAW data reconstruction."
                },
                "authors": [
                    {
                        "name": "Hongjun Chen"
                    },
                    {
                        "name": "Wencheng Han"
                    },
                    {
                        "name": "Huan Zheng"
                    },
                    {
                        "name": "Jianbing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Shen"
                },
                "author": "Jianbing Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11714v1",
                "updated": "2024-11-18T16:42:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    42,
                    7,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:42:07Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    42,
                    7,
                    0,
                    323,
                    0
                ],
                "title": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via\n  Skill Library and Tactile Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via\n  Skill Library and Tactile Representation"
                },
                "summary": "Deploying robots in open-world environments involves complex tasks\ncharacterized by long sequences and rich interactions, necessitating efficient\ntransfer of robotic skills across diverse and complex scenarios. To address\nthis challenge, we propose a skill library framework based on knowledge graphs,\nwhich endows robots with high-level skill awareness and spatial semantic\nunderstanding. The framework hierarchically organizes operational knowledge by\nconstructing a \"task graph\" and a \"scene graph\" to represent task and scene\nsemantic information, respectively. We introduce a \"state graph\" to facilitate\ninteraction between high-level task planning and low-level scene information.\nFurthermore, we propose a hierarchical transfer framework for operational\nskills. At the task level, the framework integrates contextual learning and\nchain-of-thought prompting within a four-stage prompt paradigm, leveraging\nlarge language models' (LLMs) reasoning and generalization capabilities to\nachieve task-level subtask sequence transfer. At the motion level, an adaptive\ntrajectory transfer method is developed using the A* algorithm and the skill\nlibrary, enabling motion-level adaptive trajectory transfer. At the physical\nlevel, we introduce an adaptive contour extraction and posture perception\nmethod based on tactile perception. This method dynamically obtains\nhigh-precision contour and posture information from visual-tactile texture data\nand adjusts transferred skills, such as contact positions and postures, to\nensure effectiveness in new environments. Experimental results validate the\neffectiveness of the proposed methods. Project\nwebsite:https://github.com/MingchaoQi/skill_transfer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying robots in open-world environments involves complex tasks\ncharacterized by long sequences and rich interactions, necessitating efficient\ntransfer of robotic skills across diverse and complex scenarios. To address\nthis challenge, we propose a skill library framework based on knowledge graphs,\nwhich endows robots with high-level skill awareness and spatial semantic\nunderstanding. The framework hierarchically organizes operational knowledge by\nconstructing a \"task graph\" and a \"scene graph\" to represent task and scene\nsemantic information, respectively. We introduce a \"state graph\" to facilitate\ninteraction between high-level task planning and low-level scene information.\nFurthermore, we propose a hierarchical transfer framework for operational\nskills. At the task level, the framework integrates contextual learning and\nchain-of-thought prompting within a four-stage prompt paradigm, leveraging\nlarge language models' (LLMs) reasoning and generalization capabilities to\nachieve task-level subtask sequence transfer. At the motion level, an adaptive\ntrajectory transfer method is developed using the A* algorithm and the skill\nlibrary, enabling motion-level adaptive trajectory transfer. At the physical\nlevel, we introduce an adaptive contour extraction and posture perception\nmethod based on tactile perception. This method dynamically obtains\nhigh-precision contour and posture information from visual-tactile texture data\nand adjusts transferred skills, such as contact positions and postures, to\nensure effectiveness in new environments. Experimental results validate the\neffectiveness of the proposed methods. Project\nwebsite:https://github.com/MingchaoQi/skill_transfer"
                },
                "authors": [
                    {
                        "name": "Mingchao Qi"
                    },
                    {
                        "name": "Yuanjin Li"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Zhengxiong Liu"
                    },
                    {
                        "name": "Panfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panfeng Huang"
                },
                "author": "Panfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11707v1",
                "updated": "2024-11-18T16:34:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    34,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:34:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    34,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models"
                },
                "summary": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data."
                },
                "authors": [
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Guoqiang Ma"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11694v1",
                "updated": "2024-11-18T16:15:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:15:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search"
                },
                "summary": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "LLM;Complex Reasoning;Math",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v1",
                "updated": "2024-11-18T16:09:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the\n  Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the\n  Physical World"
                },
                "summary": "Robotic manipulation refers to the autonomous handling and interaction of\nrobots with objects using advanced techniques in robotics and artificial\nintelligence. The advent of powerful tools such as large language models (LLMs)\nand large vision-language models (LVLMs) has significantly enhanced the\ncapabilities of these robots in environmental perception and decision-making.\nHowever, the introduction of these intelligent agents has led to security\nthreats such as jailbreak attacks and adversarial attacks.\n  In this research, we take a further step by proposing a backdoor attack\nspecifically targeting robotic manipulation and, for the first time,\nimplementing backdoor attack in the physical world. By embedding a backdoor\nvisual language model into the visual perception module within the robotic\nsystem, we successfully mislead the robotic arm's operation in the physical\nworld, given the presence of common items as triggers. Experimental evaluations\nin the physical world demonstrate the effectiveness of the proposed backdoor\nattack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation refers to the autonomous handling and interaction of\nrobots with objects using advanced techniques in robotics and artificial\nintelligence. The advent of powerful tools such as large language models (LLMs)\nand large vision-language models (LVLMs) has significantly enhanced the\ncapabilities of these robots in environmental perception and decision-making.\nHowever, the introduction of these intelligent agents has led to security\nthreats such as jailbreak attacks and adversarial attacks.\n  In this research, we take a further step by proposing a backdoor attack\nspecifically targeting robotic manipulation and, for the first time,\nimplementing backdoor attack in the physical world. By embedding a backdoor\nvisual language model into the visual perception module within the robotic\nsystem, we successfully mislead the robotic arm's operation in the physical\nworld, given the presence of common items as triggers. Experimental evaluations\nin the physical world demonstrate the effectiveness of the proposed backdoor\nattack."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "Initial version with preliminary results. We welcome any feedback or\n  suggestions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11678v1",
                "updated": "2024-11-18T15:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    30,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T15:59:30Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    59,
                    30,
                    0,
                    323,
                    0
                ],
                "title": "Analysis of Hardware Synthesis Strategies for Machine Learning in\n  Collider Trigger and Data Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Hardware Synthesis Strategies for Machine Learning in\n  Collider Trigger and Data Acquisition"
                },
                "summary": "To fully exploit the physics potential of current and future high energy\nparticle colliders, machine learning (ML) can be implemented in detector\nelectronics for intelligent data processing and acquisition. The implementation\nof ML in real-time at colliders requires very low latencies that are\nunachievable with a software-based approach, requiring optimization and\nsynthesis of ML algorithms for deployment on hardware. An analysis of neural\nnetwork inference efficiency is presented, focusing on the application of\ncollider trigger algorithms in field programmable gate arrays (FPGAs).\nTrade-offs are evaluated between two frameworks, the SLAC Neural Network\nLibrary (SNL) and hls4ml, in terms of resources and latency for different model\nsizes. Results highlight the strengths and limitations of each approach,\noffering valuable insights for optimizing real-time neural network deployments\nat colliders. This work aims to guide researchers and engineers in selecting\nthe most suitable hardware and software configurations for real-time,\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To fully exploit the physics potential of current and future high energy\nparticle colliders, machine learning (ML) can be implemented in detector\nelectronics for intelligent data processing and acquisition. The implementation\nof ML in real-time at colliders requires very low latencies that are\nunachievable with a software-based approach, requiring optimization and\nsynthesis of ML algorithms for deployment on hardware. An analysis of neural\nnetwork inference efficiency is presented, focusing on the application of\ncollider trigger algorithms in field programmable gate arrays (FPGAs).\nTrade-offs are evaluated between two frameworks, the SLAC Neural Network\nLibrary (SNL) and hls4ml, in terms of resources and latency for different model\nsizes. Results highlight the strengths and limitations of each approach,\noffering valuable insights for optimizing real-time neural network deployments\nat colliders. This work aims to guide researchers and engineers in selecting\nthe most suitable hardware and software configurations for real-time,\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Haoyi Jia"
                    },
                    {
                        "name": "Abhilasha Dave"
                    },
                    {
                        "name": "Julia Gonski"
                    },
                    {
                        "name": "Ryan Herbst"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Herbst"
                },
                "author": "Ryan Herbst",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11672v1",
                "updated": "2024-11-18T15:51:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    51,
                    45,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T15:51:45Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    51,
                    45,
                    0,
                    323,
                    0
                ],
                "title": "Artificial Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Scientific Discovery"
                },
                "summary": "Rooted in the explosion of deep learning over the past decade, this thesis\nspans from AlphaGo to ChatGPT to empirically examine the fundamental concepts\nneeded to realize the vision of an artificial scientist: a machine with the\ncapacity to autonomously generate original research and contribute to the\nexpansion of human knowledge. The investigation begins with {\\sc Olivaw}, an\nAlphaGo Zero-like agent that discovers Othello knowledge from scratch but is\nunable to communicate it. This realization leads to the development of the\nExplanatory Learning (EL) framework, a formalization of the problem faced by a\nscientist when trying to explain a new phenomenon to their peers. The effective\nEL prescriptions allow us to crack Zendo, a board game simulating the\nscientific endeavor. This success comes with a fundamental insight: an\nartificial scientist must develop its own interpretation of the language used\nto explain its findings. This perspective then leads us to see modern\nmultimodal models as interpreters, and to devise a new way to build\ninterpretable and cost-effective CLIP-like models: by coupling two unimodal\nmodels using little multimodal data and no further training. Finally, we\ndiscuss what ChatGPT and its siblings are still missing to become artificial\nscientists, and introduce Odeen, a benchmark about interpreting explanations\nthat sees LLMs going no further than random chance while being instead fully\nsolved by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rooted in the explosion of deep learning over the past decade, this thesis\nspans from AlphaGo to ChatGPT to empirically examine the fundamental concepts\nneeded to realize the vision of an artificial scientist: a machine with the\ncapacity to autonomously generate original research and contribute to the\nexpansion of human knowledge. The investigation begins with {\\sc Olivaw}, an\nAlphaGo Zero-like agent that discovers Othello knowledge from scratch but is\nunable to communicate it. This realization leads to the development of the\nExplanatory Learning (EL) framework, a formalization of the problem faced by a\nscientist when trying to explain a new phenomenon to their peers. The effective\nEL prescriptions allow us to crack Zendo, a board game simulating the\nscientific endeavor. This success comes with a fundamental insight: an\nartificial scientist must develop its own interpretation of the language used\nto explain its findings. This perspective then leads us to see modern\nmultimodal models as interpreters, and to devise a new way to build\ninterpretable and cost-effective CLIP-like models: by coupling two unimodal\nmodels using little multimodal data and no further training. Finally, we\ndiscuss what ChatGPT and its siblings are still missing to become artificial\nscientists, and introduce Odeen, a benchmark about interpreting explanations\nthat sees LLMs going no further than random chance while being instead fully\nsolved by humans."
                },
                "authors": [
                    {
                        "name": "Antonio Norelli"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Norelli"
                },
                "author": "Antonio Norelli",
                "arxiv_comment": "PhD thesis, 123 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12804v2",
                "updated": "2024-11-18T15:41:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    41,
                    24,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-24T16:31:11Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    16,
                    31,
                    11,
                    0,
                    176,
                    0
                ],
                "title": "Modulating Language Model Experiences through Frictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modulating Language Model Experiences through Frictions"
                },
                "summary": "Language models are transforming the ways that their users engage with the\nworld. Despite impressive capabilities, over-consumption of language model\noutputs risks propagating unchecked errors in the short-term and damaging human\ncapabilities for critical thinking in the long-term. How can we develop\nscaffolding around language models to curate more appropriate use? We propose\nselective frictions for language model experiences, inspired by behavioral\nscience interventions, to dampen misuse. Frictions involve small modifications\nto a user's experience, e.g., the addition of a button impeding model access\nand reminding a user of their expertise relative to the model. Through a user\nstudy with real humans, we observe shifts in user behavior from the imposition\nof a friction over LLMs in the context of a multi-topic question-answering task\nas a representative task that people may use LLMs for, e.g., in education and\ninformation retrieval. We find that frictions modulate over-reliance by driving\ndown users' click rates while minimally affecting accuracy for those topics.\nYet, frictions may have unintended effects. We find marked differences in\nusers' click behaviors even on topics where frictions were not provisioned. Our\ncontributions motivate further study of human-AI behavioral interaction to\ninform more effective and appropriate LLM use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are transforming the ways that their users engage with the\nworld. Despite impressive capabilities, over-consumption of language model\noutputs risks propagating unchecked errors in the short-term and damaging human\ncapabilities for critical thinking in the long-term. How can we develop\nscaffolding around language models to curate more appropriate use? We propose\nselective frictions for language model experiences, inspired by behavioral\nscience interventions, to dampen misuse. Frictions involve small modifications\nto a user's experience, e.g., the addition of a button impeding model access\nand reminding a user of their expertise relative to the model. Through a user\nstudy with real humans, we observe shifts in user behavior from the imposition\nof a friction over LLMs in the context of a multi-topic question-answering task\nas a representative task that people may use LLMs for, e.g., in education and\ninformation retrieval. We find that frictions modulate over-reliance by driving\ndown users' click rates while minimally affecting accuracy for those topics.\nYet, frictions may have unintended effects. We find marked differences in\nusers' click behaviors even on topics where frictions were not provisioned. Our\ncontributions motivate further study of human-AI behavioral interaction to\ninform more effective and appropriate LLM use."
                },
                "authors": [
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Malak Sadek"
                    },
                    {
                        "name": "Holli Sargeant"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Umang Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Umang Bhatt"
                },
                "author": "Umang Bhatt",
                "arxiv_comment": "NeurIPS Workshop on Behavioral ML; non-archival",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v6",
                "updated": "2024-11-18T15:41:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    41,
                    1,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing them shows limited performance. In this work, we facilitate utilizing\nLLMs in an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to guide the LLM to refine the modified molecule. We\nconduct experiments across both single- and multi-property objectives with 2\nthresholds, where $\\text{Re}^3$DF shows significant improvements. Particularly,\nfor 20 single-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 16.95%\nand 20.76% under loose (\\texttt{l}) and strict (\\texttt{s}) thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing them shows limited performance. In this work, we facilitate utilizing\nLLMs in an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to guide the LLM to refine the modified molecule. We\nconduct experiments across both single- and multi-property objectives with 2\nthresholds, where $\\text{Re}^3$DF shows significant improvements. Particularly,\nfor 20 single-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 16.95%\nand 20.76% under loose (\\texttt{l}) and strict (\\texttt{s}) thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11211v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11211v3",
                "updated": "2024-11-18T14:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    43,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-07-15T19:53:02Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    19,
                    53,
                    2,
                    0,
                    197,
                    0
                ],
                "title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion"
                },
                "summary": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues."
                },
                "authors": [
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Kyra Ahrens"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Published at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11211v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08745v2",
                "updated": "2024-11-18T14:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    41,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-13T16:26:19Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    26,
                    19,
                    2,
                    318,
                    0
                ],
                "title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers"
                },
                "summary": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models."
                },
                "authors": [
                    {
                        "name": "Clment Dumas"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "12 pages, 10 figures, previous version published under the title \"How\n  Do Llamas Process Multilingual Text? A Latent Exploration through Activation\n  Patching\" at the ICML 2024 mechanistic interpretability workshop at\n  https://openreview.net/forum?id=0ku2hIm4BS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07302v2",
                "updated": "2024-11-18T14:40:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    14,
                    40,
                    54,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-11T14:30:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    14,
                    30,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BertaQA: How Much Do Language Models Know About Local Culture?"
                },
                "summary": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA."
                },
                "authors": [
                    {
                        "name": "Julen Etxaniz"
                    },
                    {
                        "name": "Gorka Azkune"
                    },
                    {
                        "name": "Aitor Soroa"
                    },
                    {
                        "name": "Oier Lopez de Lacalle"
                    },
                    {
                        "name": "Mikel Artetxe"
                    }
                ],
                "author_detail": {
                    "name": "Mikel Artetxe"
                },
                "author": "Mikel Artetxe",
                "arxiv_comment": "NEURIPS Datasets & Benchmarks 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11582v1",
                "updated": "2024-11-18T13:59:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    59,
                    29,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    59,
                    29,
                    0,
                    323,
                    0
                ],
                "title": "Exploring LLMs for Verifying Technical System Specifications Against\n  Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Verifying Technical System Specifications Against\n  Requirements"
                },
                "summary": "Requirements engineering is a knowledge intensive process and crucial for the\nsuccess of engineering projects. The field of knowledge-based requirements\nengineering (KBRE) aims to support engineers by providing knowledge to assist\nin the elicitation, validation, and management of system requirements. The\nadvent of large language models (LLMs) opens new opportunities in the field of\nKBRE. This work experimentally investigates the potential of LLMs in\nrequirements verification. Therein, LLMs are provided with a set of\nrequirements and a textual system specification and are prompted to assess\nwhich requirements are fulfilled by the system specification. Different\nexperimental variables such as system specification complexity, the number of\nrequirements, and prompting strategies were analyzed. Formal rule-based systems\nserve as a benchmark to compare LLM performance to. Requirements and system\nspecifications are derived from the smart-grid domain. Results show that\nadvanced LLMs, like GPT-4o and Claude 3.5 Sonnet, achieved f1-scores between 79\n% and 94 % in identifying non-fulfilled requirements, indicating potential for\nLLMs to be leveraged for requirements verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements engineering is a knowledge intensive process and crucial for the\nsuccess of engineering projects. The field of knowledge-based requirements\nengineering (KBRE) aims to support engineers by providing knowledge to assist\nin the elicitation, validation, and management of system requirements. The\nadvent of large language models (LLMs) opens new opportunities in the field of\nKBRE. This work experimentally investigates the potential of LLMs in\nrequirements verification. Therein, LLMs are provided with a set of\nrequirements and a textual system specification and are prompted to assess\nwhich requirements are fulfilled by the system specification. Different\nexperimental variables such as system specification complexity, the number of\nrequirements, and prompting strategies were analyzed. Formal rule-based systems\nserve as a benchmark to compare LLM performance to. Requirements and system\nspecifications are derived from the smart-grid domain. Results show that\nadvanced LLMs, like GPT-4o and Claude 3.5 Sonnet, achieved f1-scores between 79\n% and 94 % in identifying non-fulfilled requirements, indicating potential for\nLLMs to be leveraged for requirements verification."
                },
                "authors": [
                    {
                        "name": "Lasse M. Reinpold"
                    },
                    {
                        "name": "Marvin Schieseck"
                    },
                    {
                        "name": "Lukas P. Wagner"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "arxiv_comment": "Submitted to 3rd IEEE Industrial Electronics Society Annual Online\n  Conference (ONCON)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v1",
                "updated": "2024-11-18T13:57:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agents Social Interaction Simulations on One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agents Social Interaction Simulations on One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (\\emph{i.e.}, X, Reddit) with more realistic\nlarge language model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (\\emph{i.e.},\ndynamic social networks and post information), diverse action spaces\n(\\emph{i.e.}, following, commenting), and recommendation systems (\\emph{i.e.},\ninterest-based and hot-score-based). Additionally, OASIS supports large-scale\nuser simulations, capable of modeling up to one million users. With these\nfeatures, OASIS can be easily extended to different social media platforms to\nstudy large-scale group phenomena and behaviors. We replicate various social\nphenomena, including information spreading, group polarization, and herd\neffects across X and Reddit platforms. Moreover, we provide observations of\nsocial phenomena at different agent group scales. We observe that the larger\nagent group scale leads to more enhanced group dynamics and more diverse and\nhelpful agents' opinions. These findings demonstrate OASIS's potential as a\npowerful tool for studying complex systems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (\\emph{i.e.}, X, Reddit) with more realistic\nlarge language model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (\\emph{i.e.},\ndynamic social networks and post information), diverse action spaces\n(\\emph{i.e.}, following, commenting), and recommendation systems (\\emph{i.e.},\ninterest-based and hot-score-based). Additionally, OASIS supports large-scale\nuser simulations, capable of modeling up to one million users. With these\nfeatures, OASIS can be easily extended to different social media platforms to\nstudy large-scale group phenomena and behaviors. We replicate various social\nphenomena, including information spreading, group polarization, and herd\neffects across X and Reddit platforms. Moreover, we provide observations of\nsocial phenomena at different agent group scales. We observe that the larger\nagent group scale leads to more enhanced group dynamics and more diverse and\nhelpful agents' opinions. These findings demonstrate OASIS's potential as a\npowerful tool for studying complex systems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11560v1",
                "updated": "2024-11-18T13:26:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    26,
                    9,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:26:09Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    26,
                    9,
                    0,
                    323,
                    0
                ],
                "title": "Topology-aware Preemptive Scheduling for Co-located LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology-aware Preemptive Scheduling for Co-located LLM Workloads"
                },
                "summary": "Hosting diverse large language model workloads in a unified resource pool\nthrough co-location is cost-effective. For example, long-running chat services\ngenerally follow diurnal traffic patterns, which inspire co-location of batch\njobs to fulfill resource valleys between successive peaks, and thus to saturate\nresource allocation in cluster-wide scope. These heterogeneous workloads often\nhave different business priorities, and therefore preemption can be leveraged\nfor resource elasticity. However, workloads often have distinct topology\npreferences as well. The resources released by lower-priority instances may\nfail to meet the requirements of high-priority online services which are\nusually latency-sensitive. The root cause behind such mis-match is a lack of\ntopology awareness of resource scheduler, especially during preemption. To\nbridge this gap, we develop a fine-grained topology-aware method for preemptive\nscheduling of hybrid workloads. The method ensures that the resources freed by\npreempted tasks adhere to the topological affinity needs of high-priority\npreemptors in a guaranteed or best-effort manner. This dynamic alignment\nsignificantly increases the efficiency of preemption and improves overall\nscheduled performance for LLM workloads by $55\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hosting diverse large language model workloads in a unified resource pool\nthrough co-location is cost-effective. For example, long-running chat services\ngenerally follow diurnal traffic patterns, which inspire co-location of batch\njobs to fulfill resource valleys between successive peaks, and thus to saturate\nresource allocation in cluster-wide scope. These heterogeneous workloads often\nhave different business priorities, and therefore preemption can be leveraged\nfor resource elasticity. However, workloads often have distinct topology\npreferences as well. The resources released by lower-priority instances may\nfail to meet the requirements of high-priority online services which are\nusually latency-sensitive. The root cause behind such mis-match is a lack of\ntopology awareness of resource scheduler, especially during preemption. To\nbridge this gap, we develop a fine-grained topology-aware method for preemptive\nscheduling of hybrid workloads. The method ensures that the resources freed by\npreempted tasks adhere to the topological affinity needs of high-priority\npreemptors in a guaranteed or best-effort manner. This dynamic alignment\nsignificantly increases the efficiency of preemption and improves overall\nscheduled performance for LLM workloads by $55\\%$."
                },
                "authors": [
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Jinjie Yang"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_comment": "17 Pages, 11 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06913v2",
                "updated": "2024-11-18T13:15:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    15,
                    41,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-09T14:12:51Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    12,
                    51,
                    2,
                    283,
                    0
                ],
                "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning"
                },
                "summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict, on the other hand, emerges as the LLM's\nknowledge evolves during SFT, allowing it to answer questions that were\npreviously unanswerable. Yet, these now-answerable training samples still\nretain the original \"I don't know\" supervision signals based on the initial LLM\nstate, resulting in inconsistencies. These conflicts cause the trained LLM to\nmisclassify known questions as unknown, resulting in over-refusal. To address\nthis issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware\nInstructions Tuning (CRaFT). CRaFT centers on two main contributions: First, we\nadditionally incorporate response certainty to selectively filter and modify\ndata, reducing static conflicts. Second, we implement preliminary rehearsal\ntraining to characterize changes in the LLM's knowledge state, which helps\nmitigate dynamic conflicts during the fine-tuning process. We conducted\nextensive experiments on open-ended question answering and multiple-choice\nquestion task. Experiment results show that CRaFT can improve LLM's overall\nperformance during the RAIT process. Source code and training data will be\nreleased at Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict, on the other hand, emerges as the LLM's\nknowledge evolves during SFT, allowing it to answer questions that were\npreviously unanswerable. Yet, these now-answerable training samples still\nretain the original \"I don't know\" supervision signals based on the initial LLM\nstate, resulting in inconsistencies. These conflicts cause the trained LLM to\nmisclassify known questions as unknown, resulting in over-refusal. To address\nthis issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware\nInstructions Tuning (CRaFT). CRaFT centers on two main contributions: First, we\nadditionally incorporate response certainty to selectively filter and modify\ndata, reducing static conflicts. Second, we implement preliminary rehearsal\ntraining to characterize changes in the LLM's knowledge state, which helps\nmitigate dynamic conflicts during the fine-tuning process. We conducted\nextensive experiments on open-ended question answering and multiple-choice\nquestion task. Experiment results show that CRaFT can improve LLM's overall\nperformance during the RAIT process. Source code and training data will be\nreleased at Github."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Junyuan Gao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v1",
                "updated": "2024-11-18T13:01:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11532v1",
                "updated": "2024-11-18T12:41:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "title": "A Code Knowledge Graph-Enhanced System for LLM-Based Fuzz Driver\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Code Knowledge Graph-Enhanced System for LLM-Based Fuzz Driver\n  Generation"
                },
                "summary": "The rapid development of large language models (LLMs) with advanced\nprogramming capabilities has paved the way for innovative approaches in\nsoftware testing. Fuzz testing, a cornerstone for improving software\nreliability and detecting vulnerabilities, often relies on manually written\nfuzz drivers, limiting scalability and efficiency. To address this challenge,\nwe propose CodeGraphGPT, a novel system that integrates code knowledge graphs\nwith an LLM-powered intelligent agent to automate the fuzz driver generation\nprocess. By framing fuzz driver creation as a code generation task,\nCodeGraphGPT leverages program analysis to construct a knowledge graph of code\nrepositories, where nodes represent code entities, such as functions or files,\nand edges capture their relationships. This enables the system to generate\ntailored fuzz drivers and input seeds, resolve compilation errors, and analyze\ncrash reports, all while adapting to specific API usage scenarios.\nAdditionally, querying the knowledge graph helps identify precise testing\ntargets and contextualize the purpose of each fuzz driver within the fuzzing\nloop. We evaluated CodeGraphGPT on eight open-source software projects,\nachieving an average improvement of 8.73\\% in code coverage compared to\nstate-of-the-art methods. Moreover, it reduced the manual workload in crash\ncase analysis by 84.4\\% and identified 11 real-world bugs, including nine\npreviously unreported ones. This work highlights how integrating LLMs with code\nknowledge graphs enhances fuzz driver generation, offering an efficient\nsolution for vulnerability detection and software quality improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) with advanced\nprogramming capabilities has paved the way for innovative approaches in\nsoftware testing. Fuzz testing, a cornerstone for improving software\nreliability and detecting vulnerabilities, often relies on manually written\nfuzz drivers, limiting scalability and efficiency. To address this challenge,\nwe propose CodeGraphGPT, a novel system that integrates code knowledge graphs\nwith an LLM-powered intelligent agent to automate the fuzz driver generation\nprocess. By framing fuzz driver creation as a code generation task,\nCodeGraphGPT leverages program analysis to construct a knowledge graph of code\nrepositories, where nodes represent code entities, such as functions or files,\nand edges capture their relationships. This enables the system to generate\ntailored fuzz drivers and input seeds, resolve compilation errors, and analyze\ncrash reports, all while adapting to specific API usage scenarios.\nAdditionally, querying the knowledge graph helps identify precise testing\ntargets and contextualize the purpose of each fuzz driver within the fuzzing\nloop. We evaluated CodeGraphGPT on eight open-source software projects,\nachieving an average improvement of 8.73\\% in code coverage compared to\nstate-of-the-art methods. Moreover, it reduced the manual workload in crash\ncase analysis by 84.4\\% and identified 11 real-world bugs, including nine\npreviously unreported ones. This work highlights how integrating LLMs with code\nknowledge graphs enhances fuzz driver generation, offering an efficient\nsolution for vulnerability detection and software quality improvement."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Ting Zhou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11531v1",
                "updated": "2024-11-18T12:40:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    40,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:40:51Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    40,
                    51,
                    0,
                    323,
                    0
                ],
                "title": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality"
                },
                "summary": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval."
                },
                "authors": [
                    {
                        "name": "Viktoriia Chekalina"
                    },
                    {
                        "name": "Anton Razzigaev"
                    },
                    {
                        "name": "Elizaveta Goncharova"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Kuznetsov"
                },
                "author": "Andrey Kuznetsov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16937v2",
                "updated": "2024-11-18T12:36:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    36,
                    13,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-17T09:39:34Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    39,
                    34,
                    0,
                    169,
                    0
                ],
                "title": "A Complete Survey on LLM-based AI Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Complete Survey on LLM-based AI Chatbots"
                },
                "summary": "The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI."
                },
                "authors": [
                    {
                        "name": "Sumit Kumar Dam"
                    },
                    {
                        "name": "Choong Seon Hong"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Chaoning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chaoning Zhang"
                },
                "author": "Chaoning Zhang",
                "arxiv_comment": "23 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11521v1",
                "updated": "2024-11-18T12:31:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    31,
                    22,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T12:31:22Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    31,
                    22,
                    0,
                    323,
                    0
                ],
                "title": "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions"
                },
                "summary": "Individuals have been increasingly interacting with online Large Language\nModels (LLMs), both in their work and personal lives. These interactions raise\nprivacy issues as the LLMs are typically hosted by third-parties who can gather\na variety of sensitive information about users and their companies. Text\nSanitization techniques have been proposed in the literature and can be used to\nsanitize user prompts before sending them to the LLM. However, sanitization has\nan impact on the downstream task performed by the LLM, and often to such an\nextent that it leads to unacceptable results for the user. This is not just a\nminor annoyance, with clear monetary consequences as LLM services charge on a\nper use basis as well as great amount of computing resources wasted. We propose\nan architecture leveraging a Small Language Model (SLM) at the user-side to\nhelp estimate the impact of sanitization on a prompt before it is sent to the\nLLM, thus preventing resource losses.\n  Our evaluation of this architecture revealed a significant problem with text\nsanitization based on Differential Privacy, on which we want to draw the\nattention of the community for further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individuals have been increasingly interacting with online Large Language\nModels (LLMs), both in their work and personal lives. These interactions raise\nprivacy issues as the LLMs are typically hosted by third-parties who can gather\na variety of sensitive information about users and their companies. Text\nSanitization techniques have been proposed in the literature and can be used to\nsanitize user prompts before sending them to the LLM. However, sanitization has\nan impact on the downstream task performed by the LLM, and often to such an\nextent that it leads to unacceptable results for the user. This is not just a\nminor annoyance, with clear monetary consequences as LLM services charge on a\nper use basis as well as great amount of computing resources wasted. We propose\nan architecture leveraging a Small Language Model (SLM) at the user-side to\nhelp estimate the impact of sanitization on a prompt before it is sent to the\nLLM, thus preventing resource losses.\n  Our evaluation of this architecture revealed a significant problem with text\nsanitization based on Differential Privacy, on which we want to draw the\nattention of the community for further investigation."
                },
                "authors": [
                    {
                        "name": "Robin Carpentier"
                    },
                    {
                        "name": "Benjamin Zi Hao Zhao"
                    },
                    {
                        "name": "Hassan Jameel Asghar"
                    },
                    {
                        "name": "Dali Kaafar"
                    }
                ],
                "author_detail": {
                    "name": "Dali Kaafar"
                },
                "author": "Dali Kaafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14148v2",
                "updated": "2024-11-18T11:58:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    58,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-18T03:34:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    34,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment"
                },
                "summary": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models."
                },
                "authors": [
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11493v1",
                "updated": "2024-11-18T11:55:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    55,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    55,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "LSRAM: A Lightweight Autoscaling and SLO Resource Allocation Framework\n  for Microservices Based on Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSRAM: A Lightweight Autoscaling and SLO Resource Allocation Framework\n  for Microservices Based on Gradient Descent"
                },
                "summary": "Microservices architecture has become the dominant architecture in cloud\ncomputing paradigm with its advantages of facilitating development, deployment,\nmodularity and scalability. The workflow of microservices architecture is\ntransparent to the users, who are concerned with the quality of service (QoS).\nTaking Service Level Objective (SLO) as an important indicator of system\nresource scaling can effectively ensure user's QoS, but how to quickly allocate\nend-to-end SLOs to each microservice in a complete service so that it can\nobtain the optimal SLO resource allocation scheme is still a challenging\nproblem. Existing microservice autoscaling frameworks based on SLO resources\noften have heavy and complex models that demand substantial time and\ncomputational resources to get a suitable resource allocation scheme. Moreover,\nwhen the system environment or microservice application changes, these methods\nrequire significant time and resources for model retraining. In this paper, we\npropose LSRAM, a lightweight SLO resource allocation management framework based\non the gradient descent method to overcome the limitation of existing methods\nin terms of heavy model, time-consuming, poor scalability, and difficulty in\nretraining. LSRAM has two stages: at stage one, the lightweight SLO resource\nallocation model from LSRAM can quickly compute the appropriate SLO resources\nfor each microservice; at stage two, LSRAM's SLO resource update model enables\nthe entire framework to quickly adapt to changes in the cluster environment\n(e.g. load and applications). Additionally, LSRAM can effectively handle bursty\ntraffic and highly fluctuating load application scenarios. Compared to\nstate-of-the-art SLO allocation frameworks, LSRAM not only guarantees users'\nQoS but also reduces resource usage by 17%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microservices architecture has become the dominant architecture in cloud\ncomputing paradigm with its advantages of facilitating development, deployment,\nmodularity and scalability. The workflow of microservices architecture is\ntransparent to the users, who are concerned with the quality of service (QoS).\nTaking Service Level Objective (SLO) as an important indicator of system\nresource scaling can effectively ensure user's QoS, but how to quickly allocate\nend-to-end SLOs to each microservice in a complete service so that it can\nobtain the optimal SLO resource allocation scheme is still a challenging\nproblem. Existing microservice autoscaling frameworks based on SLO resources\noften have heavy and complex models that demand substantial time and\ncomputational resources to get a suitable resource allocation scheme. Moreover,\nwhen the system environment or microservice application changes, these methods\nrequire significant time and resources for model retraining. In this paper, we\npropose LSRAM, a lightweight SLO resource allocation management framework based\non the gradient descent method to overcome the limitation of existing methods\nin terms of heavy model, time-consuming, poor scalability, and difficulty in\nretraining. LSRAM has two stages: at stage one, the lightweight SLO resource\nallocation model from LSRAM can quickly compute the appropriate SLO resources\nfor each microservice; at stage two, LSRAM's SLO resource update model enables\nthe entire framework to quickly adapt to changes in the cluster environment\n(e.g. load and applications). Additionally, LSRAM can effectively handle bursty\ntraffic and highly fluctuating load application scenarios. Compared to\nstate-of-the-art SLO allocation frameworks, LSRAM not only guarantees users'\nQoS but also reduces resource usage by 17%."
                },
                "authors": [
                    {
                        "name": "Kan Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "22 pages",
                "arxiv_journal_ref": "Software: Practice and Experience 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12138v2",
                "updated": "2024-11-18T11:29:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    29,
                    47,
                    0,
                    323,
                    0
                ],
                "published": "2024-04-18T12:40:59Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    12,
                    40,
                    59,
                    3,
                    109,
                    0
                ],
                "title": "Character is Destiny: Can Role-Playing Language Agents Make\n  Persona-Driven Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Character is Destiny: Can Role-Playing Language Agents Make\n  Persona-Driven Decisions?"
                },
                "summary": "Can Large Language Models (LLMs) simulate humans in making important\ndecisions? Recent research has unveiled the potential of using LLMs to develop\nrole-playing language agents (RPLAs), mimicking mainly the knowledge and tones\nof various characters. However, imitative decision-making necessitates a more\nnuanced understanding of personas. In this paper, we benchmark the ability of\nLLMs in persona-driven decision-making. Specifically, we investigate whether\nLLMs can predict characters' decisions provided by the preceding stories in\nhigh-quality novels. Leveraging character analyses written by literary experts,\nwe construct a dataset LIFECHOICE comprising 1,462 characters' decision points\nfrom 388 books. Then, we conduct comprehensive experiments on LIFECHOICE, with\nvarious LLMs and RPLA methodologies. The results demonstrate that\nstate-of-the-art LLMs exhibit promising capabilities in this task, yet\nsubstantial room for improvement remains. Hence, we further propose the CHARMAP\nmethod, which adopts persona-based memory retrieval and significantly advances\nRPLAs on this task, achieving 5.03% increase in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models (LLMs) simulate humans in making important\ndecisions? Recent research has unveiled the potential of using LLMs to develop\nrole-playing language agents (RPLAs), mimicking mainly the knowledge and tones\nof various characters. However, imitative decision-making necessitates a more\nnuanced understanding of personas. In this paper, we benchmark the ability of\nLLMs in persona-driven decision-making. Specifically, we investigate whether\nLLMs can predict characters' decisions provided by the preceding stories in\nhigh-quality novels. Leveraging character analyses written by literary experts,\nwe construct a dataset LIFECHOICE comprising 1,462 characters' decision points\nfrom 388 books. Then, we conduct comprehensive experiments on LIFECHOICE, with\nvarious LLMs and RPLA methodologies. The results demonstrate that\nstate-of-the-art LLMs exhibit promising capabilities in this task, yet\nsubstantial room for improvement remains. Hence, we further propose the CHARMAP\nmethod, which adopts persona-based memory retrieval and significantly advances\nRPLAs on this task, achieving 5.03% increase in accuracy."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Xiaoqing Dong"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12737v2",
                "updated": "2024-11-18T11:21:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    21,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-04-19T09:30:07Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    9,
                    30,
                    7,
                    4,
                    110,
                    0
                ],
                "title": "LLM App Store Analysis: A Vision and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM App Store Analysis: A Vision and Roadmap"
                },
                "summary": "The rapid growth and popularity of large language model (LLM) app stores have\ncreated new opportunities and challenges for researchers, developers, users,\nand app store managers. As the LLM app ecosystem continues to evolve, it is\ncrucial to understand the current landscape and identify potential areas for\nfuture research and development. This paper presents a forward-looking analysis\nof LLM app stores, focusing on key aspects such as data mining, security risk\nidentification, development assistance, and market dynamics. Our comprehensive\nexamination extends to the intricate relationships between various stakeholders\nand the technological advancements driving the ecosystem's growth. We explore\nthe ethical considerations and potential societal impacts of widespread LLM app\nadoption, highlighting the need for responsible innovation and governance\nframeworks. By examining these aspects, we aim to provide a vision for future\nresearch directions and highlight the importance of collaboration among\nstakeholders to address the challenges and opportunities within the LLM app\necosystem. The insights and recommendations provided in this paper serve as a\nfoundation for driving innovation, ensuring responsible development, and\ncreating a thriving, user-centric LLM app landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth and popularity of large language model (LLM) app stores have\ncreated new opportunities and challenges for researchers, developers, users,\nand app store managers. As the LLM app ecosystem continues to evolve, it is\ncrucial to understand the current landscape and identify potential areas for\nfuture research and development. This paper presents a forward-looking analysis\nof LLM app stores, focusing on key aspects such as data mining, security risk\nidentification, development assistance, and market dynamics. Our comprehensive\nexamination extends to the intricate relationships between various stakeholders\nand the technological advancements driving the ecosystem's growth. We explore\nthe ethical considerations and potential societal impacts of widespread LLM app\nadoption, highlighting the need for responsible innovation and governance\nframeworks. By examining these aspects, we aim to provide a vision for future\nresearch directions and highlight the importance of collaboration among\nstakeholders to address the challenges and opportunities within the LLM app\necosystem. The insights and recommendations provided in this paper serve as a\nfoundation for driving innovation, ensuring responsible development, and\ncreating a thriving, user-centric LLM app landscape."
                },
                "authors": [
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18009v2",
                "updated": "2024-11-18T11:15:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    15,
                    56,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-28T09:50:46Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    9,
                    50,
                    46,
                    1,
                    149,
                    0
                ],
                "title": "Exploring Context Window of Large Language Models via Decomposed\n  Positional Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Context Window of Large Language Models via Decomposed\n  Positional Vectors"
                },
                "summary": "Transformer-based large language models (LLMs) typically have a limited\ncontext window, resulting in significant performance degradation when\nprocessing text beyond the length of the context window. Extensive studies have\nbeen proposed to extend the context window and achieve length extrapolation of\nLLMs, but there is still a lack of in-depth interpretation of these approaches.\nIn this study, we explore the positional information within and beyond the\ncontext window for deciphering the underlying mechanism of LLMs. By using a\nmean-based decomposition method, we disentangle positional vectors from hidden\nstates of LLMs and analyze their formation and effect on attention.\nFurthermore, when texts exceed the context window, we analyze the change of\npositional vectors in two settings, i.e., direct extrapolation and context\nwindow extension. Based on our findings, we design two training-free context\nwindow extension methods, positional vector replacement and attention window\nextension. Experimental results show that our methods can effectively extend\nthe context window length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) typically have a limited\ncontext window, resulting in significant performance degradation when\nprocessing text beyond the length of the context window. Extensive studies have\nbeen proposed to extend the context window and achieve length extrapolation of\nLLMs, but there is still a lack of in-depth interpretation of these approaches.\nIn this study, we explore the positional information within and beyond the\ncontext window for deciphering the underlying mechanism of LLMs. By using a\nmean-based decomposition method, we disentangle positional vectors from hidden\nstates of LLMs and analyze their formation and effect on attention.\nFurthermore, when texts exceed the context window, we analyze the change of\npositional vectors in two settings, i.e., direct extrapolation and context\nwindow extension. Based on our findings, we design two training-free context\nwindow extension methods, positional vector replacement and attention window\nextension. Experimental results show that our methods can effectively extend\nthe context window length."
                },
                "authors": [
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingbing Wang"
                    },
                    {
                        "name": "Zhen Tian"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted by Neurips 2024 as a spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v4",
                "updated": "2024-11-18T10:32:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    32,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "arxiv_comment": "Presented at the Scientific Methods for Understanding Deep Learning\n  (SciForDL) workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11449v1",
                "updated": "2024-11-18T10:31:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    31,
                    24,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T10:31:24Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    31,
                    24,
                    0,
                    323,
                    0
                ],
                "title": "Deliberative XAI: How Explanations Impact Understanding and\n  Decision-Making of AI Novices in Collective and Individual Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberative XAI: How Explanations Impact Understanding and\n  Decision-Making of AI Novices in Collective and Individual Settings"
                },
                "summary": "XAI research often focuses on settings where people learn about and assess\nalgorithmic systems individually. However, as more public AI systems are\ndeployed, it becomes essential for XAI to facilitate collective understanding\nand deliberation. We conducted a task-based interview study involving 8 focus\ngroups and 12 individual interviews to explore how explanations can support AI\nnovices in understanding and forming opinions about AI systems. Participants\nreceived a collection of explanations organized into four information\ncategories to solve tasks and decide about a system's deployment. These\nexplanations improved or calibrated participants' self-reported understanding\nand decision confidence and facilitated group discussions. Participants valued\nboth technical and contextual information and the self-directed and modular\nexplanation structure. Our contributions include an explanation approach that\nfacilitates both individual and collaborative interaction and explanation\ndesign recommendations, including active and controllable exploration,\ndifferent levels of information detail and breadth, and adaptations to the\nneeds of decision subjects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAI research often focuses on settings where people learn about and assess\nalgorithmic systems individually. However, as more public AI systems are\ndeployed, it becomes essential for XAI to facilitate collective understanding\nand deliberation. We conducted a task-based interview study involving 8 focus\ngroups and 12 individual interviews to explore how explanations can support AI\nnovices in understanding and forming opinions about AI systems. Participants\nreceived a collection of explanations organized into four information\ncategories to solve tasks and decide about a system's deployment. These\nexplanations improved or calibrated participants' self-reported understanding\nand decision confidence and facilitated group discussions. Participants valued\nboth technical and contextual information and the self-directed and modular\nexplanation structure. Our contributions include an explanation approach that\nfacilitates both individual and collaborative interaction and explanation\ndesign recommendations, including active and controllable exploration,\ndifferent levels of information detail and breadth, and adaptations to the\nneeds of decision subjects."
                },
                "authors": [
                    {
                        "name": "Timothe Schmude"
                    },
                    {
                        "name": "Laura Koesten"
                    },
                    {
                        "name": "Torsten Mller"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Tschiatschek"
                },
                "author": "Sebastian Tschiatschek",
                "arxiv_comment": "24 pages main text, 7 figures, 4 tables, supplementary material\n  included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11435v1",
                "updated": "2024-11-18T10:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    4,
                    10,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T10:04:10Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    4,
                    10,
                    0,
                    323,
                    0
                ],
                "title": "GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced\n  Aesthetic Text Glyph Layouts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced\n  Aesthetic Text Glyph Layouts"
                },
                "summary": "Text logo design heavily relies on the creativity and expertise of\nprofessional designers, in which arranging element layouts is one of the most\nimportant procedures. However, few attention has been paid to this specific\ntask which needs to take precise textural details and user constraints into\nconsideration, but only on the broader tasks such as document/poster layout\ngeneration. In this paper, we propose a VLM-based framework that generates\ncontent-aware text logo layouts by integrating multi-modal inputs with user\nconstraints, supporting a more flexible and stable layout design in real-world\napplications. We introduce two model techniques to reduce the computation for\nprocessing multiple glyph images simultaneously, while does not face\nperformance degradation. To support instruction-tuning of out model, we\nconstruct two extensive text logo datasets, which are 5x more larger than the\nexisting public dataset. Except for the geometric annotations (e.g. text masks\nand character recognition), we also compliment with comprehensive layout\ndescriptions in natural language format, for more effective training to have\nreasoning ability when dealing with complex layouts and custom user\nconstraints. Experimental studies demonstrate the effectiveness of our proposed\nmodel and datasets, when comparing with previous methods in various benchmarks\nto evaluate geometric aesthetics and human preferences. The code and datasets\nwill be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text logo design heavily relies on the creativity and expertise of\nprofessional designers, in which arranging element layouts is one of the most\nimportant procedures. However, few attention has been paid to this specific\ntask which needs to take precise textural details and user constraints into\nconsideration, but only on the broader tasks such as document/poster layout\ngeneration. In this paper, we propose a VLM-based framework that generates\ncontent-aware text logo layouts by integrating multi-modal inputs with user\nconstraints, supporting a more flexible and stable layout design in real-world\napplications. We introduce two model techniques to reduce the computation for\nprocessing multiple glyph images simultaneously, while does not face\nperformance degradation. To support instruction-tuning of out model, we\nconstruct two extensive text logo datasets, which are 5x more larger than the\nexisting public dataset. Except for the geometric annotations (e.g. text masks\nand character recognition), we also compliment with comprehensive layout\ndescriptions in natural language format, for more effective training to have\nreasoning ability when dealing with complex layouts and custom user\nconstraints. Experimental studies demonstrate the effectiveness of our proposed\nmodel and datasets, when comparing with previous methods in various benchmarks\nto evaluate geometric aesthetics and human preferences. The code and datasets\nwill be publicly available."
                },
                "authors": [
                    {
                        "name": "Junwen He"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Jun-Yan He"
                    },
                    {
                        "name": "Chenyang Li"
                    },
                    {
                        "name": "Hanyuan Chen"
                    },
                    {
                        "name": "Jin-Peng Lan"
                    },
                    {
                        "name": "Bin Luo"
                    },
                    {
                        "name": "Yifeng Geng"
                    }
                ],
                "author_detail": {
                    "name": "Yifeng Geng"
                },
                "author": "Yifeng Geng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08449v2",
                "updated": "2024-11-18T09:57:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    57,
                    4,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-13T09:11:56Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    9,
                    11,
                    56,
                    2,
                    318,
                    0
                ],
                "title": "Towards Evaluating Large Language Models for Graph Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating Large Language Models for Graph Query Generation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the landscape of Generative\nArtificial Intelligence (GenAI), with innovative LLM-backed solutions emerging\nrapidly. However, when applied to database technologies, specifically query\ngeneration for graph databases and Knowledge Graphs (KGs), LLMs still face\nsignificant challenges. While research on LLM-driven query generation for\nStructured Query Language (SQL) exists, similar systems for graph databases\nremain underdeveloped. This paper presents a comparative study addressing the\nchallenge of generating Cypher queries a powerful language for interacting with\ngraph databases using open-access LLMs. We rigorously evaluate several LLM\nagents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a\nlocally deployed Llama 3.1 8B) using a designed few-shot learning prompt and\nRetrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)\nreasoning. Our empirical analysis of query generation accuracy reveals that\nClaude Sonnet 3.5 outperforms its counterparts in this specific domain.\nFurther, we highlight promising future research directions to address the\nidentified limitations and advance LLM-driven query generation for graph\ndatabases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the landscape of Generative\nArtificial Intelligence (GenAI), with innovative LLM-backed solutions emerging\nrapidly. However, when applied to database technologies, specifically query\ngeneration for graph databases and Knowledge Graphs (KGs), LLMs still face\nsignificant challenges. While research on LLM-driven query generation for\nStructured Query Language (SQL) exists, similar systems for graph databases\nremain underdeveloped. This paper presents a comparative study addressing the\nchallenge of generating Cypher queries a powerful language for interacting with\ngraph databases using open-access LLMs. We rigorously evaluate several LLM\nagents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a\nlocally deployed Llama 3.1 8B) using a designed few-shot learning prompt and\nRetrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)\nreasoning. Our empirical analysis of query generation accuracy reveals that\nClaude Sonnet 3.5 outperforms its counterparts in this specific domain.\nFurther, we highlight promising future research directions to address the\nidentified limitations and advance LLM-driven query generation for graph\ndatabases."
                },
                "authors": [
                    {
                        "name": "Siraj Munir"
                    },
                    {
                        "name": "Alessandro Aldini"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Aldini"
                },
                "author": "Alessandro Aldini",
                "arxiv_comment": "Paper accepted and will be presented at CSCI2024 in December 2024,\n  Later will be published at Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11424v1",
                "updated": "2024-11-18T09:50:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    50,
                    54,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:50:54Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    50,
                    54,
                    0,
                    323,
                    0
                ],
                "title": "Membership Inference Attack against Long-Context Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attack against Long-Context Large Language Models"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled them to overcome\ntheir context window limitations, and demonstrate exceptional retrieval and\nreasoning capacities on longer context. Quesion-answering systems augmented\nwith Long-Context Language Models (LCLMs) can automatically search massive\nexternal data and incorporate it into their contexts, enabling faithful\npredictions and reducing issues such as hallucinations and knowledge staleness.\nExisting studies targeting LCLMs mainly concentrate on addressing the so-called\nlost-in-the-middle problem or improving the inference effiencicy, leaving their\nprivacy risks largely unexplored. In this paper, we aim to bridge this gap and\nargue that integrating all information into the long context makes it a\nrepository of sensitive information, which often contains private data such as\nmedical records or personal identities. We further investigate the membership\nprivacy within LCLMs external context, with the aim of determining whether a\ngiven document or sequence is included in the LCLMs context. Our basic idea is\nthat if a document lies in the context, it will exhibit a low generation loss\nor a high degree of semantic similarity to the contents generated by LCLMs. We\nfor the first time propose six membership inference attack (MIA) strategies\ntailored for LCLMs and conduct extensive experiments on various popular models.\nEmpirical results demonstrate that our attacks can accurately infer membership\nstatus in most cases, e.g., 90.66% attack F1-score on Multi-document QA\ndatasets with LongChat-7b-v1.5-32k, highlighting significant risks of\nmembership leakage within LCLMs input contexts. Furthermore, we examine the\nunderlying reasons why LCLMs are susceptible to revealing such membership\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled them to overcome\ntheir context window limitations, and demonstrate exceptional retrieval and\nreasoning capacities on longer context. Quesion-answering systems augmented\nwith Long-Context Language Models (LCLMs) can automatically search massive\nexternal data and incorporate it into their contexts, enabling faithful\npredictions and reducing issues such as hallucinations and knowledge staleness.\nExisting studies targeting LCLMs mainly concentrate on addressing the so-called\nlost-in-the-middle problem or improving the inference effiencicy, leaving their\nprivacy risks largely unexplored. In this paper, we aim to bridge this gap and\nargue that integrating all information into the long context makes it a\nrepository of sensitive information, which often contains private data such as\nmedical records or personal identities. We further investigate the membership\nprivacy within LCLMs external context, with the aim of determining whether a\ngiven document or sequence is included in the LCLMs context. Our basic idea is\nthat if a document lies in the context, it will exhibit a low generation loss\nor a high degree of semantic similarity to the contents generated by LCLMs. We\nfor the first time propose six membership inference attack (MIA) strategies\ntailored for LCLMs and conduct extensive experiments on various popular models.\nEmpirical results demonstrate that our attacks can accurately infer membership\nstatus in most cases, e.g., 90.66% attack F1-score on Multi-document QA\ndatasets with LongChat-7b-v1.5-32k, highlighting significant risks of\nmembership leakage within LCLMs input contexts. Furthermore, we examine the\nunderlying reasons why LCLMs are susceptible to revealing such membership\ninformation."
                },
                "authors": [
                    {
                        "name": "Zixiong Wang"
                    },
                    {
                        "name": "Gaoyang Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Chen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Wang"
                },
                "author": "Chen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18492v3",
                "updated": "2024-11-18T09:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    44,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-28T18:01:52Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    18,
                    1,
                    52,
                    1,
                    149,
                    0
                ],
                "title": "LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance"
                },
                "summary": "Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code can\nbe found at https://github.com/felixbmuller/llms-memorization-copyright.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code can\nbe found at https://github.com/felixbmuller/llms-memorization-copyright."
                },
                "authors": [
                    {
                        "name": "Felix B Mueller"
                    },
                    {
                        "name": "Rebekka Grge"
                    },
                    {
                        "name": "Anna K Bernzen"
                    },
                    {
                        "name": "Janna C Pirk"
                    },
                    {
                        "name": "Maximilian Poretschkin"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Poretschkin"
                },
                "author": "Maximilian Poretschkin",
                "arxiv_comment": "10 pages, 3 figures, AIES 2024 conference",
                "arxiv_journal_ref": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,\n  7(1), 984-996, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11410v1",
                "updated": "2024-11-18T09:30:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    30,
                    14,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:30:14Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    30,
                    14,
                    0,
                    323,
                    0
                ],
                "title": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Multi-Parameter Constraint Inconsistencies in Python Data\n  Science Libraries"
                },
                "summary": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose an MPDetector for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detect\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI- and Data-intensive software systems rely heavily on data science\nand machine learning libraries that provide essential algorithmic\nimplementations and computational frameworks. These libraries expose complex\nAPIs whose correct usage has to follow constraints among multiple\ninterdependent parameters. Developers using these APIs are expected to learn\nabout the constraints through the provided documentations and any discrepancy\nmay lead to unexpected behaviors. However, maintaining correct and consistent\nmulti-parameter constraints in API documentations remains a significant\nchallenge for API compatibility and reliability. To address this challenge, we\npropose an MPDetector for detecting inconsistencies between code and\ndocumentation, specifically focusing on multi-parameter constraints. MPDetector\nidentifies these constraints at the code level by exploring execution paths\nthrough symbolic execution and further extracts corresponding constraints from\ndocumentation using large language models (LLMs). We propose a customized fuzzy\nconstraint logic to reconcile the unpredictability of LLM outputs and detect\nlogical inconsistencies between the code and documentation constraints. We\ncollected and constructed two datasets from four popular data science libraries\nand evaluated MPDetector on them. The results demonstrate that MPDetector can\neffectively detect inconsistency issues with the precision of 92.8%. We further\nreported 14 detected inconsistency issues to the library developers, who have\nconfirmed 11 issues at the time of writing."
                },
                "authors": [
                    {
                        "name": "Xiufeng Xu"
                    },
                    {
                        "name": "Fuman Xie"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Guangdong Bai"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11407v1",
                "updated": "2024-11-18T09:28:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    28,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:28:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    28,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on\n  Large Language Models"
                },
                "summary": "The widespread deployment of large language models (LLMs) across various\ndomains has showcased their immense potential while exposing significant safety\nvulnerabilities. A major concern is ensuring that LLM-generated content aligns\nwith human values. Existing jailbreak techniques reveal how this alignment can\nbe compromised through specific prompts or adversarial suffixes. In this study,\nwe introduce a new threat: LLMs' bias toward authority. While this inherent\nbias can improve the quality of outputs generated by LLMs, it also introduces a\npotential vulnerability, increasing the risk of producing harmful content.\nNotably, the biases in LLMs is the varying levels of trust given to different\ntypes of authoritative information in harmful queries. For example, malware\ndevelopment often favors trust GitHub. To better reveal the risks with LLM, we\npropose DarkCite, an adaptive authority citation matcher and generator designed\nfor a black-box setting. DarkCite matches optimal citation types to specific\nrisk types and generates authoritative citations relevant to harmful\ninstructions, enabling more effective jailbreak attacks on aligned LLMs.Our\nexperiments show that DarkCite achieves a higher attack success rate (e.g.,\nLLama-2 at 76% versus 68%) than previous methods. To counter this risk, we\npropose an authenticity and harm verification defense strategy, raising the\naverage defense pass rate (DPR) from 11% to 74%. More importantly, the ability\nto link citations to the content they encompass has become a foundational\nfunction in LLMs, amplifying the influence of LLMs' bias toward authority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large language models (LLMs) across various\ndomains has showcased their immense potential while exposing significant safety\nvulnerabilities. A major concern is ensuring that LLM-generated content aligns\nwith human values. Existing jailbreak techniques reveal how this alignment can\nbe compromised through specific prompts or adversarial suffixes. In this study,\nwe introduce a new threat: LLMs' bias toward authority. While this inherent\nbias can improve the quality of outputs generated by LLMs, it also introduces a\npotential vulnerability, increasing the risk of producing harmful content.\nNotably, the biases in LLMs is the varying levels of trust given to different\ntypes of authoritative information in harmful queries. For example, malware\ndevelopment often favors trust GitHub. To better reveal the risks with LLM, we\npropose DarkCite, an adaptive authority citation matcher and generator designed\nfor a black-box setting. DarkCite matches optimal citation types to specific\nrisk types and generates authoritative citations relevant to harmful\ninstructions, enabling more effective jailbreak attacks on aligned LLMs.Our\nexperiments show that DarkCite achieves a higher attack success rate (e.g.,\nLLama-2 at 76% versus 68%) than previous methods. To counter this risk, we\npropose an authenticity and harm verification defense strategy, raising the\naverage defense pass rate (DPR) from 11% to 74%. More importantly, the ability\nto link citations to the content they encompass has become a foundational\nfunction in LLMs, amplifying the influence of LLMs' bias toward authority."
                },
                "authors": [
                    {
                        "name": "Xikang Yang"
                    },
                    {
                        "name": "Xuehai Tang"
                    },
                    {
                        "name": "Jizhong Han"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18191v3",
                "updated": "2024-11-18T09:26:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    26,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-02-28T09:27:29Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    27,
                    29,
                    2,
                    59,
                    0
                ],
                "title": "Clustering and Ranking: Diversity-preserved Instruction Selection\n  through Expert-aligned Quality Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering and Ranking: Diversity-preserved Instruction Selection\n  through Expert-aligned Quality Estimation"
                },
                "summary": "With contributions from the open-source community, a vast amount of\ninstruction tuning (IT) data has emerged. Given the significant resource\nallocation required for training and evaluating models, it is advantageous to\nhave an efficient method for selecting high-quality IT data. However, existing\nmethods for instruction data selection have limitations such as relying on\nfragile external APIs, being affected by biases in GPT models, or reducing the\ndiversity of the selected instruction dataset. In this paper, we propose an\nindustrial-friendly, expert-aligned and diversity-preserved instruction data\nselection method: Clustering and Ranking (CaR). CaR employs a two-step process:\nfirst, it ranks instruction pairs using a high-accuracy (84.25%) scoring model\naligned with expert preferences; second, it preserves dataset diversity through\nclustering. In our experiment, CaR efficiently selected a mere 1.96% of\nAlpaca's IT data, yet the resulting AlpaCaR model surpassed Alpaca's\nperformance by an average of 32.1% in GPT-4 evaluations. Moreover, we find that\ndata selecting is a consistent paradigm whether the pre-trained model is more\ncapable or the model parameters scaling up. Our approach employs compact models\nwith 550M parameters and incurs just 11.2% of the financial outlay of current\nmethods, enhancing its industrial deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With contributions from the open-source community, a vast amount of\ninstruction tuning (IT) data has emerged. Given the significant resource\nallocation required for training and evaluating models, it is advantageous to\nhave an efficient method for selecting high-quality IT data. However, existing\nmethods for instruction data selection have limitations such as relying on\nfragile external APIs, being affected by biases in GPT models, or reducing the\ndiversity of the selected instruction dataset. In this paper, we propose an\nindustrial-friendly, expert-aligned and diversity-preserved instruction data\nselection method: Clustering and Ranking (CaR). CaR employs a two-step process:\nfirst, it ranks instruction pairs using a high-accuracy (84.25%) scoring model\naligned with expert preferences; second, it preserves dataset diversity through\nclustering. In our experiment, CaR efficiently selected a mere 1.96% of\nAlpaca's IT data, yet the resulting AlpaCaR model surpassed Alpaca's\nperformance by an average of 32.1% in GPT-4 evaluations. Moreover, we find that\ndata selecting is a consistent paradigm whether the pre-trained model is more\ncapable or the model parameters scaling up. Our approach employs compact models\nwith 550M parameters and incurs just 11.2% of the financial outlay of current\nmethods, enhancing its industrial deployability."
                },
                "authors": [
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Yilun Liu"
                    },
                    {
                        "name": "Chi Hu"
                    },
                    {
                        "name": "Weibin Meng"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Xiaofeng Zhao"
                    },
                    {
                        "name": "Hongxia Ma"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted by EMNLP2024",
                "arxiv_journal_ref": "https://aclanthology.org/2024.emnlp-main.28/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11401v1",
                "updated": "2024-11-18T09:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?"
                },
                "summary": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence."
                },
                "authors": [
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Ahmad Tayeb"
                    },
                    {
                        "name": "Ozren Dabi"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14259v2",
                "updated": "2024-11-18T09:19:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    19,
                    25,
                    0,
                    323,
                    0
                ],
                "published": "2024-02-22T03:46:08Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    3,
                    46,
                    8,
                    3,
                    53,
                    0
                ],
                "title": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form\n  Medical Question Answering Applications and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form\n  Medical Question Answering Applications and Beyond"
                },
                "summary": "Uncertainty estimation is crucial for the reliability of safety-critical\nhuman and artificial intelligence (AI) interaction systems, particularly in the\ndomain of healthcare engineering. However, a robust and general uncertainty\nmeasure for free-form answers has not been well-established in open-ended\nmedical question-answering (QA) tasks, where generative inequality introduces a\nlarge number of irrelevant words and sequences within the generated set for\nuncertainty quantification (UQ), which can lead to biases. This paper\nintroduces Word-Sequence Entropy (WSE), a method that calibrates uncertainty at\nboth the word and sequence levels, considering semantic relevance. WSE\nquantifies uncertainty in a way that is more closely aligned with the\nreliability of LLMs during uncertainty quantification (UQ). We compare WSE with\nsix baseline methods on five free-form medical QA datasets, utilizing seven\npopular large language models (LLMs). Experimental results demonstrate that WSE\nexhibits superior performance in UQ under two standard criteria for correctness\nevaluation. Additionally, in terms of real-world medical QA applications, the\nperformance of LLMs is significantly enhanced (e.g., a 6.36% improvement in\nmodel accuracy on the COVID-QA dataset) by employing responses with lower\nuncertainty that are identified by WSE as final answers, without any additional\ntask-specific fine-tuning or architectural modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is crucial for the reliability of safety-critical\nhuman and artificial intelligence (AI) interaction systems, particularly in the\ndomain of healthcare engineering. However, a robust and general uncertainty\nmeasure for free-form answers has not been well-established in open-ended\nmedical question-answering (QA) tasks, where generative inequality introduces a\nlarge number of irrelevant words and sequences within the generated set for\nuncertainty quantification (UQ), which can lead to biases. This paper\nintroduces Word-Sequence Entropy (WSE), a method that calibrates uncertainty at\nboth the word and sequence levels, considering semantic relevance. WSE\nquantifies uncertainty in a way that is more closely aligned with the\nreliability of LLMs during uncertainty quantification (UQ). We compare WSE with\nsix baseline methods on five free-form medical QA datasets, utilizing seven\npopular large language models (LLMs). Experimental results demonstrate that WSE\nexhibits superior performance in UQ under two standard criteria for correctness\nevaluation. Additionally, in terms of real-world medical QA applications, the\nperformance of LLMs is significantly enhanced (e.g., a 6.36% improvement in\nmodel accuracy on the COVID-QA dataset) by employing responses with lower\nuncertainty that are identified by WSE as final answers, without any additional\ntask-specific fine-tuning or architectural modifications."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Chenxi Yuan"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ren Wang"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "Accepted by Engineering Applications of Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20911v2",
                "updated": "2024-11-18T09:15:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    15,
                    46,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-28T10:43:34Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    43,
                    34,
                    0,
                    302,
                    0
                ],
                "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks"
                },
                "summary": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis"
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Ateniese"
                },
                "author": "Giuseppe Ateniese",
                "arxiv_comment": "v0.2 (evaluated on more agents)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11389v1",
                "updated": "2024-11-18T09:03:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    3,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T09:03:51Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    3,
                    51,
                    0,
                    323,
                    0
                ],
                "title": "Adapting to Cyber Threats: A Phishing Evolution Network (PEN) Framework\n  for Phishing Generation and Analyzing Evolution Patterns using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting to Cyber Threats: A Phishing Evolution Network (PEN) Framework\n  for Phishing Generation and Analyzing Evolution Patterns using Large Language\n  Models"
                },
                "summary": "Phishing remains a pervasive cyber threat, as attackers craft deceptive\nemails to lure victims into revealing sensitive information. While Artificial\nIntelligence (AI), particularly deep learning, has become a key component in\ndefending against phishing attacks, these approaches face critical limitations.\nThe scarcity of publicly available, diverse, and updated data, largely due to\nprivacy concerns, constrains their effectiveness. As phishing tactics evolve\nrapidly, models trained on limited, outdated data struggle to detect new,\nsophisticated deception strategies, leaving systems vulnerable to an\never-growing array of attacks. Addressing this gap is essential to\nstrengthening defenses in an increasingly hostile cyber landscape. To address\nthis gap, we propose the Phishing Evolution Network (PEN), a framework\nleveraging large language models (LLMs) and adversarial training mechanisms to\ncontinuously generate high quality and realistic diverse phishing samples, and\nanalyze features of LLM-provided phishing to understand evolving phishing\npatterns. We evaluate the quality and diversity of phishing samples generated\nby PEN and find that it produces over 80% realistic phishing samples,\neffectively expanding phishing datasets across seven dominant types. These\nPEN-generated samples enhance the performance of current phishing detectors,\nleading to a 40% improvement in detection accuracy. Additionally, the use of\nPEN significantly boosts model robustness, reducing detectors' sensitivity to\nperturbations by up to 60%, thereby decreasing attack success rates under\nadversarial conditions. When we analyze the phishing patterns that are used in\nLLM-generated phishing, the cognitive complexity and the tone of time\nlimitation are detected with statistically significant differences compared\nwith existing phishing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing remains a pervasive cyber threat, as attackers craft deceptive\nemails to lure victims into revealing sensitive information. While Artificial\nIntelligence (AI), particularly deep learning, has become a key component in\ndefending against phishing attacks, these approaches face critical limitations.\nThe scarcity of publicly available, diverse, and updated data, largely due to\nprivacy concerns, constrains their effectiveness. As phishing tactics evolve\nrapidly, models trained on limited, outdated data struggle to detect new,\nsophisticated deception strategies, leaving systems vulnerable to an\never-growing array of attacks. Addressing this gap is essential to\nstrengthening defenses in an increasingly hostile cyber landscape. To address\nthis gap, we propose the Phishing Evolution Network (PEN), a framework\nleveraging large language models (LLMs) and adversarial training mechanisms to\ncontinuously generate high quality and realistic diverse phishing samples, and\nanalyze features of LLM-provided phishing to understand evolving phishing\npatterns. We evaluate the quality and diversity of phishing samples generated\nby PEN and find that it produces over 80% realistic phishing samples,\neffectively expanding phishing datasets across seven dominant types. These\nPEN-generated samples enhance the performance of current phishing detectors,\nleading to a 40% improvement in detection accuracy. Additionally, the use of\nPEN significantly boosts model robustness, reducing detectors' sensitivity to\nperturbations by up to 60%, thereby decreasing attack success rates under\nadversarial conditions. When we analyze the phishing patterns that are used in\nLLM-generated phishing, the cognitive complexity and the tone of time\nlimitation are detected with statistically significant differences compared\nwith existing phishing."
                },
                "authors": [
                    {
                        "name": "Fengchao Chen"
                    },
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Van Nguyen"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Carsten Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rudolph"
                },
                "author": "Carsten Rudolph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11371v1",
                "updated": "2024-11-18T08:34:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    34,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T08:34:38Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    34,
                    38,
                    0,
                    323,
                    0
                ],
                "title": "Rethinking Thinking Tokens: Understanding Why They Underperform in\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Thinking Tokens: Understanding Why They Underperform in\n  Practice"
                },
                "summary": "Thinking Tokens (TT) have been proposed as an unsupervised method to\nfacilitate reasoning in language models. However, despite their conceptual\nappeal, our findings show that TTs marginally improves performance and\nconsistently underperforms compared to Chain-of-Thought (CoT) reasoning across\nmultiple benchmarks. We hypothesize that this underperformance stems from the\nreliance on a single embedding for TTs, which results in inconsistent learning\nsignals and introduces noisy gradients. This paper provides a comprehensive\nempirical analysis to validate this hypothesis and discusses the implications\nfor future research on unsupervised reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Tokens (TT) have been proposed as an unsupervised method to\nfacilitate reasoning in language models. However, despite their conceptual\nappeal, our findings show that TTs marginally improves performance and\nconsistently underperforms compared to Chain-of-Thought (CoT) reasoning across\nmultiple benchmarks. We hypothesize that this underperformance stems from the\nreliance on a single embedding for TTs, which results in inconsistent learning\nsignals and introduces noisy gradients. This paper provides a comprehensive\nempirical analysis to validate this hypothesis and discusses the implications\nfor future research on unsupervised reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Sreeram Vennam"
                    },
                    {
                        "name": "David Valente"
                    },
                    {
                        "name": "David Herel"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00499v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00499v3",
                "updated": "2024-11-18T08:33:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    33,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-29T17:33:07Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    17,
                    33,
                    7,
                    5,
                    181,
                    0
                ],
                "title": "ConU: Conformal Uncertainty in Large Language Models with Correctness\n  Coverage Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConU: Conformal Uncertainty in Large Language Models with Correctness\n  Coverage Guarantees"
                },
                "summary": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks\nremains an open challenge, exacerbated by the closed-source nature of the\nlatest large language models (LLMs). This study investigates applying conformal\nprediction (CP), which can transform any heuristic uncertainty notion into\nrigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We\nintroduce a novel uncertainty measure based on self-consistency theory, and\nthen develop a conformal uncertainty criterion by integrating the uncertainty\ncondition aligned with correctness into the CP algorithm. Empirical evaluations\nindicate that our uncertainty measure outperforms prior state-of-the-art\nmethods. Furthermore, we achieve strict control over the correctness coverage\nrate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning\ngeneral-purpose and medical scenarios. Additionally, the calibrated prediction\nsets with small size further highlights the efficiency of our method in\nproviding trustworthy guarantees for practical open-ended NLG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks\nremains an open challenge, exacerbated by the closed-source nature of the\nlatest large language models (LLMs). This study investigates applying conformal\nprediction (CP), which can transform any heuristic uncertainty notion into\nrigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We\nintroduce a novel uncertainty measure based on self-consistency theory, and\nthen develop a conformal uncertainty criterion by integrating the uncertainty\ncondition aligned with correctness into the CP algorithm. Empirical evaluations\nindicate that our uncertainty measure outperforms prior state-of-the-art\nmethods. Furthermore, we achieve strict control over the correctness coverage\nrate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning\ngeneral-purpose and medical scenarios. Additionally, the calibrated prediction\nsets with small size further highlights the efficiency of our method in\nproviding trustworthy guarantees for practical open-ended NLG applications."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Hengtao Shen"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Zhu"
                },
                "author": "Xiaofeng Zhu",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00499v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00499v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10370v2",
                "updated": "2024-11-18T08:29:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    8,
                    29,
                    8,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-16T18:03:41Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    18,
                    3,
                    41,
                    3,
                    137,
                    0
                ],
                "title": "Grounded 3D-LLM with Referent Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded 3D-LLM with Referent Tokens"
                },
                "summary": "Prior studies on 3D scene understanding have primarily developed specialized\nmodels for specific tasks or required task-specific fine-tuning. In this study,\nwe propose Grounded 3D-LLM, which explores the potential of 3D large\nmulti-modal models (3D LMMs) to consolidate various 3D vision tasks within a\nunified generative framework. The model uses scene referent tokens as special\nnoun phrases to reference 3D scenes, enabling it to handle sequences that\ninterleave 3D and textual data. Per-task instruction-following templates are\nemployed to ensure natural and diversity in translating 3D vision tasks into\nlanguage formats. To facilitate the use of referent tokens in subsequent\nlanguage modeling, we provide a large-scale, automatically curated grounded\nscene-text dataset with over 1 million phrase-to-region correspondences and\nintroduce Contrastive Language-Scene Pre-training (CLASP) to perform\nphrase-level scene-text alignment using this data. Our comprehensive evaluation\ncovers open-ended tasks like dense captioning and 3D question answering,\nalongside close-ended tasks such as object detection and language grounding.\nExperiments across multiple 3D benchmarks reveal the leading performance and\nthe broad applicability of Grounded 3D-LLM. Code and datasets are available at\nthe https://groundedscenellm.github.io/grounded_3d-llm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies on 3D scene understanding have primarily developed specialized\nmodels for specific tasks or required task-specific fine-tuning. In this study,\nwe propose Grounded 3D-LLM, which explores the potential of 3D large\nmulti-modal models (3D LMMs) to consolidate various 3D vision tasks within a\nunified generative framework. The model uses scene referent tokens as special\nnoun phrases to reference 3D scenes, enabling it to handle sequences that\ninterleave 3D and textual data. Per-task instruction-following templates are\nemployed to ensure natural and diversity in translating 3D vision tasks into\nlanguage formats. To facilitate the use of referent tokens in subsequent\nlanguage modeling, we provide a large-scale, automatically curated grounded\nscene-text dataset with over 1 million phrase-to-region correspondences and\nintroduce Contrastive Language-Scene Pre-training (CLASP) to perform\nphrase-level scene-text alignment using this data. Our comprehensive evaluation\ncovers open-ended tasks like dense captioning and 3D question answering,\nalongside close-ended tasks such as object detection and language grounding.\nExperiments across multiple 3D benchmarks reveal the leading performance and\nthe broad applicability of Grounded 3D-LLM. Code and datasets are available at\nthe https://groundedscenellm.github.io/grounded_3d-llm.github.io."
                },
                "authors": [
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Ruiyuan Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11350v1",
                "updated": "2024-11-18T07:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    39,
                    46,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T07:39:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    39,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Zero-Shot Load Forecasting with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Load Forecasting with Large Language Models"
                },
                "summary": "Deep learning models have shown strong performance in load forecasting, but\nthey generally require large amounts of data for model training before being\napplied to new scenarios, which limits their effectiveness in data-scarce\nscenarios. Inspired by the great success of pre-trained language models (LLMs)\nin natural language processing, this paper proposes a zero-shot load\nforecasting approach using an advanced LLM framework denoted as the Chronos\nmodel. By utilizing its extensive pre-trained knowledge, the Chronos model\nenables accurate load forecasting in data-scarce scenarios without the need for\nextensive data-specific training. Simulation results across five real-world\ndatasets demonstrate that the Chronos model significantly outperforms nine\npopular baseline models for both deterministic and probabilistic load\nforecasting with various forecast horizons (e.g., 1 to 48 hours), even though\nthe Chronos model is neither tailored nor fine-tuned to these specific load\ndatasets. Notably, Chronos reduces root mean squared error (RMSE), continuous\nranked probability score (CRPS), and quantile score (QS) by approximately\n7.34%-84.30%, 19.63%-60.06%, and 22.83%-54.49%, respectively, compared to\nbaseline models. These results highlight the superiority and flexibility of the\nChronos model, positioning it as an effective solution in data-scarce\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have shown strong performance in load forecasting, but\nthey generally require large amounts of data for model training before being\napplied to new scenarios, which limits their effectiveness in data-scarce\nscenarios. Inspired by the great success of pre-trained language models (LLMs)\nin natural language processing, this paper proposes a zero-shot load\nforecasting approach using an advanced LLM framework denoted as the Chronos\nmodel. By utilizing its extensive pre-trained knowledge, the Chronos model\nenables accurate load forecasting in data-scarce scenarios without the need for\nextensive data-specific training. Simulation results across five real-world\ndatasets demonstrate that the Chronos model significantly outperforms nine\npopular baseline models for both deterministic and probabilistic load\nforecasting with various forecast horizons (e.g., 1 to 48 hours), even though\nthe Chronos model is neither tailored nor fine-tuned to these specific load\ndatasets. Notably, Chronos reduces root mean squared error (RMSE), continuous\nranked probability score (CRPS), and quantile score (QS) by approximately\n7.34%-84.30%, 19.63%-60.06%, and 22.83%-54.49%, respectively, compared to\nbaseline models. These results highlight the superiority and flexibility of the\nChronos model, positioning it as an effective solution in data-scarce\nscenarios."
                },
                "authors": [
                    {
                        "name": "Wenlong Liao"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Mengshuo Jia"
                    },
                    {
                        "name": "Christian Rehtanz"
                    },
                    {
                        "name": "Jiannong Fang"
                    },
                    {
                        "name": "Fernando Port-Agel"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Port-Agel"
                },
                "author": "Fernando Port-Agel",
                "arxiv_comment": "21 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02156v2",
                "updated": "2024-11-18T07:36:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    36,
                    36,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-03T02:36:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    2,
                    36,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "The why, what, and how of AI-based coding in scientific research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The why, what, and how of AI-based coding in scientific research"
                },
                "summary": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress."
                },
                "authors": [
                    {
                        "name": "Tonghe Zhuang"
                    },
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_comment": "23 pages, 7 figure, 3 boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08484v2",
                "updated": "2024-11-18T07:32:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    32,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-03-13T12:50:23Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    12,
                    50,
                    23,
                    2,
                    73,
                    0
                ],
                "title": "Targeted Efficient Fine-tuning: Optimizing Parameter Updates with\n  Data-Driven Sample Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Efficient Fine-tuning: Optimizing Parameter Updates with\n  Data-Driven Sample Selection"
                },
                "summary": "Fine-tuning all parameters of Large Language Models (LLMs) is computationally\nexpensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nselectively fine-tuning specific parameters. Most of the parameter efficient\nfine-tuning (PEFT) methods center on selecting or introducing a set of\nparameters to be fine-tuned. However, there are few methods that consider the\nimpact of data samples on parameter selecting. Representative data driven\nmethods include FISH Mask based method, which randomly selects a portion of\ndata samples as a basis when selecting parameters. However, this random data\nsample selection method cannot select optimal parameters for unstable data\ndistribution. In this work, we introduce a data-centric approach and propose\nthe Iterative Range Decreasing (IRD) algorithm to optimize the sample-parameter\npair selection in FISH Mask. IRD iteratively refines the selection by\nidentifying subsets of samples and parameters exhibiting higher Fisher\ninformation. We demonstrate the effectiveness and rationality of proposed\nstrategy by conducting experiments on GLUE benchmark. Experimental results show\nour strategy optimizes the parameter selection and achieves preferable\nperformance over some typical baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning all parameters of Large Language Models (LLMs) is computationally\nexpensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nselectively fine-tuning specific parameters. Most of the parameter efficient\nfine-tuning (PEFT) methods center on selecting or introducing a set of\nparameters to be fine-tuned. However, there are few methods that consider the\nimpact of data samples on parameter selecting. Representative data driven\nmethods include FISH Mask based method, which randomly selects a portion of\ndata samples as a basis when selecting parameters. However, this random data\nsample selection method cannot select optimal parameters for unstable data\ndistribution. In this work, we introduce a data-centric approach and propose\nthe Iterative Range Decreasing (IRD) algorithm to optimize the sample-parameter\npair selection in FISH Mask. IRD iteratively refines the selection by\nidentifying subsets of samples and parameters exhibiting higher Fisher\ninformation. We demonstrate the effectiveness and rationality of proposed\nstrategy by conducting experiments on GLUE benchmark. Experimental results show\nour strategy optimizes the parameter selection and achieves preferable\nperformance over some typical baseline methods."
                },
                "authors": [
                    {
                        "name": "Ming Dong"
                    },
                    {
                        "name": "Kang Xue"
                    },
                    {
                        "name": "Bolong Zheng"
                    },
                    {
                        "name": "Tingting He"
                    }
                ],
                "author_detail": {
                    "name": "Tingting He"
                },
                "author": "Tingting He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08807v3",
                "updated": "2024-11-18T07:30:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    30,
                    6,
                    0,
                    323,
                    0
                ],
                "published": "2024-01-16T20:13:50Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    20,
                    13,
                    50,
                    1,
                    16,
                    0
                ],
                "title": "SpecGen: Automated Generation of Formal Program Specifications via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecGen: Automated Generation of Formal Program Specifications via Large\n  Language Models"
                },
                "summary": "Formal program specifications play a crucial role in various stages of\nsoftware development. However, manually crafting formal program specifications\nis rather difficult, making the job time-consuming and labor-intensive. It is\neven more challenging to write specifications that correctly and\ncomprehensively describe the semantics of complex programs. To reduce the\nburden on software developers, automated specification generation methods have\nemerged. However, existing methods usually rely on predefined templates or\ngrammar, making them struggle to accurately describe the behavior and\nfunctionality of complex real-world programs. To tackle this challenge, we\nintroduce SpecGen, a novel technique for formal program specification\ngeneration based on Large Language Models. Our key insight is to overcome the\nlimitations of existing methods by leveraging the code comprehension capability\nof LLMs. The process of SpecGen consists of two phases. The first phase employs\na conversational approach that guides the LLM to generate appropriate\nspecifications for a given program. The second phase, designed for where the\nLLM fails to generate correct specifications, applies four mutation operators\nto the model-generated specifications and selects verifiable specifications\nfrom the mutated ones through a novel heuristic selection strategy. We evaluate\nSpecGen on two datasets, including the SV-COMP Java category benchmark and a\nmanually constructed dataset. Experimental results demonstrate that SpecGen\nsucceeds in generating verifiable specifications for 279 out of 385 programs,\noutperforming the existing purely LLM-based approaches and conventional\nspecification generation tools like Houdini and Daikon. Further investigations\non the quality of generated specifications indicate that SpecGen can\ncomprehensively articulate the behaviors of the input program.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal program specifications play a crucial role in various stages of\nsoftware development. However, manually crafting formal program specifications\nis rather difficult, making the job time-consuming and labor-intensive. It is\neven more challenging to write specifications that correctly and\ncomprehensively describe the semantics of complex programs. To reduce the\nburden on software developers, automated specification generation methods have\nemerged. However, existing methods usually rely on predefined templates or\ngrammar, making them struggle to accurately describe the behavior and\nfunctionality of complex real-world programs. To tackle this challenge, we\nintroduce SpecGen, a novel technique for formal program specification\ngeneration based on Large Language Models. Our key insight is to overcome the\nlimitations of existing methods by leveraging the code comprehension capability\nof LLMs. The process of SpecGen consists of two phases. The first phase employs\na conversational approach that guides the LLM to generate appropriate\nspecifications for a given program. The second phase, designed for where the\nLLM fails to generate correct specifications, applies four mutation operators\nto the model-generated specifications and selects verifiable specifications\nfrom the mutated ones through a novel heuristic selection strategy. We evaluate\nSpecGen on two datasets, including the SV-COMP Java category benchmark and a\nmanually constructed dataset. Experimental results demonstrate that SpecGen\nsucceeds in generating verifiable specifications for 279 out of 385 programs,\noutperforming the existing purely LLM-based approaches and conventional\nspecification generation tools like Houdini and Daikon. Further investigations\non the quality of generated specifications indicate that SpecGen can\ncomprehensively articulate the behaviors of the input program."
                },
                "authors": [
                    {
                        "name": "Lezhi Ma"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Lei Bu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bu"
                },
                "author": "Lei Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11342v1",
                "updated": "2024-11-18T07:23:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    23,
                    55,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T07:23:55Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    23,
                    55,
                    0,
                    323,
                    0
                ],
                "title": "Multi-hop Differential Topology based Algorithms for Resilient Network\n  of UAV Swarm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop Differential Topology based Algorithms for Resilient Network\n  of UAV Swarm"
                },
                "summary": "Unmanned aerial vehicle (UAV) swarm networks face severe challenges of\ncommunication network split (CNS) issues caused by massive damage in hostile\nenvironments. In this paper, we propose a new paradigm to restore network\nconnectivity by repositioning remaining UAVs based on damage information within\nlocal topologies. Particularly, the locations of destroyed UAVs distributed in\ngaps between disconnected sub-nets are considered for recovery trajectory\nplanning. Specifically, we construct the multi-hop differential sub-graph\n(MDSG) to represent local damage-varying topologies. Based on this, we develop\ntwo distinct algorithms to address CNS issues. The first approach leverages an\nartificial potential field algorithm to calculate the recovery velocities via\nMDSG, enabling simple deployment on low-intelligence UAVs. In the second\napproach, we design an MDSG-based graph convolution framework to find the\nrecovery topology for high-intelligence swarms. As per the unique topology of\nMDSG, we propose a novel bipartite graph convolution operation, enhanced with a\nbatch-processing mechanism to improve graph convolution efficiency. Simulation\nresults show that the proposed algorithms expedite the recovery with\nsignificant margin while improving the spatial coverage and topology degree\nuniformity after recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicle (UAV) swarm networks face severe challenges of\ncommunication network split (CNS) issues caused by massive damage in hostile\nenvironments. In this paper, we propose a new paradigm to restore network\nconnectivity by repositioning remaining UAVs based on damage information within\nlocal topologies. Particularly, the locations of destroyed UAVs distributed in\ngaps between disconnected sub-nets are considered for recovery trajectory\nplanning. Specifically, we construct the multi-hop differential sub-graph\n(MDSG) to represent local damage-varying topologies. Based on this, we develop\ntwo distinct algorithms to address CNS issues. The first approach leverages an\nartificial potential field algorithm to calculate the recovery velocities via\nMDSG, enabling simple deployment on low-intelligence UAVs. In the second\napproach, we design an MDSG-based graph convolution framework to find the\nrecovery topology for high-intelligence swarms. As per the unique topology of\nMDSG, we propose a novel bipartite graph convolution operation, enhanced with a\nbatch-processing mechanism to improve graph convolution efficiency. Simulation\nresults show that the proposed algorithms expedite the recovery with\nsignificant margin while improving the spatial coverage and topology degree\nuniformity after recovery."
                },
                "authors": [
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Lianghui Ding"
                    }
                ],
                "author_detail": {
                    "name": "Lianghui Ding"
                },
                "author": "Lianghui Ding",
                "arxiv_comment": "16 pages, 12figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09822v2",
                "updated": "2024-11-18T07:05:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    5,
                    33,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-16T05:39:08Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    5,
                    39,
                    8,
                    3,
                    137,
                    0
                ],
                "title": "SEEK: Semantic Reasoning for Object Goal Navigation in Real World\n  Inspection Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEEK: Semantic Reasoning for Object Goal Navigation in Real World\n  Inspection Tasks"
                },
                "summary": "This paper addresses the problem of object-goal navigation in autonomous\ninspections in real-world environments. Object-goal navigation is crucial to\nenable effective inspections in various settings, often requiring the robot to\nidentify the target object within a large search space. Current object\ninspection methods fall short of human efficiency because they typically cannot\nbootstrap prior and common sense knowledge as humans do. In this paper, we\nintroduce a framework that enables robots to use semantic knowledge from prior\nspatial configurations of the environment and semantic common sense knowledge.\nWe propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines\nsemantic prior knowledge with the robot's observations to search for and\nnavigate toward target objects more efficiently. SEEK maintains two\nrepresentations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network\n(RSN). The RSN is a compact and practical model that estimates the probability\nof finding the target object across spatial elements in the DSG. We propose a\nnovel probabilistic planning framework to search for the object using\nrelational semantic knowledge. Our simulation analyses demonstrate that SEEK\noutperforms the classical planning and Large Language Models (LLMs)-based\nmethods that are examined in this study in terms of efficiency for object-goal\ninspection tasks. We validated our approach on a physical legged robot in urban\nenvironments, showcasing its practicality and effectiveness in real-world\ninspection scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of object-goal navigation in autonomous\ninspections in real-world environments. Object-goal navigation is crucial to\nenable effective inspections in various settings, often requiring the robot to\nidentify the target object within a large search space. Current object\ninspection methods fall short of human efficiency because they typically cannot\nbootstrap prior and common sense knowledge as humans do. In this paper, we\nintroduce a framework that enables robots to use semantic knowledge from prior\nspatial configurations of the environment and semantic common sense knowledge.\nWe propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines\nsemantic prior knowledge with the robot's observations to search for and\nnavigate toward target objects more efficiently. SEEK maintains two\nrepresentations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network\n(RSN). The RSN is a compact and practical model that estimates the probability\nof finding the target object across spatial elements in the DSG. We propose a\nnovel probabilistic planning framework to search for the object using\nrelational semantic knowledge. Our simulation analyses demonstrate that SEEK\noutperforms the classical planning and Large Language Models (LLMs)-based\nmethods that are examined in this study in terms of efficiency for object-goal\ninspection tasks. We validated our approach on a physical legged robot in urban\nenvironments, showcasing its practicality and effectiveness in real-world\ninspection scenarios."
                },
                "authors": [
                    {
                        "name": "Muhammad Fadhil Ginting"
                    },
                    {
                        "name": "Sung-Kyun Kim"
                    },
                    {
                        "name": "David D. Fan"
                    },
                    {
                        "name": "Matteo Palieri"
                    },
                    {
                        "name": "Mykel J. Kochenderfer"
                    },
                    {
                        "name": "Ali-akbar Agha-Mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Ali-akbar Agha-Mohammadi"
                },
                "author": "Ali-akbar Agha-Mohammadi",
                "arxiv_journal_ref": "Proc. of Robotics: Science and Systems 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09261v2",
                "updated": "2024-11-18T06:41:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    41,
                    26,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-14T07:58:44Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    58,
                    44,
                    3,
                    319,
                    0
                ],
                "title": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming"
                },
                "summary": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design."
                },
                "authors": [
                    {
                        "name": "Umar Alkafaween"
                    },
                    {
                        "name": "Ibrahim Albluwi"
                    },
                    {
                        "name": "Paul Denny"
                    }
                ],
                "author_detail": {
                    "name": "Paul Denny"
                },
                "author": "Paul Denny",
                "arxiv_comment": "Submitted to Journal of Computer Assisted Learning; updated table\n  refs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11323v1",
                "updated": "2024-11-18T06:33:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    33,
                    5,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T06:33:05Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    33,
                    5,
                    0,
                    323,
                    0
                ],
                "title": "SayComply: Grounding Field Robotic Tasks in Operational Compliance\n  through Retrieval-Based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SayComply: Grounding Field Robotic Tasks in Operational Compliance\n  through Retrieval-Based Language Models"
                },
                "summary": "This paper addresses the problem of task planning for robots that must comply\nwith operational manuals in real-world settings. Task planning under these\nconstraints is essential for enabling autonomous robot operation in domains\nthat require adherence to domain-specific knowledge. Current methods for\ngenerating robot goals and plans rely on common sense knowledge encoded in\nlarge language models. However, these models lack grounding of robot plans to\ndomain-specific knowledge and are not easily transferable between multiple\nsites or customers with different compliance needs. In this work, we present\nSayComply, which enables grounding robotic task planning with operational\ncompliance using retrieval-based language models. We design a hierarchical\ndatabase of operational, environment, and robot embodiment manuals and\nprocedures to enable efficient retrieval of the relevant context under the\nlimited context length of the LLMs. We then design a task planner using a\ntree-based retrieval augmented generation (RAG) technique to generate robot\ntasks that follow user instructions while simultaneously complying with the\ndomain knowledge in the database. We demonstrate the benefits of our approach\nthrough simulations and hardware experiments in real-world scenarios that\nrequire precise context retrieval across various types of context,\noutperforming the standard RAG method. Our approach bridges the gap in\ndeploying robots that consistently adhere to operational protocols, offering a\nscalable and edge-deployable solution for ensuring compliance across varied and\ncomplex real-world environments. Project website: saycomply.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of task planning for robots that must comply\nwith operational manuals in real-world settings. Task planning under these\nconstraints is essential for enabling autonomous robot operation in domains\nthat require adherence to domain-specific knowledge. Current methods for\ngenerating robot goals and plans rely on common sense knowledge encoded in\nlarge language models. However, these models lack grounding of robot plans to\ndomain-specific knowledge and are not easily transferable between multiple\nsites or customers with different compliance needs. In this work, we present\nSayComply, which enables grounding robotic task planning with operational\ncompliance using retrieval-based language models. We design a hierarchical\ndatabase of operational, environment, and robot embodiment manuals and\nprocedures to enable efficient retrieval of the relevant context under the\nlimited context length of the LLMs. We then design a task planner using a\ntree-based retrieval augmented generation (RAG) technique to generate robot\ntasks that follow user instructions while simultaneously complying with the\ndomain knowledge in the database. We demonstrate the benefits of our approach\nthrough simulations and hardware experiments in real-world scenarios that\nrequire precise context retrieval across various types of context,\noutperforming the standard RAG method. Our approach bridges the gap in\ndeploying robots that consistently adhere to operational protocols, offering a\nscalable and edge-deployable solution for ensuring compliance across varied and\ncomplex real-world environments. Project website: saycomply.github.io."
                },
                "authors": [
                    {
                        "name": "Muhammad Fadhil Ginting"
                    },
                    {
                        "name": "Dong-Ki Kim"
                    },
                    {
                        "name": "Sung-Kyun Kim"
                    },
                    {
                        "name": "Bandi Jai Krishna"
                    },
                    {
                        "name": "Mykel J. Kochenderfer"
                    },
                    {
                        "name": "Shayegan Omidshafiei"
                    },
                    {
                        "name": "Ali-akbar Agha-mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Ali-akbar Agha-mohammadi"
                },
                "author": "Ali-akbar Agha-mohammadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19979v3",
                "updated": "2024-11-18T06:28:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    28,
                    1,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-30T06:07:12Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    7,
                    12,
                    0,
                    274,
                    0
                ],
                "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model"
                },
                "summary": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimi Suzuki"
                },
                "author": "Yoshimi Suzuki",
                "arxiv_comment": "Long paper accepted to EMNLP 2024 Main. 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v2",
                "updated": "2024-11-18T06:14:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    6,
                    14,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08527v2",
                "updated": "2024-11-18T05:47:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    47,
                    10,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-12T08:31:34Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    8,
                    31,
                    34,
                    2,
                    164,
                    0
                ],
                "title": "Optimized Feature Generation for Tabular Data via LLMs with Decision\n  Tree Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Feature Generation for Tabular Data via LLMs with Decision\n  Tree Reasoning"
                },
                "summary": "In tabular prediction tasks, tree-based models combined with automated\nfeature engineering methods often outperform deep learning approaches that rely\non learned representations. While these feature engineering techniques are\neffective, they typically depend on a pre-defined search space and primarily\nuse validation scores for feature selection, thereby missing valuable insights\nfrom previous experiments. To address these limitations, we propose a novel\ntabular learning framework that utilizes large language models (LLMs), termed\nOptimizing Column feature generator with decision Tree reasoning (OCTree). Our\nkey idea is to leverage the reasoning capabilities of LLMs to identify\neffective feature generation rules without manually specifying the search space\nand provide language-based reasoning information highlighting past experiments\nas feedback for iterative rule improvements. We use decision trees to convey\nthis reasoning information, as they can be easily represented in natural\nlanguage, effectively providing knowledge from prior experiments (i.e., the\nimpact of the generated features on performance) to the LLMs. Our empirical\nresults demonstrate that OCTree consistently enhances the performance of\nvarious prediction models across diverse benchmarks, outperforming competing\nautomated feature engineering methods. Code is available at\nhttps://github.com/jaehyun513/OCTree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In tabular prediction tasks, tree-based models combined with automated\nfeature engineering methods often outperform deep learning approaches that rely\non learned representations. While these feature engineering techniques are\neffective, they typically depend on a pre-defined search space and primarily\nuse validation scores for feature selection, thereby missing valuable insights\nfrom previous experiments. To address these limitations, we propose a novel\ntabular learning framework that utilizes large language models (LLMs), termed\nOptimizing Column feature generator with decision Tree reasoning (OCTree). Our\nkey idea is to leverage the reasoning capabilities of LLMs to identify\neffective feature generation rules without manually specifying the search space\nand provide language-based reasoning information highlighting past experiments\nas feedback for iterative rule improvements. We use decision trees to convey\nthis reasoning information, as they can be easily represented in natural\nlanguage, effectively providing knowledge from prior experiments (i.e., the\nimpact of the generated features on performance) to the LLMs. Our empirical\nresults demonstrate that OCTree consistently enhances the performance of\nvarious prediction models across diverse benchmarks, outperforming competing\nautomated feature engineering methods. Code is available at\nhttps://github.com/jaehyun513/OCTree."
                },
                "authors": [
                    {
                        "name": "Jaehyun Nam"
                    },
                    {
                        "name": "Kyuyoung Kim"
                    },
                    {
                        "name": "Seunghyuk Oh"
                    },
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11295v1",
                "updated": "2024-11-18T05:41:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    41,
                    27,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:41:27Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    41,
                    27,
                    0,
                    323,
                    0
                ],
                "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource\n  Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource\n  Language Translation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of tasks and domains. However, their performance in low-resource\nlanguage translation, particularly when translating into these languages,\nremains underexplored. This gap poses significant challenges, as linguistic\nbarriers hinder the cultural preservation and development of minority\ncommunities. To address this issue, this paper introduces a novel\nretrieval-based method that enhances translation quality for low-resource\nlanguages by focusing on key terms, which involves translating keywords and\nretrieving corresponding examples from existing data. To evaluate the\neffectiveness of this method, we conducted experiments translating from English\ninto three low-resource languages: Cherokee, a critically endangered indigenous\nlanguage of North America; Tibetan, a historically and culturally significant\nlanguage in Asia; and Manchu, a language with few remaining speakers. Our\ncomparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B,\nhighlights the significant challenges these models face when translating into\nlow-resource languages. In contrast, our retrieval-based method shows promise\nin improving both word-level accuracy and overall semantic understanding by\nleveraging existing resources more effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of tasks and domains. However, their performance in low-resource\nlanguage translation, particularly when translating into these languages,\nremains underexplored. This gap poses significant challenges, as linguistic\nbarriers hinder the cultural preservation and development of minority\ncommunities. To address this issue, this paper introduces a novel\nretrieval-based method that enhances translation quality for low-resource\nlanguages by focusing on key terms, which involves translating keywords and\nretrieving corresponding examples from existing data. To evaluate the\neffectiveness of this method, we conducted experiments translating from English\ninto three low-resource languages: Cherokee, a critically endangered indigenous\nlanguage of North America; Tibetan, a historically and culturally significant\nlanguage in Asia; and Manchu, a language with few remaining speakers. Our\ncomparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B,\nhighlights the significant challenges these models face when translating into\nlow-resource languages. In contrast, our retrieval-based method shows promise\nin improving both word-level accuracy and overall semantic understanding by\nleveraging existing resources more effectively."
                },
                "authors": [
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Huaqin Zhao"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Constance Owl"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Claudio Saunt"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03816v3",
                "updated": "2024-11-18T05:36:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    36,
                    16,
                    0,
                    323,
                    0
                ],
                "published": "2024-06-06T07:40:00Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    7,
                    40,
                    0,
                    3,
                    158,
                    0
                ],
                "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"
                },
                "summary": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM. We release all code at\nhttps://github.com/THUDM/ReST-MCTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM. We release all code at\nhttps://github.com/THUDM/ReST-MCTS."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07950v3",
                "updated": "2024-11-18T05:30:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    30,
                    50,
                    0,
                    323,
                    0
                ],
                "published": "2024-01-15T20:22:21Z",
                "published_parsed": [
                    2024,
                    1,
                    15,
                    20,
                    22,
                    21,
                    0,
                    15,
                    0
                ],
                "title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for\n  Training Scientific Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for\n  Training Scientific Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown promise in assisting scientific\ndiscovery. However, such applications are currently limited by LLMs'\ndeficiencies in understanding intricate scientific concepts, deriving symbolic\nequations, and solving advanced numerical calculations. To bridge these gaps,\nwe introduce SciInstruct, a suite of scientific instructions for training\nscientific language models capable of college-level scientific reasoning.\nCentral to our approach is a novel self-reflective instruction annotation\nframework to address the data scarcity challenge in the science domain. This\nframework leverages existing LLMs to generate step-by-step reasoning for\nunlabelled scientific questions, followed by a process of self-reflective\ncritic-and-revise. Applying this framework, we curated a diverse and\nhigh-quality dataset encompassing physics, chemistry, math, and formal proofs.\nWe analyze the curated SciInstruct from multiple interesting perspectives\n(e.g., domain, scale, source, question type, answer length, etc.). To verify\nthe effectiveness of SciInstruct, we fine-tuned different language models with\nSciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B:\nMetaMath, enhancing their scientific and mathematical reasoning capabilities,\nwithout sacrificing the language understanding capabilities of the base model.\nWe release all codes and SciInstruct at https://github.com/THUDM/SciGLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in assisting scientific\ndiscovery. However, such applications are currently limited by LLMs'\ndeficiencies in understanding intricate scientific concepts, deriving symbolic\nequations, and solving advanced numerical calculations. To bridge these gaps,\nwe introduce SciInstruct, a suite of scientific instructions for training\nscientific language models capable of college-level scientific reasoning.\nCentral to our approach is a novel self-reflective instruction annotation\nframework to address the data scarcity challenge in the science domain. This\nframework leverages existing LLMs to generate step-by-step reasoning for\nunlabelled scientific questions, followed by a process of self-reflective\ncritic-and-revise. Applying this framework, we curated a diverse and\nhigh-quality dataset encompassing physics, chemistry, math, and formal proofs.\nWe analyze the curated SciInstruct from multiple interesting perspectives\n(e.g., domain, scale, source, question type, answer length, etc.). To verify\nthe effectiveness of SciInstruct, we fine-tuned different language models with\nSciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B:\nMetaMath, enhancing their scientific and mathematical reasoning capabilities,\nwithout sacrificing the language understanding capabilities of the base model.\nWe release all codes and SciInstruct at https://github.com/THUDM/SciGLM."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "Accepted to NeurIPS D&B Track 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14782v2",
                "updated": "2024-11-18T05:27:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    27,
                    38,
                    0,
                    323,
                    0
                ],
                "published": "2023-11-24T16:32:47Z",
                "published_parsed": [
                    2023,
                    11,
                    24,
                    16,
                    32,
                    47,
                    4,
                    328,
                    0
                ],
                "title": "Understanding the Role of Textual Prompts in LLM for Time Series\n  Forecasting: an Adapter View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Role of Textual Prompts in LLM for Time Series\n  Forecasting: an Adapter View"
                },
                "summary": "In the burgeoning domain of Large Language Models (LLMs), there is a growing\ninterest in applying LLM to time series forecasting, with multiple studies\nfocused on leveraging textual prompts to further enhance the predictive\nprowess. This study aims to understand how and why the integration of textual\nprompts into LLM can effectively improve the prediction accuracy of time\nseries, which is not obvious at the glance, given the significant domain gap\nbetween texts and time series. Our extensive examination leads us to believe\nthat (a) adding text prompts is roughly equivalent to introducing additional\nadapters, and (b) It is the introduction of learnable parameters rather than\ntextual information that aligns the LLM with the time series forecasting task,\nultimately enhancing prediction accuracy. Inspired by this discovery, we\ndeveloped four adapters that explicitly address the gap between LLM and time\nseries, and further improve the prediction accuracy. Overall,our work\nhighlights how textual prompts enhance LLM accuracy in time series forecasting\nand suggests new avenues for continually improving LLM-based time series\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the burgeoning domain of Large Language Models (LLMs), there is a growing\ninterest in applying LLM to time series forecasting, with multiple studies\nfocused on leveraging textual prompts to further enhance the predictive\nprowess. This study aims to understand how and why the integration of textual\nprompts into LLM can effectively improve the prediction accuracy of time\nseries, which is not obvious at the glance, given the significant domain gap\nbetween texts and time series. Our extensive examination leads us to believe\nthat (a) adding text prompts is roughly equivalent to introducing additional\nadapters, and (b) It is the introduction of learnable parameters rather than\ntextual information that aligns the LLM with the time series forecasting task,\nultimately enhancing prediction accuracy. Inspired by this discovery, we\ndeveloped four adapters that explicitly address the gap between LLM and time\nseries, and further improve the prediction accuracy. Overall,our work\nhighlights how textual prompts enhance LLM accuracy in time series forecasting\nand suggests new avenues for continually improving LLM-based time series\nanalysis."
                },
                "authors": [
                    {
                        "name": "Peisong Niu"
                    },
                    {
                        "name": "Tian Zhou"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Liang Sun"
                    },
                    {
                        "name": "Rong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Rong Jin"
                },
                "author": "Rong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.14782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12311v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12311v3",
                "updated": "2024-11-18T05:23:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    23,
                    42,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-16T07:24:28Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    7,
                    24,
                    28,
                    2,
                    290,
                    0
                ],
                "title": "Open Domain Question Answering with Conflicting Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Domain Question Answering with Conflicting Contexts"
                },
                "summary": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Qiang Ning"
                    },
                    {
                        "name": "Kishaloy Halder"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Zheng Qi"
                    },
                    {
                        "name": "Phu Mon Htut"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Neha Anna John"
                    },
                    {
                        "name": "Bonan Min"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12311v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01585v3",
                "updated": "2024-11-18T05:18:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    18,
                    51,
                    0,
                    323,
                    0
                ],
                "published": "2024-08-02T21:54:13Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    21,
                    54,
                    13,
                    4,
                    215,
                    0
                ],
                "title": "LibreLog: Accurate and Efficient Unsupervised Log Parsing Using\n  Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibreLog: Accurate and Efficient Unsupervised Log Parsing Using\n  Open-Source Large Language Models"
                },
                "summary": "Log parsing is a critical step that transforms unstructured log data into\nstructured formats, facilitating subsequent log-based analysis. Traditional\nsyntax-based log parsers are efficient and effective, but they often experience\ndecreased accuracy when processing logs that deviate from the predefined rules.\nRecently, large language models (LLM) based log parsers have shown superior\nparsing accuracy. However, existing LLM-based parsers face three main\nchallenges: 1)time-consuming and labor-intensive manual labeling for\nfine-tuning or in-context learning, 2)increased parsing costs due to the vast\nvolume of log data and limited context size of LLMs, and 3)privacy risks from\nusing commercial models like ChatGPT with sensitive log information. To\novercome these limitations, this paper introduces LibreLog, an unsupervised log\nparsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance\nprivacy and reduce operational costs while achieving state-of-the-art parsing\naccuracy. LibreLog first groups logs with similar static text but varying\ndynamic variables using a fixed-depth grouping tree. It then parses logs within\nthese groups using three components: i)similarity scoring-based retrieval\naugmented generation: selects diverse logs within each group based on Jaccard\nsimilarity, helping the LLM distinguish between static text and dynamic\nvariables; ii)self-reflection: iteratively query LLMs to refine log templates\nto improve parsing accuracy; and iii) log template memory: stores parsed\ntemplates to reduce LLM queries for improved parsing efficiency. Our evaluation\non LogHub-2.0 shows that LibreLog achieves 25% higher parsing accuracy and\nprocesses logs 2.7 times faster compared to state-of-the-art LLM-based parsers.\nIn short, LibreLog addresses privacy and cost concerns of using commercial LLMs\nwhile achieving state-of-the-arts parsing efficiency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log parsing is a critical step that transforms unstructured log data into\nstructured formats, facilitating subsequent log-based analysis. Traditional\nsyntax-based log parsers are efficient and effective, but they often experience\ndecreased accuracy when processing logs that deviate from the predefined rules.\nRecently, large language models (LLM) based log parsers have shown superior\nparsing accuracy. However, existing LLM-based parsers face three main\nchallenges: 1)time-consuming and labor-intensive manual labeling for\nfine-tuning or in-context learning, 2)increased parsing costs due to the vast\nvolume of log data and limited context size of LLMs, and 3)privacy risks from\nusing commercial models like ChatGPT with sensitive log information. To\novercome these limitations, this paper introduces LibreLog, an unsupervised log\nparsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance\nprivacy and reduce operational costs while achieving state-of-the-art parsing\naccuracy. LibreLog first groups logs with similar static text but varying\ndynamic variables using a fixed-depth grouping tree. It then parses logs within\nthese groups using three components: i)similarity scoring-based retrieval\naugmented generation: selects diverse logs within each group based on Jaccard\nsimilarity, helping the LLM distinguish between static text and dynamic\nvariables; ii)self-reflection: iteratively query LLMs to refine log templates\nto improve parsing accuracy; and iii) log template memory: stores parsed\ntemplates to reduce LLM queries for improved parsing efficiency. Our evaluation\non LogHub-2.0 shows that LibreLog achieves 25% higher parsing accuracy and\nprocesses logs 2.7 times faster compared to state-of-the-art LLM-based parsers.\nIn short, LibreLog addresses privacy and cost concerns of using commercial LLMs\nwhile achieving state-of-the-arts parsing efficiency and accuracy."
                },
                "authors": [
                    {
                        "name": "Zeyang Ma"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11289v1",
                "updated": "2024-11-18T05:17:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    17,
                    27,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:17:27Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    17,
                    27,
                    0,
                    323,
                    0
                ],
                "title": "LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large\n  Language Models"
                },
                "summary": "Creating high-quality, large-scale datasets for large language models (LLMs)\noften relies on resource-intensive, GPU-accelerated models for quality\nfiltering, making the process time-consuming and costly. This dependence on\nGPUs limits accessibility for organizations lacking significant computational\ninfrastructure. To address this issue, we introduce the Lightweight,\nPurpose-driven (LP) Data Pipeline, a framework that operates entirely on CPUs\nto streamline the processes of dataset extraction, filtering, and curation.\nBased on our four core principles, the LP Data Pipeline significantly reduces\npreparation time and cost while maintaining high data quality. Importantly, our\npipeline enables the creation of purpose-driven datasets tailored to specific\ndomains and languages, enhancing the applicability of LLMs in specialized\ncontexts. We anticipate that our pipeline will lower the barriers to LLM\ndevelopment, enabling a wide range of organizations to access LLMs more easily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating high-quality, large-scale datasets for large language models (LLMs)\noften relies on resource-intensive, GPU-accelerated models for quality\nfiltering, making the process time-consuming and costly. This dependence on\nGPUs limits accessibility for organizations lacking significant computational\ninfrastructure. To address this issue, we introduce the Lightweight,\nPurpose-driven (LP) Data Pipeline, a framework that operates entirely on CPUs\nto streamline the processes of dataset extraction, filtering, and curation.\nBased on our four core principles, the LP Data Pipeline significantly reduces\npreparation time and cost while maintaining high data quality. Importantly, our\npipeline enables the creation of purpose-driven datasets tailored to specific\ndomains and languages, enhancing the applicability of LLMs in specialized\ncontexts. We anticipate that our pipeline will lower the barriers to LLM\ndevelopment, enabling a wide range of organizations to access LLMs more easily."
                },
                "authors": [
                    {
                        "name": "Yungi Kim"
                    },
                    {
                        "name": "Hyunsoo Ha"
                    },
                    {
                        "name": "Seonghoon Yang"
                    },
                    {
                        "name": "Sukyung Lee"
                    },
                    {
                        "name": "Jihoo Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanjun Park"
                },
                "author": "Chanjun Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11285v1",
                "updated": "2024-11-18T05:11:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    11,
                    29,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:11:29Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    11,
                    29,
                    0,
                    323,
                    0
                ],
                "title": "Zero-Shot Automatic Annotation and Instance Segmentation using\n  LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for\n  Deep Learning Model Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Automatic Annotation and Instance Segmentation using\n  LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for\n  Deep Learning Model Development"
                },
                "summary": "Currently, deep learning-based instance segmentation for various applications\n(e.g., Agriculture) is predominantly performed using a labor-intensive process\ninvolving extensive field data collection using sophisticated sensors, followed\nby careful manual annotation of images, presenting significant logistical and\nfinancial challenges to researchers and organizations. The process also slows\ndown the model development and training process. In this study, we presented a\nnovel method for deep learning-based instance segmentation of apples in\ncommercial orchards that eliminates the need for labor-intensive field data\ncollection and manual annotation. Utilizing a Large Language Model (LLM), we\nsynthetically generated orchard images and automatically annotated them using\nthe Segment Anything Model (SAM) integrated with a YOLO11 base model. This\nmethod significantly reduces reliance on physical sensors and manual data\nprocessing, presenting a major advancement in \"Agricultural AI\". The synthetic,\nauto-annotated dataset was used to train the YOLO11 model for Apple instance\nsegmentation, which was then validated on real orchard images. The results\nshowed that the automatically generated annotations achieved a Dice Coefficient\nof 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask\nannotations. All YOLO11 configurations, trained solely on these synthetic\ndatasets with automated annotations, accurately recognized and delineated\napples, highlighting the method's efficacy. Specifically, the YOLO11m-seg\nconfiguration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on\ntest images collected from a commercial orchard. Additionally, the YOLO11l-seg\nconfiguration outperformed other models in validation on 40 LLM-generated\nimages, achieving the highest mask precision and mAP@50 metrics.\n  Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, deep learning-based instance segmentation for various applications\n(e.g., Agriculture) is predominantly performed using a labor-intensive process\ninvolving extensive field data collection using sophisticated sensors, followed\nby careful manual annotation of images, presenting significant logistical and\nfinancial challenges to researchers and organizations. The process also slows\ndown the model development and training process. In this study, we presented a\nnovel method for deep learning-based instance segmentation of apples in\ncommercial orchards that eliminates the need for labor-intensive field data\ncollection and manual annotation. Utilizing a Large Language Model (LLM), we\nsynthetically generated orchard images and automatically annotated them using\nthe Segment Anything Model (SAM) integrated with a YOLO11 base model. This\nmethod significantly reduces reliance on physical sensors and manual data\nprocessing, presenting a major advancement in \"Agricultural AI\". The synthetic,\nauto-annotated dataset was used to train the YOLO11 model for Apple instance\nsegmentation, which was then validated on real orchard images. The results\nshowed that the automatically generated annotations achieved a Dice Coefficient\nof 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask\nannotations. All YOLO11 configurations, trained solely on these synthetic\ndatasets with automated annotations, accurately recognized and delineated\napples, highlighting the method's efficacy. Specifically, the YOLO11m-seg\nconfiguration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on\ntest images collected from a commercial orchard. Additionally, the YOLO11l-seg\nconfiguration outperformed other models in validation on 40 LLM-generated\nimages, achieving the highest mask precision and mAP@50 metrics.\n  Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Achyut Paudel"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06220v2",
                "updated": "2024-11-18T05:00:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    0,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-10T05:08:26Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    8,
                    26,
                    1,
                    254,
                    0
                ],
                "title": "CerviXpert: A Multi-Structural Convolutional Neural Network for\n  Predicting Cervix Type and Cervical Cell Abnormalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CerviXpert: A Multi-Structural Convolutional Neural Network for\n  Predicting Cervix Type and Cervical Cell Abnormalities"
                },
                "summary": "Cervical cancer is a major cause of cancer-related mortality among women\nworldwide, and its survival rate improves significantly with early detection.\nTraditional diagnostic methods such as Pap smears and cervical biopsies rely\nheavily on cytologist expertise, making the process prone to human error. This\nstudy introduces CerviXpert, a multi-structural convolutional neural network\nmodel designed to efficiently classify cervix types and detect cervical cell\nabnormalities. CerviXpert is built as a computationally efficient model that\nclassifies cervical cancer using images from the publicly available SiPaKMeD\ndataset. The model architecture emphasizes simplicity, using a limited number\nof convolutional layers followed by max pooling and dense layers, trained from\nscratch.\n  We assessed the performance of CerviXpert against other state of the art\nconvolutional neural network models including ResNet50, VGG16, MobileNetV2, and\nInceptionV3, evaluating them on accuracy, computational efficiency, and\nrobustness using five fold cross validation. CerviXpert achieved an accuracy of\n98.04 percent in classifying cervical cell abnormalities into three classes and\n98.60 percent for five class cervix type classification, outperforming\nMobileNetV2 and InceptionV3 in both accuracy and computational requirements. It\nshowed comparable results to ResNet50 and VGG16 while reducing computational\ncomplexity and resource needs.\n  CerviXpert provides an effective solution for cervical cancer screening and\ndiagnosis, balancing accuracy with computational efficiency. Its streamlined\ndesign enables deployment in resource constrained environments, potentially\nenhancing early detection and management of cervical cancer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cervical cancer is a major cause of cancer-related mortality among women\nworldwide, and its survival rate improves significantly with early detection.\nTraditional diagnostic methods such as Pap smears and cervical biopsies rely\nheavily on cytologist expertise, making the process prone to human error. This\nstudy introduces CerviXpert, a multi-structural convolutional neural network\nmodel designed to efficiently classify cervix types and detect cervical cell\nabnormalities. CerviXpert is built as a computationally efficient model that\nclassifies cervical cancer using images from the publicly available SiPaKMeD\ndataset. The model architecture emphasizes simplicity, using a limited number\nof convolutional layers followed by max pooling and dense layers, trained from\nscratch.\n  We assessed the performance of CerviXpert against other state of the art\nconvolutional neural network models including ResNet50, VGG16, MobileNetV2, and\nInceptionV3, evaluating them on accuracy, computational efficiency, and\nrobustness using five fold cross validation. CerviXpert achieved an accuracy of\n98.04 percent in classifying cervical cell abnormalities into three classes and\n98.60 percent for five class cervix type classification, outperforming\nMobileNetV2 and InceptionV3 in both accuracy and computational requirements. It\nshowed comparable results to ResNet50 and VGG16 while reducing computational\ncomplexity and resource needs.\n  CerviXpert provides an effective solution for cervical cancer screening and\ndiagnosis, balancing accuracy with computational efficiency. Its streamlined\ndesign enables deployment in resource constrained environments, potentially\nenhancing early detection and management of cervical cancer."
                },
                "authors": [
                    {
                        "name": "Rashik Shahriar Akash"
                    },
                    {
                        "name": "Radiful Islam"
                    },
                    {
                        "name": "S. M. Saiful Islam Badhon"
                    },
                    {
                        "name": "K. S. M. Tozammel Hossain"
                    }
                ],
                "author_detail": {
                    "name": "K. S. M. Tozammel Hossain"
                },
                "author": "K. S. M. Tozammel Hossain",
                "arxiv_doi": "10.1177/20552076241295440",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/20552076241295440",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 figures, 9 tables",
                "arxiv_journal_ref": "DIGITAL HEALTH, Vol. 10, 2024,",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11266v1",
                "updated": "2024-11-18T03:45:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T03:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "VersaTune: Fine-Tuning Multi-Ability LLMs Efficiently",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaTune: Fine-Tuning Multi-Ability LLMs Efficiently"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable capabilities in handling\nmultiple tasks across domains due to their emergent properties. These\ncapabilities are further augmented during the Supervised Fine-Tuning (SFT)\nphase. Despite their potential, existing work mainly focuses on domain-specific\nenhancements during fine-tuning, the challenge of which lies in catastrophic\nforgetting of knowledge across other domains. In this study, we introduce\nVersaTune, a novel data composition framework designed for enhancing LLMs'\noverall multi-ability performances during fine-tuning. We categorize knowledge\ninto distinct domains including law, medicine, finance, science, code. We begin\nwith detecting the distribution of domain-specific knowledge within the base\nmodel, followed by the composition of training data that aligns with the\nmodel's existing knowledge distribution. During the fine-tuning process,\nweights of different domains are dynamically adjusted based on their learnable\npotential and forgetting degree. Experimental results demonstrate that\nVersaTune achieves significant improvements in multi-domain performance, with a\n35.21% enhancement in comprehensive multi-domain tasks. Additionally, in\nscenarios where specific domain optimization is required, VersaTune reduces the\ndegradation of performance in other domains by 38.77%, without compromising the\ntarget domain's training efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable capabilities in handling\nmultiple tasks across domains due to their emergent properties. These\ncapabilities are further augmented during the Supervised Fine-Tuning (SFT)\nphase. Despite their potential, existing work mainly focuses on domain-specific\nenhancements during fine-tuning, the challenge of which lies in catastrophic\nforgetting of knowledge across other domains. In this study, we introduce\nVersaTune, a novel data composition framework designed for enhancing LLMs'\noverall multi-ability performances during fine-tuning. We categorize knowledge\ninto distinct domains including law, medicine, finance, science, code. We begin\nwith detecting the distribution of domain-specific knowledge within the base\nmodel, followed by the composition of training data that aligns with the\nmodel's existing knowledge distribution. During the fine-tuning process,\nweights of different domains are dynamically adjusted based on their learnable\npotential and forgetting degree. Experimental results demonstrate that\nVersaTune achieves significant improvements in multi-domain performance, with a\n35.21% enhancement in comprehensive multi-domain tasks. Additionally, in\nscenarios where specific domain optimization is required, VersaTune reduces the\ndegradation of performance in other domains by 38.77%, without compromising the\ntarget domain's training efficacy."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v5",
                "updated": "2024-11-18T03:17:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    17,
                    32,
                    0,
                    323,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Lawrence KQ Yan"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Silin Chen"
                },
                "author": "Silin Chen",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.10285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.10285v2",
                "updated": "2024-11-18T02:50:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    50,
                    46,
                    0,
                    323,
                    0
                ],
                "published": "2022-11-18T15:17:28Z",
                "published_parsed": [
                    2022,
                    11,
                    18,
                    15,
                    17,
                    28,
                    4,
                    322,
                    0
                ],
                "title": "A Fair Loss Function for Network Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fair Loss Function for Network Pruning"
                },
                "summary": "Model pruning can enable the deployment of neural networks in environments\nwith resource constraints. While pruning may have a small effect on the overall\nperformance of the model, it can exacerbate existing biases into the model such\nthat subsets of samples see significantly degraded performance. In this paper,\nwe introduce the performance weighted loss function, a simple modified\ncross-entropy loss function that can be used to limit the introduction of\nbiases during pruning. Experiments using the CelebA, Fitzpatrick17k and\nCIFAR-10 datasets demonstrate that the proposed method is a simple and\neffective tool that can enable existing pruning methods to be used in fairness\nsensitive contexts. Code used to produce all experiments contained in this\npaper can be found at https://github.com/robbiemeyer/pw_loss_pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model pruning can enable the deployment of neural networks in environments\nwith resource constraints. While pruning may have a small effect on the overall\nperformance of the model, it can exacerbate existing biases into the model such\nthat subsets of samples see significantly degraded performance. In this paper,\nwe introduce the performance weighted loss function, a simple modified\ncross-entropy loss function that can be used to limit the introduction of\nbiases during pruning. Experiments using the CelebA, Fitzpatrick17k and\nCIFAR-10 datasets demonstrate that the proposed method is a simple and\neffective tool that can enable existing pruning methods to be used in fairness\nsensitive contexts. Code used to produce all experiments contained in this\npaper can be found at https://github.com/robbiemeyer/pw_loss_pruning."
                },
                "authors": [
                    {
                        "name": "Robbie Meyer"
                    },
                    {
                        "name": "Alexander Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Wong"
                },
                "author": "Alexander Wong",
                "arxiv_comment": "[v1] Trustworthy and Socially Responsible Machine Learning (TSRML\n  2022) workshop co-located with NeurIPS 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.10285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.10285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18634v2",
                "updated": "2024-11-18T02:42:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    42,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-05-28T22:33:02Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    22,
                    33,
                    2,
                    1,
                    149,
                    0
                ],
                "title": "A Theoretical Understanding of Self-Correction through In-context\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Theoretical Understanding of Self-Correction through In-context\n  Alignment"
                },
                "summary": "Going beyond mimicking limited human experiences, recent studies show initial\nevidence that, like humans, large language models (LLMs) are capable of\nimproving their abilities purely by self-correction, i.e., correcting previous\nresponses through self-examination, in certain circumstances. Nevertheless,\nlittle is known about how such capabilities arise. In this work, based on a\nsimplified setup akin to an alignment task, we theoretically analyze\nself-correction from an in-context learning perspective, showing that when LLMs\ngive relatively accurate self-examinations as rewards, they are capable of\nrefining responses in an in-context way. Notably, going beyond previous\ntheories on over-simplified linear transformers, our theoretical construction\nunderpins the roles of several key designs of realistic transformers for\nself-correction: softmax attention, multi-head attention, and the MLP block. We\nvalidate these findings extensively on synthetic datasets. Inspired by these\nfindings, we also illustrate novel applications of self-correction, such as\ndefending against LLM jailbreaks, where a simple self-correction step does make\na large difference. We believe that these findings will inspire further\nresearch on understanding, exploiting, and enhancing self-correction for\nbuilding better foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going beyond mimicking limited human experiences, recent studies show initial\nevidence that, like humans, large language models (LLMs) are capable of\nimproving their abilities purely by self-correction, i.e., correcting previous\nresponses through self-examination, in certain circumstances. Nevertheless,\nlittle is known about how such capabilities arise. In this work, based on a\nsimplified setup akin to an alignment task, we theoretically analyze\nself-correction from an in-context learning perspective, showing that when LLMs\ngive relatively accurate self-examinations as rewards, they are capable of\nrefining responses in an in-context way. Notably, going beyond previous\ntheories on over-simplified linear transformers, our theoretical construction\nunderpins the roles of several key designs of realistic transformers for\nself-correction: softmax attention, multi-head attention, and the MLP block. We\nvalidate these findings extensively on synthetic datasets. Inspired by these\nfindings, we also illustrate novel applications of self-correction, such as\ndefending against LLM jailbreaks, where a simple self-correction step does make\na large difference. We believe that these findings will inspire further\nresearch on understanding, exploiting, and enhancing self-correction for\nbuilding better foundation models."
                },
                "authors": [
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yuyang Wu"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Stefanie Jegelka"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15297v2",
                "updated": "2024-11-18T02:13:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    13,
                    31,
                    0,
                    323,
                    0
                ],
                "published": "2024-10-20T05:57:10Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    5,
                    57,
                    10,
                    6,
                    294,
                    0
                ],
                "title": "Redefining Proactivity for Information Seeking Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Proactivity for Information Seeking Dialogue"
                },
                "summary": "Information-Seeking Dialogue (ISD) agents aim to provide accurate responses\nto user queries. While proficient in directly addressing user queries, these\nagents, as well as LLMs in general, predominantly exhibit reactive behavior,\nlacking the ability to generate proactive responses that actively engage users\nin sustained conversations. However, existing definitions of proactive dialogue\nin this context do not focus on how each response actively engages the user and\nsustains the conversation. Hence, we present a new definition of proactivity\nthat focuses on enhancing the `proactiveness' of each generated response via\nthe introduction of new information related to the initial query. To this end,\nwe construct a proactive dialogue dataset comprising 2,000 single-turn\nconversations, and introduce several automatic metrics to evaluate response\n`proactiveness' which achieved high correlation with human annotation.\nAdditionally, we introduce two innovative Chain-of-Thought (CoT) prompts, the\n3-step CoT and the 3-in-1 CoT prompts, which consistently outperform standard\nprompts by up to 90% in the zero-shot setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Seeking Dialogue (ISD) agents aim to provide accurate responses\nto user queries. While proficient in directly addressing user queries, these\nagents, as well as LLMs in general, predominantly exhibit reactive behavior,\nlacking the ability to generate proactive responses that actively engage users\nin sustained conversations. However, existing definitions of proactive dialogue\nin this context do not focus on how each response actively engages the user and\nsustains the conversation. Hence, we present a new definition of proactivity\nthat focuses on enhancing the `proactiveness' of each generated response via\nthe introduction of new information related to the initial query. To this end,\nwe construct a proactive dialogue dataset comprising 2,000 single-turn\nconversations, and introduce several automatic metrics to evaluate response\n`proactiveness' which achieved high correlation with human annotation.\nAdditionally, we introduce two innovative Chain-of-Thought (CoT) prompts, the\n3-step CoT and the 3-in-1 CoT prompts, which consistently outperform standard\nprompts by up to 90% in the zero-shot setting."
                },
                "authors": [
                    {
                        "name": "Jing Yang Lee"
                    },
                    {
                        "name": "Seokhwan Kim"
                    },
                    {
                        "name": "Kartik Mehta"
                    },
                    {
                        "name": "Jiun-Yu Kao"
                    },
                    {
                        "name": "Yu-Hsiang Lin"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11223v1",
                "updated": "2024-11-18T01:25:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    25,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T01:25:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    25,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Efficient Transfer Learning for Video-language Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Transfer Learning for Video-language Foundation Models"
                },
                "summary": "Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video."
                },
                "authors": [
                    {
                        "name": "Haoxing Chen"
                    },
                    {
                        "name": "Zizheng Huang"
                    },
                    {
                        "name": "Yan Hong"
                    },
                    {
                        "name": "Yanshuo Wang"
                    },
                    {
                        "name": "Zhongcai Lyu"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Zhangxuan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Zhangxuan Gu"
                },
                "author": "Zhangxuan Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11217v1",
                "updated": "2024-11-18T01:06:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    6,
                    12,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T01:06:12Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    6,
                    12,
                    0,
                    323,
                    0
                ],
                "title": "MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs"
                },
                "summary": "Efficient deployment of large language models, particularly Mixture of\nExperts (MoE), on resource-constrained platforms presents significant\nchallenges, especially in terms of computational efficiency and memory\nutilization. The MoE architecture, renowned for its ability to increase model\ncapacity without a proportional increase in inference cost, greatly reduces the\ntoken generation latency compared with dense models. However, the large model\nsize makes MoE models inaccessible to individuals without high-end GPUs. In\nthis paper, we propose a high-throughput MoE batch inference system, that\nsignificantly outperforms past work. MoE-Lightning introduces a novel\nCPU-GPU-I/O pipelining schedule, CGOPipe, with paged weights to achieve high\nresource utilization, and a performance model, HRM, based on a Hierarchical\nRoofline Model we introduce to help find policies with higher throughput than\nexisting systems. MoE-Lightning can achieve up to 10.3x higher throughput than\nstate-of-the-art offloading-enabled LLM inference systems for Mixtral 8x7B on a\nsingle T4 GPU (16GB). When the theoretical system throughput is bounded by the\nGPU memory, MoE-Lightning can reach the throughput upper bound with 2-3x less\nCPU memory, significantly increasing resource utilization. MoE-Lightning also\nsupports efficient batch inference for much larger MoEs (e.g., Mixtral 8x22B\nand DBRX) on multiple low-cost GPUs (e.g., 2-4 T4).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of large language models, particularly Mixture of\nExperts (MoE), on resource-constrained platforms presents significant\nchallenges, especially in terms of computational efficiency and memory\nutilization. The MoE architecture, renowned for its ability to increase model\ncapacity without a proportional increase in inference cost, greatly reduces the\ntoken generation latency compared with dense models. However, the large model\nsize makes MoE models inaccessible to individuals without high-end GPUs. In\nthis paper, we propose a high-throughput MoE batch inference system, that\nsignificantly outperforms past work. MoE-Lightning introduces a novel\nCPU-GPU-I/O pipelining schedule, CGOPipe, with paged weights to achieve high\nresource utilization, and a performance model, HRM, based on a Hierarchical\nRoofline Model we introduce to help find policies with higher throughput than\nexisting systems. MoE-Lightning can achieve up to 10.3x higher throughput than\nstate-of-the-art offloading-enabled LLM inference systems for Mixtral 8x7B on a\nsingle T4 GPU (16GB). When the theoretical system throughput is bounded by the\nGPU memory, MoE-Lightning can reach the throughput upper bound with 2-3x less\nCPU memory, significantly increasing resource utilization. MoE-Lightning also\nsupports efficient batch inference for much larger MoEs (e.g., Mixtral 8x22B\nand DBRX) on multiple low-cost GPUs (e.g., 2-4 T4)."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Tyler Griggs"
                    },
                    {
                        "name": "Peter Schafhalter"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11206v1",
                "updated": "2024-11-17T23:40:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    23,
                    40,
                    0,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T23:40:00Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    23,
                    40,
                    0,
                    6,
                    322,
                    0
                ],
                "title": "Capturing Sparks of Abstraction for the ARC Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Sparks of Abstraction for the ARC Challenge"
                },
                "summary": "Excellent progress has been made recently in solving ARC Challenge problems.\nHowever, it seems that new techniques may be required to push beyond 60%\naccuracy. Even commercial Large Language Models (LLMs) struggle to 'understand'\nmany of the problems (when given the input and output grids), which makes\ndiscovering solutions by LLM-lead program search somewhat futile.\n  In this work, LLM 'understanding' is attempted from a stronger starting\nposition : An LLM is given complete solutions to tasks in code, and then asked\nto explain how the task is being solved at various levels of abstraction.\nSpecifically, the LLM was given code solutions implemented in arc-dsl-llm (an\nLLM-legible version of Hodel's arc-dsl to obtain: (a) commented code; (b) code\nrefactored into reusable functional chunks; (c) problem solution steps; and (d)\nhigh-level problem-solving tactics.\n  We demonstrate that 'Sparks of Abstraction' can be extracted from the LLM\noutput - in a form that could be used in downstream tasks with Local LLMs\neligible to enter the ARC Prize.\n  Both the arc-dsl-llm DSL framework (with the re-engineered solutions) and the\nGemini LLM-generated data (along with the generation code) are made Open\nSource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excellent progress has been made recently in solving ARC Challenge problems.\nHowever, it seems that new techniques may be required to push beyond 60%\naccuracy. Even commercial Large Language Models (LLMs) struggle to 'understand'\nmany of the problems (when given the input and output grids), which makes\ndiscovering solutions by LLM-lead program search somewhat futile.\n  In this work, LLM 'understanding' is attempted from a stronger starting\nposition : An LLM is given complete solutions to tasks in code, and then asked\nto explain how the task is being solved at various levels of abstraction.\nSpecifically, the LLM was given code solutions implemented in arc-dsl-llm (an\nLLM-legible version of Hodel's arc-dsl to obtain: (a) commented code; (b) code\nrefactored into reusable functional chunks; (c) problem solution steps; and (d)\nhigh-level problem-solving tactics.\n  We demonstrate that 'Sparks of Abstraction' can be extracted from the LLM\noutput - in a form that could be used in downstream tasks with Local LLMs\neligible to enter the ARC Prize.\n  Both the arc-dsl-llm DSL framework (with the re-engineered solutions) and the\nGemini LLM-generated data (along with the generation code) are made Open\nSource."
                },
                "authors": [
                    {
                        "name": "Martin Andrews"
                    }
                ],
                "author_detail": {
                    "name": "Martin Andrews"
                },
                "author": "Martin Andrews",
                "arxiv_comment": "Submitted as a paper entry for the 2024 ARC Prize",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06304v2",
                "updated": "2024-11-17T23:22:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    23,
                    22,
                    18,
                    6,
                    322,
                    0
                ],
                "published": "2024-10-08T19:25:26Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    25,
                    26,
                    1,
                    282,
                    0
                ],
                "title": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language\n  Model Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language\n  Model Mathematical Reasoning"
                },
                "summary": "Hallucinations in large language models (LLMs) pose significant challenges in\ntasks requiring complex multi-step reasoning, such as mathematical\nproblem-solving. Existing approaches primarily detect the presence of\nhallucinations but lack a nuanced understanding of their types and\nmanifestations. In this paper, we first introduce a comprehensive taxonomy that\ncategorizes the common hallucinations in mathematical reasoning task into six\ntypes: fabrication, factual inconsistency, context inconsistency, instruction\ninconsistency, logical inconsistency, and logical error. We then propose FG-PRM\n(Fine-Grained Process Reward Model), an augmented model designed to detect and\nmitigate hallucinations in a fine-grained, step-level manner. To address the\nlimitations of manually labeling training data, we propose an automated method\nfor generating fine-grained hallucination data using LLMs. By injecting\nhallucinations into reasoning steps of correct solutions, we create a diverse\nand balanced synthetic dataset for training FG-PRM, which consists of six\nspecialized Process Reward Models (PRMs), each tailored to detect a specific\nhallucination type. Our FG-PRM demonstrates superior performance across two key\ntasks: 1) Fine-grained hallucination detection: classifying hallucination types\nfor each reasoning step; and 2) Verification: ranking multiple LLM-generated\noutputs to select the most accurate solution, mitigating reasoning\nhallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and\nClaude-3 on fine-grained hallucination detection and substantially boosts the\nperformance of LLMs on GSM8K and MATH benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language models (LLMs) pose significant challenges in\ntasks requiring complex multi-step reasoning, such as mathematical\nproblem-solving. Existing approaches primarily detect the presence of\nhallucinations but lack a nuanced understanding of their types and\nmanifestations. In this paper, we first introduce a comprehensive taxonomy that\ncategorizes the common hallucinations in mathematical reasoning task into six\ntypes: fabrication, factual inconsistency, context inconsistency, instruction\ninconsistency, logical inconsistency, and logical error. We then propose FG-PRM\n(Fine-Grained Process Reward Model), an augmented model designed to detect and\nmitigate hallucinations in a fine-grained, step-level manner. To address the\nlimitations of manually labeling training data, we propose an automated method\nfor generating fine-grained hallucination data using LLMs. By injecting\nhallucinations into reasoning steps of correct solutions, we create a diverse\nand balanced synthetic dataset for training FG-PRM, which consists of six\nspecialized Process Reward Models (PRMs), each tailored to detect a specific\nhallucination type. Our FG-PRM demonstrates superior performance across two key\ntasks: 1) Fine-grained hallucination detection: classifying hallucination types\nfor each reasoning step; and 2) Verification: ranking multiple LLM-generated\noutputs to select the most accurate solution, mitigating reasoning\nhallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and\nClaude-3 on fine-grained hallucination detection and substantially boosts the\nperformance of LLMs on GSM8K and MATH benchmarks."
                },
                "authors": [
                    {
                        "name": "Ruosen Li"
                    },
                    {
                        "name": "Ziming Luo"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11197v1",
                "updated": "2024-11-17T23:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    23,
                    15,
                    36,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T23:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    23,
                    15,
                    36,
                    6,
                    322,
                    0
                ],
                "title": "Stealing Training Graphs from Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealing Training Graphs from Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have shown promising results in modeling graphs\nin various tasks. The training of GNNs, especially on specialized tasks such as\nbioinformatics, demands extensive expert annotations, which are expensive and\nusually contain sensitive information of data providers. The trained GNN models\nare often shared for deployment in the real world. As neural networks can\nmemorize the training samples, the model parameters of GNNs have a high risk of\nleaking private training data. Our theoretical analysis shows the strong\nconnections between trained GNN parameters and the training graphs used,\nconfirming the training graph leakage issue. However, explorations into\ntraining data leakage from trained GNNs are rather limited. Therefore, we\ninvestigate a novel problem of stealing graphs from trained GNNs. To obtain\nhigh-quality graphs that resemble the target training set, a graph diffusion\nmodel with diffusion noise optimization is deployed as a graph generator.\nFurthermore, we propose a selection method that effectively leverages GNN model\nparameters to identify training graphs from samples generated by the graph\ndiffusion model. Extensive experiments on real-world datasets demonstrate the\neffectiveness of the proposed framework in stealing training graphs from the\ntrained GNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown promising results in modeling graphs\nin various tasks. The training of GNNs, especially on specialized tasks such as\nbioinformatics, demands extensive expert annotations, which are expensive and\nusually contain sensitive information of data providers. The trained GNN models\nare often shared for deployment in the real world. As neural networks can\nmemorize the training samples, the model parameters of GNNs have a high risk of\nleaking private training data. Our theoretical analysis shows the strong\nconnections between trained GNN parameters and the training graphs used,\nconfirming the training graph leakage issue. However, explorations into\ntraining data leakage from trained GNNs are rather limited. Therefore, we\ninvestigate a novel problem of stealing graphs from trained GNNs. To obtain\nhigh-quality graphs that resemble the target training set, a graph diffusion\nmodel with diffusion noise optimization is deployed as a graph generator.\nFurthermore, we propose a selection method that effectively leverages GNN model\nparameters to identify training graphs from samples generated by the graph\ndiffusion model. Extensive experiments on real-world datasets demonstrate the\neffectiveness of the proposed framework in stealing training graphs from the\ntrained GNN."
                },
                "authors": [
                    {
                        "name": "Minhua Lin"
                    },
                    {
                        "name": "Enyan Dai"
                    },
                    {
                        "name": "Junjie Xu"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "To be appeared in KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09553v2",
                "updated": "2024-11-17T22:53:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    22,
                    53,
                    9,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-14T16:06:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    6,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations"
                },
                "summary": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. These\nmulti-class positive annotations naturally fall within the in-distribution (ID)\nset. Unlabelled pixels may contain positive classes but also negative ones,\nincluding what is typically referred to as \\emph{background} in standard\nsegmentation formulations. Here, we forgo the need for background annotation\nand consider these together with any other unseen classes as part of the OOD\nset. Our framework can integrate, at a pixel-level, any OOD detection\napproaches designed for classification tasks. To address the lack of existing\nOOD datasets and established evaluation metric for medical image segmentation,\nwe propose a cross-validation strategy that treats held-out labelled classes as\nOOD. Extensive experiments on both multi-class hyperspectral and RGB surgical\nimaging datasets demonstrate the robustness and generalisation capability of\nour proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. These\nmulti-class positive annotations naturally fall within the in-distribution (ID)\nset. Unlabelled pixels may contain positive classes but also negative ones,\nincluding what is typically referred to as \\emph{background} in standard\nsegmentation formulations. Here, we forgo the need for background annotation\nand consider these together with any other unseen classes as part of the OOD\nset. Our framework can integrate, at a pixel-level, any OOD detection\napproaches designed for classification tasks. To address the lack of existing\nOOD datasets and established evaluation metric for medical image segmentation,\nwe propose a cross-validation strategy that treats held-out labelled classes as\nOOD. Extensive experiments on both multi-class hyperspectral and RGB surgical\nimaging datasets demonstrate the robustness and generalisation capability of\nour proposed framework."
                },
                "authors": [
                    {
                        "name": "Junwen Wang"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Oscar MacCormac"
                    },
                    {
                        "name": "Jonathan Shapey"
                    },
                    {
                        "name": "Tom Vercauteren"
                    }
                ],
                "author_detail": {
                    "name": "Tom Vercauteren"
                },
                "author": "Tom Vercauteren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08869v2",
                "updated": "2024-11-17T22:45:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    22,
                    45,
                    45,
                    6,
                    322,
                    0
                ],
                "published": "2024-10-11T14:46:49Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    46,
                    49,
                    4,
                    285,
                    0
                ],
                "title": "Evolution of SAE Features Across Layers in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of SAE Features Across Layers in LLMs"
                },
                "summary": "Sparse Autoencoders for transformer-based language models are typically\ndefined independently per layer. In this work we analyze statistical\nrelationships between features in adjacent layers to understand how features\nevolve through a forward pass. We provide a graph visualization interface for\nfeatures and their most similar next-layer neighbors\n(https://stefanhex.com/spar-2024/feature-browser/), and build communities of\nrelated features across layers. We find that a considerable amount of features\nare passed through from a previous layer, some features can be expressed as\nquasi-boolean combinations of previous features, and some features become more\nspecialized in later layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders for transformer-based language models are typically\ndefined independently per layer. In this work we analyze statistical\nrelationships between features in adjacent layers to understand how features\nevolve through a forward pass. We provide a graph visualization interface for\nfeatures and their most similar next-layer neighbors\n(https://stefanhex.com/spar-2024/feature-browser/), and build communities of\nrelated features across layers. We find that a considerable amount of features\nare passed through from a previous layer, some features can be expressed as\nquasi-boolean combinations of previous features, and some features become more\nspecialized in later layers."
                },
                "authors": [
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Benjamin Lerner"
                    },
                    {
                        "name": "Michael Oesterle"
                    },
                    {
                        "name": "Ediz Ucar"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "arxiv_comment": "Presented at the Attributing Model Behavior at Scale (ATTRIB)\n  workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20181v2",
                "updated": "2024-11-17T22:23:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    22,
                    23,
                    45,
                    6,
                    322,
                    0
                ],
                "published": "2024-07-26T15:24:01Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    15,
                    24,
                    1,
                    4,
                    208,
                    0
                ],
                "title": "Blockchain for Large Language Model Security and Safety: A Holistic\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain for Large Language Model Security and Safety: A Holistic\n  Survey"
                },
                "summary": "With the growing development and deployment of large language models (LLMs)\nin both industrial and academic fields, their security and safety concerns have\nbecome increasingly critical. However, recent studies indicate that LLMs face\nnumerous vulnerabilities, including data poisoning, prompt injections, and\nunauthorized data exposure, which conventional methods have struggled to\naddress fully. In parallel, blockchain technology, known for its data\nimmutability and decentralized structure, offers a promising foundation for\nsafeguarding LLMs. In this survey, we aim to comprehensively assess how to\nleverage blockchain technology to enhance LLMs' security and safety. Besides,\nwe propose a new taxonomy of blockchain for large language models (BC4LLMs) to\nsystematically categorize related works in this emerging field. Our analysis\nincludes novel frameworks and definitions to delineate security and safety in\nthe context of BC4LLMs, highlighting potential research directions and\nchallenges at this intersection. Through this study, we aim to stimulate\ntargeted advancements in blockchain-integrated LLM security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing development and deployment of large language models (LLMs)\nin both industrial and academic fields, their security and safety concerns have\nbecome increasingly critical. However, recent studies indicate that LLMs face\nnumerous vulnerabilities, including data poisoning, prompt injections, and\nunauthorized data exposure, which conventional methods have struggled to\naddress fully. In parallel, blockchain technology, known for its data\nimmutability and decentralized structure, offers a promising foundation for\nsafeguarding LLMs. In this survey, we aim to comprehensively assess how to\nleverage blockchain technology to enhance LLMs' security and safety. Besides,\nwe propose a new taxonomy of blockchain for large language models (BC4LLMs) to\nsystematically categorize related works in this emerging field. Our analysis\nincludes novel frameworks and definitions to delineate security and safety in\nthe context of BC4LLMs, highlighting potential research directions and\nchallenges at this intersection. Through this study, we aim to stimulate\ntargeted advancements in blockchain-integrated LLM security."
                },
                "authors": [
                    {
                        "name": "Caleb Geren"
                    },
                    {
                        "name": "Amanda Board"
                    },
                    {
                        "name": "Gaby G. Dagher"
                    },
                    {
                        "name": "Tim Andersen"
                    },
                    {
                        "name": "Jun Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhuang"
                },
                "author": "Jun Zhuang",
                "arxiv_comment": "Accepted to SIGKDD Explorations, to appear Dec 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19336v3",
                "updated": "2024-11-17T19:49:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    19,
                    49,
                    58,
                    6,
                    322,
                    0
                ],
                "published": "2024-04-30T08:03:22Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    8,
                    3,
                    22,
                    1,
                    121,
                    0
                ],
                "title": "Improving LLM Classification of Logical Errors by Integrating Error\n  Relationship into Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Classification of Logical Errors by Integrating Error\n  Relationship into Prompts"
                },
                "summary": "LLMs trained in the understanding of programming syntax are now providing\neffective assistance to developers and are being used in programming education\nsuch as in generation of coding problem examples or providing code\nexplanations. A key aspect of programming education is understanding and\ndealing with error message. However, 'logical errors' in which the program\noperates against the programmer's intentions do not receive error messages from\nthe compiler. In this study, building on existing research on programming\nerrors, we first define the types of logical errors that can occur in\nprogramming in general. Based on the definition, we propose an effective\napproach for detecting logical errors with LLMs that makes use of relations\namong error types in the Chain-of-Thought and Tree-of-Thought prompts. The\nexperimental results indicate that when such logical error descriptions in the\nprompt are used, the average classifition performance is about 21% higher than\nthe ones without them. We also conducted an experiment for exploiting the\nrelations among errors in generating a new logical error dataset using LLMs. As\nthere is very limited dataset for logical errors such benchmark dataset can be\nvery useful for various programming related applications. We expect that our\nwork can assist novice programmers in identifying the causes of code errors and\ncorrect them more effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs trained in the understanding of programming syntax are now providing\neffective assistance to developers and are being used in programming education\nsuch as in generation of coding problem examples or providing code\nexplanations. A key aspect of programming education is understanding and\ndealing with error message. However, 'logical errors' in which the program\noperates against the programmer's intentions do not receive error messages from\nthe compiler. In this study, building on existing research on programming\nerrors, we first define the types of logical errors that can occur in\nprogramming in general. Based on the definition, we propose an effective\napproach for detecting logical errors with LLMs that makes use of relations\namong error types in the Chain-of-Thought and Tree-of-Thought prompts. The\nexperimental results indicate that when such logical error descriptions in the\nprompt are used, the average classifition performance is about 21% higher than\nthe ones without them. We also conducted an experiment for exploiting the\nrelations among errors in generating a new logical error dataset using LLMs. As\nthere is very limited dataset for logical errors such benchmark dataset can be\nvery useful for various programming related applications. We expect that our\nwork can assist novice programmers in identifying the causes of code errors and\ncorrect them more effectively."
                },
                "authors": [
                    {
                        "name": "Yanggyu Lee"
                    },
                    {
                        "name": "Suchae Jeong"
                    },
                    {
                        "name": "Jihie Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihie Kim"
                },
                "author": "Jihie Kim",
                "arxiv_comment": "Published in ITS 2024 (Best Paper Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05168v2",
                "updated": "2024-11-17T17:26:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    17,
                    26,
                    23,
                    6,
                    322,
                    0
                ],
                "published": "2024-10-07T16:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation"
                },
                "summary": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20092v2",
                "updated": "2024-11-17T17:05:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    17,
                    5,
                    3,
                    6,
                    322,
                    0
                ],
                "published": "2024-06-28T17:57:14Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    57,
                    14,
                    4,
                    180,
                    0
                ],
                "title": "Efficient Large Multi-modal Models via Visual Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Large Multi-modal Models via Visual Context Compression"
                },
                "summary": "While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in multi-modal LLMs (MLLMs) has remained a largely overlooked area. In\nthis work, we present the study on the analysis of redundancy concerning visual\ntokens and efficient training within these models. Our initial experiments show\nthat eliminating up to 70% of visual tokens at the testing stage by simply\naverage pooling only leads to a minimal 3% reduction in visual question\nanswering accuracy on the GQA benchmark, indicating significant redundancy in\nvisual context. Addressing this, we introduce Visual Context Compressor, which\nreduces the number of visual tokens to enhance training and inference\nefficiency without sacrificing performance. To minimize information loss caused\nby the compression on visual tokens while maintaining training efficiency, we\ndevelop LLaVolta as a light and staged training scheme that incorporates\nstage-wise visual context compression to progressively compress the visual\ntokens from heavily to lightly compression during training, yielding no loss of\ninformation when testing. Extensive experiments demonstrate that our approach\nenhances the performance of MLLMs in both image-language and video-language\nunderstanding, while also significantly cutting training costs and improving\ninference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in multi-modal LLMs (MLLMs) has remained a largely overlooked area. In\nthis work, we present the study on the analysis of redundancy concerning visual\ntokens and efficient training within these models. Our initial experiments show\nthat eliminating up to 70% of visual tokens at the testing stage by simply\naverage pooling only leads to a minimal 3% reduction in visual question\nanswering accuracy on the GQA benchmark, indicating significant redundancy in\nvisual context. Addressing this, we introduce Visual Context Compressor, which\nreduces the number of visual tokens to enhance training and inference\nefficiency without sacrificing performance. To minimize information loss caused\nby the compression on visual tokens while maintaining training efficiency, we\ndevelop LLaVolta as a light and staged training scheme that incorporates\nstage-wise visual context compression to progressively compress the visual\ntokens from heavily to lightly compression during training, yielding no loss of\ninformation when testing. Extensive experiments demonstrate that our approach\nenhances the performance of MLLMs in both image-language and video-language\nunderstanding, while also significantly cutting training costs and improving\ninference efficiency."
                },
                "authors": [
                    {
                        "name": "Jieneng Chen"
                    },
                    {
                        "name": "Luoxin Ye"
                    },
                    {
                        "name": "Ju He"
                    },
                    {
                        "name": "Zhao-Yang Wang"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Alan Yuille"
                    }
                ],
                "author_detail": {
                    "name": "Alan Yuille"
                },
                "author": "Alan Yuille",
                "arxiv_comment": "NeurIPS 2024 Camera Ready; Code is available at\n  https://github.com/Beckschen/LLaVolta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05558v2",
                "updated": "2024-11-17T17:00:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    17,
                    0,
                    11,
                    6,
                    322,
                    0
                ],
                "published": "2024-10-07T23:36:05Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    23,
                    36,
                    5,
                    0,
                    281,
                    0
                ],
                "title": "Narrative-of-Thought: Improving Temporal Reasoning of Large Language\n  Models via Recounted Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative-of-Thought: Improving Temporal Reasoning of Large Language\n  Models via Recounted Narratives"
                },
                "summary": "Reasoning about time and temporal relations is an integral aspect of human\ncognition, essential for perceiving the world and navigating our experiences.\nThough large language models (LLMs) have demonstrated impressive performance in\nmany reasoning tasks, temporal reasoning remains challenging due to its\nintrinsic complexity. In this work, we first study an essential task of\ntemporal reasoning -- temporal graph generation, to unveil LLMs' inherent,\nglobal reasoning capabilities. We show that this task presents great challenges\neven for the most powerful LLMs, such as GPT-3.5/4. We also notice a\nsignificant performance gap by small models (<10B) that lag behind LLMs by 50%.\nNext, we study how to close this gap with a budget constraint, e.g., not using\nmodel finetuning. We propose a new prompting technique tailored for temporal\nreasoning, Narrative-of-Thought (NoT), that first converts the events set to a\nPython class, then prompts a small model to generate a temporally grounded\nnarrative, guiding the final generation of a temporal graph. Extensive\nexperiments showcase the efficacy of NoT in improving various metrics. Notably,\nNoT attains the highest F1 on the Schema-11 evaluation set, while securing an\noverall F1 on par with GPT-3.5. NoT also achieves the best structural\nsimilarity across the board, even compared with GPT-3.5/4. Our code is\navailable at https://github.com/launchnlp/NoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about time and temporal relations is an integral aspect of human\ncognition, essential for perceiving the world and navigating our experiences.\nThough large language models (LLMs) have demonstrated impressive performance in\nmany reasoning tasks, temporal reasoning remains challenging due to its\nintrinsic complexity. In this work, we first study an essential task of\ntemporal reasoning -- temporal graph generation, to unveil LLMs' inherent,\nglobal reasoning capabilities. We show that this task presents great challenges\neven for the most powerful LLMs, such as GPT-3.5/4. We also notice a\nsignificant performance gap by small models (<10B) that lag behind LLMs by 50%.\nNext, we study how to close this gap with a budget constraint, e.g., not using\nmodel finetuning. We propose a new prompting technique tailored for temporal\nreasoning, Narrative-of-Thought (NoT), that first converts the events set to a\nPython class, then prompts a small model to generate a temporally grounded\nnarrative, guiding the final generation of a temporal graph. Extensive\nexperiments showcase the efficacy of NoT in improving various metrics. Notably,\nNoT attains the highest F1 on the Schema-11 evaluation set, while securing an\noverall F1 on par with GPT-3.5. NoT also achieves the best structural\nsimilarity across the board, even compared with GPT-3.5/4. Our code is\navailable at https://github.com/launchnlp/NoT."
                },
                "authors": [
                    {
                        "name": "Xinliang Frederick Zhang"
                    },
                    {
                        "name": "Nick Beauchamp"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "arxiv_comment": "EMNLP'24 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20098v2",
                "updated": "2024-11-17T16:11:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    16,
                    11,
                    0,
                    6,
                    322,
                    0
                ],
                "published": "2024-06-28T17:59:46Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    59,
                    46,
                    4,
                    180,
                    0
                ],
                "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs"
                },
                "summary": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose $\\texttt{Web2Code}$, a benchmark consisting of a new\nlarge-scale webpage-to-code dataset for instruction tuning and an evaluation\nframework for the webpage understanding and HTML code translation abilities of\nMLLMs. For dataset construction, we leverage pretrained LLMs to enhance\nexisting webpage-to-code datasets as well as generate a diverse pool of new\nwebpages rendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain. We hope\nour work will contribute to the development of general MLLMs suitable for\nweb-based content generation and task automation. Our data and code are\navailable at https://github.com/MBZUAI-LLM/web2code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose $\\texttt{Web2Code}$, a benchmark consisting of a new\nlarge-scale webpage-to-code dataset for instruction tuning and an evaluation\nframework for the webpage understanding and HTML code translation abilities of\nMLLMs. For dataset construction, we leverage pretrained LLMs to enhance\nexisting webpage-to-code datasets as well as generate a diverse pool of new\nwebpages rendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain. We hope\nour work will contribute to the development of general MLLMs suitable for\nweb-based content generation and task automation. Our data and code are\navailable at https://github.com/MBZUAI-LLM/web2code."
                },
                "authors": [
                    {
                        "name": "Sukmin Yun"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Rusiru Thushara"
                    },
                    {
                        "name": "Mohammad Qazim Bhat"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Zutao Jiang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Jinhong Wang"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Camera-ready Version. Website at\n  https://mbzuai-llm.github.io/webpage2code/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11114v1",
                "updated": "2024-11-17T16:08:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    16,
                    8,
                    34,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T16:08:34Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    16,
                    8,
                    34,
                    6,
                    322,
                    0
                ],
                "title": "JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of\n  Representation and Circuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of\n  Representation and Circuit"
                },
                "summary": "Despite the outstanding performance of Large language models (LLMs) in\ndiverse tasks, they are vulnerable to jailbreak attacks, wherein adversarial\nprompts are crafted to bypass their security mechanisms and elicit unexpected\nresponses.Although jailbreak attacks are prevalent, the understanding of their\nunderlying mechanisms remains limited. Recent studies have explain typical\njailbreaking behavior (e.g., the degree to which the model refuses to respond)\nof LLMs by analyzing the representation shifts in their latent space caused by\njailbreak prompts or identifying key neurons that contribute to the success of\nthese attacks. However, these studies neither explore diverse jailbreak\npatterns nor provide a fine-grained explanation from the failure of circuit to\nthe changes of representational, leaving significant gaps in uncovering the\njailbreak mechanism. In this paper, we propose JailbreakLens, an interpretation\nframework that analyzes jailbreak mechanisms from both representation (which\nreveals how jailbreaks alter the model's harmfulness perception) and circuit\nperspectives (which uncovers the causes of these deceptions by identifying key\ncircuits contributing to the vulnerability), tracking their evolution\nthroughout the entire response generation process. We then conduct an in-depth\nevaluation of jailbreak behavior on four mainstream LLMs under seven jailbreak\nstrategies. Our evaluation finds that jailbreak prompts amplify components that\nreinforce affirmative responses while suppressing those that produce refusal.\nAlthough this manipulation shifts model representations toward safe clusters to\ndeceive the LLM, leading it to provide detailed responses instead of refusals,\nit still produce abnormal activation which can be caught in the circuit\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the outstanding performance of Large language models (LLMs) in\ndiverse tasks, they are vulnerable to jailbreak attacks, wherein adversarial\nprompts are crafted to bypass their security mechanisms and elicit unexpected\nresponses.Although jailbreak attacks are prevalent, the understanding of their\nunderlying mechanisms remains limited. Recent studies have explain typical\njailbreaking behavior (e.g., the degree to which the model refuses to respond)\nof LLMs by analyzing the representation shifts in their latent space caused by\njailbreak prompts or identifying key neurons that contribute to the success of\nthese attacks. However, these studies neither explore diverse jailbreak\npatterns nor provide a fine-grained explanation from the failure of circuit to\nthe changes of representational, leaving significant gaps in uncovering the\njailbreak mechanism. In this paper, we propose JailbreakLens, an interpretation\nframework that analyzes jailbreak mechanisms from both representation (which\nreveals how jailbreaks alter the model's harmfulness perception) and circuit\nperspectives (which uncovers the causes of these deceptions by identifying key\ncircuits contributing to the vulnerability), tracking their evolution\nthroughout the entire response generation process. We then conduct an in-depth\nevaluation of jailbreak behavior on four mainstream LLMs under seven jailbreak\nstrategies. Our evaluation finds that jailbreak prompts amplify components that\nreinforce affirmative responses while suppressing those that produce refusal.\nAlthough this manipulation shifts model representations toward safe clusters to\ndeceive the LLM, leading it to provide detailed responses instead of refusals,\nit still produce abnormal activation which can be caught in the circuit\nanalysis."
                },
                "authors": [
                    {
                        "name": "Zeqing He"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Huiyu Xu"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Chun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Chen"
                },
                "author": "Chun Chen",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18528v2",
                "updated": "2024-11-17T15:09:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    15,
                    9,
                    54,
                    6,
                    322,
                    0
                ],
                "published": "2024-06-26T17:56:29Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    17,
                    56,
                    29,
                    2,
                    178,
                    0
                ],
                "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine\n  Translation and Summarization Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine\n  Translation and Summarization Evaluation"
                },
                "summary": "Large language models (LLMs) have revolutionized NLP research. Notably,\nin-context learning enables their use as evaluation metrics for natural\nlanguage generation, making them particularly advantageous in low-resource\nscenarios and time-restricted applications. In this work, we introduce PrExMe,\na large-scale Prompt Exploration for Metrics, where we evaluate more than 720\nprompt templates for open-source LLM-based metrics on machine translation (MT)\nand summarization datasets, totalling over 6.6M evaluations. This extensive\ncomparison (1) benchmarks recent open-source LLMs as metrics and (2) explores\nthe stability and variability of different prompting strategies. We discover\nthat, on the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized NLP research. Notably,\nin-context learning enables their use as evaluation metrics for natural\nlanguage generation, making them particularly advantageous in low-resource\nscenarios and time-restricted applications. In this work, we introduce PrExMe,\na large-scale Prompt Exploration for Metrics, where we evaluate more than 720\nprompt templates for open-source LLM-based metrics on machine translation (MT)\nand summarization datasets, totalling over 6.6M evaluations. This extensive\ncomparison (1) benchmarks recent open-source LLMs as metrics and (2) explores\nthe stability and variability of different prompting strategies. We discover\nthat, on the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations."
                },
                "authors": [
                    {
                        "name": "Christoph Leiter"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "EMNLP 2024 main; camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11082v1",
                "updated": "2024-11-17T14:15:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    15,
                    54,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:15:54Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    15,
                    54,
                    6,
                    322,
                    0
                ],
                "title": "STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage\n  Synergistic Training of Deep Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage\n  Synergistic Training of Deep Spiking Neural Networks"
                },
                "summary": "The prevailing of artificial intelligence-of-things calls for higher\nenergy-efficient edge computing paradigms, such as neuromorphic agents\nleveraging brain-inspired spiking neural network (SNN) models based on\nspatiotemporally sparse binary activations. However, the lack of efficient and\nhigh-accuracy deep SNN learning algorithms prevents them from practical edge\ndeployments with a strictly bounded cost. In this paper, we propose a\nspatiotemporal orthogonal propagation (STOP) algorithm to tack this challenge.\nOur algorithm enables fully synergistic learning of synaptic weights as well as\nfiring thresholds and leakage factors in spiking neurons to improve SNN\naccuracy, while under a unified temporally-forward trace-based framework to\nmitigate the huge memory requirement for storing neural states of all\ntime-steps in the forward pass. Characteristically, the spatially-backward\nneuronal errors and temporally-forward traces propagate orthogonally to and\nindependently of each other, substantially reducing computational overhead. Our\nSTOP algorithm obtained high recognition accuracies of 99.53%, 94.84%, 74.92%,\n98.26% and 77.10% on the MNIST, CIFAR-10, CIFAR-100, DVS-Gesture and\nDVS-CIFAR10 datasets with adequate SNNs of intermediate scales from LeNet-5 to\nResNet-18. Compared with other deep SNN training works, our method is more\nplausible for edge intelligent scenarios where resources are limited but\nhigh-accuracy in-situ learning is desired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing of artificial intelligence-of-things calls for higher\nenergy-efficient edge computing paradigms, such as neuromorphic agents\nleveraging brain-inspired spiking neural network (SNN) models based on\nspatiotemporally sparse binary activations. However, the lack of efficient and\nhigh-accuracy deep SNN learning algorithms prevents them from practical edge\ndeployments with a strictly bounded cost. In this paper, we propose a\nspatiotemporal orthogonal propagation (STOP) algorithm to tack this challenge.\nOur algorithm enables fully synergistic learning of synaptic weights as well as\nfiring thresholds and leakage factors in spiking neurons to improve SNN\naccuracy, while under a unified temporally-forward trace-based framework to\nmitigate the huge memory requirement for storing neural states of all\ntime-steps in the forward pass. Characteristically, the spatially-backward\nneuronal errors and temporally-forward traces propagate orthogonally to and\nindependently of each other, substantially reducing computational overhead. Our\nSTOP algorithm obtained high recognition accuracies of 99.53%, 94.84%, 74.92%,\n98.26% and 77.10% on the MNIST, CIFAR-10, CIFAR-100, DVS-Gesture and\nDVS-CIFAR10 datasets with adequate SNNs of intermediate scales from LeNet-5 to\nResNet-18. Compared with other deep SNN training works, our method is more\nplausible for edge intelligent scenarios where resources are limited but\nhigh-accuracy in-situ learning is desired."
                },
                "authors": [
                    {
                        "name": "Haoran Gao"
                    },
                    {
                        "name": "Xichuan Zhou"
                    },
                    {
                        "name": "Yingcheng Lin"
                    },
                    {
                        "name": "Min Tian"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Cong Shi"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shi"
                },
                "author": "Cong Shi",
                "arxiv_comment": "13 pages (exclude supplementary), 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]