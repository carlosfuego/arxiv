[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v2",
                "updated": "2024-10-07T15:24:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    24,
                    10,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Lukáš Ondráček"
                    },
                    {
                        "name": "Ondřej Mička"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Mička"
                },
                "author": "Ondřej Mička",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.05269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05269v1",
                "updated": "2024-10-07T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    58,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    58,
                    0,
                    281,
                    0
                ],
                "title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models"
                },
                "summary": "Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Ninareh Mehrabi"
                    },
                    {
                        "name": "Palash Goyal"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aram Galstyan"
                    }
                ],
                "author_detail": {
                    "name": "Aram Galstyan"
                },
                "author": "Aram Galstyan",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/DataAdvisor/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05267v1",
                "updated": "2024-10-07T17:59:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    48,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:48Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    48,
                    0,
                    281,
                    0
                ],
                "title": "Grounding Partially-Defined Events in Multimodal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Partially-Defined Events in Multimodal Data"
                },
                "summary": "How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems."
                },
                "authors": [
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Reno Kriz"
                    },
                    {
                        "name": "David Etter"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Alexander Martin"
                    },
                    {
                        "name": "Cameron Carpenter"
                    },
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "arxiv_comment": "Preprint; 9 pages; 2024 EMNLP Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11839v2",
                "updated": "2024-10-07T17:59:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    42,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-17T17:59:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    59,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models"
                },
                "summary": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "James Y. Huang"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05262v1",
                "updated": "2024-10-07T17:58:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    58,
                    47,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:58:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    58,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles"
                },
                "summary": "As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\""
                },
                "authors": [
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Ke Fang"
                    },
                    {
                        "name": "Yunfeng Shi"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05254v1",
                "updated": "2024-10-07T17:55:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:55:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments"
                },
                "summary": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents."
                },
                "authors": [
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Omer Madmon"
                    },
                    {
                        "name": "Itamar Reinman"
                    },
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    }
                ],
                "author_detail": {
                    "name": "Moshe Tennenholtz"
                },
                "author": "Moshe Tennenholtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05252v1",
                "updated": "2024-10-07T17:55:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    10,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:55:10Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    10,
                    0,
                    281,
                    0
                ],
                "title": "Causal Micro-Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Micro-Narratives"
                },
                "summary": "We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research."
                },
                "authors": [
                    {
                        "name": "Mourad Heddaya"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Rob Voigt"
                    },
                    {
                        "name": "Alexander Zentefis"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Zentefis"
                },
                "author": "Alexander Zentefis",
                "arxiv_comment": "Accepted to EMNLP 2024 Workshop on Narrative Understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05248v1",
                "updated": "2024-10-07T17:52:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    52,
                    21,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:52:21Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    52,
                    21,
                    0,
                    281,
                    0
                ],
                "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe"
                },
                "summary": "To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications."
                },
                "authors": [
                    {
                        "name": "Yuxin Xiao"
                    },
                    {
                        "name": "Shujian Zhang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Sanqiang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sanqiang Zhao"
                },
                "author": "Sanqiang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17975v2",
                "updated": "2024-10-07T17:49:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    49,
                    13,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-25T23:12:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    23,
                    12,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)"
                },
                "summary": "Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs."
                },
                "authors": [
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Shubham Jain"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05234v1",
                "updated": "2024-10-07T17:41:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    41,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:41:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    41,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields\n  in Unsupervised Deformable Image Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields\n  in Unsupervised Deformable Image Registration"
                },
                "summary": "Deformable image registration aims to precisely align medical images from\ndifferent modalities or times. Traditional deep learning methods, while\neffective, often lack interpretability, real-time observability and adjustment\ncapacity during registration inference. Denoising diffusion models present an\nalternative by reformulating registration as iterative image denoising.\nHowever, existing diffusion registration approaches do not fully harness\ncapabilities, neglecting the critical sampling phase that enables continuous\nobservability during the inference. Hence, we introduce DiffuseReg, an\ninnovative diffusion-based method that denoises deformation fields instead of\nimages for improved transparency. We also propose a novel denoising network\nupon Swin Transformer, which better integrates moving and fixed images with\ndiffusion time step throughout the denoising process. Furthermore, we enhance\ncontrol over the denoising registration process with a novel similarity\nconsistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg\noutperforms existing diffusion registration methods by 1.32 in Dice score. The\nsampling process in DiffuseReg enables real-time output observability and\nadjustment unmatched by previous deep models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable image registration aims to precisely align medical images from\ndifferent modalities or times. Traditional deep learning methods, while\neffective, often lack interpretability, real-time observability and adjustment\ncapacity during registration inference. Denoising diffusion models present an\nalternative by reformulating registration as iterative image denoising.\nHowever, existing diffusion registration approaches do not fully harness\ncapabilities, neglecting the critical sampling phase that enables continuous\nobservability during the inference. Hence, we introduce DiffuseReg, an\ninnovative diffusion-based method that denoises deformation fields instead of\nimages for improved transparency. We also propose a novel denoising network\nupon Swin Transformer, which better integrates moving and fixed images with\ndiffusion time step throughout the denoising process. Furthermore, we enhance\ncontrol over the denoising registration process with a novel similarity\nconsistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg\noutperforms existing diffusion registration methods by 1.32 in Dice score. The\nsampling process in DiffuseReg enables real-time output observability and\nadjustment unmatched by previous deep models."
                },
                "authors": [
                    {
                        "name": "Yongtai Zhuo"
                    },
                    {
                        "name": "Yiqing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yiqing Shen"
                },
                "author": "Yiqing Shen",
                "arxiv_comment": "MICCAI 2024, W-AM-067, https://github.com/YutaZhuo/DiffuseReg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05229v1",
                "updated": "2024-10-07T17:36:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    36,
                    37,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:36:37Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    36,
                    37,
                    0,
                    281,
                    0
                ],
                "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Hooman Shahrokhi"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    }
                ],
                "author_detail": {
                    "name": "Mehrdad Farajtabar"
                },
                "author": "Mehrdad Farajtabar",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05224v1",
                "updated": "2024-10-07T17:29:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    29,
                    40,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:29:40Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    29,
                    40,
                    0,
                    281,
                    0
                ],
                "title": "Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates"
                },
                "summary": "Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules."
                },
                "authors": [
                    {
                        "name": "Avanika Narayan"
                    },
                    {
                        "name": "Mayee F. Chen"
                    },
                    {
                        "name": "Kush Bhatia"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13356v2",
                "updated": "2024-10-07T17:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    27,
                    30,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-19T09:03:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    3,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attack"
                },
                "summary": "Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in LLMs. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of targeted relearning attacks. With access to only a small and\npotentially loosely related set of data, we find that we can \"jog\" the memory\nof unlearned models to reverse the effects of unlearning. For example, we show\nthat relearning on public medical articles can lead an unlearned LLM to output\nharmful knowledge about bioweapons, and relearning general wiki information\nabout the book series Harry Potter can force the model to output verbatim\nmemorized text. We formalize this unlearning-relearning pipeline, explore the\nattack across three popular unlearning benchmarks, and discuss future\ndirections and guidelines that result from our study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in LLMs. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of targeted relearning attacks. With access to only a small and\npotentially loosely related set of data, we find that we can \"jog\" the memory\nof unlearned models to reverse the effects of unlearning. For example, we show\nthat relearning on public medical articles can lead an unlearned LLM to output\nharmful knowledge about bioweapons, and relearning general wiki information\nabout the book series Harry Potter can force the model to output verbatim\nmemorized text. We formalize this unlearning-relearning pipeline, explore the\nattack across three popular unlearning benchmarks, and discuss future\ndirections and guidelines that result from our study."
                },
                "authors": [
                    {
                        "name": "Shengyuan Hu"
                    },
                    {
                        "name": "Yiwei Fu"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "26 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05222v1",
                "updated": "2024-10-07T17:26:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    26,
                    31,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:26:31Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    26,
                    31,
                    0,
                    281,
                    0
                ],
                "title": "Precise Model Benchmarking with Only a Few Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise Model Benchmarking with Only a Few Observations"
                },
                "summary": "How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach."
                },
                "authors": [
                    {
                        "name": "Riccardo Fogliato"
                    },
                    {
                        "name": "Pratik Patil"
                    },
                    {
                        "name": "Nil-Jana Akpinar"
                    },
                    {
                        "name": "Mathew Monfort"
                    }
                ],
                "author_detail": {
                    "name": "Mathew Monfort"
                },
                "author": "Mathew Monfort",
                "arxiv_comment": "To appear at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15877v3",
                "updated": "2024-10-07T17:23:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    23,
                    30,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-22T15:52:04Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    15,
                    52,
                    4,
                    5,
                    174,
                    0
                ],
                "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions"
                },
                "summary": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area."
                },
                "authors": [
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Minh Chien Vu"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Imam Nur Bani Yusuf"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Indraneil Paul"
                    },
                    {
                        "name": "Simon Brunner"
                    },
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Thong Hoang"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Xiaoheng Hong"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Prateek Yadav"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro Von Werra"
                    }
                ],
                "author_detail": {
                    "name": "Leandro Von Werra"
                },
                "author": "Leandro Von Werra",
                "arxiv_comment": "44 pages, 14 figures, 7 tables, built with love by the BigCode\n  community :)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05218v1",
                "updated": "2024-10-07T17:22:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    22,
                    56,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:22:56Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    22,
                    56,
                    0,
                    281,
                    0
                ],
                "title": "Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Toni J. B. Liu"
                    },
                    {
                        "name": "Nicolas Boullé"
                    },
                    {
                        "name": "Raphaël Sarfati"
                    },
                    {
                        "name": "Christopher J. Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J. Earls"
                },
                "author": "Christopher J. Earls",
                "arxiv_comment": "Under review as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05217v1",
                "updated": "2024-10-07T17:21:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    46,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:21:46Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    46,
                    0,
                    281,
                    0
                ],
                "title": "Organizing Unstructured Image Collections using Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizing Unstructured Image Collections using Natural Language"
                },
                "summary": "Organizing unstructured visual data into semantic clusters is a key challenge\nin computer vision. Traditional deep clustering (DC) approaches focus on a\nsingle partition of data, while multiple clustering (MC) methods address this\nlimitation by uncovering distinct clustering solutions. The rise of large\nlanguage models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing\nusers to define clustering criteria in natural language. However, manually\nspecifying criteria for large datasets is impractical. In this work, we\nintroduce the task Semantic Multiple Clustering (SMC) that aims to\nautomatically discover clustering criteria from large image collections,\nuncovering interpretable substructures without requiring human input. Our\nframework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a\nproxy to concurrently reason over large image collections, discover\npartitioning criteria, expressed in natural language, and reveal semantic\nsubstructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c\nbenchmarks, each containing four grouping criteria and ground-truth\nannotations. We apply TeDeSC to various applications, such as discovering\nbiases and analyzing social media image popularity, demonstrating its utility\nas a tool for automatically organizing image collections and revealing novel\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizing unstructured visual data into semantic clusters is a key challenge\nin computer vision. Traditional deep clustering (DC) approaches focus on a\nsingle partition of data, while multiple clustering (MC) methods address this\nlimitation by uncovering distinct clustering solutions. The rise of large\nlanguage models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing\nusers to define clustering criteria in natural language. However, manually\nspecifying criteria for large datasets is impractical. In this work, we\nintroduce the task Semantic Multiple Clustering (SMC) that aims to\nautomatically discover clustering criteria from large image collections,\nuncovering interpretable substructures without requiring human input. Our\nframework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a\nproxy to concurrently reason over large image collections, discover\npartitioning criteria, expressed in natural language, and reveal semantic\nsubstructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c\nbenchmarks, each containing four grouping criteria and ground-truth\nannotations. We apply TeDeSC to various applications, such as discovering\nbiases and analyzing social media image popularity, demonstrating its utility\nas a tool for automatically organizing image collections and revealing novel\ninsights."
                },
                "authors": [
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Gianni Franchi"
                    },
                    {
                        "name": "Subhankar Roy"
                    },
                    {
                        "name": "Elisa Ricci"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Ricci"
                },
                "author": "Elisa Ricci",
                "arxiv_comment": "Preprint. Project webpage: https://oatmealliu.github.io/smc.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.02233v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.02233v3",
                "updated": "2024-10-07T17:21:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2023-09-05T13:39:38Z",
                "published_parsed": [
                    2023,
                    9,
                    5,
                    13,
                    39,
                    38,
                    1,
                    248,
                    0
                ],
                "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)"
                },
                "summary": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%."
                },
                "authors": [
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "This version has been accepted and published at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.02233v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.02233v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15652v2",
                "updated": "2024-10-07T17:21:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    0,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-24T15:47:35Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    15,
                    47,
                    35,
                    4,
                    145,
                    0
                ],
                "title": "$$\\mathbf{L^2\\cdot M = C^2}$$ Large Language Models are Covert Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$$\\mathbf{L^2\\cdot M = C^2}$$ Large Language Models are Covert Channels"
                },
                "summary": "Large Language Models (LLMs) have gained significant popularity recently.\nLLMs are susceptible to various attacks but can also improve the security of\ndiverse systems. However, besides enabling more secure systems, how well do\nopen source LLMs behave as covertext distributions to, e.g., facilitate\ncensorship-resistant communication? In this paper, we explore open-source\nLLM-based covert channels. We empirically measure the security vs. capacity of\nan open-source LLM model (Llama-7B) to assess its performance as a covert\nchannel. Although our results indicate that such channels are not likely to\nachieve high practical bitrates, we also show that the chance for an adversary\nto detect covert communication is low. To ensure our results can be used with\nthe least effort as a general reference, we employ a conceptually simple and\nconcise scheme and only assume public models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant popularity recently.\nLLMs are susceptible to various attacks but can also improve the security of\ndiverse systems. However, besides enabling more secure systems, how well do\nopen source LLMs behave as covertext distributions to, e.g., facilitate\ncensorship-resistant communication? In this paper, we explore open-source\nLLM-based covert channels. We empirically measure the security vs. capacity of\nan open-source LLM model (Llama-7B) to assess its performance as a covert\nchannel. Although our results indicate that such channels are not likely to\nachieve high practical bitrates, we also show that the chance for an adversary\nto detect covert communication is low. To ensure our results can be used with\nthe least effort as a general reference, we employ a conceptually simple and\nconcise scheme and only assume public models."
                },
                "authors": [
                    {
                        "name": "Simen Gaure"
                    },
                    {
                        "name": "Stefanos Koffas"
                    },
                    {
                        "name": "Stjepan Picek"
                    },
                    {
                        "name": "Sondre Rønjom"
                    }
                ],
                "author_detail": {
                    "name": "Sondre Rønjom"
                },
                "author": "Sondre Rønjom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05209v1",
                "updated": "2024-10-07T17:14:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    14,
                    6,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:14:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    14,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "Neural Networks for cosmological model selection and feature importance\n  using Cosmic Microwave Background data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Networks for cosmological model selection and feature importance\n  using Cosmic Microwave Background data"
                },
                "summary": "The measurements of the temperature and polarisation anisotropies of the\nCosmic Microwave Background (CMB) by the ESA Planck mission have strongly\nsupported the current concordance model of cosmology. However, the latest\ncosmological data release from ESA Planck mission still has a powerful\npotential to test new data science algorithms and inference techniques. In this\npaper, we use advanced Machine Learning (ML) algorithms, such as Neural\nNetworks (NNs), to discern among different underlying cosmological models at\nthe angular power spectra level, using both temperature and polarisation Planck\n18 data. We test two different models beyond $\\Lambda$CDM: a modified gravity\nmodel: the Hu-Sawicki model, and an alternative inflationary model: a\nfeature-template in the primordial power spectrum. Furthermore, we also\nimplemented an interpretability method based on SHAP values to evaluate the\nlearning process and identify the most relevant elements that drive our\narchitecture to certain outcomes. We find that our NN is able to distinguish\nbetween different angular power spectra successfully for both alternative\nmodels and $\\Lambda$CDM. We conclude by explaining how archival scientific data\nhas still a strong potential to test novel data science algorithms that are\ninteresting for the next generation of cosmological experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurements of the temperature and polarisation anisotropies of the\nCosmic Microwave Background (CMB) by the ESA Planck mission have strongly\nsupported the current concordance model of cosmology. However, the latest\ncosmological data release from ESA Planck mission still has a powerful\npotential to test new data science algorithms and inference techniques. In this\npaper, we use advanced Machine Learning (ML) algorithms, such as Neural\nNetworks (NNs), to discern among different underlying cosmological models at\nthe angular power spectra level, using both temperature and polarisation Planck\n18 data. We test two different models beyond $\\Lambda$CDM: a modified gravity\nmodel: the Hu-Sawicki model, and an alternative inflationary model: a\nfeature-template in the primordial power spectrum. Furthermore, we also\nimplemented an interpretability method based on SHAP values to evaluate the\nlearning process and identify the most relevant elements that drive our\narchitecture to certain outcomes. We find that our NN is able to distinguish\nbetween different angular power spectra successfully for both alternative\nmodels and $\\Lambda$CDM. We conclude by explaining how archival scientific data\nhas still a strong potential to test novel data science algorithms that are\ninteresting for the next generation of cosmological experiments."
                },
                "authors": [
                    {
                        "name": "I. Ocampo"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "S. Nesseris"
                    }
                ],
                "author_detail": {
                    "name": "S. Nesseris"
                },
                "author": "S. Nesseris",
                "arxiv_comment": "24 pages, 9 figures, 2 tables, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06369v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06369v4",
                "updated": "2024-10-07T17:13:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    13,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-10T15:30:13Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    15,
                    30,
                    13,
                    0,
                    162,
                    0
                ],
                "title": "Annotation alignment: Comparing LLM and human annotations of\n  conversational safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotation alignment: Comparing LLM and human annotations of\n  conversational safety"
                },
                "summary": "Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another."
                },
                "authors": [
                    {
                        "name": "Rajiv Movva"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Emma Pierson"
                    }
                ],
                "author_detail": {
                    "name": "Emma Pierson"
                },
                "author": "Emma Pierson",
                "arxiv_comment": "EMNLP 2024 (Main). Main text contains 6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06369v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06369v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15939v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15939v4",
                "updated": "2024-10-07T17:00:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    0,
                    22,
                    0,
                    281,
                    0
                ],
                "published": "2024-08-28T17:01:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    1,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "The Tidal Torque Theory Revisited. I. Protohalo Angular Momentum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tidal Torque Theory Revisited. I. Protohalo Angular Momentum"
                },
                "summary": "According to the Tidal Torque Theory (TTT), the angular momentum (AM) of\nhaloes arises from the tidal torque produced on protohaloes by the surrounding\nmass distribution. That approach, initially developed assuming protohaloes as\nrandom overdense regions in the linear density field, was extended to the more\nrealistic scenario that protohaloes are collapsing patches around peaks in the\nGaussian-smoothed linear density field. But that extension faced two\nfundamental issues: 1) the unknown mass of collapsing patches marked by\nGaussian peaks, and 2) the unknown collapse time such ellipsoidal patches.\nFurthermore, the TTT strictly holds in linear regime only, so it does not allow\none to infer from it the AM of relaxed haloes. This Paper is the first of a\nseries of two devoted to revisiting the TTT and accurately calculating the halo\nAM. In the present Paper we use the CUSP formalism, fixing all the open issues\nof the protohalo AM in the peak theory. That viewpoint is fully accomplished in\nthe sense that not only is the protohalo suffering the tidal torque identified\nto a peak, but the main mass fluctuation causing the tidal torque is also\nidentified to a peak or a hole. This way, we obtain a simple analytic\nexpression for the mean and median Lagrangian protohalo AM, which can be\nreadily implemented in galaxy formation models and be compared to the results\nof simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to the Tidal Torque Theory (TTT), the angular momentum (AM) of\nhaloes arises from the tidal torque produced on protohaloes by the surrounding\nmass distribution. That approach, initially developed assuming protohaloes as\nrandom overdense regions in the linear density field, was extended to the more\nrealistic scenario that protohaloes are collapsing patches around peaks in the\nGaussian-smoothed linear density field. But that extension faced two\nfundamental issues: 1) the unknown mass of collapsing patches marked by\nGaussian peaks, and 2) the unknown collapse time such ellipsoidal patches.\nFurthermore, the TTT strictly holds in linear regime only, so it does not allow\none to infer from it the AM of relaxed haloes. This Paper is the first of a\nseries of two devoted to revisiting the TTT and accurately calculating the halo\nAM. In the present Paper we use the CUSP formalism, fixing all the open issues\nof the protohalo AM in the peak theory. That viewpoint is fully accomplished in\nthe sense that not only is the protohalo suffering the tidal torque identified\nto a peak, but the main mass fluctuation causing the tidal torque is also\nidentified to a peak or a hole. This way, we obtain a simple analytic\nexpression for the mean and median Lagrangian protohalo AM, which can be\nreadily implemented in galaxy formation models and be compared to the results\nof simulations."
                },
                "authors": [
                    {
                        "name": "Eduard Salvador-Solé"
                    },
                    {
                        "name": "Alberto Manrique"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Manrique"
                },
                "author": "Alberto Manrique",
                "arxiv_comment": "11 pages, 2 figures, submitted to MNRAS. Text has been revised",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15939v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15939v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17972v2",
                "updated": "2024-10-07T16:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    52,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-28T08:59:38Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    8,
                    59,
                    38,
                    1,
                    149,
                    0
                ],
                "title": "Inference for the stochastic FitzHugh-Nagumo model from real action\n  potential data via approximate Bayesian computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for the stochastic FitzHugh-Nagumo model from real action\n  potential data via approximate Bayesian computation"
                },
                "summary": "The stochastic FitzHugh-Nagumo (FHN) model is a two-dimensional nonlinear\nstochastic differential equation with additive degenerate noise, whose first\ncomponent, the only one observed, describes the membrane voltage evolution of a\nsingle neuron. Due to its low-dimensionality, its analytical and numerical\ntractability and its neuronal interpretation, it has been used as a case study\nto test the performance of different statistical methods in estimating the\nunderlying model parameters. Existing methods, however, often require complete\nobservations, non-degeneracy of the noise or a complex architecture (e.g., to\nestimate the transition density of the process, \"recovering\" the unobserved\nsecond component) and they may not (satisfactorily) estimate all model\nparameters simultaneously. Moreover, these studies lack real data applications\nfor the stochastic FHN model. The proposed method tackles all challenges\n(non-globally Lipschitz drift, non-explicit solution, lack of available\ntransition density, degeneracy of the noise and partial observations). It is an\nintuitive and easy-to-implement sequential Monte Carlo approximate Bayesian\ncomputation algorithm, which relies on a recent computationally efficient and\nstructure-preserving numerical splitting scheme for synthetic data generation\nand on summary statistics exploiting the structural properties of the process.\nAll model parameters are successfully estimated from simulated data and, more\nremarkably, real action potential data of rats. The presented novel real-data\nfit may broaden the scope and credibility of this classic and widely used\nneuronal model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stochastic FitzHugh-Nagumo (FHN) model is a two-dimensional nonlinear\nstochastic differential equation with additive degenerate noise, whose first\ncomponent, the only one observed, describes the membrane voltage evolution of a\nsingle neuron. Due to its low-dimensionality, its analytical and numerical\ntractability and its neuronal interpretation, it has been used as a case study\nto test the performance of different statistical methods in estimating the\nunderlying model parameters. Existing methods, however, often require complete\nobservations, non-degeneracy of the noise or a complex architecture (e.g., to\nestimate the transition density of the process, \"recovering\" the unobserved\nsecond component) and they may not (satisfactorily) estimate all model\nparameters simultaneously. Moreover, these studies lack real data applications\nfor the stochastic FHN model. The proposed method tackles all challenges\n(non-globally Lipschitz drift, non-explicit solution, lack of available\ntransition density, degeneracy of the noise and partial observations). It is an\nintuitive and easy-to-implement sequential Monte Carlo approximate Bayesian\ncomputation algorithm, which relies on a recent computationally efficient and\nstructure-preserving numerical splitting scheme for synthetic data generation\nand on summary statistics exploiting the structural properties of the process.\nAll model parameters are successfully estimated from simulated data and, more\nremarkably, real action potential data of rats. The presented novel real-data\nfit may broaden the scope and credibility of this classic and widely used\nneuronal model."
                },
                "authors": [
                    {
                        "name": "Adeline Samson"
                    },
                    {
                        "name": "Massimiliano Tamborrino"
                    },
                    {
                        "name": "Irene Tubikanec"
                    }
                ],
                "author_detail": {
                    "name": "Irene Tubikanec"
                },
                "author": "Irene Tubikanec",
                "arxiv_comment": "30 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60H10, 60H35, 65C30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05193v1",
                "updated": "2024-10-07T16:50:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:50:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
                },
                "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tiezheng YU"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05191v1",
                "updated": "2024-10-07T16:49:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    49,
                    16,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:49:16Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    49,
                    16,
                    0,
                    281,
                    0
                ],
                "title": "LADEV: A Language-Driven Testing and Evaluation Platform for\n  Vision-Language-Action Models in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADEV: A Language-Driven Testing and Evaluation Platform for\n  Vision-Language-Action Models in Robotic Manipulation"
                },
                "summary": "Building on the advancements of Large Language Models (LLMs) and Vision\nLanguage Models (VLMs), recent research has introduced Vision-Language-Action\n(VLA) models as an integrated solution for robotic manipulation tasks. These\nmodels take camera images and natural language task instructions as input and\ndirectly generate control actions for robots to perform specified tasks,\ngreatly improving both decision-making capabilities and interaction with human\nusers. However, the data-driven nature of VLA models, combined with their lack\nof interpretability, makes the assurance of their effectiveness and robustness\na challenging task. This highlights the need for a reliable testing and\nevaluation platform. For this purpose, in this work, we propose LADEV, a\ncomprehensive and efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that automatically\ngenerates simulation environments from natural language inputs, mitigating the\nneed for manual adjustments and significantly improving testing efficiency.\nThen, to further assess the influence of language input on the VLA models, we\nimplement a paraphrase mechanism that produces diverse natural language task\ninstructions for testing. Finally, to expedite the evaluation process, we\nintroduce a batch-style method for conducting large-scale testing of VLA\nmodels. Using LADEV, we conducted experiments on several state-of-the-art VLA\nmodels, demonstrating its effectiveness as a tool for evaluating these models.\nOur results showed that LADEV not only enhances testing efficiency but also\nestablishes a solid baseline for evaluating VLA models, paving the way for the\ndevelopment of more intelligent and advanced robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the advancements of Large Language Models (LLMs) and Vision\nLanguage Models (VLMs), recent research has introduced Vision-Language-Action\n(VLA) models as an integrated solution for robotic manipulation tasks. These\nmodels take camera images and natural language task instructions as input and\ndirectly generate control actions for robots to perform specified tasks,\ngreatly improving both decision-making capabilities and interaction with human\nusers. However, the data-driven nature of VLA models, combined with their lack\nof interpretability, makes the assurance of their effectiveness and robustness\na challenging task. This highlights the need for a reliable testing and\nevaluation platform. For this purpose, in this work, we propose LADEV, a\ncomprehensive and efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that automatically\ngenerates simulation environments from natural language inputs, mitigating the\nneed for manual adjustments and significantly improving testing efficiency.\nThen, to further assess the influence of language input on the VLA models, we\nimplement a paraphrase mechanism that produces diverse natural language task\ninstructions for testing. Finally, to expedite the evaluation process, we\nintroduce a batch-style method for conducting large-scale testing of VLA\nmodels. Using LADEV, we conducted experiments on several state-of-the-art VLA\nmodels, demonstrating its effectiveness as a tool for evaluating these models.\nOur results showed that LADEV not only enhances testing efficiency but also\nestablishes a solid baseline for evaluating VLA models, paving the way for the\ndevelopment of more intelligent and advanced robotic systems."
                },
                "authors": [
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Zhehua Zhou"
                    },
                    {
                        "name": "Jiayang Song"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00099v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00099v4",
                "updated": "2024-10-07T16:45:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    45,
                    42,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-30T18:00:02Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    18,
                    0,
                    2,
                    1,
                    121,
                    0
                ],
                "title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation"
                },
                "summary": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00099v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00099v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18693v2",
                "updated": "2024-10-07T16:42:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    42,
                    20,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-27T12:28:20Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    28,
                    20,
                    4,
                    271,
                    0
                ],
                "title": "Heavy Elements Abundances Inferred from the First Adiabatic Exponent in\n  the Solar Envelope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heavy Elements Abundances Inferred from the First Adiabatic Exponent in\n  the Solar Envelope"
                },
                "summary": "The first adiabatic exponent profile, noted $\\Gamma_1$, computed along\nadiabatic coordinates $(T, \\rho)$ is in the focus of our study. Under\nconditions of almost fully ionized hydrogen and helium, the $\\Gamma_1$ profile\nis quite sensitive to heavy elements ionization. $\\Gamma_1$ decreases in\nregions where an element is partially ionized. The recent helioseismic\nstructural inversion is obtained with an accuracy better than $10^{-4}$ in the\nmost of the adiabatic convective zone that allows to study ionization\nvariations. The aim is to determine the major heavy elements content in the\nsolar convective zone. The method of our research is synthesis of the\n$\\Gamma_1$ profile which is based on a linear combination of the contributions\nof individual heavy elements. The idea of the approach was proposed and\njustified by Baturin et al. (Astron. Astrophys., 660, A125, 2022). We find the\nbest approximation of the inverted profile $\\Gamma_1$ adjusting the abundances\nof major elements (C, N, O, Ne), meanwhile the abundances of elements heavier\nthan neon are fixed. We synthesize the theoretical $\\Gamma_1$ profile using the\nSAHA-S equation of state, and are able to reproduce the inverted profiles with\nan accuracy of $(1-2)\\cdot 10^{-5}$. Total mass fraction of heavy elements\nfound by this method is $Z=0.0148\\pm 0.0004$. The oxygen logarithmic abundance\nis $8.70\\pm 0.03$, carbon $8.44\\pm 0.04$, nitrogen $8.12\\pm 0.08$, and neon\n$8.17\\pm 0.09$. The obtained estimations of oxygen and carbon agree with\nspectroscopic abundances by Asplund et al. (Astron. Astrophys., 653, A141,\n2021).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first adiabatic exponent profile, noted $\\Gamma_1$, computed along\nadiabatic coordinates $(T, \\rho)$ is in the focus of our study. Under\nconditions of almost fully ionized hydrogen and helium, the $\\Gamma_1$ profile\nis quite sensitive to heavy elements ionization. $\\Gamma_1$ decreases in\nregions where an element is partially ionized. The recent helioseismic\nstructural inversion is obtained with an accuracy better than $10^{-4}$ in the\nmost of the adiabatic convective zone that allows to study ionization\nvariations. The aim is to determine the major heavy elements content in the\nsolar convective zone. The method of our research is synthesis of the\n$\\Gamma_1$ profile which is based on a linear combination of the contributions\nof individual heavy elements. The idea of the approach was proposed and\njustified by Baturin et al. (Astron. Astrophys., 660, A125, 2022). We find the\nbest approximation of the inverted profile $\\Gamma_1$ adjusting the abundances\nof major elements (C, N, O, Ne), meanwhile the abundances of elements heavier\nthan neon are fixed. We synthesize the theoretical $\\Gamma_1$ profile using the\nSAHA-S equation of state, and are able to reproduce the inverted profiles with\nan accuracy of $(1-2)\\cdot 10^{-5}$. Total mass fraction of heavy elements\nfound by this method is $Z=0.0148\\pm 0.0004$. The oxygen logarithmic abundance\nis $8.70\\pm 0.03$, carbon $8.44\\pm 0.04$, nitrogen $8.12\\pm 0.08$, and neon\n$8.17\\pm 0.09$. The obtained estimations of oxygen and carbon agree with\nspectroscopic abundances by Asplund et al. (Astron. Astrophys., 653, A141,\n2021)."
                },
                "authors": [
                    {
                        "name": "V. A. Baturin"
                    },
                    {
                        "name": "A. V. Oreshina"
                    },
                    {
                        "name": "G. Buldgen"
                    },
                    {
                        "name": "S. V. Ayukov"
                    },
                    {
                        "name": "V. K. Gryaznov"
                    },
                    {
                        "name": "I. L. Iosilevskiy"
                    },
                    {
                        "name": "A. Noels"
                    },
                    {
                        "name": "R. Scuflaire"
                    }
                ],
                "author_detail": {
                    "name": "R. Scuflaire"
                },
                "author": "R. Scuflaire",
                "arxiv_comment": "Accepted for publication in Solar Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05180v1",
                "updated": "2024-10-07T16:40:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    40,
                    21,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:40:21Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    40,
                    21,
                    0,
                    281,
                    0
                ],
                "title": "Enhancing Equity in Large Language Models for Medical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Equity in Large Language Models for Medical Applications"
                },
                "summary": "Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Wenhe Ma"
                    },
                    {
                        "name": "Sonish Sivarajkumar"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Eugene Mathew Sadhu"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Xizhi Wu"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02151v3",
                "updated": "2024-10-07T16:35:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    35,
                    15,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-02T17:58:27Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    17,
                    58,
                    27,
                    1,
                    93,
                    0
                ],
                "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks"
                },
                "summary": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flammarion"
                },
                "author": "Nicolas Flammarion",
                "arxiv_comment": "Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved\n  writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B),\n  jailbreak artifacts for all attacks are available, evaluation with different\n  judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots\n  over iterations, ablation on the suffix length for random search), examples\n  of jailbroken generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10054v2",
                "updated": "2024-10-07T16:26:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    26,
                    0,
                    0,
                    281,
                    0
                ],
                "published": "2023-11-16T17:48:55Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    17,
                    48,
                    55,
                    3,
                    320,
                    0
                ],
                "title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models"
                },
                "summary": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles."
                },
                "authors": [
                    {
                        "name": "Mingqian Zheng"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05168v1",
                "updated": "2024-10-07T16:25:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation"
                },
                "summary": "Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02902v2",
                "updated": "2024-10-07T16:25:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-03T18:48:38Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    48,
                    38,
                    3,
                    277,
                    0
                ],
                "title": "Better Instruction-Following Through Minimum Bayes Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Instruction-Following Through Minimum Bayes Risk"
                },
                "summary": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding."
                },
                "authors": [
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Sina Pakazad"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05167v1",
                "updated": "2024-10-07T16:24:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    24,
                    18,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:24:18Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    24,
                    18,
                    0,
                    281,
                    0
                ],
                "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Presto! Distilling Steps and Layers for Accelerating Music Generation"
                },
                "summary": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient,\nhigh-quality generation remains a challenge. We introduce Presto!, an approach\nto inference acceleration for score-based diffusion transformers via reducing\nboth sampling steps and cost per step. To reduce steps, we develop a new\nscore-based distribution matching distillation (DMD) method for the EDM-family\nof diffusion models, the first GAN-based distillation method for TTM. To reduce\nthe cost per step, we develop a simple, but powerful improvement to a recent\nlayer distillation method that improves learning via better preserving hidden\nstate variance. Finally, we combine our step and layer distillation methods\ntogether for a dual-faceted approach. We evaluate our step and layer\ndistillation methods independently and show each yield best-in-class\nperformance. Our combined distillation method can generate high-quality outputs\nwith improved diversity, accelerating our base model by 10-18x (230/435ms\nlatency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --\nthe fastest high-quality TTM to our knowledge. Sound examples can be found at\nhttps://presto-music.github.io/web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient,\nhigh-quality generation remains a challenge. We introduce Presto!, an approach\nto inference acceleration for score-based diffusion transformers via reducing\nboth sampling steps and cost per step. To reduce steps, we develop a new\nscore-based distribution matching distillation (DMD) method for the EDM-family\nof diffusion models, the first GAN-based distillation method for TTM. To reduce\nthe cost per step, we develop a simple, but powerful improvement to a recent\nlayer distillation method that improves learning via better preserving hidden\nstate variance. Finally, we combine our step and layer distillation methods\ntogether for a dual-faceted approach. We evaluate our step and layer\ndistillation methods independently and show each yield best-in-class\nperformance. Our combined distillation method can generate high-quality outputs\nwith improved diversity, accelerating our base model by 10-18x (230/435ms\nlatency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --\nthe fastest high-quality TTM to our knowledge. Sound examples can be found at\nhttps://presto-music.github.io/web/."
                },
                "authors": [
                    {
                        "name": "Zachary Novack"
                    },
                    {
                        "name": "Ge Zhu"
                    },
                    {
                        "name": "Jonah Casebeer"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Nicholas J. Bryan"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas J. Bryan"
                },
                "author": "Nicholas J. Bryan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05165v1",
                "updated": "2024-10-07T16:23:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    23,
                    36,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:23:36Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    23,
                    36,
                    0,
                    281,
                    0
                ],
                "title": "Efficient Inference for Large Language Model-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Large Language Model-based Generative\n  Recommendation"
                },
                "summary": "Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05109v2",
                "updated": "2024-10-07T16:21:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    21,
                    29,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-07T18:58:50Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    18,
                    58,
                    50,
                    2,
                    38,
                    0
                ],
                "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding"
                },
                "summary": "To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding frame-work. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of lightweight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads: a sequentially-dependent\ndrop-in replacement for standard draft heads that significantly improves the\naccuracy of draft head speculation. We further explore the design space of\nHydra head training objectives and architectures, and propose a carefully tuned\nHydra head recipe, which we call Hydra++, that improves decoding throughput by\nup to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding\nrespectively. Overall, Hydra heads are a simple and well-motivated intervention\non standard draft heads that significantly improve the end-to-end speed of\ndraft head-based speculative decoding. We make our code publicly available at\nhttps://github.com/zankner/Hydra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding frame-work. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of lightweight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads: a sequentially-dependent\ndrop-in replacement for standard draft heads that significantly improves the\naccuracy of draft head speculation. We further explore the design space of\nHydra head training objectives and architectures, and propose a carefully tuned\nHydra head recipe, which we call Hydra++, that improves decoding throughput by\nup to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding\nrespectively. Overall, Hydra heads are a simple and well-motivated intervention\non standard draft heads that significantly improve the end-to-end speed of\ndraft head-based speculative decoding. We make our code publicly available at\nhttps://github.com/zankner/Hydra."
                },
                "authors": [
                    {
                        "name": "Zachary Ankner"
                    },
                    {
                        "name": "Rishab Parthasarathy"
                    },
                    {
                        "name": "Aniruddha Nrusimha"
                    },
                    {
                        "name": "Christopher Rinard"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "William Brandon"
                    }
                ],
                "author_detail": {
                    "name": "William Brandon"
                },
                "author": "William Brandon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14577v2",
                "updated": "2024-10-07T16:01:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    1,
                    49,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-23T13:51:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    51,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "Representation noising effectively prevents harmful fine-tuning on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation noising effectively prevents harmful fine-tuning on LLMs"
                },
                "summary": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Kai Williams"
                    },
                    {
                        "name": "Łukasz Bartoszcze"
                    },
                    {
                        "name": "David Atanasov"
                    },
                    {
                        "name": "Robie Gonzales"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    },
                    {
                        "name": "Carsten Maple"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz",
                "arxiv_comment": "Published in NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03853v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03853v5",
                "updated": "2024-10-07T15:46:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    46,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-06T19:07:38Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    19,
                    7,
                    38,
                    2,
                    340,
                    0
                ],
                "title": "Dr. Jekyll and Mr. Hyde: Two Faces of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr. Jekyll and Mr. Hyde: Two Faces of LLMs"
                },
                "summary": "Recently, we have witnessed a rise in the use of Large Language Models\n(LLMs), especially in applications like chatbots. Safety mechanisms are\nimplemented to prevent improper responses from these chatbots. In this work, we\nbypass these measures for ChatGPT and Gemini by making them impersonate complex\npersonas with personality characteristics that are not aligned with a truthful\nassistant. First, we create elaborate biographies of these personas, which we\nthen use in a new session with the same chatbots. Our conversations then follow\na role-play style to elicit prohibited responses. Using personas, we show that\nprohibited responses are provided, making it possible to obtain unauthorized,\nillegal, or harmful information in both ChatGPT and Gemini. We also introduce\nseveral ways of activating such adversarial personas, showing that both\nchatbots are vulnerable to this attack. With the same principle, we introduce\ntwo defenses that push the model to interpret trustworthy personalities and\nmake it more robust against such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, we have witnessed a rise in the use of Large Language Models\n(LLMs), especially in applications like chatbots. Safety mechanisms are\nimplemented to prevent improper responses from these chatbots. In this work, we\nbypass these measures for ChatGPT and Gemini by making them impersonate complex\npersonas with personality characteristics that are not aligned with a truthful\nassistant. First, we create elaborate biographies of these personas, which we\nthen use in a new session with the same chatbots. Our conversations then follow\na role-play style to elicit prohibited responses. Using personas, we show that\nprohibited responses are provided, making it possible to obtain unauthorized,\nillegal, or harmful information in both ChatGPT and Gemini. We also introduce\nseveral ways of activating such adversarial personas, showing that both\nchatbots are vulnerable to this attack. With the same principle, we introduce\ntwo defenses that push the model to interpret trustworthy personalities and\nmake it more robust against such attacks."
                },
                "authors": [
                    {
                        "name": "Matteo Gioele Collu"
                    },
                    {
                        "name": "Tom Janssen-Groesbeek"
                    },
                    {
                        "name": "Stefanos Koffas"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Stjepan Picek"
                    }
                ],
                "author_detail": {
                    "name": "Stjepan Picek"
                },
                "author": "Stjepan Picek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03853v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03853v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00126v2",
                "updated": "2024-10-07T15:44:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    44,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-29T21:05:37Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    21,
                    5,
                    37,
                    3,
                    60,
                    0
                ],
                "title": "FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition"
                },
                "summary": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Wang"
                    },
                    {
                        "name": "Lingfei Wu"
                    },
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "arxiv_comment": "Accepted at EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03036v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03036v4",
                "updated": "2024-10-07T15:36:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    36,
                    18,
                    0,
                    281,
                    0
                ],
                "published": "2024-01-05T19:19:49Z",
                "published_parsed": [
                    2024,
                    1,
                    5,
                    19,
                    19,
                    49,
                    4,
                    5,
                    0
                ],
                "title": "Modelling and calibration of pair-rule protein patterns in Drosophila\n  embryo: From Even-skipped and Fushi-tarazu to Wingless expression networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling and calibration of pair-rule protein patterns in Drosophila\n  embryo: From Even-skipped and Fushi-tarazu to Wingless expression networks"
                },
                "summary": "We modelled and calibrated the distributions of the seven-stripe patterns of\nEven-skipped (\\textit{Eve}) and Fushi-tarazu (\\textit{Ftz}) pair-rule proteins\nalong the anteroposterior axis of the \\textit{Drosphila} embryo, established\nduring early development. We have identified the putative repressive\ncombinations for five \\textit{Eve} enhancers, and we have explored the\nrelationship between \\textit{Eve} and \\textit{Ftz} for complementary patterns.\nThe regulators of \\textit{Eve} and \\textit{Ftz} are stripe-specific DNA\nenhancers with embryo position-dependent activation rates and are regulated by\nthe gap family of proteins. We achieved remarkable data matching of the\n\\textit{Eve} stripe pattern, and the calibrated model reproduces gap gene\nmutation experiments. Extended work inferring the Wingless (\\textit{Wg})\nfourteen stripe pattern from \\textit{Eve} and \\textit{Ftz} enhancers have been\nproposed, clarifying the hierarchical structure of \\textit{Drosphila}'s genetic\nexpression network during early development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We modelled and calibrated the distributions of the seven-stripe patterns of\nEven-skipped (\\textit{Eve}) and Fushi-tarazu (\\textit{Ftz}) pair-rule proteins\nalong the anteroposterior axis of the \\textit{Drosphila} embryo, established\nduring early development. We have identified the putative repressive\ncombinations for five \\textit{Eve} enhancers, and we have explored the\nrelationship between \\textit{Eve} and \\textit{Ftz} for complementary patterns.\nThe regulators of \\textit{Eve} and \\textit{Ftz} are stripe-specific DNA\nenhancers with embryo position-dependent activation rates and are regulated by\nthe gap family of proteins. We achieved remarkable data matching of the\n\\textit{Eve} stripe pattern, and the calibrated model reproduces gap gene\nmutation experiments. Extended work inferring the Wingless (\\textit{Wg})\nfourteen stripe pattern from \\textit{Eve} and \\textit{Ftz} enhancers have been\nproposed, clarifying the hierarchical structure of \\textit{Drosphila}'s genetic\nexpression network during early development."
                },
                "authors": [
                    {
                        "name": "Catarina Dias"
                    },
                    {
                        "name": "Rui Dilão"
                    }
                ],
                "author_detail": {
                    "name": "Rui Dilão"
                },
                "author": "Rui Dilão",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03036v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03036v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05130v1",
                "updated": "2024-10-07T15:34:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    34,
                    14,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T15:34:14Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    34,
                    14,
                    0,
                    281,
                    0
                ],
                "title": "Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents"
                },
                "summary": "Recent research has explored the use of Large Language Models (LLMs) for\ntackling complex graph reasoning tasks. However, due to the intricacies of\ngraph structures and the inherent limitations of LLMs in handling long text,\ncurrent approaches often fail to deliver satisfactory accuracy, even on\nsmall-scale graphs and simple tasks. To address these challenges, we introduce\nGraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent\ncollaboration strategy for explicit and precise graph reasoning. Inspired by\ndistributed graph computation theory, our framework decomposes graph problems\ninto smaller, node-centric tasks that are distributed among multiple agents.\nThe agents collaborate to solve the overall problem, significantly reducing the\namount of information and complexity handled by a single LLM, thus enhancing\nthe accuracy of graph reasoning. By simply increasing the number of agents,\nGraphAgent-Reasoner can efficiently scale to accommodate larger graphs with\nover 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework\ndemonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,\nsignificantly outperforming the best available models, both closed-source and\nfine-tuned open-source variants. Our framework also demonstrates the capability\nto handle real-world graph reasoning applications such as webpage importance\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has explored the use of Large Language Models (LLMs) for\ntackling complex graph reasoning tasks. However, due to the intricacies of\ngraph structures and the inherent limitations of LLMs in handling long text,\ncurrent approaches often fail to deliver satisfactory accuracy, even on\nsmall-scale graphs and simple tasks. To address these challenges, we introduce\nGraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent\ncollaboration strategy for explicit and precise graph reasoning. Inspired by\ndistributed graph computation theory, our framework decomposes graph problems\ninto smaller, node-centric tasks that are distributed among multiple agents.\nThe agents collaborate to solve the overall problem, significantly reducing the\namount of information and complexity handled by a single LLM, thus enhancing\nthe accuracy of graph reasoning. By simply increasing the number of agents,\nGraphAgent-Reasoner can efficiently scale to accommodate larger graphs with\nover 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework\ndemonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,\nsignificantly outperforming the best available models, both closed-source and\nfine-tuned open-source variants. Our framework also demonstrates the capability\nto handle real-world graph reasoning applications such as webpage importance\nanalysis."
                },
                "authors": [
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Xinyi Huang"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yongchao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao Liu"
                },
                "author": "Yongchao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16424v2",
                "updated": "2024-10-07T15:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    33,
                    37,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-24T08:18:19Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    8,
                    18,
                    19,
                    0,
                    176,
                    0
                ],
                "title": "Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial\n  Optimization"
                },
                "summary": "Combinatorial Optimization is crucial to numerous real-world applications,\nyet still presents challenges due to its (NP-)hard nature. Amongst existing\napproaches, heuristics often offer the best trade-off between quality and\nscalability, making them suitable for industrial use. While Reinforcement\nLearning (RL) offers a flexible framework for designing heuristics, its\nadoption over handcrafted heuristics remains incomplete within industrial\nsolvers. Existing learned methods still lack the ability to adapt to specific\ninstances and fully leverage the available computational budget. The current\nbest methods either rely on a collection of pre-trained policies, or on\ndata-inefficient fine-tuning; hence failing to fully utilize newly available\ninformation within the constraints of the budget. In response, we present\nMEMENTO, an approach that leverages memory to improve the adaptation of neural\nsolvers at inference time. MEMENTO enables updating the action distribution\ndynamically based on the outcome of previous decisions. We validate its\neffectiveness on benchmark problems, in particular Traveling Salesman and\nCapacitated Vehicle Routing, demonstrating its superiority over tree-search and\npolicy-gradient fine-tuning; and showing it can be zero-shot combined with\ndiversity-based solvers. We successfully train all RL auto-regressive solvers\non large instances, and show that MEMENTO can scale and is data-efficient.\nOverall, MEMENTO enables to push the state-of-the-art on 11 out of 12 evaluated\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Optimization is crucial to numerous real-world applications,\nyet still presents challenges due to its (NP-)hard nature. Amongst existing\napproaches, heuristics often offer the best trade-off between quality and\nscalability, making them suitable for industrial use. While Reinforcement\nLearning (RL) offers a flexible framework for designing heuristics, its\nadoption over handcrafted heuristics remains incomplete within industrial\nsolvers. Existing learned methods still lack the ability to adapt to specific\ninstances and fully leverage the available computational budget. The current\nbest methods either rely on a collection of pre-trained policies, or on\ndata-inefficient fine-tuning; hence failing to fully utilize newly available\ninformation within the constraints of the budget. In response, we present\nMEMENTO, an approach that leverages memory to improve the adaptation of neural\nsolvers at inference time. MEMENTO enables updating the action distribution\ndynamically based on the outcome of previous decisions. We validate its\neffectiveness on benchmark problems, in particular Traveling Salesman and\nCapacitated Vehicle Routing, demonstrating its superiority over tree-search and\npolicy-gradient fine-tuning; and showing it can be zero-shot combined with\ndiversity-based solvers. We successfully train all RL auto-regressive solvers\non large instances, and show that MEMENTO can scale and is data-efficient.\nOverall, MEMENTO enables to push the state-of-the-art on 11 out of 12 evaluated\ntasks."
                },
                "authors": [
                    {
                        "name": "Felix Chalumeau"
                    },
                    {
                        "name": "Refiloe Shabe"
                    },
                    {
                        "name": "Noah De Nicola"
                    },
                    {
                        "name": "Arnu Pretorius"
                    },
                    {
                        "name": "Thomas D. Barrett"
                    },
                    {
                        "name": "Nathan Grinsztajn"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Grinsztajn"
                },
                "author": "Nathan Grinsztajn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v2",
                "updated": "2024-10-07T15:24:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    24,
                    10,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17385v2",
                "updated": "2024-10-07T15:15:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    15,
                    18,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-25T09:04:21Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    9,
                    4,
                    21,
                    1,
                    177,
                    0
                ],
                "title": "Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance"
                },
                "summary": "Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency."
                },
                "authors": [
                    {
                        "name": "Manon Reusens"
                    },
                    {
                        "name": "Philipp Borchert"
                    },
                    {
                        "name": "Jochen De Weerdt"
                    },
                    {
                        "name": "Bart Baesens"
                    }
                ],
                "author_detail": {
                    "name": "Bart Baesens"
                },
                "author": "Bart Baesens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15929v2",
                "updated": "2024-10-07T15:01:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    48,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-24T23:16:57Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    23,
                    16,
                    57,
                    5,
                    55,
                    0
                ],
                "title": "Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs"
                },
                "summary": "Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels."
                },
                "authors": [
                    {
                        "name": "Isha Chaudhary"
                    },
                    {
                        "name": "Vedaant V. Jain"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05102v1",
                "updated": "2024-10-07T15:01:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    29,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T15:01:29Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    29,
                    0,
                    281,
                    0
                ],
                "title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks"
                },
                "summary": "Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods."
                },
                "authors": [
                    {
                        "name": "Fenia Christopoulou"
                    },
                    {
                        "name": "Ronald Cardenas"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "20 papges, 9 figures, 5 tables. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05099v1",
                "updated": "2024-10-07T14:55:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    55,
                    20,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:55:20Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    55,
                    20,
                    0,
                    281,
                    0
                ],
                "title": "Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances"
                },
                "summary": "Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them."
                },
                "authors": [
                    {
                        "name": "Alina Wróblewska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Wróblewska"
                },
                "author": "Alina Wróblewska",
                "arxiv_comment": "Accepted at CoNLL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02707v2",
                "updated": "2024-10-07T14:46:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    46,
                    11,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-03T17:31:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations"
                },
                "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."
                },
                "authors": [
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16671v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16671v7",
                "updated": "2024-10-07T14:44:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    44,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-26T15:47:01Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    15,
                    47,
                    1,
                    0,
                    57,
                    0
                ],
                "title": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding"
                },
                "summary": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level."
                },
                "authors": [
                    {
                        "name": "Alex Zhuang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Weiming Ren"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16671v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16671v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05090v1",
                "updated": "2024-10-07T14:42:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    42,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:42:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    42,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "HyperINF: Unleashing the HyperPower of the Schulz's Method for Data\n  Influence Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperINF: Unleashing the HyperPower of the Schulz's Method for Data\n  Influence Estimation"
                },
                "summary": "Influence functions provide a principled method to assess the contribution of\nindividual training samples to a specific target. Yet, their high computational\ncosts limit their applications on large-scale models and datasets. Existing\nmethods proposed for influence function approximation have significantly\nreduced the computational overheads. However, they mostly suffer from\ninaccurate estimation due to the lack of strong convergence guarantees from the\nalgorithm. The family of hyperpower methods are well-known for their rigorous\nconvergence guarantees on matrix inverse approximation, while the matrix\nmultiplication operation can involve intractable memory and computation costs\non large-scale models. We propose HyperINF, an efficient and accurate influence\nfunction approximation method which leverages the hyperpower method,\nspecifically Schulz's iterative algorithm.\n  To deal with the computation-intensive matrix multiplication, we incorporate\nthe generalized fisher information (GFIM) as a low-rank approximation of the\nHessian matrix, which reduces the memory and computation overheads to constant\ncosts independent of ranks on LoRA-tuned models.\n  We first demonstrate the superior accuracy and stability of \\method compared\nto other baselines through a synthetic convergence simulation for matrix\ninversion. We further validate the efficacy of \\method through extensive\nreal-world data attribution tasks, including mislabeled data detection and data\nselection for LLM and VLM fine-tuning.\n  On LoRA-tuned models, HyperINF achieves superior downstream performance with\nminimal memory and computational overhead, while other baselines suffer from\nsignificant degradation. Our codebase is available at\nhttps://github.com/Blackzxy/HyperINF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence functions provide a principled method to assess the contribution of\nindividual training samples to a specific target. Yet, their high computational\ncosts limit their applications on large-scale models and datasets. Existing\nmethods proposed for influence function approximation have significantly\nreduced the computational overheads. However, they mostly suffer from\ninaccurate estimation due to the lack of strong convergence guarantees from the\nalgorithm. The family of hyperpower methods are well-known for their rigorous\nconvergence guarantees on matrix inverse approximation, while the matrix\nmultiplication operation can involve intractable memory and computation costs\non large-scale models. We propose HyperINF, an efficient and accurate influence\nfunction approximation method which leverages the hyperpower method,\nspecifically Schulz's iterative algorithm.\n  To deal with the computation-intensive matrix multiplication, we incorporate\nthe generalized fisher information (GFIM) as a low-rank approximation of the\nHessian matrix, which reduces the memory and computation overheads to constant\ncosts independent of ranks on LoRA-tuned models.\n  We first demonstrate the superior accuracy and stability of \\method compared\nto other baselines through a synthetic convergence simulation for matrix\ninversion. We further validate the efficacy of \\method through extensive\nreal-world data attribution tasks, including mislabeled data detection and data\nselection for LLM and VLM fine-tuning.\n  On LoRA-tuned models, HyperINF achieves superior downstream performance with\nminimal memory and computational overhead, while other baselines suffer from\nsignificant degradation. Our codebase is available at\nhttps://github.com/Blackzxy/HyperINF."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Simin Fan"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05085v1",
                "updated": "2024-10-07T14:39:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    39,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:39:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    39,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification"
                },
                "summary": "Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness."
                },
                "authors": [
                    {
                        "name": "Jeremie Bogaert"
                    },
                    {
                        "name": "Marie-Catherine de Marneffe"
                    },
                    {
                        "name": "Antonin Descampe"
                    },
                    {
                        "name": "Louis Escouflaire"
                    },
                    {
                        "name": "Cedrick Fairon"
                    },
                    {
                        "name": "Francois-Xavier Standaert"
                    }
                ],
                "author_detail": {
                    "name": "Francois-Xavier Standaert"
                },
                "author": "Francois-Xavier Standaert",
                "arxiv_comment": "This paper is a faithful translation of a paper which was\n  peer-reviewed and published in the French journal Traitement Automatique des\n  Langues, n. 64",
                "arxiv_journal_ref": "Traitement Automatique des Langues 64, 2023, ATALA, Paris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14768v2",
                "updated": "2024-10-07T14:35:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    35,
                    14,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-23T16:35:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    35,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05080v1",
                "updated": "2024-10-07T14:33:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    33,
                    50,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:33:50Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    33,
                    50,
                    0,
                    281,
                    0
                ],
                "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery"
                },
                "summary": "The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research."
                },
                "authors": [
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Shijie Chen"
                    },
                    {
                        "name": "Yuting Ning"
                    },
                    {
                        "name": "Qianheng Zhang"
                    },
                    {
                        "name": "Boshi Wang"
                    },
                    {
                        "name": "Botao Yu"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Zitong Lu"
                    },
                    {
                        "name": "Vishal Dey"
                    },
                    {
                        "name": "Mingyi Xue"
                    },
                    {
                        "name": "Frazier N. Baker"
                    },
                    {
                        "name": "Benjamin Burns"
                    },
                    {
                        "name": "Daniel Adu-Ampratwum"
                    },
                    {
                        "name": "Xuhui Huang"
                    },
                    {
                        "name": "Xia Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "55 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05077v1",
                "updated": "2024-10-07T14:31:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    31,
                    43,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:31:43Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    31,
                    43,
                    0,
                    281,
                    0
                ],
                "title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering"
                },
                "summary": "Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points."
                },
                "authors": [
                    {
                        "name": "Francesco Maria Molfese"
                    },
                    {
                        "name": "Simone Conia"
                    },
                    {
                        "name": "Riccardo Orlando"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12034v2",
                "updated": "2024-10-07T14:27:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    27,
                    56,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-17T19:06:54Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    19,
                    6,
                    54,
                    0,
                    169,
                    0
                ],
                "title": "Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts"
                },
                "summary": "We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems."
                },
                "authors": [
                    {
                        "name": "Junmo Kang"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Jacob Hansen"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05069v1",
                "updated": "2024-10-07T14:26:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    26,
                    16,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:26:16Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    26,
                    16,
                    0,
                    281,
                    0
                ],
                "title": "Quantile regression under dependent censoring with unknown association",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile regression under dependent censoring with unknown association"
                },
                "summary": "The study of survival data often requires taking proper care of the censoring\nmechanism that prohibits complete observation of the data. Under right\ncensoring, only the first occurring event is observed: either the event of\ninterest, or a competing event like withdrawal of a subject from the study. The\ncorresponding identifiability difficulties led many authors to imposing\n(conditional) independence or a fully known dependence between survival and\ncensoring times, both of which are not always realistic. However, recent\nresults in survival literature showed that parametric copula models allow\nidentification of all model parameters, including the association parameter,\nunder appropriately chosen marginal distributions. The present paper is the\nfirst one to apply such models in a quantile regression context, hence\nbenefiting from its well-known advantages in terms of e.g. robustness and\nricher inference results. The parametric copula is supplemented with a likewise\nparametric, yet flexible, enriched asymmetric Laplace distribution for the\nsurvival times conditional on the covariates. Its asymmetric Laplace basis\nprovides its close connection to quantiles, while the extension with Laguerre\northogonal polynomials ensures sufficient flexibility for increasing polynomial\ndegrees. The distributional flavour of the quantile regression presented, comes\nwith advantages of both theoretical and computational nature. All model\nparameters are proven to be identifiable, consistent, and asymptotically\nnormal. Finally, performance of the model and of the proposed estimation\nprocedure is assessed through extensive simulation studies as well as an\napplication on liver transplant data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of survival data often requires taking proper care of the censoring\nmechanism that prohibits complete observation of the data. Under right\ncensoring, only the first occurring event is observed: either the event of\ninterest, or a competing event like withdrawal of a subject from the study. The\ncorresponding identifiability difficulties led many authors to imposing\n(conditional) independence or a fully known dependence between survival and\ncensoring times, both of which are not always realistic. However, recent\nresults in survival literature showed that parametric copula models allow\nidentification of all model parameters, including the association parameter,\nunder appropriately chosen marginal distributions. The present paper is the\nfirst one to apply such models in a quantile regression context, hence\nbenefiting from its well-known advantages in terms of e.g. robustness and\nricher inference results. The parametric copula is supplemented with a likewise\nparametric, yet flexible, enriched asymmetric Laplace distribution for the\nsurvival times conditional on the covariates. Its asymmetric Laplace basis\nprovides its close connection to quantiles, while the extension with Laguerre\northogonal polynomials ensures sufficient flexibility for increasing polynomial\ndegrees. The distributional flavour of the quantile regression presented, comes\nwith advantages of both theoretical and computational nature. All model\nparameters are proven to be identifiable, consistent, and asymptotically\nnormal. Finally, performance of the model and of the proposed estimation\nprocedure is assessed through extensive simulation studies as well as an\napplication on liver transplant data."
                },
                "authors": [
                    {
                        "name": "Myrthe D'Haen"
                    },
                    {
                        "name": "Ingrid Van Keilegom"
                    },
                    {
                        "name": "Anneleen Verhasselt"
                    }
                ],
                "author_detail": {
                    "name": "Anneleen Verhasselt"
                },
                "author": "Anneleen Verhasselt",
                "arxiv_comment": "40 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05062v1",
                "updated": "2024-10-07T14:21:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    21,
                    17,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:21:17Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    21,
                    17,
                    0,
                    281,
                    0
                ],
                "title": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks"
                },
                "summary": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence."
                },
                "authors": [
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02458v2",
                "updated": "2024-10-07T14:20:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    20,
                    42,
                    0,
                    281,
                    0
                ],
                "published": "2024-01-04T08:00:32Z",
                "published_parsed": [
                    2024,
                    1,
                    4,
                    8,
                    0,
                    32,
                    3,
                    4,
                    0
                ],
                "title": "Data-Centric Foundation Models in Computational Healthcare: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Centric Foundation Models in Computational Healthcare: A Survey"
                },
                "summary": "The advent of foundation models (FMs) as an emerging suite of AI techniques\nhas struck a wave of opportunities in computational healthcare. The interactive\nnature of these models, guided by pre-training data and human instructions, has\nignited a data-centric AI paradigm that emphasizes better data\ncharacterization, quality, and scale. In healthcare AI, obtaining and\nprocessing high-quality clinical data records has been a longstanding\nchallenge, ranging from data quantity, annotation, patient privacy, and ethics.\nIn this survey, we investigate a wide range of data-centric approaches in the\nFM era (from model pre-training to inference) towards improving the healthcare\nworkflow. We discuss key perspectives in AI security, assessment, and alignment\nwith human values. Finally, we offer a promising outlook of FM-based analytics\nto enhance the performance of patient outcome and clinical workflow in the\nevolving landscape of healthcare and medicine. We provide an up-to-date list of\nhealthcare-related foundation models and datasets at\nhttps://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models (FMs) as an emerging suite of AI techniques\nhas struck a wave of opportunities in computational healthcare. The interactive\nnature of these models, guided by pre-training data and human instructions, has\nignited a data-centric AI paradigm that emphasizes better data\ncharacterization, quality, and scale. In healthcare AI, obtaining and\nprocessing high-quality clinical data records has been a longstanding\nchallenge, ranging from data quantity, annotation, patient privacy, and ethics.\nIn this survey, we investigate a wide range of data-centric approaches in the\nFM era (from model pre-training to inference) towards improving the healthcare\nworkflow. We discuss key perspectives in AI security, assessment, and alignment\nwith human values. Finally, we offer a promising outlook of FM-based analytics\nto enhance the performance of patient outcome and clinical workflow in the\nevolving landscape of healthcare and medicine. We provide an up-to-date list of\nhealthcare-related foundation models and datasets at\nhttps://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare ."
                },
                "authors": [
                    {
                        "name": "Yunkun Zhang"
                    },
                    {
                        "name": "Jin Gao"
                    },
                    {
                        "name": "Zheling Tan"
                    },
                    {
                        "name": "Lingfeng Zhou"
                    },
                    {
                        "name": "Kexin Ding"
                    },
                    {
                        "name": "Mu Zhou"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Dequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dequan Wang"
                },
                "author": "Dequan Wang",
                "arxiv_comment": "Survey content updated to include recent research work and progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06474v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06474v3",
                "updated": "2024-10-07T14:19:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    19,
                    37,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-09T17:25:47Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    17,
                    25,
                    47,
                    1,
                    100,
                    0
                ],
                "title": "Autonomous Evaluation and Refinement of Digital Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Evaluation and Refinement of Digital Agents"
                },
                "summary": "We show that domain-general automatic evaluators can significantly improve\nthe performance of agents for web navigation and device control. We experiment\nwith multiple evaluation models that trade off between inference cost,\nmodularity of design, and accuracy. We validate the performance of these models\nin several popular benchmarks for digital agents, finding between 74.4 and\n92.9% agreement with oracle evaluation metrics. Finally, we use these\nevaluators to improve the performance of existing agents via fine-tuning and\ninference-time guidance. Without any additional supervision, we improve\nstate-of-the-art performance by 29% on the popular benchmark WebArena, and\nachieve around 75% relative improvement in device control settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that domain-general automatic evaluators can significantly improve\nthe performance of agents for web navigation and device control. We experiment\nwith multiple evaluation models that trade off between inference cost,\nmodularity of design, and accuracy. We validate the performance of these models\nin several popular benchmarks for digital agents, finding between 74.4 and\n92.9% agreement with oracle evaluation metrics. Finally, we use these\nevaluators to improve the performance of existing agents via fine-tuning and\ninference-time guidance. Without any additional supervision, we improve\nstate-of-the-art performance by 29% on the popular benchmark WebArena, and\nachieve around 75% relative improvement in device control settings."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Nicholas Tomlin"
                    },
                    {
                        "name": "Yifei Zhou"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Alane Suhr"
                    }
                ],
                "author_detail": {
                    "name": "Alane Suhr"
                },
                "author": "Alane Suhr",
                "arxiv_comment": "Published at COLM 2024. Code at\n  https://github.com/Berkeley-NLP/Agent-Eval-Refine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06474v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06474v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19745v2",
                "updated": "2024-10-07T14:17:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    17,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-29T15:40:54Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    15,
                    40,
                    54,
                    6,
                    273,
                    0
                ],
                "title": "PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead"
                },
                "summary": "Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability."
                },
                "authors": [
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Yining Qian"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Yongbo Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Jingtong Wu"
                    },
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12058v4",
                "updated": "2024-10-07T14:08:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    8,
                    13,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-17T19:50:40Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    19,
                    50,
                    40,
                    0,
                    169,
                    0
                ],
                "title": "WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions"
                },
                "summary": "Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being."
                },
                "authors": [
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Jinendra Malekar"
                    },
                    {
                        "name": "Vedant Palit"
                    },
                    {
                        "name": "Francis Ferraro"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "Accepted in BlackboxNLP @ EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05047v1",
                "updated": "2024-10-07T14:01:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    1,
                    20,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:01:20Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    1,
                    20,
                    0,
                    281,
                    0
                ],
                "title": "A test suite of prompt injection attacks for LLM-based machine\n  translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A test suite of prompt injection attacks for LLM-based machine\n  translation"
                },
                "summary": "LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied."
                },
                "authors": [
                    {
                        "name": "Antonio Valerio Miceli-Barone"
                    },
                    {
                        "name": "Zhifan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhifan Sun"
                },
                "author": "Zhifan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05045v1",
                "updated": "2024-10-07T14:00:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    0,
                    8,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:00:08Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    0,
                    8,
                    0,
                    281,
                    0
                ],
                "title": "Can LLMs plan paths with extra hints from solvers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs plan paths with extra hints from solvers?"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs."
                },
                "authors": [
                    {
                        "name": "Erik Wu"
                    },
                    {
                        "name": "Sayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Mitra"
                },
                "author": "Sayan Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07125v2",
                "updated": "2024-10-07T13:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    54,
                    37,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-09T07:05:53Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    7,
                    5,
                    53,
                    1,
                    191,
                    0
                ],
                "title": "Rapid Parameter Estimation for Merging Massive Black Hole Binaries Using\n  Continuous Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Parameter Estimation for Merging Massive Black Hole Binaries Using\n  Continuous Normalizing Flows"
                },
                "summary": "Detecting the coalescences of massive black hole binaries (MBHBs) is one of\nthe primary targets for space-based gravitational wave observatories such as\nLISA, Taiji, and Tianqin. The fast and accurate parameter estimation of merging\nMBHBs is of great significance for the global fitting of all resolvable\nsources, as well as the astrophysical interpretation of gravitational wave\nsignals. However, such analyses usually entail significant computational costs.\nTo address these challenges, inspired by the latest progress in generative\nmodels, we explore the application of continuous normalizing flows (CNFs) on\nthe parameter estimation of MBHBs. Specifically, we employ linear interpolation\nand trig interpolation methods to construct transport paths for training CNFs.\nAdditionally, we creatively introduce a parameter transformation method based\non the symmetry in the detector's response function. This transformation is\nintegrated within CNFs, allowing us to train the model using a simplified\ndataset, and then perform parameter estimation on more general data, hence also\nacting as a crucial factor in improving the training speed. In conclusion, for\nthe first time, within a comprehensive and reasonable parameter range, we have\nachieved a complete and unbiased 11-dimensional rapid inference for MBHBs in\nthe presence of astrophysical confusion noise using CNFs. In the experiments\nbased on simulated data, our model produces posterior distributions comparable\nto those obtained by nested sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the coalescences of massive black hole binaries (MBHBs) is one of\nthe primary targets for space-based gravitational wave observatories such as\nLISA, Taiji, and Tianqin. The fast and accurate parameter estimation of merging\nMBHBs is of great significance for the global fitting of all resolvable\nsources, as well as the astrophysical interpretation of gravitational wave\nsignals. However, such analyses usually entail significant computational costs.\nTo address these challenges, inspired by the latest progress in generative\nmodels, we explore the application of continuous normalizing flows (CNFs) on\nthe parameter estimation of MBHBs. Specifically, we employ linear interpolation\nand trig interpolation methods to construct transport paths for training CNFs.\nAdditionally, we creatively introduce a parameter transformation method based\non the symmetry in the detector's response function. This transformation is\nintegrated within CNFs, allowing us to train the model using a simplified\ndataset, and then perform parameter estimation on more general data, hence also\nacting as a crucial factor in improving the training speed. In conclusion, for\nthe first time, within a comprehensive and reasonable parameter range, we have\nachieved a complete and unbiased 11-dimensional rapid inference for MBHBs in\nthe presence of astrophysical confusion noise using CNFs. In the experiments\nbased on simulated data, our model produces posterior distributions comparable\nto those obtained by nested sampling."
                },
                "authors": [
                    {
                        "name": "Bo Liang"
                    },
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Yuxiang Xu"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiaotong Wei"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Li-e Qiang"
                    },
                    {
                        "name": "Ziren Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ziren Luo"
                },
                "author": "Ziren Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10389v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10389v3",
                "updated": "2024-10-07T13:27:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    27,
                    28,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-15T01:58:54Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    1,
                    58,
                    54,
                    0,
                    197,
                    0
                ],
                "title": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High\n  Quality and Efficient Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High\n  Quality and Efficient Rendering"
                },
                "summary": "Since the introduction of NeRFs, considerable attention has been focused on\nimproving their training and inference times, leading to the development of\nFast-NeRFs models. Despite demonstrating impressive rendering speed and\nquality, the rapid convergence of such models poses challenges for further\nimproving reconstruction quality. Common strategies to improve rendering\nquality involves augmenting model parameters or increasing the number of\nsampled points. However, these computationally intensive approaches encounter\nlimitations in achieving significant quality enhancements. This study\nintroduces a model-agnostic framework inspired by Sparsely-Gated Mixture of\nExperts to enhance rendering quality without escalating computational\ncomplexity. Our approach enables specialization in rendering different scene\ncomponents by employing a mixture of experts with varying resolutions. We\npresent a novel gate formulation designed to maximize expert capabilities and\npropose a resolution-based routing technique to effectively induce sparsity and\ndecompose scenes. Our work significantly improves reconstruction quality while\nmaintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the introduction of NeRFs, considerable attention has been focused on\nimproving their training and inference times, leading to the development of\nFast-NeRFs models. Despite demonstrating impressive rendering speed and\nquality, the rapid convergence of such models poses challenges for further\nimproving reconstruction quality. Common strategies to improve rendering\nquality involves augmenting model parameters or increasing the number of\nsampled points. However, these computationally intensive approaches encounter\nlimitations in achieving significant quality enhancements. This study\nintroduces a model-agnostic framework inspired by Sparsely-Gated Mixture of\nExperts to enhance rendering quality without escalating computational\ncomplexity. Our approach enables specialization in rendering different scene\ncomponents by employing a mixture of experts with varying resolutions. We\npresent a novel gate formulation designed to maximize expert capabilities and\npropose a resolution-based routing technique to effectively induce sparsity and\ndecompose scenes. Our work significantly improves reconstruction quality while\nmaintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Francesco Di Sario"
                    },
                    {
                        "name": "Riccardo Renzulli"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "name": "Marco Grangetto"
                    }
                ],
                "author_detail": {
                    "name": "Marco Grangetto"
                },
                "author": "Marco Grangetto",
                "arxiv_comment": "The paper has been accepted to the ECCV 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10389v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10389v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05020v1",
                "updated": "2024-10-07T13:20:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    20,
                    26,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:20:26Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    20,
                    26,
                    0,
                    281,
                    0
                ],
                "title": "FRIDA: Free-Rider Detection using Privacy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRIDA: Free-Rider Detection using Privacy Attacks"
                },
                "summary": "Federated learning is increasingly popular as it enables multiple parties\nwith limited datasets and resources to train a high-performing machine learning\nmodel collaboratively. However, similarly to other collaborative systems,\nfederated learning is vulnerable to free-riders -- participants who do not\ncontribute to the training but still benefit from the shared model. Free-riders\nnot only compromise the integrity of the learning process but also slow down\nthe convergence of the global model, resulting in increased costs for the\nhonest participants.\n  To address this challenge, we propose FRIDA: free-rider detection using\nprivacy attacks, a framework that leverages inference attacks to detect\nfree-riders. Unlike traditional methods that only capture the implicit effects\nof free-riding, FRIDA directly infers details of the underlying training\ndatasets, revealing characteristics that indicate free-rider behaviour. Through\nextensive experiments, we demonstrate that membership and property inference\nattacks are effective for this purpose. Our evaluation shows that FRIDA\noutperforms state-of-the-art methods, especially in non-IID settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning is increasingly popular as it enables multiple parties\nwith limited datasets and resources to train a high-performing machine learning\nmodel collaboratively. However, similarly to other collaborative systems,\nfederated learning is vulnerable to free-riders -- participants who do not\ncontribute to the training but still benefit from the shared model. Free-riders\nnot only compromise the integrity of the learning process but also slow down\nthe convergence of the global model, resulting in increased costs for the\nhonest participants.\n  To address this challenge, we propose FRIDA: free-rider detection using\nprivacy attacks, a framework that leverages inference attacks to detect\nfree-riders. Unlike traditional methods that only capture the implicit effects\nof free-riding, FRIDA directly infers details of the underlying training\ndatasets, revealing characteristics that indicate free-rider behaviour. Through\nextensive experiments, we demonstrate that membership and property inference\nattacks are effective for this purpose. Our evaluation shows that FRIDA\noutperforms state-of-the-art methods, especially in non-IID settings."
                },
                "authors": [
                    {
                        "name": "Pol G. Recasens"
                    },
                    {
                        "name": "Ádám Horváth"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    },
                    {
                        "name": "Balázs Pejó"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Pejó"
                },
                "author": "Balázs Pejó",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.09697v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.09697v6",
                "updated": "2024-10-07T13:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    17,
                    56,
                    0,
                    281,
                    0
                ],
                "published": "2023-04-19T14:41:14Z",
                "published_parsed": [
                    2023,
                    4,
                    19,
                    14,
                    41,
                    14,
                    2,
                    109,
                    0
                ],
                "title": "A Calculus for Scoped Effects & Handlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Calculus for Scoped Effects & Handlers"
                },
                "summary": "Algebraic effects & handlers have become a standard approach for side-effects\nin functional programming. Their modular composition with other effects and\nclean separation of syntax and semantics make them attractive to a wide\naudience. However, not all effects can be classified as algebraic; some need a\nmore sophisticated handling. In particular, effects that have or create a\ndelimited scope need special care, as their continuation consists of two\nparts-in and out of the scope-and their modular composition introduces\nadditional complexity. These effects are called scoped and have gained\nattention by their growing applicability and adoption in popular libraries.\nWhile calculi have been designed with algebraic effects & handlers built in to\nfacilitate their use, a calculus that supports scoped effects & handlers in a\nsimilar manner does not yet exist. This work fills this gap: we present\n$\\lambda_{\\mathit{sc}}$, a calculus with native support for both algebraic and\nscoped effects & handlers. It addresses the need for polymorphic handlers and\nexplicit clauses for forwarding unknown scoped operations to other handlers.\nOur calculus is based on Eff, an existing calculus for algebraic effects,\nextended with Koka-style row polymorphism, and consists of a formal grammar,\noperational semantics, a (type-safe) type-and-effect system and type inference.\nWe demonstrate $\\lambda_{\\mathit{sc}}$ on a range of examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algebraic effects & handlers have become a standard approach for side-effects\nin functional programming. Their modular composition with other effects and\nclean separation of syntax and semantics make them attractive to a wide\naudience. However, not all effects can be classified as algebraic; some need a\nmore sophisticated handling. In particular, effects that have or create a\ndelimited scope need special care, as their continuation consists of two\nparts-in and out of the scope-and their modular composition introduces\nadditional complexity. These effects are called scoped and have gained\nattention by their growing applicability and adoption in popular libraries.\nWhile calculi have been designed with algebraic effects & handlers built in to\nfacilitate their use, a calculus that supports scoped effects & handlers in a\nsimilar manner does not yet exist. This work fills this gap: we present\n$\\lambda_{\\mathit{sc}}$, a calculus with native support for both algebraic and\nscoped effects & handlers. It addresses the need for polymorphic handlers and\nexplicit clauses for forwarding unknown scoped operations to other handlers.\nOur calculus is based on Eff, an existing calculus for algebraic effects,\nextended with Koka-style row polymorphism, and consists of a formal grammar,\noperational semantics, a (type-safe) type-and-effect system and type inference.\nWe demonstrate $\\lambda_{\\mathit{sc}}$ on a range of examples."
                },
                "authors": [
                    {
                        "name": "Roger Bosman"
                    },
                    {
                        "name": "Birthe van den Berg"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Tom Schrijvers"
                    }
                ],
                "author_detail": {
                    "name": "Tom Schrijvers"
                },
                "author": "Tom Schrijvers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.09697v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.09697v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01104v2",
                "updated": "2024-10-07T13:13:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    13,
                    41,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-01T22:22:35Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    22,
                    22,
                    35,
                    1,
                    275,
                    0
                ],
                "title": "softmax is not enough (for sharp out-of-distribution)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "softmax is not enough (for sharp out-of-distribution)"
                },
                "summary": "A key property of reasoning systems is the ability to make sharp decisions on\ntheir input data. For contemporary AI systems, a key carrier of sharp behaviour\nis the softmax function, with its capability to perform differentiable\nquery-key lookups. It is a common belief that the predictive power of networks\nleveraging softmax arises from \"circuits\" which sharply perform certain kinds\nof computations consistently across many diverse inputs. However, for these\ncircuits to be robust, they would need to generalise well to arbitrary valid\ninputs. In this paper, we dispel this myth: even for tasks as simple as finding\nthe maximum key, any learned circuitry must disperse as the number of items\ngrows at test time. We attribute this to a fundamental limitation of the\nsoftmax function to robustly approximate sharp functions, prove this phenomenon\ntheoretically, and propose adaptive temperature as an ad-hoc technique for\nimproving the sharpness of softmax at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key property of reasoning systems is the ability to make sharp decisions on\ntheir input data. For contemporary AI systems, a key carrier of sharp behaviour\nis the softmax function, with its capability to perform differentiable\nquery-key lookups. It is a common belief that the predictive power of networks\nleveraging softmax arises from \"circuits\" which sharply perform certain kinds\nof computations consistently across many diverse inputs. However, for these\ncircuits to be robust, they would need to generalise well to arbitrary valid\ninputs. In this paper, we dispel this myth: even for tasks as simple as finding\nthe maximum key, any learned circuitry must disperse as the number of items\ngrows at test time. We attribute this to a fundamental limitation of the\nsoftmax function to robustly approximate sharp functions, prove this phenomenon\ntheoretically, and propose adaptive temperature as an ad-hoc technique for\nimproving the sharpness of softmax at inference time."
                },
                "authors": [
                    {
                        "name": "Petar Veličković"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "arxiv_comment": "Comments welcome. 15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19145v2",
                "updated": "2024-10-07T13:07:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    7,
                    51,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-27T21:15:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    21,
                    15,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "Novel scaling laws to derive spatially resolved flare and CME parameters\n  from sun-as-a-star observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel scaling laws to derive spatially resolved flare and CME parameters\n  from sun-as-a-star observables"
                },
                "summary": "Coronal mass ejections (CMEs) are often associated with X-ray (SXR) flares\npowered by magnetic reconnection in the low-corona, while the CME shocks in the\nupper corona and interplanetary (IP) space accelerate electrons often producing\nthe type-II radio bursts. The CME and the reconnection event are part of the\nsame energy release process as highlighted by the correlation between\nreconnection flux ($\\phi_{rec}$) that quantifies the strength of the released\nmagnetic free energy during SXR flare, and the CME kinetic energy that drives\nthe IP shocks leading to type-II bursts. Unlike the sun, these physical\nparameters cannot be directly inferred in stellar observations. Hence, scaling\nlaws between unresolved sun-as-a-star observables, namely SXR luminosity\n($L_X$) and type-II luminosity ($L_R$), and the physical properties of the\nassociated dynamical events are crucial. Such scaling laws also provide\ninsights into the interconnections between the particle acceleration processes\nacross low-corona to IP space during solar-stellar 'flare- CME- type-II'\nevents. Using long-term solar data in SXR to radio waveband, we derive a\nscaling law between two novel power metrics for the flare and CME-associated\nprocesses. The metrics of 'flare power' ($P_{flare}=\\sqrt{L_X\\phi_{rec}}$) and\n'CME power' ($P_{CME}= \\sqrt{L_R {V_{CME}}^2}$), where $V_{CME}$ is the CME\nspeed, scale as $P_{flare}\\propto P_{CME}^{0.76 \\pm 0.04}$. Besides, $L_X$ and\n$\\phi_{rec}$ show power-law trends with $P_{CME}$ with indices of 1.12$\\pm$0.05\nand 0.61$\\pm$0.05 respectively. These power-laws help infer the spatially\nresolved physical parameters, $V_{CME}$ and $\\phi_{rec}$, from disk-averaged\nobservables, $L_X$ and $L_R$ during solar-stellar 'flare- CME- type-II' events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronal mass ejections (CMEs) are often associated with X-ray (SXR) flares\npowered by magnetic reconnection in the low-corona, while the CME shocks in the\nupper corona and interplanetary (IP) space accelerate electrons often producing\nthe type-II radio bursts. The CME and the reconnection event are part of the\nsame energy release process as highlighted by the correlation between\nreconnection flux ($\\phi_{rec}$) that quantifies the strength of the released\nmagnetic free energy during SXR flare, and the CME kinetic energy that drives\nthe IP shocks leading to type-II bursts. Unlike the sun, these physical\nparameters cannot be directly inferred in stellar observations. Hence, scaling\nlaws between unresolved sun-as-a-star observables, namely SXR luminosity\n($L_X$) and type-II luminosity ($L_R$), and the physical properties of the\nassociated dynamical events are crucial. Such scaling laws also provide\ninsights into the interconnections between the particle acceleration processes\nacross low-corona to IP space during solar-stellar 'flare- CME- type-II'\nevents. Using long-term solar data in SXR to radio waveband, we derive a\nscaling law between two novel power metrics for the flare and CME-associated\nprocesses. The metrics of 'flare power' ($P_{flare}=\\sqrt{L_X\\phi_{rec}}$) and\n'CME power' ($P_{CME}= \\sqrt{L_R {V_{CME}}^2}$), where $V_{CME}$ is the CME\nspeed, scale as $P_{flare}\\propto P_{CME}^{0.76 \\pm 0.04}$. Besides, $L_X$ and\n$\\phi_{rec}$ show power-law trends with $P_{CME}$ with indices of 1.12$\\pm$0.05\nand 0.61$\\pm$0.05 respectively. These power-laws help infer the spatially\nresolved physical parameters, $V_{CME}$ and $\\phi_{rec}$, from disk-averaged\nobservables, $L_X$ and $L_R$ during solar-stellar 'flare- CME- type-II' events."
                },
                "authors": [
                    {
                        "name": "Atul Mohan"
                    },
                    {
                        "name": "Natchimuthuk Gopalswamy"
                    },
                    {
                        "name": "Hemapriya Raju"
                    },
                    {
                        "name": "Sachiko Akiyama"
                    }
                ],
                "author_detail": {
                    "name": "Sachiko Akiyama"
                },
                "author": "Sachiko Akiyama",
                "arxiv_comment": "Accepted in A & A Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04996v1",
                "updated": "2024-10-07T12:52:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    52,
                    38,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T12:52:38Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    52,
                    38,
                    0,
                    281,
                    0
                ],
                "title": "Assumption-Lean Post-Integrated Inference with Negative Control Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption-Lean Post-Integrated Inference with Negative Control Outcomes"
                },
                "summary": "Data integration has become increasingly common in aligning multiple\nheterogeneous datasets. With high-dimensional outcomes, data integration\nmethods aim to extract low-dimensional embeddings of observations to remove\nunwanted variations, such as batch effects and unmeasured covariates, inherent\nin data collected from different sources. However, multiple hypothesis testing\nafter data integration can be substantially biased due to the data-dependent\nintegration processes. To address this challenge, we introduce a robust\npost-integrated inference (PII) method that adjusts for latent heterogeneity\nusing negative control outcomes. By leveraging causal interpretations, we\nderive nonparametric identification conditions that form the basis of our PII\napproach.\n  Our assumption-lean semiparametric inference method extends robustness and\ngenerality to projected direct effect estimands that account for mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide\ndeterministic quantifications of the bias of target estimands induced by\nestimated embeddings and finite-sample linear expansions of the estimators with\nuniform concentration bounds on the residuals for all outcomes.\n  The proposed doubly robust estimators are consistent and efficient under\nminimal assumptions, facilitating data-adaptive estimation with machine\nlearning algorithms. Using random forests, we evaluate empirical statistical\nerrors in simulations and analyze single-cell CRISPR perturbed datasets with\npotential unmeasured confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data integration has become increasingly common in aligning multiple\nheterogeneous datasets. With high-dimensional outcomes, data integration\nmethods aim to extract low-dimensional embeddings of observations to remove\nunwanted variations, such as batch effects and unmeasured covariates, inherent\nin data collected from different sources. However, multiple hypothesis testing\nafter data integration can be substantially biased due to the data-dependent\nintegration processes. To address this challenge, we introduce a robust\npost-integrated inference (PII) method that adjusts for latent heterogeneity\nusing negative control outcomes. By leveraging causal interpretations, we\nderive nonparametric identification conditions that form the basis of our PII\napproach.\n  Our assumption-lean semiparametric inference method extends robustness and\ngenerality to projected direct effect estimands that account for mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide\ndeterministic quantifications of the bias of target estimands induced by\nestimated embeddings and finite-sample linear expansions of the estimators with\nuniform concentration bounds on the residuals for all outcomes.\n  The proposed doubly robust estimators are consistent and efficient under\nminimal assumptions, facilitating data-adaptive estimation with machine\nlearning algorithms. Using random forests, we evaluate empirical statistical\nerrors in simulations and analyze single-cell CRISPR perturbed datasets with\npotential unmeasured confounders."
                },
                "authors": [
                    {
                        "name": "Jin-Hong Du"
                    },
                    {
                        "name": "Kathryn Roeder"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "arxiv_comment": "29 pages for main text, and 18 pages for appendix, 9 figures for main\n  text, 4 figures for appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04992v1",
                "updated": "2024-10-07T12:48:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    48,
                    3,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T12:48:03Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    48,
                    3,
                    0,
                    281,
                    0
                ],
                "title": "MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic\n  Compartment Neurons for Stress Detection using Physiological Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic\n  Compartment Neurons for Stress Detection using Physiological Signals"
                },
                "summary": "Long short-term memory (LSTM) has emerged as a definitive network for\nanalyzing and inferring time series data. LSTM has the capability to extract\nspectral features and a mixture of temporal features. Due to this benefit, a\nsimilar feature extraction method is explored for the spiking counterparts\ntargeting time-series data. Though LSTMs perform well in their spiking form,\nthey tend to be compute and power intensive. Addressing this issue, this work\nproposes Multi-Compartment Leaky (MCLeaky) neuron as a viable alternative for\nefficient processing of time series data. The MCLeaky neuron, derived from the\nLeaky Integrate and Fire (LIF) neuron model, contains multiple memristive\nsynapses interlinked to form a memory component, which emulates the human\nbrain's Hippocampus region. The proposed MCLeaky neuron based Spiking Neural\nNetwork model and its quantized variant were benchmarked against\nstate-of-the-art (SOTA) Spiking LSTMs to perform human stress detection, by\ncomparing compute requirements, latency and real-world performances on unseen\ndata with models derived through Neural Architecture Search (NAS). Results show\nthat networks with MCLeaky activation neuron managed a superior accuracy of\n98.8% to detect stress based on Electrodermal Activity (EDA) signals, better\nthan any other investigated models, while using 20% less parameters on average.\nMCLeaky neuron was also tested for various signals including EDA Wrist and\nChest, Temperature, ECG, and combinations of them. Quantized MCLeaky model was\nalso derived and validated to forecast their performance on hardware\narchitectures, which resulted in 91.84% accuracy. The neurons were evaluated\nfor multiple modalities of data towards stress detection, which resulted in\nenergy savings of 25.12x to 39.20x and EDP gains of 52.37x to 81.9x over ANNs,\nwhile offering a best accuracy of 98.8% when compared with the rest of the SOTA\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long short-term memory (LSTM) has emerged as a definitive network for\nanalyzing and inferring time series data. LSTM has the capability to extract\nspectral features and a mixture of temporal features. Due to this benefit, a\nsimilar feature extraction method is explored for the spiking counterparts\ntargeting time-series data. Though LSTMs perform well in their spiking form,\nthey tend to be compute and power intensive. Addressing this issue, this work\nproposes Multi-Compartment Leaky (MCLeaky) neuron as a viable alternative for\nefficient processing of time series data. The MCLeaky neuron, derived from the\nLeaky Integrate and Fire (LIF) neuron model, contains multiple memristive\nsynapses interlinked to form a memory component, which emulates the human\nbrain's Hippocampus region. The proposed MCLeaky neuron based Spiking Neural\nNetwork model and its quantized variant were benchmarked against\nstate-of-the-art (SOTA) Spiking LSTMs to perform human stress detection, by\ncomparing compute requirements, latency and real-world performances on unseen\ndata with models derived through Neural Architecture Search (NAS). Results show\nthat networks with MCLeaky activation neuron managed a superior accuracy of\n98.8% to detect stress based on Electrodermal Activity (EDA) signals, better\nthan any other investigated models, while using 20% less parameters on average.\nMCLeaky neuron was also tested for various signals including EDA Wrist and\nChest, Temperature, ECG, and combinations of them. Quantized MCLeaky model was\nalso derived and validated to forecast their performance on hardware\narchitectures, which resulted in 91.84% accuracy. The neurons were evaluated\nfor multiple modalities of data towards stress detection, which resulted in\nenergy savings of 25.12x to 39.20x and EDP gains of 52.37x to 81.9x over ANNs,\nwhile offering a best accuracy of 98.8% when compared with the rest of the SOTA\nimplementations."
                },
                "authors": [
                    {
                        "name": "Ajay B. S."
                    },
                    {
                        "name": "Phani Pavan K"
                    },
                    {
                        "name": "Madhav Rao"
                    }
                ],
                "author_detail": {
                    "name": "Madhav Rao"
                },
                "author": "Madhav Rao",
                "arxiv_comment": "13 pages, 15 figures. Applied to IEEE Transactions on Computer Aided\n  Design Journal. Awaiting a verdict",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19339v2",
                "updated": "2024-10-07T12:05:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    5,
                    55,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-28T12:49:16Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    12,
                    49,
                    16,
                    5,
                    272,
                    0
                ],
                "title": "Visual Question Decomposition on Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Decomposition on Multimodal Large Language Models"
                },
                "summary": "Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets."
                },
                "authors": [
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Bailan He"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    },
                    {
                        "name": "Jindong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Gu"
                },
                "author": "Jindong Gu",
                "arxiv_comment": "Accepted to EMNLP2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16397v3",
                "updated": "2024-10-07T12:05:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    5,
                    12,
                    0,
                    281,
                    0
                ],
                "published": "2023-09-28T12:44:51Z",
                "published_parsed": [
                    2023,
                    9,
                    28,
                    12,
                    44,
                    51,
                    3,
                    271,
                    0
                ],
                "title": "Uncertainty-Aware Decision Transformer for Stochastic Driving\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Decision Transformer for Stochastic Driving\n  Environments"
                },
                "summary": "Offline Reinforcement Learning (RL) enables policy learning without active\ninteractions, making it especially appealing for self-driving tasks. Recent\nsuccesses of Transformers inspire casting offline RL as sequence modeling,\nwhich, however, fails in stochastic environments with incorrect assumptions\nthat identical actions can consistently achieve the same goal. In this paper,\nwe introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in\nstochastic driving environments without introducing additional transition or\ncomplex generative models. Specifically, UNREST estimates uncertainties by\nconditional mutual information between transitions and returns. Discovering\n'uncertainty accumulation' and 'temporal locality' properties of driving\nenvironments, we replace the global returns in decision transformers with\ntruncated returns less affected by environments to learn from actual outcomes\nof actions rather than environment transitions. We also dynamically evaluate\nuncertainty at inference for cautious planning. Extensive experiments\ndemonstrate UNREST's superior performance in various driving scenarios and the\npower of our uncertainty estimation strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Reinforcement Learning (RL) enables policy learning without active\ninteractions, making it especially appealing for self-driving tasks. Recent\nsuccesses of Transformers inspire casting offline RL as sequence modeling,\nwhich, however, fails in stochastic environments with incorrect assumptions\nthat identical actions can consistently achieve the same goal. In this paper,\nwe introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in\nstochastic driving environments without introducing additional transition or\ncomplex generative models. Specifically, UNREST estimates uncertainties by\nconditional mutual information between transitions and returns. Discovering\n'uncertainty accumulation' and 'temporal locality' properties of driving\nenvironments, we replace the global returns in decision transformers with\ntruncated returns less affected by environments to learn from actual outcomes\nof actions rather than environment transitions. We also dynamically evaluate\nuncertainty at inference for cautious planning. Extensive experiments\ndemonstrate UNREST's superior performance in various driving scenarios and the\npower of our uncertainty estimation strategy."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Qiao Sun"
                    },
                    {
                        "name": "Fang Da"
                    },
                    {
                        "name": "Hang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhao"
                },
                "author": "Hang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15341v2",
                "updated": "2024-10-07T12:04:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    4,
                    11,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-09T21:09:47Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    21,
                    9,
                    47,
                    0,
                    253,
                    0
                ],
                "title": "StructuReiser: A Structure-preserving Video Stylization Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructuReiser: A Structure-preserving Video Stylization Method"
                },
                "summary": "We introduce StructuReiser, a novel video-to-video translation method that\ntransforms input videos into stylized sequences using a set of user-provided\nkeyframes. Unlike existing approaches, StructuReiser maintains strict adherence\nto the structural elements of the target video, preserving the original\nidentity while seamlessly applying the desired stylistic transformations. This\nenables a level of control and consistency that was previously unattainable\nwith traditional text-driven or keyframe-based methods. Furthermore,\nStructuReiser supports real-time inference and custom keyframe editing, making\nit ideal for interactive applications and expanding the possibilities for\ncreative expression and video manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce StructuReiser, a novel video-to-video translation method that\ntransforms input videos into stylized sequences using a set of user-provided\nkeyframes. Unlike existing approaches, StructuReiser maintains strict adherence\nto the structural elements of the target video, preserving the original\nidentity while seamlessly applying the desired stylistic transformations. This\nenables a level of control and consistency that was previously unattainable\nwith traditional text-driven or keyframe-based methods. Furthermore,\nStructuReiser supports real-time inference and custom keyframe editing, making\nit ideal for interactive applications and expanding the possibilities for\ncreative expression and video manipulation."
                },
                "authors": [
                    {
                        "name": "Radim Spetlik"
                    },
                    {
                        "name": "David Futschik"
                    },
                    {
                        "name": "Daniel Sykora"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sykora"
                },
                "author": "Daniel Sykora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04961v1",
                "updated": "2024-10-07T12:00:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    0,
                    17,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T12:00:17Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    0,
                    17,
                    0,
                    281,
                    0
                ],
                "title": "Changing-Look Inspirals: Trends and Switches in AGN Disk Emission as\n  Signposts for Merging Black Hole Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Changing-Look Inspirals: Trends and Switches in AGN Disk Emission as\n  Signposts for Merging Black Hole Binaries"
                },
                "summary": "Using grid-based hydrodynamics simulations and analytic modeling, we compute\nthe electromagnetic (EM) signatures of gravitational wave (GW) driven inspirals\nof massive black hole binaries that accrete gas from circumbinary disks,\nexploring the effects of varying gas temperatures, viscosity laws, and binary\nmass ratios. Our main finding is that active galactic nuclei (AGN's) that host\ninspiraling binaries can exhibit two sub-types of long-term secular variability\npatterns: Type-A events which dim before merger and brighten afterward, and\nType-B events which brighten before merger and dim afterward. In both types the\nmerger coincides with a long-lasting chromatic change of the AGN appearance.\nThe sub-types correspond to the direction of angular momentum transfer between\nthe binary and the disk, and could thus have correlated GW signatures if the\ngas-induced torque can be inferred from GW phase drift measurements by LISA.\nThe long-term brightness trends are caused by steady weakening of the\ndisk-binary torque that accompanies orbital decay, it induces a hysteresis\neffect whereby the disk \"remembers\" the history of the binary's contraction. We\nillustrate the effect using a reduced model problem of an axisymmetric thin\ndisk subjected at its inner edge to the weakening torque of an inspiraling\nbinary. The model problem yields a new class of self-similar disk solutions,\nwhich capture salient features of the multi-dimensional hydrodynamics\nsimulations. We use these solutions to derive variable AGN disk emission\nsignatures within years to decades of massive black hole binary mergers in\nAGN's. Spectral changes of Mrk 1018 might have been triggered by an\ninspiral-merger event.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using grid-based hydrodynamics simulations and analytic modeling, we compute\nthe electromagnetic (EM) signatures of gravitational wave (GW) driven inspirals\nof massive black hole binaries that accrete gas from circumbinary disks,\nexploring the effects of varying gas temperatures, viscosity laws, and binary\nmass ratios. Our main finding is that active galactic nuclei (AGN's) that host\ninspiraling binaries can exhibit two sub-types of long-term secular variability\npatterns: Type-A events which dim before merger and brighten afterward, and\nType-B events which brighten before merger and dim afterward. In both types the\nmerger coincides with a long-lasting chromatic change of the AGN appearance.\nThe sub-types correspond to the direction of angular momentum transfer between\nthe binary and the disk, and could thus have correlated GW signatures if the\ngas-induced torque can be inferred from GW phase drift measurements by LISA.\nThe long-term brightness trends are caused by steady weakening of the\ndisk-binary torque that accompanies orbital decay, it induces a hysteresis\neffect whereby the disk \"remembers\" the history of the binary's contraction. We\nillustrate the effect using a reduced model problem of an axisymmetric thin\ndisk subjected at its inner edge to the weakening torque of an inspiraling\nbinary. The model problem yields a new class of self-similar disk solutions,\nwhich capture salient features of the multi-dimensional hydrodynamics\nsimulations. We use these solutions to derive variable AGN disk emission\nsignatures within years to decades of massive black hole binary mergers in\nAGN's. Spectral changes of Mrk 1018 might have been triggered by an\ninspiral-merger event."
                },
                "authors": [
                    {
                        "name": "Jonathan Zrake"
                    },
                    {
                        "name": "Madeline Clyburn"
                    },
                    {
                        "name": "Samuel Feyan"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Feyan"
                },
                "author": "Samuel Feyan",
                "arxiv_comment": "Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10805v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10805v4",
                "updated": "2024-10-07T11:52:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    52,
                    50,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-15T15:20:40Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    15,
                    20,
                    40,
                    0,
                    197,
                    0
                ],
                "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning."
                },
                "authors": [
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Muzhi Li"
                    },
                    {
                        "name": "Huaren Qu"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10805v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10805v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02525v3",
                "updated": "2024-10-07T11:52:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    52,
                    19,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-03T07:27:33Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    7,
                    27,
                    33,
                    2,
                    94,
                    0
                ],
                "title": "Large Language Model for Vulnerability Detection and Repair: Literature\n  Review and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Vulnerability Detection and Repair: Literature\n  Review and the Road Ahead"
                },
                "summary": "The significant advancements in Large Language Models (LLMs) have resulted in\ntheir widespread adoption across various tasks within Software Engineering\n(SE), including vulnerability detection and repair. Numerous studies have\ninvestigated the application of LLMs to enhance vulnerability detection and\nrepair tasks. Despite the increasing research interest, there is currently no\nexisting survey that focuses on the utilization of LLMs for vulnerability\ndetection and repair. In this paper, we aim to bridge this gap by offering a\nsystematic literature review of approaches aimed at improving vulnerability\ndetection and repair through the utilization of LLMs. The review encompasses\nresearch work from leading SE, AI, and Security conferences and journals,\nencompassing 43 papers published across 25 distinct venues, along with 15\nhigh-quality preprint papers, bringing the total to 58 papers. By answering\nthree key research questions, we aim to (1) summarize the LLMs employed in the\nrelevant literature, (2) categorize various LLM adaptation techniques in\nvulnerability detection, and (3) classify various LLM adaptation techniques in\nvulnerability repair. Based on our findings, we have identified a series of\nlimitations of existing studies. Additionally, we have outlined a roadmap\nhighlighting potential opportunities that we believe are pertinent and crucial\nfor future research endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements in Large Language Models (LLMs) have resulted in\ntheir widespread adoption across various tasks within Software Engineering\n(SE), including vulnerability detection and repair. Numerous studies have\ninvestigated the application of LLMs to enhance vulnerability detection and\nrepair tasks. Despite the increasing research interest, there is currently no\nexisting survey that focuses on the utilization of LLMs for vulnerability\ndetection and repair. In this paper, we aim to bridge this gap by offering a\nsystematic literature review of approaches aimed at improving vulnerability\ndetection and repair through the utilization of LLMs. The review encompasses\nresearch work from leading SE, AI, and Security conferences and journals,\nencompassing 43 papers published across 25 distinct venues, along with 15\nhigh-quality preprint papers, bringing the total to 58 papers. By answering\nthree key research questions, we aim to (1) summarize the LLMs employed in the\nrelevant literature, (2) categorize various LLM adaptation techniques in\nvulnerability detection, and (3) classify various LLM adaptation techniques in\nvulnerability repair. Based on our findings, we have identified a series of\nlimitations of existing studies. Additionally, we have outlined a roadmap\nhighlighting potential opportunities that we believe are pertinent and crucial\nfor future research endeavors."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Sicong Cao"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04949v1",
                "updated": "2024-10-07T11:45:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    45,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:45:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    45,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law"
                },
                "summary": "Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches."
                },
                "authors": [
                    {
                        "name": "Yongming Chen"
                    },
                    {
                        "name": "Miner Chen"
                    },
                    {
                        "name": "Ye Zhu"
                    },
                    {
                        "name": "Juan Pei"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Songan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Songan Zhang"
                },
                "author": "Songan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14883v2",
                "updated": "2024-10-07T11:37:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    37,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-23T10:09:46Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    10,
                    9,
                    46,
                    1,
                    114,
                    0
                ],
                "title": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans"
                },
                "summary": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference."
                },
                "authors": [
                    {
                        "name": "Vittoria Dentella"
                    },
                    {
                        "name": "Fritz Guenther"
                    },
                    {
                        "name": "Evelina Leivada"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Leivada"
                },
                "author": "Evelina Leivada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2111.06235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2111.06235v2",
                "updated": "2024-10-07T11:35:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    35,
                    10,
                    0,
                    281,
                    0
                ],
                "published": "2021-11-11T14:33:07Z",
                "published_parsed": [
                    2021,
                    11,
                    11,
                    14,
                    33,
                    7,
                    3,
                    315,
                    0
                ],
                "title": "Limits of Multilayer Diffusion Network Inference in Social Media\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of Multilayer Diffusion Network Inference in Social Media\n  Research"
                },
                "summary": "Information on social media spreads through an underlying diffusion network\nthat connects people of common interests and opinions. This diffusion network\noften comprises multiple layers, each capturing the spreading dynamics of a\ncertain type of information characterized by, for example, topic, language, or\nattitude. Researchers have previously proposed methods to infer these\nunderlying multilayer diffusion networks from observed spreading patterns, but\nlittle is known about how well these methods perform across the range of\nrealistic spreading data. In this paper, we conduct an extensive series of\nsynthetic data experiments to systematically analyze the performance of the\nmultilayer diffusion network inference framework, under varied network\nstructure (e.g. density, number of layers) and information diffusion settings\n(e.g. cascade size, layer mixing) that are designed to mimic real-world\nspreading on social media. Our results show extreme performance variation of\nthe inference framework: notably, it achieves much higher accuracy when\ninferring a denser diffusion network, while it fails to decompose the diffusion\nnetwork correctly when most cascades in the data reach a limited audience. In\ndemonstrating the conditions under which the inference accuracy is extremely\nlow, our paper highlights the need to carefully evaluate the applicability of\nthe inference before running it on real data. Practically, our results serve as\na reference for this evaluation, and our publicly available implementation,\nwhich outperforms previous implementations in accuracy, supports further\ntesting under personalized settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information on social media spreads through an underlying diffusion network\nthat connects people of common interests and opinions. This diffusion network\noften comprises multiple layers, each capturing the spreading dynamics of a\ncertain type of information characterized by, for example, topic, language, or\nattitude. Researchers have previously proposed methods to infer these\nunderlying multilayer diffusion networks from observed spreading patterns, but\nlittle is known about how well these methods perform across the range of\nrealistic spreading data. In this paper, we conduct an extensive series of\nsynthetic data experiments to systematically analyze the performance of the\nmultilayer diffusion network inference framework, under varied network\nstructure (e.g. density, number of layers) and information diffusion settings\n(e.g. cascade size, layer mixing) that are designed to mimic real-world\nspreading on social media. Our results show extreme performance variation of\nthe inference framework: notably, it achieves much higher accuracy when\ninferring a denser diffusion network, while it fails to decompose the diffusion\nnetwork correctly when most cascades in the data reach a limited audience. In\ndemonstrating the conditions under which the inference accuracy is extremely\nlow, our paper highlights the need to carefully evaluate the applicability of\nthe inference before running it on real data. Practically, our results serve as\na reference for this evaluation, and our publicly available implementation,\nwhich outperforms previous implementations in accuracy, supports further\ntesting under personalized settings."
                },
                "authors": [
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Ted Hsuan Yun Chen"
                    },
                    {
                        "name": "Mikko Kivelä"
                    }
                ],
                "author_detail": {
                    "name": "Mikko Kivelä"
                },
                "author": "Mikko Kivelä",
                "arxiv_doi": "10.1609/icwsm.v16i1.19365",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/icwsm.v16i1.19365",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2111.06235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2111.06235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ICWSM 2022",
                "arxiv_journal_ref": "Proceedings of the International AAAI Conference on Web and Social\n  Media 16 (2022) 1145-1156",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04931v1",
                "updated": "2024-10-07T11:24:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    24,
                    29,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:24:29Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    24,
                    29,
                    0,
                    281,
                    0
                ],
                "title": "The Role of Governments in Increasing Interconnected Post-Deployment\n  Monitoring of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Governments in Increasing Interconnected Post-Deployment\n  Monitoring of AI"
                },
                "summary": "Language-based AI systems are diffusing into society, bringing positive and\nnegative impacts. Mitigating negative impacts depends on accurate impact\nassessments, drawn from an empirical evidence base that makes causal\nconnections between AI usage and impacts. Interconnected post-deployment\nmonitoring combines information about model integration and use, application\nuse, and incidents and impacts. For example, inference time monitoring of\nchain-of-thought reasoning can be combined with long-term monitoring of\nsectoral AI diffusion, impacts and incidents. Drawing on information sharing\nmechanisms in other industries, we highlight example data sources and specific\ndata points that governments could collect to inform AI risk management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-based AI systems are diffusing into society, bringing positive and\nnegative impacts. Mitigating negative impacts depends on accurate impact\nassessments, drawn from an empirical evidence base that makes causal\nconnections between AI usage and impacts. Interconnected post-deployment\nmonitoring combines information about model integration and use, application\nuse, and incidents and impacts. For example, inference time monitoring of\nchain-of-thought reasoning can be combined with long-term monitoring of\nsectoral AI diffusion, impacts and incidents. Drawing on information sharing\nmechanisms in other industries, we highlight example data sources and specific\ndata points that governments could collect to inform AI risk management."
                },
                "authors": [
                    {
                        "name": "Merlin Stein"
                    },
                    {
                        "name": "Jamie Bernardi"
                    },
                    {
                        "name": "Connor Dunlop"
                    }
                ],
                "author_detail": {
                    "name": "Connor Dunlop"
                },
                "author": "Connor Dunlop",
                "arxiv_comment": "7 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04927v1",
                "updated": "2024-10-07T11:19:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    19,
                    5,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:19:05Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    19,
                    5,
                    0,
                    281,
                    0
                ],
                "title": "FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services"
                },
                "summary": "Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS."
                },
                "authors": [
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Guanhua Ye"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Viet Hung Nguyen"
                },
                "author": "Quoc Viet Hung Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04925v1",
                "updated": "2024-10-07T11:17:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    17,
                    5,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:17:05Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    17,
                    5,
                    0,
                    281,
                    0
                ],
                "title": "Intent Classification for Bank Chatbots through LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Classification for Bank Chatbots through LLM Fine-Tuning"
                },
                "summary": "This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication."
                },
                "authors": [
                    {
                        "name": "Bibiána Lajčinová"
                    },
                    {
                        "name": "Patrik Valábek"
                    },
                    {
                        "name": "Michal Spišiak"
                    }
                ],
                "author_detail": {
                    "name": "Michal Spišiak"
                },
                "author": "Michal Spišiak",
                "arxiv_comment": "7 pages, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17263v2",
                "updated": "2024-10-07T11:15:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    15,
                    41,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-25T04:07:22Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    4,
                    7,
                    22,
                    1,
                    177,
                    0
                ],
                "title": "Efficient, Multimodal, and Derivative-Free Bayesian Inference With\n  Fisher-Rao Gradient Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient, Multimodal, and Derivative-Free Bayesian Inference With\n  Fisher-Rao Gradient Flows"
                },
                "summary": "In this paper, we study efficient approximate sampling for probability\ndistributions known up to normalization constants. We specifically focus on a\nproblem class arising in Bayesian inference for large-scale inverse problems in\nscience and engineering applications. The computational challenges we address\nwith the proposed methodology are: (i) the need for repeated evaluations of\nexpensive forward models; (ii) the potential existence of multiple modes; and\n(iii) the fact that gradient of, or adjoint solver for, the forward model might\nnot be feasible.\n  While existing Bayesian inference methods meet some of these challenges\nindividually, we propose a framework that tackles all three systematically. Our\napproach builds upon the Fisher-Rao gradient flow in probability space,\nyielding a dynamical system for probability densities that converges towards\nthe target distribution at a uniform exponential rate. This rapid convergence\nis advantageous for the computational burden outlined in (i). We apply Gaussian\nmixture approximations with operator splitting techniques to simulate the flow\nnumerically; the resulting approximation can capture multiple modes thus\naddressing (ii). Furthermore, we employ the Kalman methodology to facilitate a\nderivative-free update of these Gaussian components and their respective\nweights, addressing the issue in (iii).\n  The proposed methodology results in an efficient derivative-free sampler\nflexible enough to handle multi-modal distributions: Gaussian Mixture Kalman\nInversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically\nand numerically in several experiments with multimodal target distributions,\nincluding proof-of-concept and two-dimensional examples, as well as a\nlarge-scale application: recovering the Navier-Stokes initial condition from\nsolution data at positive times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study efficient approximate sampling for probability\ndistributions known up to normalization constants. We specifically focus on a\nproblem class arising in Bayesian inference for large-scale inverse problems in\nscience and engineering applications. The computational challenges we address\nwith the proposed methodology are: (i) the need for repeated evaluations of\nexpensive forward models; (ii) the potential existence of multiple modes; and\n(iii) the fact that gradient of, or adjoint solver for, the forward model might\nnot be feasible.\n  While existing Bayesian inference methods meet some of these challenges\nindividually, we propose a framework that tackles all three systematically. Our\napproach builds upon the Fisher-Rao gradient flow in probability space,\nyielding a dynamical system for probability densities that converges towards\nthe target distribution at a uniform exponential rate. This rapid convergence\nis advantageous for the computational burden outlined in (i). We apply Gaussian\nmixture approximations with operator splitting techniques to simulate the flow\nnumerically; the resulting approximation can capture multiple modes thus\naddressing (ii). Furthermore, we employ the Kalman methodology to facilitate a\nderivative-free update of these Gaussian components and their respective\nweights, addressing the issue in (iii).\n  The proposed methodology results in an efficient derivative-free sampler\nflexible enough to handle multi-modal distributions: Gaussian Mixture Kalman\nInversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically\nand numerically in several experiments with multimodal target distributions,\nincluding proof-of-concept and two-dimensional examples, as well as a\nlarge-scale application: recovering the Navier-Stokes initial condition from\nsolution data at positive times."
                },
                "authors": [
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Daniel Zhengyu Huang"
                    },
                    {
                        "name": "Jiaoyang Huang"
                    },
                    {
                        "name": "Sebastian Reich"
                    },
                    {
                        "name": "Andrew M. Stuart"
                    }
                ],
                "author_detail": {
                    "name": "Andrew M. Stuart"
                },
                "author": "Andrew M. Stuart",
                "arxiv_comment": "42 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04917v1",
                "updated": "2024-10-07T11:07:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    7,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:07:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    7,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "Why am I seeing this: Democratizing End User Auditing for Online Content\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why am I seeing this: Democratizing End User Auditing for Online Content\n  Recommendations"
                },
                "summary": "Personalized recommendation systems tailor content based on user attributes,\nwhich are either provided or inferred from private data. Research suggests that\nusers often hypothesize about reasons behind contents they encounter (e.g., \"I\nsee this jewelry ad because I am a woman\"), but they lack the means to confirm\nthese hypotheses due to the opaqueness of these systems. This hinders informed\ndecision-making about privacy and system use and contributes to the lack of\nalgorithmic accountability. To address these challenges, we introduce a new\ninteractive sandbox approach. This approach creates sets of synthetic user\npersonas and corresponding personal data that embody realistic variations in\npersonal attributes, allowing users to test their hypotheses by observing how a\nwebsite's algorithms respond to these personas. We tested the sandbox in the\ncontext of targeted advertisement. Our user study demonstrates its usability,\nusefulness, and effectiveness in empowering end-user auditing in a case study\nof targeting ads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized recommendation systems tailor content based on user attributes,\nwhich are either provided or inferred from private data. Research suggests that\nusers often hypothesize about reasons behind contents they encounter (e.g., \"I\nsee this jewelry ad because I am a woman\"), but they lack the means to confirm\nthese hypotheses due to the opaqueness of these systems. This hinders informed\ndecision-making about privacy and system use and contributes to the lack of\nalgorithmic accountability. To address these challenges, we introduce a new\ninteractive sandbox approach. This approach creates sets of synthetic user\npersonas and corresponding personal data that embody realistic variations in\npersonal attributes, allowing users to test their hypotheses by observing how a\nwebsite's algorithms respond to these personas. We tested the sandbox in the\ncontext of targeted advertisement. Our user study demonstrates its usability,\nusefulness, and effectiveness in empowering end-user auditing in a case study\nof targeting ads."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Leyang Li"
                    },
                    {
                        "name": "Luke Cao"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Toby Jia-jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-jun Li"
                },
                "author": "Toby Jia-jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07578v2",
                "updated": "2024-10-07T11:04:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    4,
                    31,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-11T19:10:29Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    10,
                    29,
                    2,
                    255,
                    0
                ],
                "title": "A Novel Mathematical Framework for Objective Characterization of Ideas\n  through Vector Embeddings in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Mathematical Framework for Objective Characterization of Ideas\n  through Vector Embeddings in LLM"
                },
                "summary": "The demand for innovation in product design necessitates a prolific ideation\nphase. Conversational AI (CAI) systems that use Large Language Models (LLMs)\nsuch as GPT (Generative Pre-trained Transformer) have been shown to be fruitful\nin augmenting human creativity, providing numerous novel and diverse ideas.\nDespite the success in ideation quantity, the qualitative assessment of these\nideas remains challenging and traditionally reliant on expert human evaluation.\nThis method suffers from limitations such as human judgment errors, bias, and\noversight. Addressing this gap, our study introduces a comprehensive\nmathematical framework for automated analysis to objectively evaluate the\nplethora of ideas generated by CAI systems and/or humans. This framework is\nparticularly advantageous for novice designers who lack experience in selecting\npromising ideas. By converting the ideas into higher dimensional vectors and\nquantitatively measuring the diversity between them using tools such as UMAP,\nDBSCAN and PCA, the proposed method provides a reliable and objective way of\nselecting the most promising ideas, thereby enhancing the efficiency of the\nideation phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for innovation in product design necessitates a prolific ideation\nphase. Conversational AI (CAI) systems that use Large Language Models (LLMs)\nsuch as GPT (Generative Pre-trained Transformer) have been shown to be fruitful\nin augmenting human creativity, providing numerous novel and diverse ideas.\nDespite the success in ideation quantity, the qualitative assessment of these\nideas remains challenging and traditionally reliant on expert human evaluation.\nThis method suffers from limitations such as human judgment errors, bias, and\noversight. Addressing this gap, our study introduces a comprehensive\nmathematical framework for automated analysis to objectively evaluate the\nplethora of ideas generated by CAI systems and/or humans. This framework is\nparticularly advantageous for novice designers who lack experience in selecting\npromising ideas. By converting the ideas into higher dimensional vectors and\nquantitatively measuring the diversity between them using tools such as UMAP,\nDBSCAN and PCA, the proposed method provides a reliable and objective way of\nselecting the most promising ideas, thereby enhancing the efficiency of the\nideation phase."
                },
                "authors": [
                    {
                        "name": "B. Sankar"
                    },
                    {
                        "name": "Dibakar Sen"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Sen"
                },
                "author": "Dibakar Sen",
                "arxiv_comment": "20 pages, 12 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53A45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14140v2",
                "updated": "2024-10-07T10:57:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    10,
                    57,
                    11,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-20T09:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    9,
                    32,
                    45,
                    3,
                    172,
                    0
                ],
                "title": "Nonparametric Jackknife Instrumental Variable Estimation and Confounding\n  Robust Surrogate Indices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Jackknife Instrumental Variable Estimation and Confounding\n  Robust Surrogate Indices"
                },
                "summary": "Jackknife instrumental variable estimation (JIVE) is a classic method to\nleverage many weak instrumental variables (IVs) to estimate linear structural\nmodels, overcoming the bias of standard methods like two-stage least squares.\nIn this paper, we extend the jackknife approach to nonparametric IV (NPIV)\nmodels with many weak IVs. Since NPIV characterizes the structural regression\nas having residuals projected onto the IV being zero, existing approaches\nminimize an estimate of the average squared projected residuals, but their\nestimates are biased under many weak IVs. We introduce an IV splitting device\ninspired by JIVE to remove this bias, and by carefully studying this split-IV\nempirical process we establish learning rates that depend on generic complexity\nmeasures of the nonparametric hypothesis class. We then turn to leveraging this\nfor semiparametric inference on average treatment effects (ATEs) on unobserved\nlong-term outcomes predicted from short-term surrogates, using historical\nexperiments as IVs to learn this nonparametric predictive relationship even in\nthe presence of confounding between short- and long-term observations. Using\nsplit-IV estimates of a debiasing nuisance, we develop asymptotically normal\nestimates for predicted ATEs, enabling inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jackknife instrumental variable estimation (JIVE) is a classic method to\nleverage many weak instrumental variables (IVs) to estimate linear structural\nmodels, overcoming the bias of standard methods like two-stage least squares.\nIn this paper, we extend the jackknife approach to nonparametric IV (NPIV)\nmodels with many weak IVs. Since NPIV characterizes the structural regression\nas having residuals projected onto the IV being zero, existing approaches\nminimize an estimate of the average squared projected residuals, but their\nestimates are biased under many weak IVs. We introduce an IV splitting device\ninspired by JIVE to remove this bias, and by carefully studying this split-IV\nempirical process we establish learning rates that depend on generic complexity\nmeasures of the nonparametric hypothesis class. We then turn to leveraging this\nfor semiparametric inference on average treatment effects (ATEs) on unobserved\nlong-term outcomes predicted from short-term surrogates, using historical\nexperiments as IVs to learn this nonparametric predictive relationship even in\nthe presence of confounding between short- and long-term observations. Using\nsplit-IV estimates of a debiasing nuisance, we develop asymptotically normal\nestimates for predicted ATEs, enabling inference."
                },
                "authors": [
                    {
                        "name": "Aurélien Bibaut"
                    },
                    {
                        "name": "Nathan Kallus"
                    },
                    {
                        "name": "Apoorva Lal"
                    }
                ],
                "author_detail": {
                    "name": "Apoorva Lal"
                },
                "author": "Apoorva Lal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12510v2",
                "updated": "2024-10-07T10:31:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    10,
                    31,
                    58,
                    0,
                    281,
                    0
                ],
                "published": "2024-03-19T07:24:54Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    7,
                    24,
                    54,
                    1,
                    79,
                    0
                ],
                "title": "Generalized Consistency Trajectory Models for Image Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Consistency Trajectory Models for Image Manipulation"
                },
                "summary": "Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing."
                },
                "authors": [
                    {
                        "name": "Beomsu Kim"
                    },
                    {
                        "name": "Jaemin Kim"
                    },
                    {
                        "name": "Jeongsol Kim"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04834v2",
                "updated": "2024-10-07T10:28:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    10,
                    28,
                    25,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-07T07:05:40Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    7,
                    5,
                    40,
                    6,
                    98,
                    0
                ],
                "title": "LLM-Based Multi-Agent Systems for Software Engineering: Vision and the\n  Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Agent Systems for Software Engineering: Vision and the\n  Road Ahead"
                },
                "summary": "Integrating Large Language Models(LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\ncompetitive to human planning and reasoning. This paper envisions the evolution\nof LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted\nsoftware engineering challenges. LMA systems introduce numerous benefits,\nincluding enhanced robustness through collaborative cross-examination,\nautonomous problem-solving, and scalable solutions to complex software\nprojects. By examining the role of LMA systems in future software engineering\npractices, this vision paper highlights the potential applications and emerging\nchallenges. We further point to specific opportunities for research and\nconclude with a research agenda with a set of research questions to guide\nfuture research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models(LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\ncompetitive to human planning and reasoning. This paper envisions the evolution\nof LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted\nsoftware engineering challenges. LMA systems introduce numerous benefits,\nincluding enhanced robustness through collaborative cross-examination,\nautonomous problem-solving, and scalable solutions to complex software\nprojects. By examining the role of LMA systems in future software engineering\npractices, this vision paper highlights the potential applications and emerging\nchallenges. We further point to specific opportunities for research and\nconclude with a research agenda with a set of research questions to guide\nfuture research directions."
                },
                "authors": [
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05930v4",
                "updated": "2024-10-07T09:58:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    58,
                    48,
                    0,
                    281,
                    0
                ],
                "published": "2024-01-11T14:09:09Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    14,
                    9,
                    9,
                    3,
                    11,
                    0
                ],
                "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully"
                },
                "summary": "Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Tianhang Zhang"
                    },
                    {
                        "name": "Hai Hu"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08227v2",
                "updated": "2024-10-07T09:51:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    51,
                    46,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-11T07:01:50Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    7,
                    1,
                    50,
                    3,
                    193,
                    0
                ],
                "title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs"
                },
                "summary": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets."
                },
                "authors": [
                    {
                        "name": "Chihcheng Hsieh"
                    },
                    {
                        "name": "Catarina Moreira"
                    },
                    {
                        "name": "Isabel Blanco Nobre"
                    },
                    {
                        "name": "Sandra Costa Sousa"
                    },
                    {
                        "name": "Chun Ouyang"
                    },
                    {
                        "name": "Margot Brereton"
                    },
                    {
                        "name": "Joaquim Jorge"
                    },
                    {
                        "name": "Jacinto C. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Jacinto C. Nascimento"
                },
                "author": "Jacinto C. Nascimento",
                "arxiv_comment": "we introduce a pioneering approach to clinical data augmentation that\n  employs large language models (LLMs) to generate patient contextual synthetic\n  data. It preserves the integrity of real patient data while enriching the\n  dataset with contextually relevant synthetic features, significantly\n  enhancing model performance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.1; J.3; H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15625v2",
                "updated": "2024-10-07T09:49:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    49,
                    8,
                    0,
                    281,
                    0
                ],
                "published": "2024-08-28T08:25:22Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    22,
                    2,
                    241,
                    0
                ],
                "title": "CBF-LLM: Safe Control for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBF-LLM: Safe Control for LLM Alignment"
                },
                "summary": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks."
                },
                "authors": [
                    {
                        "name": "Yuya Miyaoka"
                    },
                    {
                        "name": "Masaki Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Masaki Inoue"
                },
                "author": "Masaki Inoue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16665v2",
                "updated": "2024-10-07T09:46:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    46,
                    7,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-23T17:32:02Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    32,
                    2,
                    1,
                    205,
                    0
                ],
                "title": "A Framework for Pupil Tracking with Event Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Pupil Tracking with Event Cameras"
                },
                "summary": "Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction."
                },
                "authors": [
                    {
                        "name": "Khadija Iddrisu"
                    },
                    {
                        "name": "Waseem Shariff"
                    },
                    {
                        "name": "Suzanne Little"
                    }
                ],
                "author_detail": {
                    "name": "Suzanne Little"
                },
                "author": "Suzanne Little",
                "arxiv_comment": "This paper is a preprint of a paper submitted to the 26th Irish\n  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the\n  copy of record will be available at IET Digital Library",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15325v2",
                "updated": "2024-10-07T09:40:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    40,
                    7,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-22T02:06:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    2,
                    6,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Odyssey: Empowering Minecraft Agents with Open-World Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Odyssey: Empowering Minecraft Agents with Open-World Skills"
                },
                "summary": "Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions."
                },
                "authors": [
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Zhenyu Cui"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Yuxuan Zheng"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04869v1",
                "updated": "2024-10-07T09:36:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    36,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T09:36:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    36,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "Active Inference for Closed-loop transmit beamsteering in Fetal Doppler\n  Ultrasound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference for Closed-loop transmit beamsteering in Fetal Doppler\n  Ultrasound"
                },
                "summary": "Doppler ultrasound is widely used to monitor fetal heart rate during labor\nand pregnancy. Unfortunately, it is highly sensitive to fetal and maternal\nmovements, which can cause the displacement of the fetal heart with respect to\nthe ultrasound beam, in turn reducing the Doppler signal-to-noise ratio and\nleading to erratic, noisy, or missing heart rate readings. To tackle this\nissue, we augment the conventional Doppler ultrasound system with a rational\nagent that autonomously steers the ultrasound beam to track the position of the\nfetal heart. The proposed cognitive ultrasound system leverages a sequential\nMonte Carlo method to infer the fetal heart position from the power Doppler\nsignal, and employs a greedy information-seeking criterion to select the\nsteering angle that minimizes the positional uncertainty for future timesteps.\nThe fetal heart rate is then calculated using the Doppler signal at the\nestimated fetal heart position. Our results show that the system can accurately\ntrack the fetal heart position across challenging signal-to-noise ratio\nscenarios, mainly thanks to its dynamic transmit beam steering capability.\nAdditionally, we find that optimizing the transmit beamsteering to minimize\npositional uncertainty also optimizes downstream heart rate estimation\nperformance. In conclusion, this work showcases the power of closed-loop\ncognitive ultrasound in boosting the capabilities of traditional systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doppler ultrasound is widely used to monitor fetal heart rate during labor\nand pregnancy. Unfortunately, it is highly sensitive to fetal and maternal\nmovements, which can cause the displacement of the fetal heart with respect to\nthe ultrasound beam, in turn reducing the Doppler signal-to-noise ratio and\nleading to erratic, noisy, or missing heart rate readings. To tackle this\nissue, we augment the conventional Doppler ultrasound system with a rational\nagent that autonomously steers the ultrasound beam to track the position of the\nfetal heart. The proposed cognitive ultrasound system leverages a sequential\nMonte Carlo method to infer the fetal heart position from the power Doppler\nsignal, and employs a greedy information-seeking criterion to select the\nsteering angle that minimizes the positional uncertainty for future timesteps.\nThe fetal heart rate is then calculated using the Doppler signal at the\nestimated fetal heart position. Our results show that the system can accurately\ntrack the fetal heart position across challenging signal-to-noise ratio\nscenarios, mainly thanks to its dynamic transmit beam steering capability.\nAdditionally, we find that optimizing the transmit beamsteering to minimize\npositional uncertainty also optimizes downstream heart rate estimation\nperformance. In conclusion, this work showcases the power of closed-loop\ncognitive ultrasound in boosting the capabilities of traditional systems."
                },
                "authors": [
                    {
                        "name": "Beatrice Federici"
                    },
                    {
                        "name": "Ruud JG van Sloun"
                    },
                    {
                        "name": "Massimo Mischi"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Mischi"
                },
                "author": "Massimo Mischi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04853v1",
                "updated": "2024-10-07T09:16:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    16,
                    58,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T09:16:58Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    16,
                    58,
                    0,
                    281,
                    0
                ],
                "title": "TimeCNN: Refining Cross-Variable Interaction on Time Point for Time\n  Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeCNN: Refining Cross-Variable Interaction on Time Point for Time\n  Series Forecasting"
                },
                "summary": "Time series forecasting is extensively applied across diverse domains.\nTransformer-based models demonstrate significant potential in modeling\ncross-time and cross-variable interaction. However, we notice that the\ncross-variable correlation of multivariate time series demonstrates\nmultifaceted (positive and negative correlations) and dynamic progression over\ntime, which is not well captured by existing Transformer-based models. To\naddress this issue, we propose a TimeCNN model to refine cross-variable\ninteractions to enhance time series forecasting. Its key innovation is\ntimepoint-independent, where each time point has an independent convolution\nkernel, allowing each time point to have its independent model to capture\nrelationships among variables. This approach effectively handles both positive\nand negative correlations and adapts to the evolving nature of variable\nrelationships over time. Extensive experiments conducted on 12 real-world\ndatasets demonstrate that TimeCNN consistently outperforms state-of-the-art\nmodels. Notably, our model achieves significant reductions in computational\nrequirements (approximately 60.46%) and parameter count (about 57.50%), while\ndelivering inference speeds 3 to 4 times faster than the benchmark iTransformer\nmodel",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting is extensively applied across diverse domains.\nTransformer-based models demonstrate significant potential in modeling\ncross-time and cross-variable interaction. However, we notice that the\ncross-variable correlation of multivariate time series demonstrates\nmultifaceted (positive and negative correlations) and dynamic progression over\ntime, which is not well captured by existing Transformer-based models. To\naddress this issue, we propose a TimeCNN model to refine cross-variable\ninteractions to enhance time series forecasting. Its key innovation is\ntimepoint-independent, where each time point has an independent convolution\nkernel, allowing each time point to have its independent model to capture\nrelationships among variables. This approach effectively handles both positive\nand negative correlations and adapts to the evolving nature of variable\nrelationships over time. Extensive experiments conducted on 12 real-world\ndatasets demonstrate that TimeCNN consistently outperforms state-of-the-art\nmodels. Notably, our model achieves significant reductions in computational\nrequirements (approximately 60.46%) and parameter count (about 57.50%), while\ndelivering inference speeds 3 to 4 times faster than the benchmark iTransformer\nmodel"
                },
                "authors": [
                    {
                        "name": "Ao Hu"
                    },
                    {
                        "name": "Dongkai Wang"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Shiyi Qi"
                    },
                    {
                        "name": "Liangjian Wen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Jiang Duan"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Duan"
                },
                "author": "Jiang Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09887v2",
                "updated": "2024-10-07T09:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    15,
                    7,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-13T13:27:57Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    27,
                    57,
                    5,
                    195,
                    0
                ],
                "title": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization\n  Modeling"
                },
                "summary": "Large language models (LLMs) have exhibited their problem-solving abilities\nin mathematical reasoning. Solving realistic optimization (OPT) problems in\napplication scenarios requires advanced and applied mathematics ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose OptiBench, a\nbenchmark for End-to-end optimization problem-solving with human-readable\ninputs and outputs. OptiBench contains rich optimization problems, including\nlinear and nonlinear programming with or without tabular data, which can\ncomprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are\nrequired to call a code solver to provide precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and\nclosed-source LLMs (e.g., GPT-4), we further propose a data synthesis method\nnamely ReSocratic. Unlike general data synthesis methods that proceed from\nquestions to answers, \\ReSocratic first incrementally synthesizes formatted\noptimization demonstration with mathematical formulations step by step and then\nback-translates the generated demonstrations into questions. Based on this, we\nsynthesize the ReSocratic-29k dataset. We further conduct supervised\nfine-tuning with ReSocratic-29k on multiple open-source models. Experimental\nresults show that ReSocratic-29k significantly improves the performance of\nopen-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited their problem-solving abilities\nin mathematical reasoning. Solving realistic optimization (OPT) problems in\napplication scenarios requires advanced and applied mathematics ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose OptiBench, a\nbenchmark for End-to-end optimization problem-solving with human-readable\ninputs and outputs. OptiBench contains rich optimization problems, including\nlinear and nonlinear programming with or without tabular data, which can\ncomprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are\nrequired to call a code solver to provide precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and\nclosed-source LLMs (e.g., GPT-4), we further propose a data synthesis method\nnamely ReSocratic. Unlike general data synthesis methods that proceed from\nquestions to answers, \\ReSocratic first incrementally synthesizes formatted\noptimization demonstration with mathematical formulations step by step and then\nback-translates the generated demonstrations into questions. Based on this, we\nsynthesize the ReSocratic-29k dataset. We further conduct supervised\nfine-tuning with ReSocratic-29k on multiple open-source models. Experimental\nresults show that ReSocratic-29k significantly improves the performance of\nopen-source models."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.05269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05269v1",
                "updated": "2024-10-07T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    58,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    58,
                    0,
                    281,
                    0
                ],
                "title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models"
                },
                "summary": "Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Ninareh Mehrabi"
                    },
                    {
                        "name": "Palash Goyal"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aram Galstyan"
                    }
                ],
                "author_detail": {
                    "name": "Aram Galstyan"
                },
                "author": "Aram Galstyan",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/DataAdvisor/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05267v1",
                "updated": "2024-10-07T17:59:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    48,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:48Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    48,
                    0,
                    281,
                    0
                ],
                "title": "Grounding Partially-Defined Events in Multimodal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Partially-Defined Events in Multimodal Data"
                },
                "summary": "How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems."
                },
                "authors": [
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Reno Kriz"
                    },
                    {
                        "name": "David Etter"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Alexander Martin"
                    },
                    {
                        "name": "Cameron Carpenter"
                    },
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "arxiv_comment": "Preprint; 9 pages; 2024 EMNLP Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11839v2",
                "updated": "2024-10-07T17:59:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    42,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-17T17:59:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    59,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models"
                },
                "summary": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "James Y. Huang"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05262v1",
                "updated": "2024-10-07T17:58:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    58,
                    47,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:58:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    58,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles"
                },
                "summary": "As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\""
                },
                "authors": [
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Ke Fang"
                    },
                    {
                        "name": "Yunfeng Shi"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05261v1",
                "updated": "2024-10-07T17:58:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    58,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:58:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    58,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and\n  Grounding with 16x Fewer Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and\n  Grounding with 16x Fewer Tokens"
                },
                "summary": "Reading dense text and locating objects within images are fundamental\nabilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.\nPrevious LVLMs, including superior proprietary models like GPT-4o, have\nstruggled to excel in both tasks simultaneously. Moreover, previous LVLMs with\nfine-grained perception cost thousands of tokens per image, making them\nresource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient\nfine-grained perception and demonstrating cutting-edge performance across\ngeneral-purpose, OCR, and grounding tasks with 16 times fewer image tokens.\nCritical improvements include: (1) Token Compression: Building on the efficient\narchitecture of its predecessor, TextHawk2 significantly reduces the number of\ntokens per image by 16 times, facilitating training and deployment of the\nTextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We\nenhance the visual encoder through LVLM co-training, unlocking its potential\nfor previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:\nWe maintain a comparable scale of 100 million samples while diversifying the\nsources of pre-training data. We assess TextHawk2 across multiple benchmarks,\nwhere it consistently delivers superior performance and outperforms\nclosed-source models of similar scale, such as achieving 78.4% accuracy on\nOCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%\naccuracy@0.5 on RefCOCOg-test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading dense text and locating objects within images are fundamental\nabilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.\nPrevious LVLMs, including superior proprietary models like GPT-4o, have\nstruggled to excel in both tasks simultaneously. Moreover, previous LVLMs with\nfine-grained perception cost thousands of tokens per image, making them\nresource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient\nfine-grained perception and demonstrating cutting-edge performance across\ngeneral-purpose, OCR, and grounding tasks with 16 times fewer image tokens.\nCritical improvements include: (1) Token Compression: Building on the efficient\narchitecture of its predecessor, TextHawk2 significantly reduces the number of\ntokens per image by 16 times, facilitating training and deployment of the\nTextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We\nenhance the visual encoder through LVLM co-training, unlocking its potential\nfor previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:\nWe maintain a comparable scale of 100 million samples while diversifying the\nsources of pre-training data. We assess TextHawk2 across multiple benchmarks,\nwhere it consistently delivers superior performance and outperforms\nclosed-source models of similar scale, such as achieving 78.4% accuracy on\nOCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%\naccuracy@0.5 on RefCOCOg-test."
                },
                "authors": [
                    {
                        "name": "Ya-Qi Yu"
                    },
                    {
                        "name": "Minghui Liao"
                    },
                    {
                        "name": "Jiwen Zhang"
                    },
                    {
                        "name": "Jihao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jihao Wu"
                },
                "author": "Jihao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05254v1",
                "updated": "2024-10-07T17:55:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:55:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments"
                },
                "summary": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents."
                },
                "authors": [
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Omer Madmon"
                    },
                    {
                        "name": "Itamar Reinman"
                    },
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    }
                ],
                "author_detail": {
                    "name": "Moshe Tennenholtz"
                },
                "author": "Moshe Tennenholtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05252v1",
                "updated": "2024-10-07T17:55:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    10,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:55:10Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    55,
                    10,
                    0,
                    281,
                    0
                ],
                "title": "Causal Micro-Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Micro-Narratives"
                },
                "summary": "We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research."
                },
                "authors": [
                    {
                        "name": "Mourad Heddaya"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Rob Voigt"
                    },
                    {
                        "name": "Alexander Zentefis"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Zentefis"
                },
                "author": "Alexander Zentefis",
                "arxiv_comment": "Accepted to EMNLP 2024 Workshop on Narrative Understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05248v1",
                "updated": "2024-10-07T17:52:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    52,
                    21,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:52:21Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    52,
                    21,
                    0,
                    281,
                    0
                ],
                "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe"
                },
                "summary": "To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications."
                },
                "authors": [
                    {
                        "name": "Yuxin Xiao"
                    },
                    {
                        "name": "Shujian Zhang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Sanqiang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sanqiang Zhao"
                },
                "author": "Sanqiang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17975v2",
                "updated": "2024-10-07T17:49:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    49,
                    13,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-25T23:12:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    23,
                    12,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)"
                },
                "summary": "Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs."
                },
                "authors": [
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Shubham Jain"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05229v1",
                "updated": "2024-10-07T17:36:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    36,
                    37,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:36:37Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    36,
                    37,
                    0,
                    281,
                    0
                ],
                "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Hooman Shahrokhi"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    }
                ],
                "author_detail": {
                    "name": "Mehrdad Farajtabar"
                },
                "author": "Mehrdad Farajtabar",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05224v1",
                "updated": "2024-10-07T17:29:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    29,
                    40,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:29:40Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    29,
                    40,
                    0,
                    281,
                    0
                ],
                "title": "Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates"
                },
                "summary": "Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules."
                },
                "authors": [
                    {
                        "name": "Avanika Narayan"
                    },
                    {
                        "name": "Mayee F. Chen"
                    },
                    {
                        "name": "Kush Bhatia"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13356v2",
                "updated": "2024-10-07T17:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    27,
                    30,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-19T09:03:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    3,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attack"
                },
                "summary": "Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in LLMs. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of targeted relearning attacks. With access to only a small and\npotentially loosely related set of data, we find that we can \"jog\" the memory\nof unlearned models to reverse the effects of unlearning. For example, we show\nthat relearning on public medical articles can lead an unlearned LLM to output\nharmful knowledge about bioweapons, and relearning general wiki information\nabout the book series Harry Potter can force the model to output verbatim\nmemorized text. We formalize this unlearning-relearning pipeline, explore the\nattack across three popular unlearning benchmarks, and discuss future\ndirections and guidelines that result from our study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in LLMs. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of targeted relearning attacks. With access to only a small and\npotentially loosely related set of data, we find that we can \"jog\" the memory\nof unlearned models to reverse the effects of unlearning. For example, we show\nthat relearning on public medical articles can lead an unlearned LLM to output\nharmful knowledge about bioweapons, and relearning general wiki information\nabout the book series Harry Potter can force the model to output verbatim\nmemorized text. We formalize this unlearning-relearning pipeline, explore the\nattack across three popular unlearning benchmarks, and discuss future\ndirections and guidelines that result from our study."
                },
                "authors": [
                    {
                        "name": "Shengyuan Hu"
                    },
                    {
                        "name": "Yiwei Fu"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "26 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05222v1",
                "updated": "2024-10-07T17:26:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    26,
                    31,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:26:31Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    26,
                    31,
                    0,
                    281,
                    0
                ],
                "title": "Precise Model Benchmarking with Only a Few Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise Model Benchmarking with Only a Few Observations"
                },
                "summary": "How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach."
                },
                "authors": [
                    {
                        "name": "Riccardo Fogliato"
                    },
                    {
                        "name": "Pratik Patil"
                    },
                    {
                        "name": "Nil-Jana Akpinar"
                    },
                    {
                        "name": "Mathew Monfort"
                    }
                ],
                "author_detail": {
                    "name": "Mathew Monfort"
                },
                "author": "Mathew Monfort",
                "arxiv_comment": "To appear at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15877v3",
                "updated": "2024-10-07T17:23:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    23,
                    30,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-22T15:52:04Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    15,
                    52,
                    4,
                    5,
                    174,
                    0
                ],
                "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions"
                },
                "summary": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area."
                },
                "authors": [
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Minh Chien Vu"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Imam Nur Bani Yusuf"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Indraneil Paul"
                    },
                    {
                        "name": "Simon Brunner"
                    },
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Thong Hoang"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Xiaoheng Hong"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Prateek Yadav"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro Von Werra"
                    }
                ],
                "author_detail": {
                    "name": "Leandro Von Werra"
                },
                "author": "Leandro Von Werra",
                "arxiv_comment": "44 pages, 14 figures, 7 tables, built with love by the BigCode\n  community :)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05218v1",
                "updated": "2024-10-07T17:22:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    22,
                    56,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:22:56Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    22,
                    56,
                    0,
                    281,
                    0
                ],
                "title": "Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Toni J. B. Liu"
                    },
                    {
                        "name": "Nicolas Boullé"
                    },
                    {
                        "name": "Raphaël Sarfati"
                    },
                    {
                        "name": "Christopher J. Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J. Earls"
                },
                "author": "Christopher J. Earls",
                "arxiv_comment": "Under review as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05217v1",
                "updated": "2024-10-07T17:21:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    46,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:21:46Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    46,
                    0,
                    281,
                    0
                ],
                "title": "Organizing Unstructured Image Collections using Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizing Unstructured Image Collections using Natural Language"
                },
                "summary": "Organizing unstructured visual data into semantic clusters is a key challenge\nin computer vision. Traditional deep clustering (DC) approaches focus on a\nsingle partition of data, while multiple clustering (MC) methods address this\nlimitation by uncovering distinct clustering solutions. The rise of large\nlanguage models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing\nusers to define clustering criteria in natural language. However, manually\nspecifying criteria for large datasets is impractical. In this work, we\nintroduce the task Semantic Multiple Clustering (SMC) that aims to\nautomatically discover clustering criteria from large image collections,\nuncovering interpretable substructures without requiring human input. Our\nframework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a\nproxy to concurrently reason over large image collections, discover\npartitioning criteria, expressed in natural language, and reveal semantic\nsubstructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c\nbenchmarks, each containing four grouping criteria and ground-truth\nannotations. We apply TeDeSC to various applications, such as discovering\nbiases and analyzing social media image popularity, demonstrating its utility\nas a tool for automatically organizing image collections and revealing novel\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizing unstructured visual data into semantic clusters is a key challenge\nin computer vision. Traditional deep clustering (DC) approaches focus on a\nsingle partition of data, while multiple clustering (MC) methods address this\nlimitation by uncovering distinct clustering solutions. The rise of large\nlanguage models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing\nusers to define clustering criteria in natural language. However, manually\nspecifying criteria for large datasets is impractical. In this work, we\nintroduce the task Semantic Multiple Clustering (SMC) that aims to\nautomatically discover clustering criteria from large image collections,\nuncovering interpretable substructures without requiring human input. Our\nframework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a\nproxy to concurrently reason over large image collections, discover\npartitioning criteria, expressed in natural language, and reveal semantic\nsubstructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c\nbenchmarks, each containing four grouping criteria and ground-truth\nannotations. We apply TeDeSC to various applications, such as discovering\nbiases and analyzing social media image popularity, demonstrating its utility\nas a tool for automatically organizing image collections and revealing novel\ninsights."
                },
                "authors": [
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Gianni Franchi"
                    },
                    {
                        "name": "Subhankar Roy"
                    },
                    {
                        "name": "Elisa Ricci"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Ricci"
                },
                "author": "Elisa Ricci",
                "arxiv_comment": "Preprint. Project webpage: https://oatmealliu.github.io/smc.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.02233v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.02233v3",
                "updated": "2024-10-07T17:21:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2023-09-05T13:39:38Z",
                "published_parsed": [
                    2023,
                    9,
                    5,
                    13,
                    39,
                    38,
                    1,
                    248,
                    0
                ],
                "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)"
                },
                "summary": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%."
                },
                "authors": [
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "This version has been accepted and published at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.02233v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.02233v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15652v2",
                "updated": "2024-10-07T17:21:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    0,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-24T15:47:35Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    15,
                    47,
                    35,
                    4,
                    145,
                    0
                ],
                "title": "$$\\mathbf{L^2\\cdot M = C^2}$$ Large Language Models are Covert Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$$\\mathbf{L^2\\cdot M = C^2}$$ Large Language Models are Covert Channels"
                },
                "summary": "Large Language Models (LLMs) have gained significant popularity recently.\nLLMs are susceptible to various attacks but can also improve the security of\ndiverse systems. However, besides enabling more secure systems, how well do\nopen source LLMs behave as covertext distributions to, e.g., facilitate\ncensorship-resistant communication? In this paper, we explore open-source\nLLM-based covert channels. We empirically measure the security vs. capacity of\nan open-source LLM model (Llama-7B) to assess its performance as a covert\nchannel. Although our results indicate that such channels are not likely to\nachieve high practical bitrates, we also show that the chance for an adversary\nto detect covert communication is low. To ensure our results can be used with\nthe least effort as a general reference, we employ a conceptually simple and\nconcise scheme and only assume public models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant popularity recently.\nLLMs are susceptible to various attacks but can also improve the security of\ndiverse systems. However, besides enabling more secure systems, how well do\nopen source LLMs behave as covertext distributions to, e.g., facilitate\ncensorship-resistant communication? In this paper, we explore open-source\nLLM-based covert channels. We empirically measure the security vs. capacity of\nan open-source LLM model (Llama-7B) to assess its performance as a covert\nchannel. Although our results indicate that such channels are not likely to\nachieve high practical bitrates, we also show that the chance for an adversary\nto detect covert communication is low. To ensure our results can be used with\nthe least effort as a general reference, we employ a conceptually simple and\nconcise scheme and only assume public models."
                },
                "authors": [
                    {
                        "name": "Simen Gaure"
                    },
                    {
                        "name": "Stefanos Koffas"
                    },
                    {
                        "name": "Stjepan Picek"
                    },
                    {
                        "name": "Sondre Rønjom"
                    }
                ],
                "author_detail": {
                    "name": "Sondre Rønjom"
                },
                "author": "Sondre Rønjom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06369v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06369v4",
                "updated": "2024-10-07T17:13:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    13,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-10T15:30:13Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    15,
                    30,
                    13,
                    0,
                    162,
                    0
                ],
                "title": "Annotation alignment: Comparing LLM and human annotations of\n  conversational safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotation alignment: Comparing LLM and human annotations of\n  conversational safety"
                },
                "summary": "Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another."
                },
                "authors": [
                    {
                        "name": "Rajiv Movva"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Emma Pierson"
                    }
                ],
                "author_detail": {
                    "name": "Emma Pierson"
                },
                "author": "Emma Pierson",
                "arxiv_comment": "EMNLP 2024 (Main). Main text contains 6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06369v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06369v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05193v1",
                "updated": "2024-10-07T16:50:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:50:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
                },
                "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tiezheng YU"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05191v1",
                "updated": "2024-10-07T16:49:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    49,
                    16,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:49:16Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    49,
                    16,
                    0,
                    281,
                    0
                ],
                "title": "LADEV: A Language-Driven Testing and Evaluation Platform for\n  Vision-Language-Action Models in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADEV: A Language-Driven Testing and Evaluation Platform for\n  Vision-Language-Action Models in Robotic Manipulation"
                },
                "summary": "Building on the advancements of Large Language Models (LLMs) and Vision\nLanguage Models (VLMs), recent research has introduced Vision-Language-Action\n(VLA) models as an integrated solution for robotic manipulation tasks. These\nmodels take camera images and natural language task instructions as input and\ndirectly generate control actions for robots to perform specified tasks,\ngreatly improving both decision-making capabilities and interaction with human\nusers. However, the data-driven nature of VLA models, combined with their lack\nof interpretability, makes the assurance of their effectiveness and robustness\na challenging task. This highlights the need for a reliable testing and\nevaluation platform. For this purpose, in this work, we propose LADEV, a\ncomprehensive and efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that automatically\ngenerates simulation environments from natural language inputs, mitigating the\nneed for manual adjustments and significantly improving testing efficiency.\nThen, to further assess the influence of language input on the VLA models, we\nimplement a paraphrase mechanism that produces diverse natural language task\ninstructions for testing. Finally, to expedite the evaluation process, we\nintroduce a batch-style method for conducting large-scale testing of VLA\nmodels. Using LADEV, we conducted experiments on several state-of-the-art VLA\nmodels, demonstrating its effectiveness as a tool for evaluating these models.\nOur results showed that LADEV not only enhances testing efficiency but also\nestablishes a solid baseline for evaluating VLA models, paving the way for the\ndevelopment of more intelligent and advanced robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the advancements of Large Language Models (LLMs) and Vision\nLanguage Models (VLMs), recent research has introduced Vision-Language-Action\n(VLA) models as an integrated solution for robotic manipulation tasks. These\nmodels take camera images and natural language task instructions as input and\ndirectly generate control actions for robots to perform specified tasks,\ngreatly improving both decision-making capabilities and interaction with human\nusers. However, the data-driven nature of VLA models, combined with their lack\nof interpretability, makes the assurance of their effectiveness and robustness\na challenging task. This highlights the need for a reliable testing and\nevaluation platform. For this purpose, in this work, we propose LADEV, a\ncomprehensive and efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that automatically\ngenerates simulation environments from natural language inputs, mitigating the\nneed for manual adjustments and significantly improving testing efficiency.\nThen, to further assess the influence of language input on the VLA models, we\nimplement a paraphrase mechanism that produces diverse natural language task\ninstructions for testing. Finally, to expedite the evaluation process, we\nintroduce a batch-style method for conducting large-scale testing of VLA\nmodels. Using LADEV, we conducted experiments on several state-of-the-art VLA\nmodels, demonstrating its effectiveness as a tool for evaluating these models.\nOur results showed that LADEV not only enhances testing efficiency but also\nestablishes a solid baseline for evaluating VLA models, paving the way for the\ndevelopment of more intelligent and advanced robotic systems."
                },
                "authors": [
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Zhehua Zhou"
                    },
                    {
                        "name": "Jiayang Song"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18074v2",
                "updated": "2024-10-07T16:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    46,
                    42,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-25T14:28:58Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    14,
                    28,
                    58,
                    3,
                    207,
                    0
                ],
                "title": "Principal-Agent Reinforcement Learning: Orchestrating AI Agents with\n  Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal-Agent Reinforcement Learning: Orchestrating AI Agents with\n  Contracts"
                },
                "summary": "The increasing deployment of AI is shaping the future landscape of the\ninternet, which is set to become an integrated ecosystem of AI agents.\nOrchestrating the interaction among AI agents necessitates decentralized,\nself-sustaining mechanisms that harmonize the tension between individual\ninterests and social welfare. In this paper we tackle this challenge by\nsynergizing reinforcement learning with principal-agent theory from economics.\nTaken separately, the former allows unrealistic freedom of intervention, while\nthe latter struggles to scale in sequential settings. Combining them achieves\nthe best of both worlds. We propose a framework where a principal guides an\nagent in a Markov Decision Process (MDP) using a series of contracts, which\nspecify payments by the principal based on observable outcomes of the agent's\nactions. We present and analyze a meta-algorithm that iteratively optimizes the\npolicies of the principal and agent, showing its equivalence to a contraction\noperator on the principal's Q-function, and its convergence to subgame-perfect\nequilibrium. We then scale our algorithm with deep Q-learning and analyze its\nconvergence in the presence of approximation error, both theoretically and\nthrough experiments with randomly generated binary game-trees. Extending our\nframework to multiple agents, we apply our methodology to the combinatorial\nCoin Game. Addressing this multi-agent sequential social dilemma is a promising\nfirst step toward scaling our approach to more complex, real-world instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of AI is shaping the future landscape of the\ninternet, which is set to become an integrated ecosystem of AI agents.\nOrchestrating the interaction among AI agents necessitates decentralized,\nself-sustaining mechanisms that harmonize the tension between individual\ninterests and social welfare. In this paper we tackle this challenge by\nsynergizing reinforcement learning with principal-agent theory from economics.\nTaken separately, the former allows unrealistic freedom of intervention, while\nthe latter struggles to scale in sequential settings. Combining them achieves\nthe best of both worlds. We propose a framework where a principal guides an\nagent in a Markov Decision Process (MDP) using a series of contracts, which\nspecify payments by the principal based on observable outcomes of the agent's\nactions. We present and analyze a meta-algorithm that iteratively optimizes the\npolicies of the principal and agent, showing its equivalence to a contraction\noperator on the principal's Q-function, and its convergence to subgame-perfect\nequilibrium. We then scale our algorithm with deep Q-learning and analyze its\nconvergence in the presence of approximation error, both theoretically and\nthrough experiments with randomly generated binary game-trees. Extending our\nframework to multiple agents, we apply our methodology to the combinatorial\nCoin Game. Addressing this multi-agent sequential social dilemma is a promising\nfirst step toward scaling our approach to more complex, real-world instances."
                },
                "authors": [
                    {
                        "name": "Dima Ivanov"
                    },
                    {
                        "name": "Paul Dütting"
                    },
                    {
                        "name": "Inbal Talgam-Cohen"
                    },
                    {
                        "name": "Tonghan Wang"
                    },
                    {
                        "name": "David C. Parkes"
                    }
                ],
                "author_detail": {
                    "name": "David C. Parkes"
                },
                "author": "David C. Parkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00099v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00099v4",
                "updated": "2024-10-07T16:45:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    45,
                    42,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-30T18:00:02Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    18,
                    0,
                    2,
                    1,
                    121,
                    0
                ],
                "title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation"
                },
                "summary": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00099v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00099v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05180v1",
                "updated": "2024-10-07T16:40:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    40,
                    21,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:40:21Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    40,
                    21,
                    0,
                    281,
                    0
                ],
                "title": "Enhancing Equity in Large Language Models for Medical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Equity in Large Language Models for Medical Applications"
                },
                "summary": "Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Wenhe Ma"
                    },
                    {
                        "name": "Sonish Sivarajkumar"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Eugene Mathew Sadhu"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Xizhi Wu"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02151v3",
                "updated": "2024-10-07T16:35:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    35,
                    15,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-02T17:58:27Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    17,
                    58,
                    27,
                    1,
                    93,
                    0
                ],
                "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks"
                },
                "summary": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flammarion"
                },
                "author": "Nicolas Flammarion",
                "arxiv_comment": "Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved\n  writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B),\n  jailbreak artifacts for all attacks are available, evaluation with different\n  judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots\n  over iterations, ablation on the suffix length for random search), examples\n  of jailbroken generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03480v2",
                "updated": "2024-10-07T16:28:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    28,
                    39,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-04T14:52:18Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    14,
                    52,
                    18,
                    4,
                    278,
                    0
                ],
                "title": "SeBS-Flow: Benchmarking Serverless Cloud Function Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeBS-Flow: Benchmarking Serverless Cloud Function Workflows"
                },
                "summary": "Serverless computing has emerged as a prominent paradigm, with a significant\nadoption rate among cloud customers. While this model offers advantages such as\nabstraction from the deployment and resource scheduling, it also poses\nlimitations in handling complex use cases due to the restricted nature of\nindividual functions. Serverless workflows address this limitation by\norchestrating multiple functions into a cohesive application. However, existing\nserverless workflow platforms exhibit significant differences in their\nprogramming models and infrastructure, making fair and consistent performance\nevaluations difficult in practice. To address this gap, we propose the first\nserverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic\nworkflow model that enables consistent benchmarking across various platforms.\nSeBS-Flow includes six real-world application benchmarks and four\nmicrobenchmarks representing different computational patterns. We conduct\ncomprehensive evaluations on three major cloud platforms, assessing\nperformance, cost, scalability, and runtime deviations. We make our benchmark\nsuite open-source, enabling rigorous and comparable evaluations of serverless\nworkflows over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has emerged as a prominent paradigm, with a significant\nadoption rate among cloud customers. While this model offers advantages such as\nabstraction from the deployment and resource scheduling, it also poses\nlimitations in handling complex use cases due to the restricted nature of\nindividual functions. Serverless workflows address this limitation by\norchestrating multiple functions into a cohesive application. However, existing\nserverless workflow platforms exhibit significant differences in their\nprogramming models and infrastructure, making fair and consistent performance\nevaluations difficult in practice. To address this gap, we propose the first\nserverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic\nworkflow model that enables consistent benchmarking across various platforms.\nSeBS-Flow includes six real-world application benchmarks and four\nmicrobenchmarks representing different computational patterns. We conduct\ncomprehensive evaluations on three major cloud platforms, assessing\nperformance, cost, scalability, and runtime deviations. We make our benchmark\nsuite open-source, enabling rigorous and comparable evaluations of serverless\nworkflows over time."
                },
                "authors": [
                    {
                        "name": "Larissa Schmid"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Alexandru Calotoiu"
                    },
                    {
                        "name": "Laurin Brandner"
                    },
                    {
                        "name": "Anne Koziolek"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10054v2",
                "updated": "2024-10-07T16:26:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    26,
                    0,
                    0,
                    281,
                    0
                ],
                "published": "2023-11-16T17:48:55Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    17,
                    48,
                    55,
                    3,
                    320,
                    0
                ],
                "title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models"
                },
                "summary": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles."
                },
                "authors": [
                    {
                        "name": "Mingqian Zheng"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05168v1",
                "updated": "2024-10-07T16:25:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation"
                },
                "summary": "Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02902v2",
                "updated": "2024-10-07T16:25:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-03T18:48:38Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    48,
                    38,
                    3,
                    277,
                    0
                ],
                "title": "Better Instruction-Following Through Minimum Bayes Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Instruction-Following Through Minimum Bayes Risk"
                },
                "summary": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding."
                },
                "authors": [
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Sina Pakazad"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05165v1",
                "updated": "2024-10-07T16:23:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    23,
                    36,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T16:23:36Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    23,
                    36,
                    0,
                    281,
                    0
                ],
                "title": "Efficient Inference for Large Language Model-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Large Language Model-based Generative\n  Recommendation"
                },
                "summary": "Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05109v2",
                "updated": "2024-10-07T16:21:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    21,
                    29,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-07T18:58:50Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    18,
                    58,
                    50,
                    2,
                    38,
                    0
                ],
                "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding"
                },
                "summary": "To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding frame-work. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of lightweight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads: a sequentially-dependent\ndrop-in replacement for standard draft heads that significantly improves the\naccuracy of draft head speculation. We further explore the design space of\nHydra head training objectives and architectures, and propose a carefully tuned\nHydra head recipe, which we call Hydra++, that improves decoding throughput by\nup to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding\nrespectively. Overall, Hydra heads are a simple and well-motivated intervention\non standard draft heads that significantly improve the end-to-end speed of\ndraft head-based speculative decoding. We make our code publicly available at\nhttps://github.com/zankner/Hydra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding frame-work. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of lightweight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads: a sequentially-dependent\ndrop-in replacement for standard draft heads that significantly improves the\naccuracy of draft head speculation. We further explore the design space of\nHydra head training objectives and architectures, and propose a carefully tuned\nHydra head recipe, which we call Hydra++, that improves decoding throughput by\nup to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding\nrespectively. Overall, Hydra heads are a simple and well-motivated intervention\non standard draft heads that significantly improve the end-to-end speed of\ndraft head-based speculative decoding. We make our code publicly available at\nhttps://github.com/zankner/Hydra."
                },
                "authors": [
                    {
                        "name": "Zachary Ankner"
                    },
                    {
                        "name": "Rishab Parthasarathy"
                    },
                    {
                        "name": "Aniruddha Nrusimha"
                    },
                    {
                        "name": "Christopher Rinard"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "William Brandon"
                    }
                ],
                "author_detail": {
                    "name": "William Brandon"
                },
                "author": "William Brandon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14577v2",
                "updated": "2024-10-07T16:01:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    1,
                    49,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-23T13:51:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    51,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "Representation noising effectively prevents harmful fine-tuning on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation noising effectively prevents harmful fine-tuning on LLMs"
                },
                "summary": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Kai Williams"
                    },
                    {
                        "name": "Łukasz Bartoszcze"
                    },
                    {
                        "name": "David Atanasov"
                    },
                    {
                        "name": "Robie Gonzales"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    },
                    {
                        "name": "Carsten Maple"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz",
                "arxiv_comment": "Published in NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03853v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03853v5",
                "updated": "2024-10-07T15:46:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    46,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-06T19:07:38Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    19,
                    7,
                    38,
                    2,
                    340,
                    0
                ],
                "title": "Dr. Jekyll and Mr. Hyde: Two Faces of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr. Jekyll and Mr. Hyde: Two Faces of LLMs"
                },
                "summary": "Recently, we have witnessed a rise in the use of Large Language Models\n(LLMs), especially in applications like chatbots. Safety mechanisms are\nimplemented to prevent improper responses from these chatbots. In this work, we\nbypass these measures for ChatGPT and Gemini by making them impersonate complex\npersonas with personality characteristics that are not aligned with a truthful\nassistant. First, we create elaborate biographies of these personas, which we\nthen use in a new session with the same chatbots. Our conversations then follow\na role-play style to elicit prohibited responses. Using personas, we show that\nprohibited responses are provided, making it possible to obtain unauthorized,\nillegal, or harmful information in both ChatGPT and Gemini. We also introduce\nseveral ways of activating such adversarial personas, showing that both\nchatbots are vulnerable to this attack. With the same principle, we introduce\ntwo defenses that push the model to interpret trustworthy personalities and\nmake it more robust against such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, we have witnessed a rise in the use of Large Language Models\n(LLMs), especially in applications like chatbots. Safety mechanisms are\nimplemented to prevent improper responses from these chatbots. In this work, we\nbypass these measures for ChatGPT and Gemini by making them impersonate complex\npersonas with personality characteristics that are not aligned with a truthful\nassistant. First, we create elaborate biographies of these personas, which we\nthen use in a new session with the same chatbots. Our conversations then follow\na role-play style to elicit prohibited responses. Using personas, we show that\nprohibited responses are provided, making it possible to obtain unauthorized,\nillegal, or harmful information in both ChatGPT and Gemini. We also introduce\nseveral ways of activating such adversarial personas, showing that both\nchatbots are vulnerable to this attack. With the same principle, we introduce\ntwo defenses that push the model to interpret trustworthy personalities and\nmake it more robust against such attacks."
                },
                "authors": [
                    {
                        "name": "Matteo Gioele Collu"
                    },
                    {
                        "name": "Tom Janssen-Groesbeek"
                    },
                    {
                        "name": "Stefanos Koffas"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Stjepan Picek"
                    }
                ],
                "author_detail": {
                    "name": "Stjepan Picek"
                },
                "author": "Stjepan Picek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03853v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03853v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00126v2",
                "updated": "2024-10-07T15:44:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    44,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-29T21:05:37Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    21,
                    5,
                    37,
                    3,
                    60,
                    0
                ],
                "title": "FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition"
                },
                "summary": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Wang"
                    },
                    {
                        "name": "Lingfei Wu"
                    },
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "arxiv_comment": "Accepted at EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05130v1",
                "updated": "2024-10-07T15:34:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    34,
                    14,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T15:34:14Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    34,
                    14,
                    0,
                    281,
                    0
                ],
                "title": "Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents"
                },
                "summary": "Recent research has explored the use of Large Language Models (LLMs) for\ntackling complex graph reasoning tasks. However, due to the intricacies of\ngraph structures and the inherent limitations of LLMs in handling long text,\ncurrent approaches often fail to deliver satisfactory accuracy, even on\nsmall-scale graphs and simple tasks. To address these challenges, we introduce\nGraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent\ncollaboration strategy for explicit and precise graph reasoning. Inspired by\ndistributed graph computation theory, our framework decomposes graph problems\ninto smaller, node-centric tasks that are distributed among multiple agents.\nThe agents collaborate to solve the overall problem, significantly reducing the\namount of information and complexity handled by a single LLM, thus enhancing\nthe accuracy of graph reasoning. By simply increasing the number of agents,\nGraphAgent-Reasoner can efficiently scale to accommodate larger graphs with\nover 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework\ndemonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,\nsignificantly outperforming the best available models, both closed-source and\nfine-tuned open-source variants. Our framework also demonstrates the capability\nto handle real-world graph reasoning applications such as webpage importance\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has explored the use of Large Language Models (LLMs) for\ntackling complex graph reasoning tasks. However, due to the intricacies of\ngraph structures and the inherent limitations of LLMs in handling long text,\ncurrent approaches often fail to deliver satisfactory accuracy, even on\nsmall-scale graphs and simple tasks. To address these challenges, we introduce\nGraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent\ncollaboration strategy for explicit and precise graph reasoning. Inspired by\ndistributed graph computation theory, our framework decomposes graph problems\ninto smaller, node-centric tasks that are distributed among multiple agents.\nThe agents collaborate to solve the overall problem, significantly reducing the\namount of information and complexity handled by a single LLM, thus enhancing\nthe accuracy of graph reasoning. By simply increasing the number of agents,\nGraphAgent-Reasoner can efficiently scale to accommodate larger graphs with\nover 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework\ndemonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,\nsignificantly outperforming the best available models, both closed-source and\nfine-tuned open-source variants. Our framework also demonstrates the capability\nto handle real-world graph reasoning applications such as webpage importance\nanalysis."
                },
                "authors": [
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Xinyi Huang"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yongchao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao Liu"
                },
                "author": "Yongchao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v2",
                "updated": "2024-10-07T15:24:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    24,
                    10,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17385v2",
                "updated": "2024-10-07T15:15:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    15,
                    18,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-25T09:04:21Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    9,
                    4,
                    21,
                    1,
                    177,
                    0
                ],
                "title": "Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance"
                },
                "summary": "Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency."
                },
                "authors": [
                    {
                        "name": "Manon Reusens"
                    },
                    {
                        "name": "Philipp Borchert"
                    },
                    {
                        "name": "Jochen De Weerdt"
                    },
                    {
                        "name": "Bart Baesens"
                    }
                ],
                "author_detail": {
                    "name": "Bart Baesens"
                },
                "author": "Bart Baesens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15929v2",
                "updated": "2024-10-07T15:01:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    48,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-24T23:16:57Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    23,
                    16,
                    57,
                    5,
                    55,
                    0
                ],
                "title": "Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs"
                },
                "summary": "Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels."
                },
                "authors": [
                    {
                        "name": "Isha Chaudhary"
                    },
                    {
                        "name": "Vedaant V. Jain"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05102v1",
                "updated": "2024-10-07T15:01:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    29,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T15:01:29Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    29,
                    0,
                    281,
                    0
                ],
                "title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks"
                },
                "summary": "Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods."
                },
                "authors": [
                    {
                        "name": "Fenia Christopoulou"
                    },
                    {
                        "name": "Ronald Cardenas"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "20 papges, 9 figures, 5 tables. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05099v1",
                "updated": "2024-10-07T14:55:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    55,
                    20,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:55:20Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    55,
                    20,
                    0,
                    281,
                    0
                ],
                "title": "Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances"
                },
                "summary": "Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them."
                },
                "authors": [
                    {
                        "name": "Alina Wróblewska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Wróblewska"
                },
                "author": "Alina Wróblewska",
                "arxiv_comment": "Accepted at CoNLL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02707v2",
                "updated": "2024-10-07T14:46:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    46,
                    11,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-03T17:31:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations"
                },
                "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."
                },
                "authors": [
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16671v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16671v7",
                "updated": "2024-10-07T14:44:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    44,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-02-26T15:47:01Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    15,
                    47,
                    1,
                    0,
                    57,
                    0
                ],
                "title": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding"
                },
                "summary": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level."
                },
                "authors": [
                    {
                        "name": "Alex Zhuang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Weiming Ren"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16671v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16671v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05090v1",
                "updated": "2024-10-07T14:42:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    42,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:42:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    42,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "HyperINF: Unleashing the HyperPower of the Schulz's Method for Data\n  Influence Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperINF: Unleashing the HyperPower of the Schulz's Method for Data\n  Influence Estimation"
                },
                "summary": "Influence functions provide a principled method to assess the contribution of\nindividual training samples to a specific target. Yet, their high computational\ncosts limit their applications on large-scale models and datasets. Existing\nmethods proposed for influence function approximation have significantly\nreduced the computational overheads. However, they mostly suffer from\ninaccurate estimation due to the lack of strong convergence guarantees from the\nalgorithm. The family of hyperpower methods are well-known for their rigorous\nconvergence guarantees on matrix inverse approximation, while the matrix\nmultiplication operation can involve intractable memory and computation costs\non large-scale models. We propose HyperINF, an efficient and accurate influence\nfunction approximation method which leverages the hyperpower method,\nspecifically Schulz's iterative algorithm.\n  To deal with the computation-intensive matrix multiplication, we incorporate\nthe generalized fisher information (GFIM) as a low-rank approximation of the\nHessian matrix, which reduces the memory and computation overheads to constant\ncosts independent of ranks on LoRA-tuned models.\n  We first demonstrate the superior accuracy and stability of \\method compared\nto other baselines through a synthetic convergence simulation for matrix\ninversion. We further validate the efficacy of \\method through extensive\nreal-world data attribution tasks, including mislabeled data detection and data\nselection for LLM and VLM fine-tuning.\n  On LoRA-tuned models, HyperINF achieves superior downstream performance with\nminimal memory and computational overhead, while other baselines suffer from\nsignificant degradation. Our codebase is available at\nhttps://github.com/Blackzxy/HyperINF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence functions provide a principled method to assess the contribution of\nindividual training samples to a specific target. Yet, their high computational\ncosts limit their applications on large-scale models and datasets. Existing\nmethods proposed for influence function approximation have significantly\nreduced the computational overheads. However, they mostly suffer from\ninaccurate estimation due to the lack of strong convergence guarantees from the\nalgorithm. The family of hyperpower methods are well-known for their rigorous\nconvergence guarantees on matrix inverse approximation, while the matrix\nmultiplication operation can involve intractable memory and computation costs\non large-scale models. We propose HyperINF, an efficient and accurate influence\nfunction approximation method which leverages the hyperpower method,\nspecifically Schulz's iterative algorithm.\n  To deal with the computation-intensive matrix multiplication, we incorporate\nthe generalized fisher information (GFIM) as a low-rank approximation of the\nHessian matrix, which reduces the memory and computation overheads to constant\ncosts independent of ranks on LoRA-tuned models.\n  We first demonstrate the superior accuracy and stability of \\method compared\nto other baselines through a synthetic convergence simulation for matrix\ninversion. We further validate the efficacy of \\method through extensive\nreal-world data attribution tasks, including mislabeled data detection and data\nselection for LLM and VLM fine-tuning.\n  On LoRA-tuned models, HyperINF achieves superior downstream performance with\nminimal memory and computational overhead, while other baselines suffer from\nsignificant degradation. Our codebase is available at\nhttps://github.com/Blackzxy/HyperINF."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Simin Fan"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05085v1",
                "updated": "2024-10-07T14:39:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    39,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:39:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    39,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification"
                },
                "summary": "Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness."
                },
                "authors": [
                    {
                        "name": "Jeremie Bogaert"
                    },
                    {
                        "name": "Marie-Catherine de Marneffe"
                    },
                    {
                        "name": "Antonin Descampe"
                    },
                    {
                        "name": "Louis Escouflaire"
                    },
                    {
                        "name": "Cedrick Fairon"
                    },
                    {
                        "name": "Francois-Xavier Standaert"
                    }
                ],
                "author_detail": {
                    "name": "Francois-Xavier Standaert"
                },
                "author": "Francois-Xavier Standaert",
                "arxiv_comment": "This paper is a faithful translation of a paper which was\n  peer-reviewed and published in the French journal Traitement Automatique des\n  Langues, n. 64",
                "arxiv_journal_ref": "Traitement Automatique des Langues 64, 2023, ATALA, Paris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14768v2",
                "updated": "2024-10-07T14:35:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    35,
                    14,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-23T16:35:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    35,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05080v1",
                "updated": "2024-10-07T14:33:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    33,
                    50,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:33:50Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    33,
                    50,
                    0,
                    281,
                    0
                ],
                "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery"
                },
                "summary": "The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research."
                },
                "authors": [
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Shijie Chen"
                    },
                    {
                        "name": "Yuting Ning"
                    },
                    {
                        "name": "Qianheng Zhang"
                    },
                    {
                        "name": "Boshi Wang"
                    },
                    {
                        "name": "Botao Yu"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Zitong Lu"
                    },
                    {
                        "name": "Vishal Dey"
                    },
                    {
                        "name": "Mingyi Xue"
                    },
                    {
                        "name": "Frazier N. Baker"
                    },
                    {
                        "name": "Benjamin Burns"
                    },
                    {
                        "name": "Daniel Adu-Ampratwum"
                    },
                    {
                        "name": "Xuhui Huang"
                    },
                    {
                        "name": "Xia Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "55 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05077v1",
                "updated": "2024-10-07T14:31:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    31,
                    43,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:31:43Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    31,
                    43,
                    0,
                    281,
                    0
                ],
                "title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering"
                },
                "summary": "Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points."
                },
                "authors": [
                    {
                        "name": "Francesco Maria Molfese"
                    },
                    {
                        "name": "Simone Conia"
                    },
                    {
                        "name": "Riccardo Orlando"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12034v2",
                "updated": "2024-10-07T14:27:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    27,
                    56,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-17T19:06:54Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    19,
                    6,
                    54,
                    0,
                    169,
                    0
                ],
                "title": "Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts"
                },
                "summary": "We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems."
                },
                "authors": [
                    {
                        "name": "Junmo Kang"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Jacob Hansen"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05062v1",
                "updated": "2024-10-07T14:21:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    21,
                    17,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:21:17Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    21,
                    17,
                    0,
                    281,
                    0
                ],
                "title": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks"
                },
                "summary": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence."
                },
                "authors": [
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19745v2",
                "updated": "2024-10-07T14:17:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    17,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-29T15:40:54Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    15,
                    40,
                    54,
                    6,
                    273,
                    0
                ],
                "title": "PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead"
                },
                "summary": "Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability."
                },
                "authors": [
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Yining Qian"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Yongbo Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Jingtong Wu"
                    },
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01386v2",
                "updated": "2024-10-07T14:14:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    14,
                    39,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-02T09:55:58Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    9,
                    55,
                    58,
                    2,
                    276,
                    0
                ],
                "title": "FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated\n  Learning Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated\n  Learning Deployments"
                },
                "summary": "This paper presents Federated Learning with Adaptive Monitoring and\nElimination (FLAME), a novel solution capable of detecting and mitigating\nconcept drift in Federated Learning (FL) Internet of Things (IoT) environments.\nConcept drift poses significant challenges for FL models deployed in dynamic\nand real-world settings. FLAME leverages an FL architecture, considers a\nreal-world FL pipeline, and proves capable of maintaining model performance and\naccuracy while addressing bandwidth and privacy constraints. Introducing\nvarious features and extensions on previous works, FLAME offers a robust\nsolution to concept drift, significantly reducing computational load and\ncommunication overhead. Compared to well-known lightweight mitigation methods,\nFLAME demonstrates superior performance in maintaining high F1 scores and\nreducing resource utilisation in large-scale IoT deployments, making it a\npromising approach for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Federated Learning with Adaptive Monitoring and\nElimination (FLAME), a novel solution capable of detecting and mitigating\nconcept drift in Federated Learning (FL) Internet of Things (IoT) environments.\nConcept drift poses significant challenges for FL models deployed in dynamic\nand real-world settings. FLAME leverages an FL architecture, considers a\nreal-world FL pipeline, and proves capable of maintaining model performance and\naccuracy while addressing bandwidth and privacy constraints. Introducing\nvarious features and extensions on previous works, FLAME offers a robust\nsolution to concept drift, significantly reducing computational load and\ncommunication overhead. Compared to well-known lightweight mitigation methods,\nFLAME demonstrates superior performance in maintaining high F1 scores and\nreducing resource utilisation in large-scale IoT deployments, making it a\npromising approach for real-world applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Stefano De Feo"
                    },
                    {
                        "name": "Aftab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Aftab Khan"
                },
                "author": "Aftab Khan",
                "arxiv_comment": "Accepted for Publication at ACM EWSN 2024 - EMERGE Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12058v4",
                "updated": "2024-10-07T14:08:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    8,
                    13,
                    0,
                    281,
                    0
                ],
                "published": "2024-06-17T19:50:40Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    19,
                    50,
                    40,
                    0,
                    169,
                    0
                ],
                "title": "WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions"
                },
                "summary": "Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being."
                },
                "authors": [
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Jinendra Malekar"
                    },
                    {
                        "name": "Vedant Palit"
                    },
                    {
                        "name": "Francis Ferraro"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "Accepted in BlackboxNLP @ EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13895v2",
                "updated": "2024-10-07T14:07:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    7,
                    47,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-18T20:48:33Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    20,
                    48,
                    33,
                    3,
                    200,
                    0
                ],
                "title": "Improving Robustness and Clinical Applicability of Automatic Respiratory\n  Sound Classification Using Deep Learning-Based Audio Enhancement: Algorithm\n  Development and Validation Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Robustness and Clinical Applicability of Automatic Respiratory\n  Sound Classification Using Deep Learning-Based Audio Enhancement: Algorithm\n  Development and Validation Study"
                },
                "summary": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions poses challenges for clinical deployment.\nAdditionally, predicting signals with only background noise could undermine\nuser trust in the system. This paper aims to investigate the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement\npreprocessing step into automatic respiratory sound classification systems to\nimprove robustness and clinical applicability. Multiple experiments were\nconducted using different audio enhancement model structures and classification\nmodels. The classification performance was compared to the baseline method of\nnoise injection data augmentation. Experiments were performed on two datasets:\nthe ICBHI respiratory sound dataset, which includes 5.5 hours of recordings,\nand the Formosa Archive of Breath Sounds (FABS) dataset, comprising 14.6 hours\nof recordings. Additionally, a physician validation study was conducted by 7\nsenior physicians to assess the clinical utility of the system.The integration\nof the audio enhancement pipeline resulted in a 21.88% increase in the ICBHI\nclassification score on the ICBHI dataset and a 4.10% improvement on the FABS\ndataset in multi-class noisy scenarios. Quantitative analysis from the\nphysician validation study revealed improvements in efficiency, diagnostic\nconfidence, and trust during model-assisted diagnosis, with workflows\nintegrating enhanced audio leading to an 11.61% increase in diagnostic\nsensitivity and facilitating high-confidence diagnoses. Incorporating an audio\nenhancement algorithm significantly enhances the robustness and clinical\nutility of automatic respiratory sound classification systems, improving\nperformance in noisy environments and fostering greater trust among medical\nprofessionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions poses challenges for clinical deployment.\nAdditionally, predicting signals with only background noise could undermine\nuser trust in the system. This paper aims to investigate the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement\npreprocessing step into automatic respiratory sound classification systems to\nimprove robustness and clinical applicability. Multiple experiments were\nconducted using different audio enhancement model structures and classification\nmodels. The classification performance was compared to the baseline method of\nnoise injection data augmentation. Experiments were performed on two datasets:\nthe ICBHI respiratory sound dataset, which includes 5.5 hours of recordings,\nand the Formosa Archive of Breath Sounds (FABS) dataset, comprising 14.6 hours\nof recordings. Additionally, a physician validation study was conducted by 7\nsenior physicians to assess the clinical utility of the system.The integration\nof the audio enhancement pipeline resulted in a 21.88% increase in the ICBHI\nclassification score on the ICBHI dataset and a 4.10% improvement on the FABS\ndataset in multi-class noisy scenarios. Quantitative analysis from the\nphysician validation study revealed improvements in efficiency, diagnostic\nconfidence, and trust during model-assisted diagnosis, with workflows\nintegrating enhanced audio leading to an 11.61% increase in diagnostic\nsensitivity and facilitating high-confidence diagnoses. Incorporating an audio\nenhancement algorithm significantly enhances the robustness and clinical\nutility of automatic respiratory sound classification systems, improving\nperformance in noisy environments and fostering greater trust among medical\nprofessionals."
                },
                "authors": [
                    {
                        "name": "Jing-Tong Tzeng"
                    },
                    {
                        "name": "Jeng-Lin Li"
                    },
                    {
                        "name": "Huan-Yu Chen"
                    },
                    {
                        "name": "Chun-Hsiang Huang"
                    },
                    {
                        "name": "Chi-Hsin Chen"
                    },
                    {
                        "name": "Cheng-Yi Fan"
                    },
                    {
                        "name": "Edward Pei-Chuan Huang"
                    },
                    {
                        "name": "Chi-Chun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Chun Lee"
                },
                "author": "Chi-Chun Lee",
                "arxiv_comment": "Demo website: https://rogertzeng.github.io/ReSC-AE/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05047v1",
                "updated": "2024-10-07T14:01:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    1,
                    20,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:01:20Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    1,
                    20,
                    0,
                    281,
                    0
                ],
                "title": "A test suite of prompt injection attacks for LLM-based machine\n  translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A test suite of prompt injection attacks for LLM-based machine\n  translation"
                },
                "summary": "LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied."
                },
                "authors": [
                    {
                        "name": "Antonio Valerio Miceli-Barone"
                    },
                    {
                        "name": "Zhifan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhifan Sun"
                },
                "author": "Zhifan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05045v1",
                "updated": "2024-10-07T14:00:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    0,
                    8,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:00:08Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    0,
                    8,
                    0,
                    281,
                    0
                ],
                "title": "Can LLMs plan paths with extra hints from solvers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs plan paths with extra hints from solvers?"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs."
                },
                "authors": [
                    {
                        "name": "Erik Wu"
                    },
                    {
                        "name": "Sayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Mitra"
                },
                "author": "Sayan Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05044v1",
                "updated": "2024-10-07T13:58:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    58,
                    40,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:58:40Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    58,
                    40,
                    0,
                    281,
                    0
                ],
                "title": "PhotoReg: Photometrically Registering 3D Gaussian Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhotoReg: Photometrically Registering 3D Gaussian Splatting Models"
                },
                "summary": "Building accurate representations of the environment is critical for\nintelligent robots to make decisions during deployment. Advances in\nphotorealistic environment models have enabled robots to develop\nhyper-realistic reconstructions, which can be used to generate images that are\nintuitive for human inspection. In particular, the recently introduced\n\\ac{3DGS}, which describes the scene with up to millions of primitive\nellipsoids, can be rendered in real time. \\ac{3DGS} has rapidly gained\nprominence. However, a critical unsolved problem persists: how can we fuse\nmultiple \\ac{3DGS} into a single coherent model? Solving this problem will\nenable robot teams to jointly build \\ac{3DGS} models of their surroundings. A\nkey insight of this work is to leverage the {duality} between photorealistic\nreconstructions, which render realistic 2D images from 3D structure, and\n\\emph{3D foundation models}, which predict 3D structure from image pairs. To\nthis end, we develop PhotoReg, a framework to register multiple photorealistic\n\\ac{3DGS} models with 3D foundation models. As \\ac{3DGS} models are generally\nbuilt from monocular camera images, they have \\emph{arbitrary scale}. To\nresolve this, PhotoReg actively enforces scale consistency among the different\n\\ac{3DGS} models by considering depth estimates within these models. Then, the\nalignment is iteratively refined with fine-grained photometric losses to\nproduce high-quality fused \\ac{3DGS} models. We rigorously evaluate PhotoReg on\nboth standard benchmark datasets and our custom-collected datasets, including\nwith two quadruped robots. The code is released at\n\\url{ziweny11.github.io/photoreg}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building accurate representations of the environment is critical for\nintelligent robots to make decisions during deployment. Advances in\nphotorealistic environment models have enabled robots to develop\nhyper-realistic reconstructions, which can be used to generate images that are\nintuitive for human inspection. In particular, the recently introduced\n\\ac{3DGS}, which describes the scene with up to millions of primitive\nellipsoids, can be rendered in real time. \\ac{3DGS} has rapidly gained\nprominence. However, a critical unsolved problem persists: how can we fuse\nmultiple \\ac{3DGS} into a single coherent model? Solving this problem will\nenable robot teams to jointly build \\ac{3DGS} models of their surroundings. A\nkey insight of this work is to leverage the {duality} between photorealistic\nreconstructions, which render realistic 2D images from 3D structure, and\n\\emph{3D foundation models}, which predict 3D structure from image pairs. To\nthis end, we develop PhotoReg, a framework to register multiple photorealistic\n\\ac{3DGS} models with 3D foundation models. As \\ac{3DGS} models are generally\nbuilt from monocular camera images, they have \\emph{arbitrary scale}. To\nresolve this, PhotoReg actively enforces scale consistency among the different\n\\ac{3DGS} models by considering depth estimates within these models. Then, the\nalignment is iteratively refined with fine-grained photometric losses to\nproduce high-quality fused \\ac{3DGS} models. We rigorously evaluate PhotoReg on\nboth standard benchmark datasets and our custom-collected datasets, including\nwith two quadruped robots. The code is released at\n\\url{ziweny11.github.io/photoreg}."
                },
                "authors": [
                    {
                        "name": "Ziwen Yuan"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Matthew Johnson-Roberson"
                    },
                    {
                        "name": "Weiming Zhi"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zhi"
                },
                "author": "Weiming Zhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04976v1",
                "updated": "2024-10-07T12:17:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    17,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T12:17:44Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    17,
                    44,
                    0,
                    281,
                    0
                ],
                "title": "Noise-Domain Non-Orthogonal Multiple Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Domain Non-Orthogonal Multiple Access"
                },
                "summary": "In this paper, we present noise-domain non-orthogonal multiple access\n(ND-NOMA), an innovative communication scheme that utilizes the modulation of\nartificial noise mean and variance to convey information. Distinct from\ntraditional methods such as power-domain non-orthogonal multiple access\n(PD-NOMA) that heavily relies on successive interference cancellation (SIC),\nND-NOMA utilizes the noise domain, considerably reducing power consumption and\nsystem complexity. Inspired by noise modulation, ND-NOMA enhances energy\nefficiency and provides lower bit error probability (BEP), making it highly\nsuitable for next-generation Internet-of-things (IoT) networks. Our theoretical\nanalyses and computer simulations reveal that ND-NOMA can achieve exceptionally\nlow bit error rates in both uplink and downlink scenarios, in the presence of\nRician fading channels. The proposed multi-user system is supported by a\nminimum distance detector for mean detection and a threshold-based detector for\nvariance detection, ensuring robust communication in low-power environments. By\nleveraging the inherent properties of noise, ND-NOMA offers a promising\nplatform for long-term deployments of low-cost and low-complexity devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present noise-domain non-orthogonal multiple access\n(ND-NOMA), an innovative communication scheme that utilizes the modulation of\nartificial noise mean and variance to convey information. Distinct from\ntraditional methods such as power-domain non-orthogonal multiple access\n(PD-NOMA) that heavily relies on successive interference cancellation (SIC),\nND-NOMA utilizes the noise domain, considerably reducing power consumption and\nsystem complexity. Inspired by noise modulation, ND-NOMA enhances energy\nefficiency and provides lower bit error probability (BEP), making it highly\nsuitable for next-generation Internet-of-things (IoT) networks. Our theoretical\nanalyses and computer simulations reveal that ND-NOMA can achieve exceptionally\nlow bit error rates in both uplink and downlink scenarios, in the presence of\nRician fading channels. The proposed multi-user system is supported by a\nminimum distance detector for mean detection and a threshold-based detector for\nvariance detection, ensuring robust communication in low-power environments. By\nleveraging the inherent properties of noise, ND-NOMA offers a promising\nplatform for long-term deployments of low-cost and low-complexity devices."
                },
                "authors": [
                    {
                        "name": "Erkin Yapici"
                    },
                    {
                        "name": "Yusuf Islam Tek"
                    },
                    {
                        "name": "Ertugrul Basar"
                    }
                ],
                "author_detail": {
                    "name": "Ertugrul Basar"
                },
                "author": "Ertugrul Basar",
                "arxiv_comment": "9 pages, 5 figures, submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19339v2",
                "updated": "2024-10-07T12:05:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    5,
                    55,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-28T12:49:16Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    12,
                    49,
                    16,
                    5,
                    272,
                    0
                ],
                "title": "Visual Question Decomposition on Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Decomposition on Multimodal Large Language Models"
                },
                "summary": "Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets."
                },
                "authors": [
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Bailan He"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    },
                    {
                        "name": "Jindong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Gu"
                },
                "author": "Jindong Gu",
                "arxiv_comment": "Accepted to EMNLP2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10805v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10805v4",
                "updated": "2024-10-07T11:52:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    52,
                    50,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-15T15:20:40Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    15,
                    20,
                    40,
                    0,
                    197,
                    0
                ],
                "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning."
                },
                "authors": [
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Muzhi Li"
                    },
                    {
                        "name": "Huaren Qu"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10805v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10805v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02525v3",
                "updated": "2024-10-07T11:52:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    52,
                    19,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-03T07:27:33Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    7,
                    27,
                    33,
                    2,
                    94,
                    0
                ],
                "title": "Large Language Model for Vulnerability Detection and Repair: Literature\n  Review and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Vulnerability Detection and Repair: Literature\n  Review and the Road Ahead"
                },
                "summary": "The significant advancements in Large Language Models (LLMs) have resulted in\ntheir widespread adoption across various tasks within Software Engineering\n(SE), including vulnerability detection and repair. Numerous studies have\ninvestigated the application of LLMs to enhance vulnerability detection and\nrepair tasks. Despite the increasing research interest, there is currently no\nexisting survey that focuses on the utilization of LLMs for vulnerability\ndetection and repair. In this paper, we aim to bridge this gap by offering a\nsystematic literature review of approaches aimed at improving vulnerability\ndetection and repair through the utilization of LLMs. The review encompasses\nresearch work from leading SE, AI, and Security conferences and journals,\nencompassing 43 papers published across 25 distinct venues, along with 15\nhigh-quality preprint papers, bringing the total to 58 papers. By answering\nthree key research questions, we aim to (1) summarize the LLMs employed in the\nrelevant literature, (2) categorize various LLM adaptation techniques in\nvulnerability detection, and (3) classify various LLM adaptation techniques in\nvulnerability repair. Based on our findings, we have identified a series of\nlimitations of existing studies. Additionally, we have outlined a roadmap\nhighlighting potential opportunities that we believe are pertinent and crucial\nfor future research endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements in Large Language Models (LLMs) have resulted in\ntheir widespread adoption across various tasks within Software Engineering\n(SE), including vulnerability detection and repair. Numerous studies have\ninvestigated the application of LLMs to enhance vulnerability detection and\nrepair tasks. Despite the increasing research interest, there is currently no\nexisting survey that focuses on the utilization of LLMs for vulnerability\ndetection and repair. In this paper, we aim to bridge this gap by offering a\nsystematic literature review of approaches aimed at improving vulnerability\ndetection and repair through the utilization of LLMs. The review encompasses\nresearch work from leading SE, AI, and Security conferences and journals,\nencompassing 43 papers published across 25 distinct venues, along with 15\nhigh-quality preprint papers, bringing the total to 58 papers. By answering\nthree key research questions, we aim to (1) summarize the LLMs employed in the\nrelevant literature, (2) categorize various LLM adaptation techniques in\nvulnerability detection, and (3) classify various LLM adaptation techniques in\nvulnerability repair. Based on our findings, we have identified a series of\nlimitations of existing studies. Additionally, we have outlined a roadmap\nhighlighting potential opportunities that we believe are pertinent and crucial\nfor future research endeavors."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Sicong Cao"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04949v1",
                "updated": "2024-10-07T11:45:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    45,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:45:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    45,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law"
                },
                "summary": "Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches."
                },
                "authors": [
                    {
                        "name": "Yongming Chen"
                    },
                    {
                        "name": "Miner Chen"
                    },
                    {
                        "name": "Ye Zhu"
                    },
                    {
                        "name": "Juan Pei"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Songan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Songan Zhang"
                },
                "author": "Songan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14883v2",
                "updated": "2024-10-07T11:37:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    37,
                    44,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-23T10:09:46Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    10,
                    9,
                    46,
                    1,
                    114,
                    0
                ],
                "title": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans"
                },
                "summary": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference."
                },
                "authors": [
                    {
                        "name": "Vittoria Dentella"
                    },
                    {
                        "name": "Fritz Guenther"
                    },
                    {
                        "name": "Evelina Leivada"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Leivada"
                },
                "author": "Evelina Leivada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04936v1",
                "updated": "2024-10-07T11:27:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    27,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:27:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    27,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Training Interactive Agent in Large FPS Game Map with Rule-enhanced\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Interactive Agent in Large FPS Game Map with Rule-enhanced\n  Reinforcement Learning"
                },
                "summary": "In the realm of competitive gaming, 3D first-person shooter (FPS) games have\ngained immense popularity, prompting the development of game AI systems to\nenhance gameplay. However, deploying game AI in practical scenarios still poses\nchallenges, particularly in large-scale and complex FPS games. In this paper,\nwe focus on the practical deployment of game AI in the online multiplayer\ncompetitive 3D FPS game called Arena Breakout, developed by Tencent Games. We\npropose a novel gaming AI system named Private Military Company Agent (PMCA),\nwhich is interactable within a large game map and engages in combat with\nplayers while utilizing tactical advantages provided by the surrounding\nterrain.\n  To address the challenges of navigation and combat in modern 3D FPS games, we\nintroduce a method that combines navigation mesh (Navmesh) and shooting-rule\nwith deep reinforcement learning (NSRL). The integration of Navmesh enhances\nthe agent's global navigation capabilities while shooting behavior is\ncontrolled using rule-based methods to ensure controllability. NSRL employs a\nDRL model to predict when to enable the navigation mesh, resulting in a diverse\nrange of behaviors for the game AI. Customized rewards for human-like behaviors\nare also employed to align PMCA's behavior with that of human players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of competitive gaming, 3D first-person shooter (FPS) games have\ngained immense popularity, prompting the development of game AI systems to\nenhance gameplay. However, deploying game AI in practical scenarios still poses\nchallenges, particularly in large-scale and complex FPS games. In this paper,\nwe focus on the practical deployment of game AI in the online multiplayer\ncompetitive 3D FPS game called Arena Breakout, developed by Tencent Games. We\npropose a novel gaming AI system named Private Military Company Agent (PMCA),\nwhich is interactable within a large game map and engages in combat with\nplayers while utilizing tactical advantages provided by the surrounding\nterrain.\n  To address the challenges of navigation and combat in modern 3D FPS games, we\nintroduce a method that combines navigation mesh (Navmesh) and shooting-rule\nwith deep reinforcement learning (NSRL). The integration of Navmesh enhances\nthe agent's global navigation capabilities while shooting behavior is\ncontrolled using rule-based methods to ensure controllability. NSRL employs a\nDRL model to predict when to enable the navigation mesh, resulting in a diverse\nrange of behaviors for the game AI. Customized rewards for human-like behaviors\nare also employed to align PMCA's behavior with that of human players."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Huan Hu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Qiyang Cao"
                    },
                    {
                        "name": "Ruochen Liu"
                    },
                    {
                        "name": "Wenya Wei"
                    },
                    {
                        "name": "Elvis S. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Elvis S. Liu"
                },
                "author": "Elvis S. Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04931v1",
                "updated": "2024-10-07T11:24:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    24,
                    29,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:24:29Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    24,
                    29,
                    0,
                    281,
                    0
                ],
                "title": "The Role of Governments in Increasing Interconnected Post-Deployment\n  Monitoring of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Governments in Increasing Interconnected Post-Deployment\n  Monitoring of AI"
                },
                "summary": "Language-based AI systems are diffusing into society, bringing positive and\nnegative impacts. Mitigating negative impacts depends on accurate impact\nassessments, drawn from an empirical evidence base that makes causal\nconnections between AI usage and impacts. Interconnected post-deployment\nmonitoring combines information about model integration and use, application\nuse, and incidents and impacts. For example, inference time monitoring of\nchain-of-thought reasoning can be combined with long-term monitoring of\nsectoral AI diffusion, impacts and incidents. Drawing on information sharing\nmechanisms in other industries, we highlight example data sources and specific\ndata points that governments could collect to inform AI risk management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-based AI systems are diffusing into society, bringing positive and\nnegative impacts. Mitigating negative impacts depends on accurate impact\nassessments, drawn from an empirical evidence base that makes causal\nconnections between AI usage and impacts. Interconnected post-deployment\nmonitoring combines information about model integration and use, application\nuse, and incidents and impacts. For example, inference time monitoring of\nchain-of-thought reasoning can be combined with long-term monitoring of\nsectoral AI diffusion, impacts and incidents. Drawing on information sharing\nmechanisms in other industries, we highlight example data sources and specific\ndata points that governments could collect to inform AI risk management."
                },
                "authors": [
                    {
                        "name": "Merlin Stein"
                    },
                    {
                        "name": "Jamie Bernardi"
                    },
                    {
                        "name": "Connor Dunlop"
                    }
                ],
                "author_detail": {
                    "name": "Connor Dunlop"
                },
                "author": "Connor Dunlop",
                "arxiv_comment": "7 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04927v1",
                "updated": "2024-10-07T11:19:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    19,
                    5,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:19:05Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    19,
                    5,
                    0,
                    281,
                    0
                ],
                "title": "FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services"
                },
                "summary": "Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS."
                },
                "authors": [
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Guanhua Ye"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Viet Hung Nguyen"
                },
                "author": "Quoc Viet Hung Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04925v1",
                "updated": "2024-10-07T11:17:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    17,
                    5,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T11:17:05Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    17,
                    5,
                    0,
                    281,
                    0
                ],
                "title": "Intent Classification for Bank Chatbots through LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Classification for Bank Chatbots through LLM Fine-Tuning"
                },
                "summary": "This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication."
                },
                "authors": [
                    {
                        "name": "Bibiána Lajčinová"
                    },
                    {
                        "name": "Patrik Valábek"
                    },
                    {
                        "name": "Michal Spišiak"
                    }
                ],
                "author_detail": {
                    "name": "Michal Spišiak"
                },
                "author": "Michal Spišiak",
                "arxiv_comment": "7 pages, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07578v2",
                "updated": "2024-10-07T11:04:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    11,
                    4,
                    31,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-11T19:10:29Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    10,
                    29,
                    2,
                    255,
                    0
                ],
                "title": "A Novel Mathematical Framework for Objective Characterization of Ideas\n  through Vector Embeddings in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Mathematical Framework for Objective Characterization of Ideas\n  through Vector Embeddings in LLM"
                },
                "summary": "The demand for innovation in product design necessitates a prolific ideation\nphase. Conversational AI (CAI) systems that use Large Language Models (LLMs)\nsuch as GPT (Generative Pre-trained Transformer) have been shown to be fruitful\nin augmenting human creativity, providing numerous novel and diverse ideas.\nDespite the success in ideation quantity, the qualitative assessment of these\nideas remains challenging and traditionally reliant on expert human evaluation.\nThis method suffers from limitations such as human judgment errors, bias, and\noversight. Addressing this gap, our study introduces a comprehensive\nmathematical framework for automated analysis to objectively evaluate the\nplethora of ideas generated by CAI systems and/or humans. This framework is\nparticularly advantageous for novice designers who lack experience in selecting\npromising ideas. By converting the ideas into higher dimensional vectors and\nquantitatively measuring the diversity between them using tools such as UMAP,\nDBSCAN and PCA, the proposed method provides a reliable and objective way of\nselecting the most promising ideas, thereby enhancing the efficiency of the\nideation phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for innovation in product design necessitates a prolific ideation\nphase. Conversational AI (CAI) systems that use Large Language Models (LLMs)\nsuch as GPT (Generative Pre-trained Transformer) have been shown to be fruitful\nin augmenting human creativity, providing numerous novel and diverse ideas.\nDespite the success in ideation quantity, the qualitative assessment of these\nideas remains challenging and traditionally reliant on expert human evaluation.\nThis method suffers from limitations such as human judgment errors, bias, and\noversight. Addressing this gap, our study introduces a comprehensive\nmathematical framework for automated analysis to objectively evaluate the\nplethora of ideas generated by CAI systems and/or humans. This framework is\nparticularly advantageous for novice designers who lack experience in selecting\npromising ideas. By converting the ideas into higher dimensional vectors and\nquantitatively measuring the diversity between them using tools such as UMAP,\nDBSCAN and PCA, the proposed method provides a reliable and objective way of\nselecting the most promising ideas, thereby enhancing the efficiency of the\nideation phase."
                },
                "authors": [
                    {
                        "name": "B. Sankar"
                    },
                    {
                        "name": "Dibakar Sen"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Sen"
                },
                "author": "Dibakar Sen",
                "arxiv_comment": "20 pages, 12 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53A45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04834v2",
                "updated": "2024-10-07T10:28:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    10,
                    28,
                    25,
                    0,
                    281,
                    0
                ],
                "published": "2024-04-07T07:05:40Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    7,
                    5,
                    40,
                    6,
                    98,
                    0
                ],
                "title": "LLM-Based Multi-Agent Systems for Software Engineering: Vision and the\n  Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Agent Systems for Software Engineering: Vision and the\n  Road Ahead"
                },
                "summary": "Integrating Large Language Models(LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\ncompetitive to human planning and reasoning. This paper envisions the evolution\nof LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted\nsoftware engineering challenges. LMA systems introduce numerous benefits,\nincluding enhanced robustness through collaborative cross-examination,\nautonomous problem-solving, and scalable solutions to complex software\nprojects. By examining the role of LMA systems in future software engineering\npractices, this vision paper highlights the potential applications and emerging\nchallenges. We further point to specific opportunities for research and\nconclude with a research agenda with a set of research questions to guide\nfuture research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models(LLMs) into autonomous agents marks a\nsignificant shift in the research landscape by offering cognitive abilities\ncompetitive to human planning and reasoning. This paper envisions the evolution\nof LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted\nsoftware engineering challenges. LMA systems introduce numerous benefits,\nincluding enhanced robustness through collaborative cross-examination,\nautonomous problem-solving, and scalable solutions to complex software\nprojects. By examining the role of LMA systems in future software engineering\npractices, this vision paper highlights the potential applications and emerging\nchallenges. We further point to specific opportunities for research and\nconclude with a research agenda with a set of research questions to guide\nfuture research directions."
                },
                "authors": [
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05930v4",
                "updated": "2024-10-07T09:58:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    58,
                    48,
                    0,
                    281,
                    0
                ],
                "published": "2024-01-11T14:09:09Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    14,
                    9,
                    9,
                    3,
                    11,
                    0
                ],
                "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully"
                },
                "summary": "Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Tianhang Zhang"
                    },
                    {
                        "name": "Hai Hu"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08227v2",
                "updated": "2024-10-07T09:51:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    51,
                    46,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-11T07:01:50Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    7,
                    1,
                    50,
                    3,
                    193,
                    0
                ],
                "title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs"
                },
                "summary": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets."
                },
                "authors": [
                    {
                        "name": "Chihcheng Hsieh"
                    },
                    {
                        "name": "Catarina Moreira"
                    },
                    {
                        "name": "Isabel Blanco Nobre"
                    },
                    {
                        "name": "Sandra Costa Sousa"
                    },
                    {
                        "name": "Chun Ouyang"
                    },
                    {
                        "name": "Margot Brereton"
                    },
                    {
                        "name": "Joaquim Jorge"
                    },
                    {
                        "name": "Jacinto C. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Jacinto C. Nascimento"
                },
                "author": "Jacinto C. Nascimento",
                "arxiv_comment": "we introduce a pioneering approach to clinical data augmentation that\n  employs large language models (LLMs) to generate patient contextual synthetic\n  data. It preserves the integrity of real patient data while enriching the\n  dataset with contextually relevant synthetic features, significantly\n  enhancing model performance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.1; J.3; H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15625v2",
                "updated": "2024-10-07T09:49:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    49,
                    8,
                    0,
                    281,
                    0
                ],
                "published": "2024-08-28T08:25:22Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    22,
                    2,
                    241,
                    0
                ],
                "title": "CBF-LLM: Safe Control for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBF-LLM: Safe Control for LLM Alignment"
                },
                "summary": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks."
                },
                "authors": [
                    {
                        "name": "Yuya Miyaoka"
                    },
                    {
                        "name": "Masaki Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Masaki Inoue"
                },
                "author": "Masaki Inoue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15325v2",
                "updated": "2024-10-07T09:40:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    40,
                    7,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-22T02:06:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    2,
                    6,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Odyssey: Empowering Minecraft Agents with Open-World Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Odyssey: Empowering Minecraft Agents with Open-World Skills"
                },
                "summary": "Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions."
                },
                "authors": [
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Zhenyu Cui"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Yuxuan Zheng"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09887v2",
                "updated": "2024-10-07T09:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    15,
                    7,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-13T13:27:57Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    27,
                    57,
                    5,
                    195,
                    0
                ],
                "title": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization\n  Modeling"
                },
                "summary": "Large language models (LLMs) have exhibited their problem-solving abilities\nin mathematical reasoning. Solving realistic optimization (OPT) problems in\napplication scenarios requires advanced and applied mathematics ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose OptiBench, a\nbenchmark for End-to-end optimization problem-solving with human-readable\ninputs and outputs. OptiBench contains rich optimization problems, including\nlinear and nonlinear programming with or without tabular data, which can\ncomprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are\nrequired to call a code solver to provide precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and\nclosed-source LLMs (e.g., GPT-4), we further propose a data synthesis method\nnamely ReSocratic. Unlike general data synthesis methods that proceed from\nquestions to answers, \\ReSocratic first incrementally synthesizes formatted\noptimization demonstration with mathematical formulations step by step and then\nback-translates the generated demonstrations into questions. Based on this, we\nsynthesize the ReSocratic-29k dataset. We further conduct supervised\nfine-tuning with ReSocratic-29k on multiple open-source models. Experimental\nresults show that ReSocratic-29k significantly improves the performance of\nopen-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited their problem-solving abilities\nin mathematical reasoning. Solving realistic optimization (OPT) problems in\napplication scenarios requires advanced and applied mathematics ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose OptiBench, a\nbenchmark for End-to-end optimization problem-solving with human-readable\ninputs and outputs. OptiBench contains rich optimization problems, including\nlinear and nonlinear programming with or without tabular data, which can\ncomprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are\nrequired to call a code solver to provide precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and\nclosed-source LLMs (e.g., GPT-4), we further propose a data synthesis method\nnamely ReSocratic. Unlike general data synthesis methods that proceed from\nquestions to answers, \\ReSocratic first incrementally synthesizes formatted\noptimization demonstration with mathematical formulations step by step and then\nback-translates the generated demonstrations into questions. Based on this, we\nsynthesize the ReSocratic-29k dataset. We further conduct supervised\nfine-tuning with ReSocratic-29k on multiple open-source models. Experimental\nresults show that ReSocratic-29k significantly improves the performance of\nopen-source models."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13538v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13538v5",
                "updated": "2024-10-07T09:11:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    11,
                    49,
                    0,
                    281,
                    0
                ],
                "published": "2023-11-22T17:24:21Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    17,
                    24,
                    21,
                    2,
                    326,
                    0
                ],
                "title": "AlignedCoT: Prompting Large Language Models via Native-Speaking\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedCoT: Prompting Large Language Models via Native-Speaking\n  Demonstrations"
                },
                "summary": "Large Language Models prompting, such as using in-context demonstrations, is\na mainstream technique for invoking LLMs to perform high-performance and solid\ncomplex reasoning (e.g., mathematical reasoning, commonsense reasoning), and\nhas the potential for further human-machine collaborative scientific findings.\nHowever, current LLMs are delicate and elusive in prompt words and styles. And\nthere is an unseen gap between LLM understanding and human-written prompts.\nThis paper introduces Alignedcot, an LLM-acquainted prompting technique that\nincludes proficient ``native-speaking'' in in-context learning for the LLMs.\nSpecifically, it achieves consistent and correct step-wise prompts in zero-shot\nscenarios by progressively probing, refining, and formatting the LLM chain of\nthoughts so that free from handcrafted few-shot demonstrations while\nmaintaining the prompt quality. We conduct experiments on mathematical\nreasoning and commonsense reasoning. We find that LLMs with Alignedcot perform\nsignificantly superior to them with human-crafted demonstrations. We further\napply Alignedcot for rewriting the GSM8K training set, resulting in a\nGSM8K-Align dataset. We observe its benefits for retrieval augmented\ngeneration. The code and data can be found at\nhttps://github.com/yangzhch6/AlignedCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models prompting, such as using in-context demonstrations, is\na mainstream technique for invoking LLMs to perform high-performance and solid\ncomplex reasoning (e.g., mathematical reasoning, commonsense reasoning), and\nhas the potential for further human-machine collaborative scientific findings.\nHowever, current LLMs are delicate and elusive in prompt words and styles. And\nthere is an unseen gap between LLM understanding and human-written prompts.\nThis paper introduces Alignedcot, an LLM-acquainted prompting technique that\nincludes proficient ``native-speaking'' in in-context learning for the LLMs.\nSpecifically, it achieves consistent and correct step-wise prompts in zero-shot\nscenarios by progressively probing, refining, and formatting the LLM chain of\nthoughts so that free from handcrafted few-shot demonstrations while\nmaintaining the prompt quality. We conduct experiments on mathematical\nreasoning and commonsense reasoning. We find that LLMs with Alignedcot perform\nsignificantly superior to them with human-crafted demonstrations. We further\napply Alignedcot for rewriting the GSM8K training set, resulting in a\nGSM8K-Align dataset. We observe its benefits for retrieval augmented\ngeneration. The code and data can be found at\nhttps://github.com/yangzhch6/AlignedCoT."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13538v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13538v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04838v1",
                "updated": "2024-10-07T08:53:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    53,
                    0,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T08:53:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    53,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation"
                },
                "summary": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Akira Kawabata"
                    },
                    {
                        "name": "Saku Sugawara"
                    }
                ],
                "author_detail": {
                    "name": "Saku Sugawara"
                },
                "author": "Saku Sugawara",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06551v2",
                "updated": "2024-10-07T08:49:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    49,
                    49,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-09T05:16:22Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    5,
                    16,
                    22,
                    1,
                    191,
                    0
                ],
                "title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators"
                },
                "summary": "Employing Large Language Models (LLMs) to assess the quality of generated\nresponses, such as prompting instruct-tuned models or fine-tuning judge models,\nhas become a widely adopted evaluation method. It is also known that such\nevaluators are vulnerable to biases, such as favoring longer responses. While\nit is important to overcome this problem, the specifics of these biases remain\nunder-explored. In this work, we qualitatively identify six types of biases\ninherent in various judge models. We propose EvalBiasBench as a meta-evaluation\ncollection of hand-crafted test cases for each bias type. Additionally, we\npresent de-biasing dataset construction methods and the associated preference\ndataset OffsetBias. Experimental results demonstrate that fine-tuning on our\ndataset significantly enhances the robustness of judge models against biases\nand improves performance across most evaluation scenarios. We release our\ndatasets and the fine-tuned judge model to public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Large Language Models (LLMs) to assess the quality of generated\nresponses, such as prompting instruct-tuned models or fine-tuning judge models,\nhas become a widely adopted evaluation method. It is also known that such\nevaluators are vulnerable to biases, such as favoring longer responses. While\nit is important to overcome this problem, the specifics of these biases remain\nunder-explored. In this work, we qualitatively identify six types of biases\ninherent in various judge models. We propose EvalBiasBench as a meta-evaluation\ncollection of hand-crafted test cases for each bias type. Additionally, we\npresent de-biasing dataset construction methods and the associated preference\ndataset OffsetBias. Experimental results demonstrate that fine-tuning on our\ndataset significantly enhances the robustness of judge models against biases\nand improves performance across most evaluation scenarios. We release our\ndatasets and the fine-tuned judge model to public."
                },
                "authors": [
                    {
                        "name": "Junsoo Park"
                    },
                    {
                        "name": "Seungyeon Jwa"
                    },
                    {
                        "name": "Meiying Ren"
                    },
                    {
                        "name": "Daeyoung Kim"
                    },
                    {
                        "name": "Sanghyuk Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyuk Choi"
                },
                "author": "Sanghyuk Choi",
                "arxiv_comment": "EMNLP2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08313v2",
                "updated": "2024-10-07T08:44:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    44,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-08-15T17:59:57Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    17,
                    59,
                    57,
                    3,
                    228,
                    0
                ],
                "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Symbolic Graphics Programs?"
                },
                "summary": "Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks."
                },
                "authors": [
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "Technical Report v2 (46 pages, 24 figures, project page:\n  https://sgp-bench.github.io/, substantial update from v1)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04834v1",
                "updated": "2024-10-07T08:44:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    44,
                    4,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T08:44:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    44,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Xin Mao"
                    },
                    {
                        "name": "Feng-Lin Li"
                    },
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Wang Chen"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02298v2",
                "updated": "2024-10-07T08:40:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    40,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-03T08:34:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    34,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models"
                },
                "summary": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yiting Dong"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07027v2",
                "updated": "2024-10-07T08:28:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    28,
                    43,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-11T14:57:42Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    14,
                    57,
                    42,
                    5,
                    132,
                    0
                ],
                "title": "TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural\n  Radiance Field Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural\n  Radiance Field Optimization"
                },
                "summary": "The reliance on accurate camera poses is a significant barrier to the\nwidespread deployment of Neural Radiance Fields (NeRF) models for 3D\nreconstruction and SLAM tasks. The existing method introduces monocular depth\npriors to jointly optimize the camera poses and NeRF, which fails to fully\nexploit the depth priors and neglects the impact of their inherent noise. In\nthis paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that\nenables training NeRF from unknown camera poses - by jointly optimizing\nlearnable parameters of the radiance field and camera poses. Our approach\nexplicitly utilizes monocular depth priors through three key advancements: 1)\nwe propose a novel depth-based ray sampling strategy based on the truncated\nnormal distribution, which improves the convergence speed and accuracy of pose\nestimation; 2) to circumvent local minima and refine depth geometry, we\nintroduce a coarse-to-fine training strategy that progressively improves the\ndepth precision; 3) we propose a more robust inter-frame point constraint that\nenhances robustness against depth noise during training. The experimental\nresults on three datasets demonstrate that TD-NeRF achieves superior\nperformance in the joint optimization of camera pose and NeRF, surpassing prior\nworks, and generates more accurate depth geometry. The implementation of our\nmethod has been released at https://github.com/nubot-nudt/TD-NeRF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reliance on accurate camera poses is a significant barrier to the\nwidespread deployment of Neural Radiance Fields (NeRF) models for 3D\nreconstruction and SLAM tasks. The existing method introduces monocular depth\npriors to jointly optimize the camera poses and NeRF, which fails to fully\nexploit the depth priors and neglects the impact of their inherent noise. In\nthis paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that\nenables training NeRF from unknown camera poses - by jointly optimizing\nlearnable parameters of the radiance field and camera poses. Our approach\nexplicitly utilizes monocular depth priors through three key advancements: 1)\nwe propose a novel depth-based ray sampling strategy based on the truncated\nnormal distribution, which improves the convergence speed and accuracy of pose\nestimation; 2) to circumvent local minima and refine depth geometry, we\nintroduce a coarse-to-fine training strategy that progressively improves the\ndepth precision; 3) we propose a more robust inter-frame point constraint that\nenhances robustness against depth noise during training. The experimental\nresults on three datasets demonstrate that TD-NeRF achieves superior\nperformance in the joint optimization of camera pose and NeRF, surpassing prior\nworks, and generates more accurate depth geometry. The implementation of our\nmethod has been released at https://github.com/nubot-nudt/TD-NeRF."
                },
                "authors": [
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Zongtan Zhou"
                    },
                    {
                        "name": "Yangbing Ge"
                    },
                    {
                        "name": "Zi Wang"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    },
                    {
                        "name": "Dewen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Dewen Hu"
                },
                "author": "Dewen Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13753v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13753v3",
                "updated": "2024-10-07T08:20:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    20,
                    55,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-22T15:38:30Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    15,
                    38,
                    30,
                    2,
                    143,
                    0
                ],
                "title": "A Dynamic Model of Performative Human-ML Collaboration: Theory and\n  Empirical Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dynamic Model of Performative Human-ML Collaboration: Theory and\n  Empirical Evidence"
                },
                "summary": "Machine learning (ML) models are increasingly used in various applications,\nfrom recommendation systems in e-commerce to diagnosis prediction in\nhealthcare. In this paper, we present a novel dynamic framework for thinking\nabout the deployment of ML models in a performative, human-ML collaborative\nsystem. In our framework, the introduction of ML recommendations changes the\ndata-generating process of human decisions, which are only a proxy to the\nground truth and which are then used to train future versions of the model. We\nshow that this dynamic process in principle can converge to different stable\npoints, i.e. where the ML model and the Human+ML system have the same\nperformance. Some of these stable points are suboptimal with respect to the\nactual ground truth. As a proof of concept, we conduct an empirical user study\nwith 1,408 participants. In the study, humans solve instances of the knapsack\nproblem with the help of machine learning predictions of varying performance.\nThis is an ideal setting because we can identify the actual ground truth, and\nevaluate the performance of human decisions supported by ML recommendations. We\nfind that for many levels of ML performance, humans can improve upon the ML\npredictions. We also find that the improvement could be even higher if humans\nrationally followed the ML recommendations. Finally, we test whether monetary\nincentives can increase the quality of human decisions, but we fail to find any\npositive effect. Using our empirical data to approximate our collaborative\nsystem suggests that the learning process would dynamically reach an\nequilibrium performance that is around 92% of the maximum knapsack value. Our\nresults have practical implications for the deployment of ML models in contexts\nwhere human decisions may deviate from the indisputable ground truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are increasingly used in various applications,\nfrom recommendation systems in e-commerce to diagnosis prediction in\nhealthcare. In this paper, we present a novel dynamic framework for thinking\nabout the deployment of ML models in a performative, human-ML collaborative\nsystem. In our framework, the introduction of ML recommendations changes the\ndata-generating process of human decisions, which are only a proxy to the\nground truth and which are then used to train future versions of the model. We\nshow that this dynamic process in principle can converge to different stable\npoints, i.e. where the ML model and the Human+ML system have the same\nperformance. Some of these stable points are suboptimal with respect to the\nactual ground truth. As a proof of concept, we conduct an empirical user study\nwith 1,408 participants. In the study, humans solve instances of the knapsack\nproblem with the help of machine learning predictions of varying performance.\nThis is an ideal setting because we can identify the actual ground truth, and\nevaluate the performance of human decisions supported by ML recommendations. We\nfind that for many levels of ML performance, humans can improve upon the ML\npredictions. We also find that the improvement could be even higher if humans\nrationally followed the ML recommendations. Finally, we test whether monetary\nincentives can increase the quality of human decisions, but we fail to find any\npositive effect. Using our empirical data to approximate our collaborative\nsystem suggests that the learning process would dynamically reach an\nequilibrium performance that is around 92% of the maximum knapsack value. Our\nresults have practical implications for the deployment of ML models in contexts\nwhere human decisions may deviate from the indisputable ground truth."
                },
                "authors": [
                    {
                        "name": "Tom Sühr"
                    },
                    {
                        "name": "Samira Samadi"
                    },
                    {
                        "name": "Chiara Farronato"
                    }
                ],
                "author_detail": {
                    "name": "Chiara Farronato"
                },
                "author": "Chiara Farronato",
                "arxiv_comment": "10 Pages and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13753v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13753v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.6; K.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04795v1",
                "updated": "2024-10-07T07:14:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    7,
                    14,
                    37,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T07:14:37Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    7,
                    14,
                    37,
                    0,
                    281,
                    0
                ],
                "title": "Representing the Under-Represented: Cultural and Core Capability\n  Benchmarks for Developing Thai Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing the Under-Represented: Cultural and Core Capability\n  Benchmarks for Developing Thai Large Language Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has highlighted the\nneed for robust evaluation frameworks that assess their core capabilities, such\nas reasoning, knowledge, and commonsense, leading to the inception of certain\nwidely-used benchmark suites such as the H6 benchmark. However, these benchmark\nsuites are primarily built for the English language, and there exists a lack\nthereof for under-represented languages, in terms of LLM development, such as\nThai. On the other hand, developing LLMs for Thai should also include enhancing\nthe cultural understanding as well as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai\nCultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough\nevaluation of various LLMs with multi-lingual capabilities, we provide a\ncomprehensive analysis of the proposed benchmarks and how they contribute to\nThai LLM development. Furthermore, we will make both the datasets and\nevaluation code publicly available to encourage further research and\ndevelopment for Thai LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has highlighted the\nneed for robust evaluation frameworks that assess their core capabilities, such\nas reasoning, knowledge, and commonsense, leading to the inception of certain\nwidely-used benchmark suites such as the H6 benchmark. However, these benchmark\nsuites are primarily built for the English language, and there exists a lack\nthereof for under-represented languages, in terms of LLM development, such as\nThai. On the other hand, developing LLMs for Thai should also include enhancing\nthe cultural understanding as well as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai\nCultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough\nevaluation of various LLMs with multi-lingual capabilities, we provide a\ncomprehensive analysis of the proposed benchmarks and how they contribute to\nThai LLM development. Furthermore, we will make both the datasets and\nevaluation code publicly available to encourage further research and\ndevelopment for Thai LLMs."
                },
                "authors": [
                    {
                        "name": "Dahyun Kim"
                    },
                    {
                        "name": "Sukyung Lee"
                    },
                    {
                        "name": "Yungi Kim"
                    },
                    {
                        "name": "Attapol Rutherford"
                    },
                    {
                        "name": "Chanjun Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanjun Park"
                },
                "author": "Chanjun Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08617v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08617v4",
                "updated": "2024-10-07T07:04:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    7,
                    4,
                    1,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-14T02:42:15Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    2,
                    42,
                    15,
                    3,
                    348,
                    0
                ],
                "title": "RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code\n  Generation Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code\n  Generation Technique"
                },
                "summary": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source. Furthermore, we have successfully\nquantized our LLM to 4-bit with a total size of 4GB, enabling it to function on\na single laptop with only slight performance degradation. This efficiency\nallows the RTL generator to serve as a local assistant for engineers, ensuring\nall design privacy concerns are addressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of RTL code (e.g., Verilog) using natural language\ninstructions and large language models (LLMs) has attracted significant\nresearch interest recently. However, most existing approaches heavily rely on\ncommercial LLMs such as ChatGPT, while open-source LLMs tailored for this\nspecific design generation task exhibit notably inferior performance. The\nabsence of high-quality open-source solutions restricts the flexibility and\ndata privacy of this emerging technique. In this study, we present a new\ncustomized LLM solution with a modest parameter count of only 7B, achieving\nbetter performance than GPT-3.5 on all representative benchmarks for RTL code\ngeneration. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.\nThis remarkable balance between accuracy and efficiency is made possible by\nleveraging our new RTL code dataset and a customized LLM algorithm, both of\nwhich have been made fully open-source. Furthermore, we have successfully\nquantized our LLM to 4-bit with a total size of 4GB, enabling it to function on\na single laptop with only slight performance degradation. This efficiency\nallows the RTL generator to serve as a local assistant for engineers, ensuring\nall design privacy concerns are addressed."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Hongce Zhang"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "Accepted by IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08617v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08617v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04790v1",
                "updated": "2024-10-07T07:02:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    7,
                    2,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T07:02:09Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    7,
                    2,
                    9,
                    0,
                    281,
                    0
                ],
                "title": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted\n  Graph for Long Document QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted\n  Graph for Long Document QA"
                },
                "summary": "In the past, Retrieval-Augmented Generation (RAG) methods split text into\nchunks to enable language models to handle long documents. Recent tree-based\nRAG methods are able to retrieve detailed information while preserving global\ncontext. However, with the advent of more powerful LLMs, such as Llama 3.1,\nwhich offer better comprehension and support for longer inputs, we found that\neven recent tree-based RAG methods perform worse than directly feeding the\nentire document into Llama 3.1, although RAG methods still hold an advantage in\nreducing computational costs. In this paper, we propose a new retrieval method,\ncalled LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph\n(GARLIC), which outperforms previous state-of-the-art baselines, including\nLlama 3.1, while retaining the computational efficiency of RAG methods. Our\nmethod introduces several improvements: (1) Rather than using a tree structure,\nwe construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many\nsummarization, where the graph edges are derived from attention mechanisms, and\neach node focuses on a single event or very few events. (2) We introduce a\nnovel retrieval method that leverages the attention weights of LLMs rather than\ndense embedding similarity. Our method allows for searching the graph along\nmultiple paths and can terminate at any depth. (3) We use the LLM to control\nthe retrieval process, enabling it to dynamically adjust the amount and depth\nof information retrieved for different queries. Experimental results show that\nour method outperforms previous state-of-the-art baselines, including Llama\n3.1, on two single-document and two multi-document QA datasets, while\nmaintaining similar computational complexity to traditional RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past, Retrieval-Augmented Generation (RAG) methods split text into\nchunks to enable language models to handle long documents. Recent tree-based\nRAG methods are able to retrieve detailed information while preserving global\ncontext. However, with the advent of more powerful LLMs, such as Llama 3.1,\nwhich offer better comprehension and support for longer inputs, we found that\neven recent tree-based RAG methods perform worse than directly feeding the\nentire document into Llama 3.1, although RAG methods still hold an advantage in\nreducing computational costs. In this paper, we propose a new retrieval method,\ncalled LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph\n(GARLIC), which outperforms previous state-of-the-art baselines, including\nLlama 3.1, while retaining the computational efficiency of RAG methods. Our\nmethod introduces several improvements: (1) Rather than using a tree structure,\nwe construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many\nsummarization, where the graph edges are derived from attention mechanisms, and\neach node focuses on a single event or very few events. (2) We introduce a\nnovel retrieval method that leverages the attention weights of LLMs rather than\ndense embedding similarity. Our method allows for searching the graph along\nmultiple paths and can terminate at any depth. (3) We use the LLM to control\nthe retrieval process, enabling it to dynamically adjust the amount and depth\nof information retrieved for different queries. Experimental results show that\nour method outperforms previous state-of-the-art baselines, including Llama\n3.1, on two single-document and two multi-document QA datasets, while\nmaintaining similar computational complexity to traditional RAG methods."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yanzheng Xiang"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07284v4",
                "updated": "2024-10-07T06:54:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    54,
                    30,
                    0,
                    281,
                    0
                ],
                "published": "2023-10-11T08:17:54Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    8,
                    17,
                    54,
                    2,
                    284,
                    0
                ],
                "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction"
                },
                "summary": "Humans can easily isolate a single speaker from a complex acoustic\nenvironment, a capability referred to as the \"Cocktail Party Effect.\" However,\nreplicating this ability has been a significant challenge in the field of\ntarget speaker extraction (TSE). Traditional TSE approaches predominantly rely\non voiceprints, which raise privacy concerns and face issues related to the\nquality and availability of enrollment samples, as well as intra-speaker\nvariability. To address these issues, this work introduces a novel text-guided\nTSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language\nmodel, LLaMA 2, processes typed text input from users to extract semantic cues.\nWe demonstrate that textual descriptions alone can effectively serve as cues\nfor extraction, thus addressing privacy concerns and reducing dependency on\nvoiceprints. Furthermore, our approach offers flexibility by allowing the user\nto specify the extraction or suppression of a speaker and enhances robustness\nagainst intra-speaker variability by incorporating context-dependent textual\ninformation. Experimental results show competitive performance with text-based\ncues alone and demonstrate the effectiveness of using text as a task selector.\nAdditionally, they achieve a new state-of-the-art when combining text-based\ncues with pre-registered cues. This work represents the first integration of\nLLMs with TSE, potentially establishing a new benchmark in solving the cocktail\nparty problem and expanding the scope of TSE applications by providing a\nversatile, privacy-conscious solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can easily isolate a single speaker from a complex acoustic\nenvironment, a capability referred to as the \"Cocktail Party Effect.\" However,\nreplicating this ability has been a significant challenge in the field of\ntarget speaker extraction (TSE). Traditional TSE approaches predominantly rely\non voiceprints, which raise privacy concerns and face issues related to the\nquality and availability of enrollment samples, as well as intra-speaker\nvariability. To address these issues, this work introduces a novel text-guided\nTSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language\nmodel, LLaMA 2, processes typed text input from users to extract semantic cues.\nWe demonstrate that textual descriptions alone can effectively serve as cues\nfor extraction, thus addressing privacy concerns and reducing dependency on\nvoiceprints. Furthermore, our approach offers flexibility by allowing the user\nto specify the extraction or suppression of a speaker and enhances robustness\nagainst intra-speaker variability by incorporating context-dependent textual\ninformation. Experimental results show competitive performance with text-based\ncues alone and demonstrate the effectiveness of using text as a task selector.\nAdditionally, they achieve a new state-of-the-art when combining text-based\ncues with pre-registered cues. This work represents the first integration of\nLLMs with TSE, potentially establishing a new benchmark in solving the cocktail\nparty problem and expanding the scope of TSE applications by providing a\nversatile, privacy-conscious solution."
                },
                "authors": [
                    {
                        "name": "Xiang Hao"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Jianwei Yu"
                    },
                    {
                        "name": "Chenglin Xu"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "arxiv_comment": "Under review, https://github.com/haoxiangsnr/llm-tse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04784v1",
                "updated": "2024-10-07T06:49:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    49,
                    41,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T06:49:41Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    49,
                    41,
                    0,
                    281,
                    0
                ],
                "title": "Formality is Favored: Unraveling the Learning Preferences of Large\n  Language Models on Data with Conflicting Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formality is Favored: Unraveling the Learning Preferences of Large\n  Language Models on Data with Conflicting Knowledge"
                },
                "summary": "Having been trained on massive pretraining data, large language models have\nshown excellent performance on many knowledge-intensive tasks. However,\npretraining data tends to contain misleading and even conflicting information,\nand it is intriguing to understand how LLMs handle these noisy data during\ntraining. In this study, we systematically analyze LLMs' learning preferences\nfor data with conflicting knowledge. We find that pretrained LLMs establish\nlearning preferences similar to humans, i.e., preferences towards formal texts\nand texts with fewer spelling errors, resulting in faster learning and more\nfavorable treatment of knowledge in data with such features when facing\nconflicts. This finding is generalizable across models and languages and is\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\ntrust data with features that signify consistency with the majority of data,\nand it is possible to instill new preferences and erase old ones by\nmanipulating the degree of consistency with the majority data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Having been trained on massive pretraining data, large language models have\nshown excellent performance on many knowledge-intensive tasks. However,\npretraining data tends to contain misleading and even conflicting information,\nand it is intriguing to understand how LLMs handle these noisy data during\ntraining. In this study, we systematically analyze LLMs' learning preferences\nfor data with conflicting knowledge. We find that pretrained LLMs establish\nlearning preferences similar to humans, i.e., preferences towards formal texts\nand texts with fewer spelling errors, resulting in faster learning and more\nfavorable treatment of knowledge in data with such features when facing\nconflicts. This finding is generalizable across models and languages and is\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\ntrust data with features that signify consistency with the majority of data,\nand it is possible to instill new preferences and erase old ones by\nmanipulating the degree of consistency with the majority data."
                },
                "authors": [
                    {
                        "name": "Jiahuan Li"
                    },
                    {
                        "name": "Yiqing Cao"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Jiajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Chen"
                },
                "author": "Jiajun Chen",
                "arxiv_comment": "accepted by EMNLP 2024, main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04780v1",
                "updated": "2024-10-07T06:45:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    45,
                    22,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T06:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    45,
                    22,
                    0,
                    281,
                    0
                ],
                "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large\n  Language Models via Deciphering Attention Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large\n  Language Models via Deciphering Attention Causality"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in\nboth industry and academia, but often suffer from biases introduced by visual\nand language priors, which can lead to multimodal hallucination. These biases\narise from the visual encoder and the Large Language Model (LLM) backbone,\naffecting the attention mechanism responsible for aligning multimodal inputs.\nExisting decoding-based mitigation methods focus on statistical correlations\nand overlook the causal relationships between attention mechanisms and model\noutput, limiting their effectiveness in addressing these biases. To tackle this\nissue, we propose a causal inference framework termed CausalMM that applies\nstructural causal modeling to MLLMs, treating modality priors as a confounder\nbetween attention mechanisms and output. Specifically, by employing backdoor\nadjustment and counterfactual reasoning at both the visual and language\nattention levels, our method mitigates the negative effects of modality priors\nand enhances the alignment of MLLM's inputs and outputs, with a maximum score\nimprovement of 65.3% on 6 VLind-Bench indicators and 164 points on MME\nBenchmark compared to conventional methods. Extensive experiments validate the\neffectiveness of our approach while being a plug-and-play solution. Our code is\navailable at: https://github.com/The-Martyr/CausalMM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in\nboth industry and academia, but often suffer from biases introduced by visual\nand language priors, which can lead to multimodal hallucination. These biases\narise from the visual encoder and the Large Language Model (LLM) backbone,\naffecting the attention mechanism responsible for aligning multimodal inputs.\nExisting decoding-based mitigation methods focus on statistical correlations\nand overlook the causal relationships between attention mechanisms and model\noutput, limiting their effectiveness in addressing these biases. To tackle this\nissue, we propose a causal inference framework termed CausalMM that applies\nstructural causal modeling to MLLMs, treating modality priors as a confounder\nbetween attention mechanisms and output. Specifically, by employing backdoor\nadjustment and counterfactual reasoning at both the visual and language\nattention levels, our method mitigates the negative effects of modality priors\nand enhances the alignment of MLLM's inputs and outputs, with a maximum score\nimprovement of 65.3% on 6 VLind-Bench indicators and 164 points on MME\nBenchmark compared to conventional methods. Extensive experiments validate the\neffectiveness of our approach while being a plug-and-play solution. Our code is\navailable at: https://github.com/The-Martyr/CausalMM"
                },
                "authors": [
                    {
                        "name": "Guanyu Zhou"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04778v1",
                "updated": "2024-10-07T06:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    36,
                    55,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T06:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    36,
                    55,
                    0,
                    281,
                    0
                ],
                "title": "MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models\n  (MLLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models\n  (MLLMs)"
                },
                "summary": "With the advent of Large Language Models (LLMs) and Multimodal\n(Visio-lingual) LLMs, a flurry of research has emerged, analyzing the\nperformance of such models across a diverse array of tasks. While most studies\nfocus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models\nthrough task accuracy (e.g., Visual Question Answering, grounding) across\nvarious datasets, our work explores the related but complementary aspect of\nconsistency - the ability of an MLLM model to produce semantically similar or\nidentical responses to semantically similar queries. We note that consistency\nis a fundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in MLLMs. Humans, in particular, are known to be highly\nconsistent (even if not always accurate) in their responses, and consistency is\ninherently expected from AI systems. Armed with this perspective, we propose\nthe MM-R$^3$ benchmark, which analyses the performance in terms of consistency\nand accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image\nRestyling, and Context Reasoning. Our analysis reveals that consistency does\nnot always align with accuracy, indicating that models with higher accuracy are\nnot necessarily more consistent, and vice versa. Furthermore, we propose a\nsimple yet effective mitigation strategy in the form of an adapter module\ntrained to minimize inconsistency across prompts. With our proposed strategy,\nwe are able to achieve absolute improvements of 5.7% and 12.5%, on average on\nwidely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over\ntheir existing counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs) and Multimodal\n(Visio-lingual) LLMs, a flurry of research has emerged, analyzing the\nperformance of such models across a diverse array of tasks. While most studies\nfocus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models\nthrough task accuracy (e.g., Visual Question Answering, grounding) across\nvarious datasets, our work explores the related but complementary aspect of\nconsistency - the ability of an MLLM model to produce semantically similar or\nidentical responses to semantically similar queries. We note that consistency\nis a fundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in MLLMs. Humans, in particular, are known to be highly\nconsistent (even if not always accurate) in their responses, and consistency is\ninherently expected from AI systems. Armed with this perspective, we propose\nthe MM-R$^3$ benchmark, which analyses the performance in terms of consistency\nand accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image\nRestyling, and Context Reasoning. Our analysis reveals that consistency does\nnot always align with accuracy, indicating that models with higher accuracy are\nnot necessarily more consistent, and vice versa. Furthermore, we propose a\nsimple yet effective mitigation strategy in the form of an adapter module\ntrained to minimize inconsistency across prompts. With our proposed strategy,\nwe are able to achieve absolute improvements of 5.7% and 12.5%, on average on\nwidely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over\ntheir existing counterparts."
                },
                "authors": [
                    {
                        "name": "Shih-Han Chou"
                    },
                    {
                        "name": "Shivam Chandhok"
                    },
                    {
                        "name": "James J. Little"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17642v2",
                "updated": "2024-10-07T06:29:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    29,
                    54,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-26T08:45:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    45,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure"
                },
                "summary": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions."
                },
                "authors": [
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Hangxin Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09592v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09592v4",
                "updated": "2024-10-07T06:20:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    20,
                    17,
                    0,
                    281,
                    0
                ],
                "published": "2023-11-16T06:05:01Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    6,
                    5,
                    1,
                    3,
                    320,
                    0
                ],
                "title": "Scalable and Adaptively Secure Any-Trust Distributed Key Generation and\n  All-hands Checkpointing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Adaptively Secure Any-Trust Distributed Key Generation and\n  All-hands Checkpointing"
                },
                "summary": "The classical distributed key generation protocols (DKG) are resurging due to\ntheir widespread applications in blockchain. While efforts have been made to\nimprove DKG communication, practical large-scale deployments are still yet to\ncome due to various challenges, including the heavy computation and\ncommunication (particularly broadcast) overhead in their adversarial cases. In\nthis paper, we propose a practical DKG for DLog-based cryptosystems, which\nachieves (quasi-)linear computation and communication per-node cost with the\nhelp of a common coin, even in the face of the maximal amount of Byzantine\nnodes. Moreover, our protocol is secure against adaptive adversaries, which can\ncorrupt less than half of all nodes. The key to our improvements lies in\ndelegating the most costly operations to an Any-Trust group together with a set\nof techniques for adaptive security. This group is randomly sampled and\nconsists of a small number of individuals. The population only trusts that at\nleast one member in the group is honest, without knowing which one. Moreover,\nwe present a generic transformer that enables us to efficiently deploy a\nconventional distributed protocol like our DKG, even when the participants have\ndifferent weights. Additionally, we introduce an extended broadcast channel\nbased on a blockchain and data dispersal network (such as IPFS), enabling\nreliable broadcasting of arbitrary-size messages at the cost of constant-size\nblockchain storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classical distributed key generation protocols (DKG) are resurging due to\ntheir widespread applications in blockchain. While efforts have been made to\nimprove DKG communication, practical large-scale deployments are still yet to\ncome due to various challenges, including the heavy computation and\ncommunication (particularly broadcast) overhead in their adversarial cases. In\nthis paper, we propose a practical DKG for DLog-based cryptosystems, which\nachieves (quasi-)linear computation and communication per-node cost with the\nhelp of a common coin, even in the face of the maximal amount of Byzantine\nnodes. Moreover, our protocol is secure against adaptive adversaries, which can\ncorrupt less than half of all nodes. The key to our improvements lies in\ndelegating the most costly operations to an Any-Trust group together with a set\nof techniques for adaptive security. This group is randomly sampled and\nconsists of a small number of individuals. The population only trusts that at\nleast one member in the group is honest, without knowing which one. Moreover,\nwe present a generic transformer that enables us to efficiently deploy a\nconventional distributed protocol like our DKG, even when the participants have\ndifferent weights. Additionally, we introduce an extended broadcast channel\nbased on a blockchain and data dispersal network (such as IPFS), enabling\nreliable broadcasting of arbitrary-size messages at the cost of constant-size\nblockchain storage."
                },
                "authors": [
                    {
                        "name": "Hanwen Feng"
                    },
                    {
                        "name": "Tiancheng Mai"
                    },
                    {
                        "name": "Qiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Tang"
                },
                "author": "Qiang Tang",
                "arxiv_doi": "10.1145/3658644.3690253",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690253",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.09592v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09592v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04772v1",
                "updated": "2024-10-07T06:15:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    15,
                    46,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T06:15:46Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    15,
                    46,
                    0,
                    281,
                    0
                ],
                "title": "From Transparency to Accountability and Back: A Discussion of Access and\n  Evidence in AI Auditing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Transparency to Accountability and Back: A Discussion of Access and\n  Evidence in AI Auditing"
                },
                "summary": "Artificial intelligence (AI) is increasingly intervening in our lives,\nraising widespread concern about its unintended and undeclared side effects.\nThese developments have brought attention to the problem of AI auditing: the\nsystematic evaluation and analysis of an AI system, its development, and its\nbehavior relative to a set of predetermined criteria. Auditing can take many\nforms, including pre-deployment risk assessments, ongoing monitoring, and\ncompliance testing. It plays a critical role in providing assurances to various\nAI stakeholders, from developers to end users. Audits may, for instance, be\nused to verify that an algorithm complies with the law, is consistent with\nindustry standards, and meets the developer's claimed specifications. However,\nthere are many operational challenges to AI auditing that complicate its\nimplementation.\n  In this work, we examine a key operational issue in AI auditing: what type of\naccess to an AI system is needed to perform a meaningful audit? Addressing this\nquestion has direct policy relevance, as it can inform AI audit guidelines and\nrequirements. We begin by discussing the factors that auditors balance when\ndetermining the appropriate type of access, and unpack the benefits and\ndrawbacks of four types of access. We conclude that, at minimum, black-box\naccess -- providing query access to a model without exposing its internal\nimplementation -- should be granted to auditors, as it balances concerns\nrelated to trade secrets, data privacy, audit standardization, and audit\nefficiency. We then suggest a framework for determining how much further access\n(in addition to black-box access) to grant auditors. We show that auditing can\nbe cast as a natural hypothesis test, draw parallels hypothesis testing and\nlegal procedure, and argue that this framing provides clear and interpretable\nguidance on audit implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is increasingly intervening in our lives,\nraising widespread concern about its unintended and undeclared side effects.\nThese developments have brought attention to the problem of AI auditing: the\nsystematic evaluation and analysis of an AI system, its development, and its\nbehavior relative to a set of predetermined criteria. Auditing can take many\nforms, including pre-deployment risk assessments, ongoing monitoring, and\ncompliance testing. It plays a critical role in providing assurances to various\nAI stakeholders, from developers to end users. Audits may, for instance, be\nused to verify that an algorithm complies with the law, is consistent with\nindustry standards, and meets the developer's claimed specifications. However,\nthere are many operational challenges to AI auditing that complicate its\nimplementation.\n  In this work, we examine a key operational issue in AI auditing: what type of\naccess to an AI system is needed to perform a meaningful audit? Addressing this\nquestion has direct policy relevance, as it can inform AI audit guidelines and\nrequirements. We begin by discussing the factors that auditors balance when\ndetermining the appropriate type of access, and unpack the benefits and\ndrawbacks of four types of access. We conclude that, at minimum, black-box\naccess -- providing query access to a model without exposing its internal\nimplementation -- should be granted to auditors, as it balances concerns\nrelated to trade secrets, data privacy, audit standardization, and audit\nefficiency. We then suggest a framework for determining how much further access\n(in addition to black-box access) to grant auditors. We show that auditing can\nbe cast as a natural hypothesis test, draw parallels hypothesis testing and\nlegal procedure, and argue that this framing provides clear and interpretable\nguidance on audit implementation."
                },
                "authors": [
                    {
                        "name": "Sarah H. Cen"
                    },
                    {
                        "name": "Rohan Alur"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Alur"
                },
                "author": "Rohan Alur",
                "arxiv_comment": "23 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01084v2",
                "updated": "2024-10-07T06:11:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    11,
                    46,
                    0,
                    281,
                    0
                ],
                "published": "2024-08-02T08:03:38Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    8,
                    3,
                    38,
                    4,
                    215,
                    0
                ],
                "title": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for\n  Handling Noisy Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for\n  Handling Noisy Contexts"
                },
                "summary": "When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge the gap between\nexternal knowledge and the LLMs' parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLMs\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge the gap between\nexternal knowledge and the LLMs' parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLMs\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation."
                },
                "authors": [
                    {
                        "name": "Youna Kim"
                    },
                    {
                        "name": "Hyuhng Joon Kim"
                    },
                    {
                        "name": "Cheonbok Park"
                    },
                    {
                        "name": "Choonghyun Park"
                    },
                    {
                        "name": "Hyunsoo Cho"
                    },
                    {
                        "name": "Junyeob Kim"
                    },
                    {
                        "name": "Kang Min Yoo"
                    },
                    {
                        "name": "Sang-goo Lee"
                    },
                    {
                        "name": "Taeuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taeuk Kim"
                },
                "author": "Taeuk Kim",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04759v1",
                "updated": "2024-10-07T05:27:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    27,
                    22,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T05:27:22Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    27,
                    22,
                    0,
                    281,
                    0
                ],
                "title": "Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM"
                },
                "summary": "This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease."
                },
                "authors": [
                    {
                        "name": "Tianhui Cai"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Zewei Zhou"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Seth Z. Zhao"
                    },
                    {
                        "name": "Zhiwen Wu"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17467v2",
                "updated": "2024-10-07T05:16:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    16,
                    25,
                    0,
                    281,
                    0
                ],
                "published": "2024-07-24T17:59:02Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    17,
                    59,
                    2,
                    2,
                    206,
                    0
                ],
                "title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models"
                },
                "summary": "Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Considering the balance between efficiency and\neffectiveness, CMR can be regarded as the optimal mixture ratio. Through\nextensive experiments, we ascertain the predictability of CMR, propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Considering the balance between efficiency and\neffectiveness, CMR can be regarded as the optimal mixture ratio. Through\nextensive experiments, we ascertain the predictability of CMR, propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources."
                },
                "authors": [
                    {
                        "name": "Jiawei Gu"
                    },
                    {
                        "name": "Zacc Yang"
                    },
                    {
                        "name": "Chuanghao Ding"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "arxiv_comment": "EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04753v1",
                "updated": "2024-10-07T05:14:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    14,
                    18,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T05:14:18Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    14,
                    18,
                    0,
                    281,
                    0
                ],
                "title": "ImProver: Agent-Based Automated Proof Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImProver: Agent-Based Automated Proof Optimization"
                },
                "summary": "Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable."
                },
                "authors": [
                    {
                        "name": "Riyaz Ahuja"
                    },
                    {
                        "name": "Jeremy Avigad"
                    },
                    {
                        "name": "Prasad Tetali"
                    },
                    {
                        "name": "Sean Welleck"
                    }
                ],
                "author_detail": {
                    "name": "Sean Welleck"
                },
                "author": "Sean Welleck",
                "arxiv_comment": "19 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]