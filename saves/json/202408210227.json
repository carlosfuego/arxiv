[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v2",
                "updated": "2024-08-19T03:18:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    18,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "HERA: High-efficiency Matrix Compression via Element Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERA: High-efficiency Matrix Compression via Element Replacement"
                },
                "summary": "Matrix quantization involves encoding matrix elements in a more\nspace-efficient manner to minimize storage requirements, with dequantization\nused to reconstruct the original matrix for practical use. We define the\nQuantization Error Minimization (QEM) problem as minimizing the difference\nbetween a matrix before and after quantization while ensuring that the\nquantized matrix occupies the same amount of memory. Matrix quantization is\nessential in various fields, including weight quantization in Large Language\nModels (LLMs), vector databases, KV cache quantization, graph compression, and\nimage compression. The growing scale of LLMs, such as GPT-4 and BERT,\nunderscores the need for matrix compression due to the large size of parameters\nand KV caches, which are stored as matrices.\n  To address the QEM problem, we introduce HETA, an algorithm that leverages\nthe local orderliness of matrix elements by iteratively swapping elements to\ncreate a locally ordered matrix. This matrix is then grouped and quantized by\ncolumns. To further improve HETA, we present two optimizations: additional\nquantization of residuals to reduce mean squared error (MSE) and the\napplication of masking and batch processing to accelerate the algorithm.\n  Our experiments show that HETA effectively reduces MSE to 12.3% of its\noriginal value at the same compression ratio, outperforming leading baseline\nalgorithms. Our contributions include formalizing the QEM problem, developing\nthe HETA algorithm, and proposing two optimizations to enhance both accuracy\nand processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization involves encoding matrix elements in a more\nspace-efficient manner to minimize storage requirements, with dequantization\nused to reconstruct the original matrix for practical use. We define the\nQuantization Error Minimization (QEM) problem as minimizing the difference\nbetween a matrix before and after quantization while ensuring that the\nquantized matrix occupies the same amount of memory. Matrix quantization is\nessential in various fields, including weight quantization in Large Language\nModels (LLMs), vector databases, KV cache quantization, graph compression, and\nimage compression. The growing scale of LLMs, such as GPT-4 and BERT,\nunderscores the need for matrix compression due to the large size of parameters\nand KV caches, which are stored as matrices.\n  To address the QEM problem, we introduce HETA, an algorithm that leverages\nthe local orderliness of matrix elements by iteratively swapping elements to\ncreate a locally ordered matrix. This matrix is then grouped and quantized by\ncolumns. To further improve HETA, we present two optimizations: additional\nquantization of residuals to reduce mean squared error (MSE) and the\napplication of masking and batch processing to accelerate the algorithm.\n  Our experiments show that HETA effectively reduces MSE to 12.3% of its\noriginal value at the same compression ratio, outperforming leading baseline\nalgorithms. Our contributions include formalizing the QEM problem, developing\nthe HETA algorithm, and proposing two optimizations to enhance both accuracy\nand processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v1",
                "updated": "2024-08-16T15:11:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.10197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10197v1",
                "updated": "2024-08-19T17:54:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    54,
                    29,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:54:29Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    54,
                    29,
                    0,
                    232,
                    0
                ],
                "title": "Demystifying the Communication Characteristics for Distributed\n  Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Communication Characteristics for Distributed\n  Transformer Models"
                },
                "summary": "Deep learning (DL) models based on the transformer architecture have\nrevolutionized many DL applications such as large language models (LLMs),\nvision transformers, audio generation, and time series prediction. Much of this\nprogress has been fueled by distributed training, yet distributed communication\nremains a substantial bottleneck to training progress. This paper examines the\ncommunication behavior of transformer models - that is, how different\nparallelism schemes used in multi-node/multi-GPU DL Training communicate data\nin the context of transformers. We use GPT-based language models as a case\nstudy of the transformer architecture due to their ubiquity. We validate the\nempirical results obtained from our communication logs using analytical models.\nAt a high level, our analysis reveals a need to optimize small message\npoint-to-point communication further, correlations between sequence length,\nper-GPU throughput, model size, and optimizations used, and where to\npotentially guide further optimizations in framework and HPC middleware design\nand optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) models based on the transformer architecture have\nrevolutionized many DL applications such as large language models (LLMs),\nvision transformers, audio generation, and time series prediction. Much of this\nprogress has been fueled by distributed training, yet distributed communication\nremains a substantial bottleneck to training progress. This paper examines the\ncommunication behavior of transformer models - that is, how different\nparallelism schemes used in multi-node/multi-GPU DL Training communicate data\nin the context of transformers. We use GPT-based language models as a case\nstudy of the transformer architecture due to their ubiquity. We validate the\nempirical results obtained from our communication logs using analytical models.\nAt a high level, our analysis reveals a need to optimize small message\npoint-to-point communication further, correlations between sequence length,\nper-GPU throughput, model size, and optimizations used, and where to\npotentially guide further optimizations in framework and HPC middleware design\nand optimization."
                },
                "authors": [
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Benjamin Michalowicz"
                    },
                    {
                        "name": "Jacob Hatef"
                    },
                    {
                        "name": "Lang Xu"
                    },
                    {
                        "name": "Mustafa Abduljabbar"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar Panda"
                },
                "author": "Dhabaleswar Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10189v1",
                "updated": "2024-08-19T17:48:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    48,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:48:11Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    48,
                    11,
                    0,
                    232,
                    0
                ],
                "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic\n  Models"
                },
                "summary": "Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models."
                },
                "authors": [
                    {
                        "name": "Aviv Bick"
                    },
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Albert Gu"
                    }
                ],
                "author_detail": {
                    "name": "Albert Gu"
                },
                "author": "Albert Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10188v2",
                "updated": "2024-08-20T17:56:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    56,
                    24,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T17:48:08Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    48,
                    8,
                    0,
                    232,
                    0
                ],
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos"
                },
                "summary": "Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first long-context Multi-Modal Sequence\nParallelism (MM-SP) system that enables long training and inference, enabling\n2M context length training on 256 GPUs without any gradient checkpointing.\nMM-SP is 2.1x - 5.7x faster than ring sequence parallelism and 1.1x - 1.4x\nfaster than Megatron context parallelism + tensor parallelism in text-only\nsettings. Moreover, it seamlessly integrates with Hugging Face Transformers.\nFor model training, we propose a five-stage pipeline comprising alignment,\npre-training, short supervised fine-tuning, context extension, and long\nsupervised fine-tuning. On datasets, we construct large-scale visual language\npre-training datasets and long video instruction-following datasets to support\nour multi-stage training process. LongVILA extends the number of frames of VILA\nfrom 8 to 1024, and improves the long video captioning score from 2.00 to 3.26\n(1.6x), achieving 99.5% accuracy in 1400-frames video (274k context length)\nneedle-in-a-haystack. LongVILA-8B demonstrates consistent accuracy improvements\non long videos in the VideoMME benchmark as the number of frames increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first long-context Multi-Modal Sequence\nParallelism (MM-SP) system that enables long training and inference, enabling\n2M context length training on 256 GPUs without any gradient checkpointing.\nMM-SP is 2.1x - 5.7x faster than ring sequence parallelism and 1.1x - 1.4x\nfaster than Megatron context parallelism + tensor parallelism in text-only\nsettings. Moreover, it seamlessly integrates with Hugging Face Transformers.\nFor model training, we propose a five-stage pipeline comprising alignment,\npre-training, short supervised fine-tuning, context extension, and long\nsupervised fine-tuning. On datasets, we construct large-scale visual language\npre-training datasets and long video instruction-following datasets to support\nour multi-stage training process. LongVILA extends the number of frames of VILA\nfrom 8 to 1024, and improves the long video captioning score from 2.00 to 3.26\n(1.6x), achieving 99.5% accuracy in 1400-frames video (274k context length)\nneedle-in-a-haystack. LongVILA-8B demonstrates consistent accuracy improvements\non long videos in the VideoMME benchmark as the number of frames increases."
                },
                "authors": [
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Ethan He"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07904v2",
                "updated": "2024-08-19T17:16:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    16,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-11T16:43:03Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    16,
                    43,
                    3,
                    3,
                    102,
                    0
                ],
                "title": "HGRN2: Gated Linear RNNs with State Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGRN2: Gated Linear RNNs with State Expansion"
                },
                "summary": "Hierarchically gated linear RNN (HGRN, \\citealt{HGRN}) has demonstrated\ncompetitive training speed and performance in language modeling while offering\nefficient inference. However, the recurrent state size of HGRN remains\nrelatively small, limiting its expressiveness. To address this issue, we\nintroduce a simple outer product-based state expansion mechanism, which\nsignificantly enlarges the recurrent state size without introducing any\nadditional parameters. This enhancement also provides a linear attention\ninterpretation for HGRN2, enabling hardware-efficient training. Our extensive\nexperiments verify the advantage of HGRN2 over HGRN consistently across\ndifferent settings and competitive with other recurrent models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchically gated linear RNN (HGRN, \\citealt{HGRN}) has demonstrated\ncompetitive training speed and performance in language modeling while offering\nefficient inference. However, the recurrent state size of HGRN remains\nrelatively small, limiting its expressiveness. To address this issue, we\nintroduce a simple outer product-based state expansion mechanism, which\nsignificantly enlarges the recurrent state size without introducing any\nadditional parameters. This enhancement also provides a linear attention\ninterpretation for HGRN2, enabling hardware-efficient training. Our extensive\nexperiments verify the advantage of HGRN2 over HGRN consistently across\ndifferent settings and competitive with other recurrent models."
                },
                "authors": [
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Accept to COLM 2024. Yiran Zhong is the corresponding author. Zhen\n  Qin and Songlin Yang contributed equally to this work. The source code is\n  available at https://github.com/OpenNLPLab/HGRN2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02138v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02138v3",
                "updated": "2024-08-19T17:16:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    16,
                    8,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-02T17:49:40Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    17,
                    49,
                    40,
                    1,
                    93,
                    0
                ],
                "title": "Topic-Based Watermarks for LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-Based Watermarks for LLM-Generated Text"
                },
                "summary": "The indistinguishability of text generated by large language models (LLMs)\nfrom human-generated text poses significant challenges. Watermarking algorithms\nare potential solutions by embedding detectable signatures within LLM-generated\noutputs. However, current watermarking schemes lack robustness to a range of\nattacks such as text substitution or manipulation, undermining their\nreliability. This paper proposes a novel topic-based watermarking algorithm for\nLLMs, designed to enhance the robustness of watermarking in LLMs. Our approach\nleverages the topics extracted from input prompts or outputs of non-watermarked\nLLMs in the generation process of watermarked text. We dynamically utilize\ntoken lists on identified topics and adjust token sampling weights accordingly.\nBy using these topic-specific token biases, we embed a topic-sensitive\nwatermarking into the generated text. We outline the theoretical framework of\nour topic-based watermarking algorithm and discuss its potential advantages in\nvarious scenarios. Additionally, we explore a comprehensive range of attacks\nagainst watermarking algorithms, including discrete alterations, paraphrasing,\nand tokenizations. We demonstrate that our proposed watermarking scheme\nclassifies various watermarked text topics with 99.99% confidence and\noutperforms existing algorithms in terms of z-score robustness and the\nfeasibility of modeling text degradation by potential attackers, while\nconsidering the trade-offs between the benefits and losses of watermarking\nLLM-generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The indistinguishability of text generated by large language models (LLMs)\nfrom human-generated text poses significant challenges. Watermarking algorithms\nare potential solutions by embedding detectable signatures within LLM-generated\noutputs. However, current watermarking schemes lack robustness to a range of\nattacks such as text substitution or manipulation, undermining their\nreliability. This paper proposes a novel topic-based watermarking algorithm for\nLLMs, designed to enhance the robustness of watermarking in LLMs. Our approach\nleverages the topics extracted from input prompts or outputs of non-watermarked\nLLMs in the generation process of watermarked text. We dynamically utilize\ntoken lists on identified topics and adjust token sampling weights accordingly.\nBy using these topic-specific token biases, we embed a topic-sensitive\nwatermarking into the generated text. We outline the theoretical framework of\nour topic-based watermarking algorithm and discuss its potential advantages in\nvarious scenarios. Additionally, we explore a comprehensive range of attacks\nagainst watermarking algorithms, including discrete alterations, paraphrasing,\nand tokenizations. We demonstrate that our proposed watermarking scheme\nclassifies various watermarked text topics with 99.99% confidence and\noutperforms existing algorithms in terms of z-score robustness and the\nfeasibility of modeling text degradation by potential attackers, while\nconsidering the trade-offs between the benefits and losses of watermarking\nLLM-generated text."
                },
                "authors": [
                    {
                        "name": "Alexander Nemecek"
                    },
                    {
                        "name": "Yuzhou Jiang"
                    },
                    {
                        "name": "Erman Ayday"
                    }
                ],
                "author_detail": {
                    "name": "Erman Ayday"
                },
                "author": "Erman Ayday",
                "arxiv_comment": "Results for proposed scheme, additional/removal of content (figures\n  and equations), 12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02138v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02138v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10159v1",
                "updated": "2024-08-19T17:09:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:09:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems predict a user's next item of interest by\nanalyzing past interactions, aligning recommendations with individual\npreferences. Leveraging the strengths of Large Language Models (LLMs) in\nknowledge comprehension and reasoning, recent approaches have applied LLMs to\nsequential recommendation through language generation paradigms. These methods\nconvert user behavior sequences into prompts for LLM fine-tuning, utilizing\nLow-Rank Adaptation (LoRA) modules to refine recommendations. However, the\nuniform application of LoRA across diverse user behaviors sometimes fails to\ncapture individual variability, leading to suboptimal performance and negative\ntransfer between disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)\nframework. iLoRA creates a diverse array of experts, each capturing specific\naspects of user preferences, and introduces a sequence representation guided\ngate function. This gate function processes historical interaction sequences to\ngenerate enriched representations, guiding the gating network to output\ncustomized expert participation weights. This tailored approach mitigates\nnegative transfer and dynamically adjusts to diverse behavior patterns.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness\nof iLoRA, highlighting its superior performance compared to existing methods in\ncapturing user-specific preferences and improving recommendation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems predict a user's next item of interest by\nanalyzing past interactions, aligning recommendations with individual\npreferences. Leveraging the strengths of Large Language Models (LLMs) in\nknowledge comprehension and reasoning, recent approaches have applied LLMs to\nsequential recommendation through language generation paradigms. These methods\nconvert user behavior sequences into prompts for LLM fine-tuning, utilizing\nLow-Rank Adaptation (LoRA) modules to refine recommendations. However, the\nuniform application of LoRA across diverse user behaviors sometimes fails to\ncapture individual variability, leading to suboptimal performance and negative\ntransfer between disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)\nframework. iLoRA creates a diverse array of experts, each capturing specific\naspects of user preferences, and introduces a sequence representation guided\ngate function. This gate function processes historical interaction sequences to\ngenerate enriched representations, guiding the gating network to output\ncustomized expert participation weights. This tailored approach mitigates\nnegative transfer and dynamically adjusts to diverse behavior patterns.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness\nof iLoRA, highlighting its superior performance compared to existing methods in\ncapturing user-specific preferences and improving recommendation accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10151v1",
                "updated": "2024-08-19T17:02:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    2,
                    6,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:02:06Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    2,
                    6,
                    0,
                    232,
                    0
                ],
                "title": "Multilingual Needle in a Haystack: Investigating Long-Context Behavior\n  of Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Needle in a Haystack: Investigating Long-Context Behavior\n  of Multilingual Large Language Models"
                },
                "summary": "While recent large language models (LLMs) demonstrate remarkable abilities in\nresponding to queries in diverse languages, their ability to handle long\nmultilingual contexts is unexplored. As such, a systematic evaluation of the\nlong-context capabilities of LLMs in multilingual settings is crucial,\nspecifically in the context of information retrieval. To address this gap, we\nintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to\nassess a model's ability to retrieve relevant information (the needle) from a\ncollection of multilingual distractor texts (the haystack). This test serves as\nan extension of the multilingual question-answering task, encompassing both\nmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs\non MLNeedle. Our findings reveal that model performance can vary significantly\nwith language and needle position. Specifically, we observe that model\nperformance is the lowest when the needle is (i) in a language outside the\nEnglish language family and (ii) located in the middle of the input context.\nFurthermore, although some models claim a context size of $8k$ tokens or\ngreater, none demonstrate satisfactory cross-lingual retrieval performance as\nthe context length increases. Our analysis provides key insights into the\nlong-context behavior of LLMs in multilingual settings to guide future\nevaluation protocols. To our knowledge, this is the first study to investigate\nthe multilingual long-context behavior of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent large language models (LLMs) demonstrate remarkable abilities in\nresponding to queries in diverse languages, their ability to handle long\nmultilingual contexts is unexplored. As such, a systematic evaluation of the\nlong-context capabilities of LLMs in multilingual settings is crucial,\nspecifically in the context of information retrieval. To address this gap, we\nintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to\nassess a model's ability to retrieve relevant information (the needle) from a\ncollection of multilingual distractor texts (the haystack). This test serves as\nan extension of the multilingual question-answering task, encompassing both\nmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs\non MLNeedle. Our findings reveal that model performance can vary significantly\nwith language and needle position. Specifically, we observe that model\nperformance is the lowest when the needle is (i) in a language outside the\nEnglish language family and (ii) located in the middle of the input context.\nFurthermore, although some models claim a context size of $8k$ tokens or\ngreater, none demonstrate satisfactory cross-lingual retrieval performance as\nthe context length increases. Our analysis provides key insights into the\nlong-context behavior of LLMs in multilingual settings to guide future\nevaluation protocols. To our knowledge, this is the first study to investigate\nthe multilingual long-context behavior of LLMs."
                },
                "authors": [
                    {
                        "name": "Amey Hengle"
                    },
                    {
                        "name": "Prasoon Bajpai"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10147v1",
                "updated": "2024-08-19T16:47:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    47,
                    46,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:47:46Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    47,
                    46,
                    0,
                    232,
                    0
                ],
                "title": "In-Context Learning with Representations: Contextual Generalization of\n  Trained Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Representations: Contextual Generalization of\n  Trained Transformers"
                },
                "summary": "In-context learning (ICL) refers to a remarkable capability of pretrained\nlarge language models, which can learn a new task given a few examples during\ninference. However, theoretical understanding of ICL is largely under-explored,\nparticularly whether transformers can be trained to generalize to unseen\nexamples in a prompt, which will require the model to acquire contextual\nknowledge of the prompt for generalization. This paper investigates the\ntraining dynamics of transformers by gradient descent through the lens of\nnon-linear regression tasks. The contextual generalization here can be attained\nvia learning the template function for each task in-context, where all template\nfunctions lie in a linear space with $m$ basis functions. We analyze the\ntraining dynamics of one-layer multi-head transformers to in-contextly predict\nunlabeled inputs given partially labeled prompts, where the labels contain\nGaussian noise and the number of examples in each prompt are not sufficient to\ndetermine the template. Under mild assumptions, we show that the training loss\nfor a one-layer multi-head transformer converges linearly to a global minimum.\nMoreover, the transformer effectively learns to perform ridge regression over\nthe basis functions. To our knowledge, this study is the first provable\ndemonstration that transformers can learn contextual (i.e., template)\ninformation to generalize to both unseen examples and tasks when prompts\ncontain only a small number of query-answer pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) refers to a remarkable capability of pretrained\nlarge language models, which can learn a new task given a few examples during\ninference. However, theoretical understanding of ICL is largely under-explored,\nparticularly whether transformers can be trained to generalize to unseen\nexamples in a prompt, which will require the model to acquire contextual\nknowledge of the prompt for generalization. This paper investigates the\ntraining dynamics of transformers by gradient descent through the lens of\nnon-linear regression tasks. The contextual generalization here can be attained\nvia learning the template function for each task in-context, where all template\nfunctions lie in a linear space with $m$ basis functions. We analyze the\ntraining dynamics of one-layer multi-head transformers to in-contextly predict\nunlabeled inputs given partially labeled prompts, where the labels contain\nGaussian noise and the number of examples in each prompt are not sufficient to\ndetermine the template. Under mild assumptions, we show that the training loss\nfor a one-layer multi-head transformer converges linearly to a global minimum.\nMoreover, the transformer effectively learns to perform ridge regression over\nthe basis functions. To our knowledge, this study is the first provable\ndemonstration that transformers can learn contextual (i.e., template)\ninformation to generalize to both unseen examples and tasks when prompts\ncontain only a small number of query-answer pairs."
                },
                "authors": [
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Yuejie Chi"
                    }
                ],
                "author_detail": {
                    "name": "Yuejie Chi"
                },
                "author": "Yuejie Chi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08808v3",
                "updated": "2024-08-20T02:32:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    32,
                    58,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-16T15:41:43Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    41,
                    43,
                    4,
                    229,
                    0
                ],
                "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Ravi Raju"
                    },
                    {
                        "name": "Swayambhoo Jain"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "arxiv_comment": "14 pages, 8 figures, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03253v5",
                "updated": "2024-08-19T16:43:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    43,
                    51,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-05T19:00:02Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    19,
                    0,
                    2,
                    1,
                    65,
                    0
                ],
                "title": "Turbocharging constraints on dark matter substructure through a\n  synthesis of strong lensing flux ratios and extended lensed arcs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbocharging constraints on dark matter substructure through a\n  synthesis of strong lensing flux ratios and extended lensed arcs"
                },
                "summary": "Strong gravitational lensing provides a purely gravitational means to infer\nproperties of dark matter halos and thereby constrain the particle nature of\ndark matter. Strong lenses sometimes appear as four lensed images of a\nbackground quasar accompanied by spatially-resolved emission from the quasar\nhost galaxy encircling the main deflector (lensed arcs). We present methodology\nto simultaneously reconstruct lensed arcs and relative image magnifications\n(flux ratios) in the presence of full populations of subhalos and line-of-sight\nhalos. To this end, we develop a new approach for multi-plane ray tracing that\naccelerates lens mass and source light reconstruction by factors of $\\sim\n100-1000$. Using simulated data, we show that simultaneous reconstruction of\nlensed arcs and flux ratios isolates small-scale perturbations to flux ratios\nby dark matter substructure from uncertainties associated with the main\ndeflector mass profile on larger angular scales. Relative to analyses that use\nonly image positions and flux ratios to constrain the lens model, incorporating\narcs strengthens likelihood ratios penalizing warm dark matter (WDM) with a\nsuppression scale $m_{\\rm{hm}} / M_{\\odot}$ in the range $\\left[10^7 -\n10^{7.5}\\right]$, $\\left[10^{7.5} - 10^{8}\\right]$, $\\left[10^8 -\n10^{8.5}\\right]$, $\\left[10^{8.5} - 10^{9}\\right]$ by factors of $1.3$, $2.5$,\n$5.6$, and $13.1$, respectively, for a cold dark matter (CDM) ground truth. The\n$95\\%$ exclusion limit improves by 0.5 dex in $\\log_{10} m_{\\rm{hm}}$. The\nenhanced sensitivity to low-mass halos enabled by these methods pushes the\nobservational frontier of substructure lensing to the threshold of galaxy\nformation, enabling stringent tests of any theory that alters the properties of\ndark matter halos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong gravitational lensing provides a purely gravitational means to infer\nproperties of dark matter halos and thereby constrain the particle nature of\ndark matter. Strong lenses sometimes appear as four lensed images of a\nbackground quasar accompanied by spatially-resolved emission from the quasar\nhost galaxy encircling the main deflector (lensed arcs). We present methodology\nto simultaneously reconstruct lensed arcs and relative image magnifications\n(flux ratios) in the presence of full populations of subhalos and line-of-sight\nhalos. To this end, we develop a new approach for multi-plane ray tracing that\naccelerates lens mass and source light reconstruction by factors of $\\sim\n100-1000$. Using simulated data, we show that simultaneous reconstruction of\nlensed arcs and flux ratios isolates small-scale perturbations to flux ratios\nby dark matter substructure from uncertainties associated with the main\ndeflector mass profile on larger angular scales. Relative to analyses that use\nonly image positions and flux ratios to constrain the lens model, incorporating\narcs strengthens likelihood ratios penalizing warm dark matter (WDM) with a\nsuppression scale $m_{\\rm{hm}} / M_{\\odot}$ in the range $\\left[10^7 -\n10^{7.5}\\right]$, $\\left[10^{7.5} - 10^{8}\\right]$, $\\left[10^8 -\n10^{8.5}\\right]$, $\\left[10^{8.5} - 10^{9}\\right]$ by factors of $1.3$, $2.5$,\n$5.6$, and $13.1$, respectively, for a cold dark matter (CDM) ground truth. The\n$95\\%$ exclusion limit improves by 0.5 dex in $\\log_{10} m_{\\rm{hm}}$. The\nenhanced sensitivity to low-mass halos enabled by these methods pushes the\nobservational frontier of substructure lensing to the threshold of galaxy\nformation, enabling stringent tests of any theory that alters the properties of\ndark matter halos."
                },
                "authors": [
                    {
                        "name": "Daniel Gilman"
                    },
                    {
                        "name": "Simon Birrer"
                    },
                    {
                        "name": "Anna Nierenberg"
                    },
                    {
                        "name": "Maverick S. H. Oh"
                    }
                ],
                "author_detail": {
                    "name": "Maverick S. H. Oh"
                },
                "author": "Maverick S. H. Oh",
                "arxiv_comment": "matches MNRAS accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10141v1",
                "updated": "2024-08-19T16:41:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    41,
                    7,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:41:07Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    41,
                    7,
                    0,
                    232,
                    0
                ],
                "title": "Instruction Finetuning for Leaderboard Generation from Empirical AI\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Finetuning for Leaderboard Generation from Empirical AI\n  Research"
                },
                "summary": "This study demonstrates the application of instruction finetuning of\npretrained Large Language Models (LLMs) to automate the generation of AI\nresearch leaderboards, extracting (Task, Dataset, Metric, Score) quadruples\nfrom articles. It aims to streamline the dissemination of advancements in AI\nresearch by transitioning from traditional, manual community curation, or\notherwise taxonomy-constrained natural language inference (NLI) models, to an\nautomated, generative LLM-based approach. Utilizing the FLAN-T5 model, this\nresearch enhances LLMs' adaptability and reliability in information extraction,\noffering a novel method for structured knowledge representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the application of instruction finetuning of\npretrained Large Language Models (LLMs) to automate the generation of AI\nresearch leaderboards, extracting (Task, Dataset, Metric, Score) quadruples\nfrom articles. It aims to streamline the dissemination of advancements in AI\nresearch by transitioning from traditional, manual community curation, or\notherwise taxonomy-constrained natural language inference (NLI) models, to an\nautomated, generative LLM-based approach. Utilizing the FLAN-T5 model, this\nresearch enhances LLMs' adaptability and reliability in information extraction,\noffering a novel method for structured knowledge representation."
                },
                "authors": [
                    {
                        "name": "Salomon Kabongo"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer D'Souza"
                },
                "author": "Jennifer D'Souza",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.02409",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10128v1",
                "updated": "2024-08-19T16:15:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    15,
                    9,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:15:09Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    15,
                    9,
                    0,
                    232,
                    0
                ],
                "title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language"
                },
                "summary": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data."
                },
                "authors": [
                    {
                        "name": "Manjil Karki"
                    },
                    {
                        "name": "Pratik Shakya"
                    },
                    {
                        "name": "Sandesh Acharya"
                    },
                    {
                        "name": "Ravi Pandit"
                    },
                    {
                        "name": "Dinesh Gothe"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Gothe"
                },
                "author": "Dinesh Gothe",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10124v1",
                "updated": "2024-08-19T16:11:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    59,
                    0,
                    232,
                    0
                ],
                "title": "Molecular Graph Representation Learning Integrating Large Language\n  Models with Domain-specific Small Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Graph Representation Learning Integrating Large Language\n  Models with Domain-specific Small Models"
                },
                "summary": "Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Yuxiang Ren"
                    },
                    {
                        "name": "Chengbin Hou"
                    },
                    {
                        "name": "Hairong Lv"
                    },
                    {
                        "name": "Xuegong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuegong Zhang"
                },
                "author": "Xuegong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.15238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.15238v2",
                "updated": "2024-08-19T15:39:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    39,
                    54,
                    0,
                    232,
                    0
                ],
                "published": "2023-09-26T20:04:48Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    20,
                    4,
                    48,
                    1,
                    269,
                    0
                ],
                "title": "Learning Using Generated Privileged Information by Text-to-Image\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Using Generated Privileged Information by Text-to-Image\n  Diffusion Models"
                },
                "summary": "Learning Using Privileged Information is a particular type of knowledge\ndistillation where the teacher model benefits from an additional data\nrepresentation during training, called privileged information, improving the\nstudent model, which does not see the extra representation. However, privileged\ninformation is rarely available in practice. To this end, we propose a text\nclassification framework that harnesses text-to-image diffusion models to\ngenerate artificial privileged information. The generated images and the\noriginal text samples are further used to train multimodal teacher models based\non state-of-the-art transformer-based architectures. Finally, the knowledge\nfrom multimodal teachers is distilled into a text-based (unimodal) student.\nHence, by employing a generative model to produce synthetic data as privileged\ninformation, we guide the training of the student model. Our framework, called\nLearning Using Generated Privileged Information (LUGPI), yields noticeable\nperformance gains on four text classification data sets, demonstrating its\npotential in text classification without any additional cost during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Using Privileged Information is a particular type of knowledge\ndistillation where the teacher model benefits from an additional data\nrepresentation during training, called privileged information, improving the\nstudent model, which does not see the extra representation. However, privileged\ninformation is rarely available in practice. To this end, we propose a text\nclassification framework that harnesses text-to-image diffusion models to\ngenerate artificial privileged information. The generated images and the\noriginal text samples are further used to train multimodal teacher models based\non state-of-the-art transformer-based architectures. Finally, the knowledge\nfrom multimodal teachers is distilled into a text-based (unimodal) student.\nHence, by employing a generative model to produce synthetic data as privileged\ninformation, we guide the training of the student model. Our framework, called\nLearning Using Generated Privileged Information (LUGPI), yields noticeable\nperformance gains on four text classification data sets, demonstrating its\npotential in text classification without any additional cost during inference."
                },
                "authors": [
                    {
                        "name": "Rafael-Edy Menadil"
                    },
                    {
                        "name": "Mariana-Iuliana Georgescu"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Tudor Ionescu"
                },
                "author": "Radu Tudor Ionescu",
                "arxiv_comment": "Accepted at ICPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.15238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.15238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10094v1",
                "updated": "2024-08-19T15:32:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    32,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:32:59Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    32,
                    59,
                    0,
                    232,
                    0
                ],
                "title": "Shape inference in three-dimensional steady state supersonic flows using\n  ODIL and JAX-Fluids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape inference in three-dimensional steady state supersonic flows using\n  ODIL and JAX-Fluids"
                },
                "summary": "We propose a novel method for inferring the shape of a solid obstacle and its\nflow field in three-dimensional, steady state supersonic flows. The method\ncombines the optimization of a discrete loss (ODIL) technique with the\nautomatically differentiable JAX-Fluids computational fluid dynamics (CFD)\nsolver to study joint reconstruction of flow field and obstacle shape. ODIL\nminimizes the discrete residual of the governing partial differential equation\n(PDE) by gradient-descent-based algorithms. The ODIL framework inherits the\ncharacteristics of the chosen numerical discretizations of the underlying PDE,\nincluding their consistency and stability. The discrete residuals and their\nautomatic differentiation gradients are computed by the JAX-Fluids solver which\nprovides nonlinear shock-capturing schemes and level-set based immersed solid\nboundaries. We test the approach on challenging inverse problems, including the\nshape inference of a solid obstacle in three-dimensional steady state\nsupersonic flow. We show that the nonlinear shock-capturing discretization in\ncombination with the level-set based interface representation allows for\naccurate inference of the obstacle shape and its flow field. The proposed\napproach opens new avenues for solving complex inverse problems in supersonic\naerodynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel method for inferring the shape of a solid obstacle and its\nflow field in three-dimensional, steady state supersonic flows. The method\ncombines the optimization of a discrete loss (ODIL) technique with the\nautomatically differentiable JAX-Fluids computational fluid dynamics (CFD)\nsolver to study joint reconstruction of flow field and obstacle shape. ODIL\nminimizes the discrete residual of the governing partial differential equation\n(PDE) by gradient-descent-based algorithms. The ODIL framework inherits the\ncharacteristics of the chosen numerical discretizations of the underlying PDE,\nincluding their consistency and stability. The discrete residuals and their\nautomatic differentiation gradients are computed by the JAX-Fluids solver which\nprovides nonlinear shock-capturing schemes and level-set based immersed solid\nboundaries. We test the approach on challenging inverse problems, including the\nshape inference of a solid obstacle in three-dimensional steady state\nsupersonic flow. We show that the nonlinear shock-capturing discretization in\ncombination with the level-set based interface representation allows for\naccurate inference of the obstacle shape and its flow field. The proposed\napproach opens new avenues for solving complex inverse problems in supersonic\naerodynamics."
                },
                "authors": [
                    {
                        "name": "Aaron B. Buhendwa"
                    },
                    {
                        "name": "Deniz A. Bezgin"
                    },
                    {
                        "name": "Petr Karnakov"
                    },
                    {
                        "name": "Nikolaus A. Adams"
                    },
                    {
                        "name": "Petros Koumoutsakos"
                    }
                ],
                "author_detail": {
                    "name": "Petros Koumoutsakos"
                },
                "author": "Petros Koumoutsakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10091v1",
                "updated": "2024-08-19T15:31:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    31,
                    54,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:31:54Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    31,
                    54,
                    0,
                    232,
                    0
                ],
                "title": "Non-Plug-In Estimators Could Outperform Plug-In Estimators: a Cautionary\n  Note and a Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Plug-In Estimators Could Outperform Plug-In Estimators: a Cautionary\n  Note and a Diagnosis"
                },
                "summary": "Objectives: Highly flexible nonparametric estimators have gained popularity\nin causal inference and epidemiology. Popular examples of such estimators\ninclude targeted maximum likelihood estimators (TMLE) and double machine\nlearning (DML). TMLE is often argued or suggested to be better than DML\nestimators and several other estimators in small to moderate samples -- even if\nthey share the same large-sample properties -- because TMLE is a plug-in\nestimator and respects the known bounds on the parameter, while other\nestimators might fall outside the known bounds and yield absurd estimates.\nHowever, this argument is not a rigorously proven result and may fail in\ncertain cases. Methods: In a carefully chosen simulation setting, I compare the\nperformance of several versions of TMLE and DML estimators of the average\ntreatment effect among treated in small to moderate samples. Results: In this\nsimulation setting, DML estimators outperforms some versions of TMLE in small\nsamples. TMLE fluctuations are unstable, and hence empirically checking the\nmagnitude of the TMLE fluctuation might alert cases where TMLE might perform\npoorly. Conclusions: As a plug-in estimator, TMLE is not guaranteed to\noutperform non-plug-in counterparts such as DML estimators in small samples.\nChecking the fluctuation magnitude might be a useful diagnosis for TMLE. More\nrigorous theoretical justification is needed to understand and compare the\nfinite-sample performance of these highly flexible estimators in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Highly flexible nonparametric estimators have gained popularity\nin causal inference and epidemiology. Popular examples of such estimators\ninclude targeted maximum likelihood estimators (TMLE) and double machine\nlearning (DML). TMLE is often argued or suggested to be better than DML\nestimators and several other estimators in small to moderate samples -- even if\nthey share the same large-sample properties -- because TMLE is a plug-in\nestimator and respects the known bounds on the parameter, while other\nestimators might fall outside the known bounds and yield absurd estimates.\nHowever, this argument is not a rigorously proven result and may fail in\ncertain cases. Methods: In a carefully chosen simulation setting, I compare the\nperformance of several versions of TMLE and DML estimators of the average\ntreatment effect among treated in small to moderate samples. Results: In this\nsimulation setting, DML estimators outperforms some versions of TMLE in small\nsamples. TMLE fluctuations are unstable, and hence empirically checking the\nmagnitude of the TMLE fluctuation might alert cases where TMLE might perform\npoorly. Conclusions: As a plug-in estimator, TMLE is not guaranteed to\noutperform non-plug-in counterparts such as DML estimators in small samples.\nChecking the fluctuation magnitude might be a useful diagnosis for TMLE. More\nrigorous theoretical justification is needed to understand and compare the\nfinite-sample performance of these highly flexible estimators in general."
                },
                "authors": [
                    {
                        "name": "Hongxiang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Qiu"
                },
                "author": "Hongxiang Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12021v2",
                "updated": "2024-08-19T15:28:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    28,
                    37,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-27T22:20:39Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    22,
                    20,
                    39,
                    3,
                    179,
                    0
                ],
                "title": "Adaptive Draft-Verification for Efficient Large Language Model Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Draft-Verification for Efficient Large Language Model Decoding"
                },
                "summary": "Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications."
                },
                "authors": [
                    {
                        "name": "Xukun Liu"
                    },
                    {
                        "name": "Bowen Lei"
                    },
                    {
                        "name": "Ruqi Zhang"
                    },
                    {
                        "name": "Dongkuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dongkuan Xu"
                },
                "author": "Dongkuan Xu",
                "arxiv_comment": "Under review of Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10086v1",
                "updated": "2024-08-19T15:27:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    27,
                    25,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:27:25Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    27,
                    25,
                    0,
                    232,
                    0
                ],
                "title": "ARMADA: Attribute-Based Multimodal Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARMADA: Attribute-Based Multimodal Data Augmentation"
                },
                "summary": "In Multimodal Language Models (MLMs), the cost of manually annotating\nhigh-quality image-text pair data for fine-tuning and alignment is extremely\nhigh. While existing multimodal data augmentation frameworks propose ways to\naugment image-text pairs, they either suffer from semantic inconsistency\nbetween texts and images, or generate unrealistic images, causing knowledge gap\nwith real world examples. To address these issues, we propose Attribute-based\nMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentation\nmethod via knowledge-guided manipulation of visual attributes of the mentioned\nentities. Specifically, we extract entities and their visual attributes from\nthe original text data, then search for alternative values for the visual\nattributes under the guidance of knowledge bases (KBs) and large language\nmodels (LLMs). We then utilize an image-editing model to edit the images with\nthe extracted attributes. ARMADA is a novel multimodal data generation\nframework that: (i) extracts knowledge-grounded attributes from symbolic KBs\nfor semantically consistent yet distinctive image-text pair generation, (ii)\ngenerates visually similar images of disparate categories using neighboring\nentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs\nto modulate auxiliary visual attributes such as backgrounds for more robust\nrepresentation of original entities. Our empirical results over four downstream\ntasks demonstrate the efficacy of our framework to produce high-quality data\nand enhance the model performance. This also highlights the need to leverage\nexternal knowledge proxies for enhanced interpretability and real-world\ngrounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Multimodal Language Models (MLMs), the cost of manually annotating\nhigh-quality image-text pair data for fine-tuning and alignment is extremely\nhigh. While existing multimodal data augmentation frameworks propose ways to\naugment image-text pairs, they either suffer from semantic inconsistency\nbetween texts and images, or generate unrealistic images, causing knowledge gap\nwith real world examples. To address these issues, we propose Attribute-based\nMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentation\nmethod via knowledge-guided manipulation of visual attributes of the mentioned\nentities. Specifically, we extract entities and their visual attributes from\nthe original text data, then search for alternative values for the visual\nattributes under the guidance of knowledge bases (KBs) and large language\nmodels (LLMs). We then utilize an image-editing model to edit the images with\nthe extracted attributes. ARMADA is a novel multimodal data generation\nframework that: (i) extracts knowledge-grounded attributes from symbolic KBs\nfor semantically consistent yet distinctive image-text pair generation, (ii)\ngenerates visually similar images of disparate categories using neighboring\nentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs\nto modulate auxiliary visual attributes such as backgrounds for more robust\nrepresentation of original entities. Our empirical results over four downstream\ntasks demonstrate the efficacy of our framework to produce high-quality data\nand enhance the model performance. This also highlights the need to leverage\nexternal knowledge proxies for enhanced interpretability and real-world\ngrounding."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Jin"
                    },
                    {
                        "name": "Jeonghwan Kim"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Kuan-Hao Huang"
                    },
                    {
                        "name": "Te-Lin Wu"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10075v1",
                "updated": "2024-08-19T15:18:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    18,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:18:30Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    18,
                    30,
                    0,
                    232,
                    0
                ],
                "title": "Personalizing Reinforcement Learning from Human Feedback with\n  Variational Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing Reinforcement Learning from Human Feedback with\n  Variational Preference Learning"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for\naligning foundation models to human values and preferences. However, current\nRLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these\ndifferences arise, traditional RLHF frameworks simply average over them,\nleading to inaccurate rewards and poor performance for individual subgroups. To\naddress the need for pluralistic alignment, we develop a class of multimodal\nRLHF methods. Our proposed techniques are based on a latent variable\nformulation - inferring a novel user-specific latent and learning reward models\nand policies conditioned on this latent without additional user-specific data.\nWhile conceptually simple, we show that in practice, this reward modeling\nrequires careful algorithmic considerations around model architecture and\nreward scaling. To empirically validate our proposed technique, we first show\nthat it can provide a way to combat underspecification in simulated control\nproblems, inferring and optimizing user-specific reward functions. Next, we\nconduct experiments on pluralistic language datasets representing diverse user\npreferences and demonstrate improved reward function accuracy. We additionally\nshow the benefits of this probabilistic framework in terms of measuring\nuncertainty, and actively learning user preferences. This work enables learning\nfrom diverse populations of users with divergent preferences, an important\nchallenge that naturally occurs in problems from robot learning to foundation\nmodel alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for\naligning foundation models to human values and preferences. However, current\nRLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these\ndifferences arise, traditional RLHF frameworks simply average over them,\nleading to inaccurate rewards and poor performance for individual subgroups. To\naddress the need for pluralistic alignment, we develop a class of multimodal\nRLHF methods. Our proposed techniques are based on a latent variable\nformulation - inferring a novel user-specific latent and learning reward models\nand policies conditioned on this latent without additional user-specific data.\nWhile conceptually simple, we show that in practice, this reward modeling\nrequires careful algorithmic considerations around model architecture and\nreward scaling. To empirically validate our proposed technique, we first show\nthat it can provide a way to combat underspecification in simulated control\nproblems, inferring and optimizing user-specific reward functions. Next, we\nconduct experiments on pluralistic language datasets representing diverse user\npreferences and demonstrate improved reward function accuracy. We additionally\nshow the benefits of this probabilistic framework in terms of measuring\nuncertainty, and actively learning user preferences. This work enables learning\nfrom diverse populations of users with divergent preferences, an important\nchallenge that naturally occurs in problems from robot learning to foundation\nmodel alignment."
                },
                "authors": [
                    {
                        "name": "Sriyash Poddar"
                    },
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Hamish Ivison"
                    },
                    {
                        "name": "Abhishek Gupta"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "arxiv_comment": "weirdlabuw.github.io/vpl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10053v1",
                "updated": "2024-08-19T14:48:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    48,
                    4,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T14:48:04Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    48,
                    4,
                    0,
                    232,
                    0
                ],
                "title": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory"
                },
                "summary": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Xuebing Zhou"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03009v2",
                "updated": "2024-08-19T14:47:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    47,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-02-05T13:47:53Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    13,
                    47,
                    53,
                    0,
                    36,
                    0
                ],
                "title": "UniMem: Towards a Unified View of Long-Context Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMem: Towards a Unified View of Long-Context Large Language Models"
                },
                "summary": "Long-context processing is a critical ability that constrains the\napplicability of large language models (LLMs). Although there exist various\nmethods devoted to enhancing the long-context processing ability of LLMs, they\nare developed in an isolated manner and lack systematic analysis and\nintegration of their strengths, hindering further developments. In this paper,\nwe introduce UniMem, a Unified framework that reformulates existing\nlong-context methods from the view of Memory augmentation of LLMs.\nDistinguished by its four core dimensions-Memory Management, Memory Writing,\nMemory Reading, and Memory Injection, UniMem empowers researchers to conduct\nsystematic exploration of long-context methods. We re-formulate 16 existing\nmethods based on UniMem and analyze four representative methods:\nTransformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent\nUniMem forms to reveal their design principles and strengths. Based on these\nanalyses, we propose UniMix, an innovative approach that integrates the\nstrengths of these algorithms. Experimental results show that UniMix achieves\nsuperior performance in handling long contexts with significantly lower\nperplexity than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context processing is a critical ability that constrains the\napplicability of large language models (LLMs). Although there exist various\nmethods devoted to enhancing the long-context processing ability of LLMs, they\nare developed in an isolated manner and lack systematic analysis and\nintegration of their strengths, hindering further developments. In this paper,\nwe introduce UniMem, a Unified framework that reformulates existing\nlong-context methods from the view of Memory augmentation of LLMs.\nDistinguished by its four core dimensions-Memory Management, Memory Writing,\nMemory Reading, and Memory Injection, UniMem empowers researchers to conduct\nsystematic exploration of long-context methods. We re-formulate 16 existing\nmethods based on UniMem and analyze four representative methods:\nTransformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent\nUniMem forms to reveal their design principles and strengths. Based on these\nanalyses, we propose UniMix, an innovative approach that integrates the\nstrengths of these algorithms. Experimental results show that UniMix achieves\nsuperior performance in handling long contexts with significantly lower\nperplexity than baselines."
                },
                "authors": [
                    {
                        "name": "Junjie Fang"
                    },
                    {
                        "name": "Likai Tang"
                    },
                    {
                        "name": "Hongzhe Bi"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Si Sun"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Haolun Li"
                    },
                    {
                        "name": "Yongjian Li"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Xiaodong Shi"
                    },
                    {
                        "name": "Sen Song"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04920v2",
                "updated": "2024-08-19T14:45:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    45,
                    5,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-07T13:24:19Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    13,
                    24,
                    19,
                    4,
                    159,
                    0
                ],
                "title": "Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online\n  Coverage Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online\n  Coverage Path Planning"
                },
                "summary": "Sim-to-real transfer presents a difficult challenge, where models trained in\nsimulation are to be deployed in the real world. The distribution shift between\nthe two settings leads to biased representations of the dynamics, and thus to\nsuboptimal predictions in the real-world environment. In this work, we tackle\nthe challenge of sim-to-real transfer of reinforcement learning (RL) agents for\ncoverage path planning (CPP). In CPP, the task is for a robot to find a path\nthat covers every point of a confined area. Specifically, we consider the case\nwhere the environment is unknown, and the agent needs to plan the path online\nwhile mapping the environment. We bridge the sim-to-real gap through a\nsemi-virtual environment, including a real robot and real-time aspects, while\nutilizing a simulated sensor and obstacles to enable environment randomization\nand automated episode resetting. We investigate what level of fine-tuning is\nneeded for adapting to a realistic setting, comparing to an agent trained\nsolely in simulation. We find that a high inference frequency allows\nfirst-order Markovian policies to transfer directly from simulation, while\nhigher-order policies can be fine-tuned to further reduce the sim-to-real gap.\nMoreover, they can operate at a lower frequency, thus reducing computational\nrequirements. In both cases, our approaches transfer state-of-the-art results\nfrom simulation to the real domain, where direct learning would take in the\norder of weeks with manual interaction, that is, it would be completely\ninfeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-real transfer presents a difficult challenge, where models trained in\nsimulation are to be deployed in the real world. The distribution shift between\nthe two settings leads to biased representations of the dynamics, and thus to\nsuboptimal predictions in the real-world environment. In this work, we tackle\nthe challenge of sim-to-real transfer of reinforcement learning (RL) agents for\ncoverage path planning (CPP). In CPP, the task is for a robot to find a path\nthat covers every point of a confined area. Specifically, we consider the case\nwhere the environment is unknown, and the agent needs to plan the path online\nwhile mapping the environment. We bridge the sim-to-real gap through a\nsemi-virtual environment, including a real robot and real-time aspects, while\nutilizing a simulated sensor and obstacles to enable environment randomization\nand automated episode resetting. We investigate what level of fine-tuning is\nneeded for adapting to a realistic setting, comparing to an agent trained\nsolely in simulation. We find that a high inference frequency allows\nfirst-order Markovian policies to transfer directly from simulation, while\nhigher-order policies can be fine-tuned to further reduce the sim-to-real gap.\nMoreover, they can operate at a lower frequency, thus reducing computational\nrequirements. In both cases, our approaches transfer state-of-the-art results\nfrom simulation to the real domain, where direct learning would take in the\norder of weeks with manual interaction, that is, it would be completely\ninfeasible."
                },
                "authors": [
                    {
                        "name": "Arvi Jonnarth"
                    },
                    {
                        "name": "Ola Johansson"
                    },
                    {
                        "name": "Michael Felsberg"
                    }
                ],
                "author_detail": {
                    "name": "Michael Felsberg"
                },
                "author": "Michael Felsberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10039v1",
                "updated": "2024-08-19T14:31:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    31,
                    57,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T14:31:57Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    31,
                    57,
                    0,
                    232,
                    0
                ],
                "title": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis"
                },
                "summary": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task."
                },
                "authors": [
                    {
                        "name": "Ruihui Hou"
                    },
                    {
                        "name": "Shencheng Chen"
                    },
                    {
                        "name": "Yongqi Fan"
                    },
                    {
                        "name": "Lifeng Zhu"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Tong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Tong Ruan"
                },
                "author": "Tong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06009v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06009v3",
                "updated": "2024-08-19T14:24:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    24,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-09T21:07:16Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    21,
                    7,
                    16,
                    5,
                    69,
                    0
                ],
                "title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and\n  Limitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and\n  Limitations"
                },
                "summary": "Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope."
                },
                "authors": [
                    {
                        "name": "Swapnaja Achintalwar"
                    },
                    {
                        "name": "Adriana Alvarado Garcia"
                    },
                    {
                        "name": "Ateret Anaby-Tavor"
                    },
                    {
                        "name": "Ioana Baldini"
                    },
                    {
                        "name": "Sara E. Berger"
                    },
                    {
                        "name": "Bishwaranjan Bhattacharjee"
                    },
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Lamogha Chiazor"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Kirushikesh DB"
                    },
                    {
                        "name": "Rogério Abreu de Paula"
                    },
                    {
                        "name": "Pierre Dognin"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Soumya Ghosh"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "George Kour"
                    },
                    {
                        "name": "Ja Young Lee"
                    },
                    {
                        "name": "Nishtha Madaan"
                    },
                    {
                        "name": "Sameep Mehta"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Manish Nagireddy"
                    },
                    {
                        "name": "Inkit Padhi"
                    },
                    {
                        "name": "David Piorkowski"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    },
                    {
                        "name": "Hendrik Strobelt"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Christoph Tillmann"
                    },
                    {
                        "name": "Aashka Trivedi"
                    },
                    {
                        "name": "Kush R. Varshney"
                    },
                    {
                        "name": "Dennis Wei"
                    },
                    {
                        "name": "Shalisha Witherspooon"
                    },
                    {
                        "name": "Marcel Zalmanovici"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Zalmanovici"
                },
                "author": "Marcel Zalmanovici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06009v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06009v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10013v1",
                "updated": "2024-08-19T14:09:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    9,
                    48,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T14:09:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    9,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "TBA: Faster Large Language Model Training Using SSD-Based Activation\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TBA: Faster Large Language Model Training Using SSD-Based Activation\n  Offloading"
                },
                "summary": "The growth rate of the GPU memory capacity has not been able to keep up with\nthat of the size of large language models (LLMs), hindering the model training\nprocess. In particular, activations -- the intermediate tensors produced during\nforward propagation and reused in backward propagation -- dominate the GPU\nmemory use. To address this challenge, we propose TBA to efficiently offload\nactivations to high-capacity NVMe SSDs. This approach reduces GPU memory usage\nwithout impacting performance by adaptively overlapping data transfers with\ncomputation. TBA is compatible with popular deep learning frameworks like\nPyTorch, Megatron, and DeepSpeed, and it employs techniques such as tensor\ndeduplication, forwarding, and adaptive offloading to further enhance\nefficiency. We conduct extensive experiments on GPT, BERT, and T5. Results\ndemonstrate that TBA effectively reduces 47% of the activation peak memory\nusage. At the same time, TBA perfectly overlaps the I/O with the computation\nand incurs negligible performance overhead. We introduce the\nrecompute-offload-keep (ROK) curve to compare the TBA offloading with other two\ntensor placement strategies, keeping activations in memory and layerwise full\nrecomputation. We find that TBA achieves better memory savings than layerwise\nfull recomputation while retaining the performance of keeping the activations\nin memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth rate of the GPU memory capacity has not been able to keep up with\nthat of the size of large language models (LLMs), hindering the model training\nprocess. In particular, activations -- the intermediate tensors produced during\nforward propagation and reused in backward propagation -- dominate the GPU\nmemory use. To address this challenge, we propose TBA to efficiently offload\nactivations to high-capacity NVMe SSDs. This approach reduces GPU memory usage\nwithout impacting performance by adaptively overlapping data transfers with\ncomputation. TBA is compatible with popular deep learning frameworks like\nPyTorch, Megatron, and DeepSpeed, and it employs techniques such as tensor\ndeduplication, forwarding, and adaptive offloading to further enhance\nefficiency. We conduct extensive experiments on GPT, BERT, and T5. Results\ndemonstrate that TBA effectively reduces 47% of the activation peak memory\nusage. At the same time, TBA perfectly overlaps the I/O with the computation\nand incurs negligible performance overhead. We introduce the\nrecompute-offload-keep (ROK) curve to compare the TBA offloading with other two\ntensor placement strategies, keeping activations in memory and layerwise full\nrecomputation. We find that TBA achieves better memory savings than layerwise\nfull recomputation while retaining the performance of keeping the activations\nin memory."
                },
                "authors": [
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Mert Hidayetoğlu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Steven Sam Lumetta"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11657v2",
                "updated": "2024-08-19T14:01:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    1,
                    19,
                    0,
                    232,
                    0
                ],
                "published": "2024-02-18T17:28:15Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    17,
                    28,
                    15,
                    6,
                    49,
                    0
                ],
                "title": "On the importance of assessing topological convergence in Bayesian\n  phylogenetic inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the importance of assessing topological convergence in Bayesian\n  phylogenetic inference"
                },
                "summary": "Modern phylogenetics research is often performed within a Bayesian framework,\nusing sampling algorithms such as Markov chain Monte Carlo (MCMC) to\napproximate the posterior distribution. These algorithms require careful\nevaluation of the quality of the generated samples. Within the field of\nphylogenetics, one frequently adopted diagnostic approach is to evaluate the\neffective sample size (ESS) and to investigate trace graphs of the sampled\nparameters. A major limitation of these approaches is that they are developed\nfor continuous parameters and therefore incompatible with a crucial parameter\nin these inferences: the tree topology. Several recent advancements have aimed\nat extending these diagnostics to topological space. In this reflection paper,\nwe present two case studies - one on Ebola virus and one on HIV - illustrating\nhow these topological diagnostics can contain information not found in standard\ndiagnostics, and how decisions regarding which of these diagnostics to compute\ncan impact inferences regarding MCMC convergence and mixing. Our results show\nthe importance of running multiple replicate analyses and of carefully\nassessing topological convergence using the output of these replicate analyses.\nTo this end, we illustrate different ways of assessing and visualizing the\ntopological convergence of these replicates. Given the major importance of\ndetecting convergence and mixing issues in Bayesian phylogenetic analyses, the\nlack of a unified approach to this problem warrants further action, especially\nnow that additional tools are becoming available to researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern phylogenetics research is often performed within a Bayesian framework,\nusing sampling algorithms such as Markov chain Monte Carlo (MCMC) to\napproximate the posterior distribution. These algorithms require careful\nevaluation of the quality of the generated samples. Within the field of\nphylogenetics, one frequently adopted diagnostic approach is to evaluate the\neffective sample size (ESS) and to investigate trace graphs of the sampled\nparameters. A major limitation of these approaches is that they are developed\nfor continuous parameters and therefore incompatible with a crucial parameter\nin these inferences: the tree topology. Several recent advancements have aimed\nat extending these diagnostics to topological space. In this reflection paper,\nwe present two case studies - one on Ebola virus and one on HIV - illustrating\nhow these topological diagnostics can contain information not found in standard\ndiagnostics, and how decisions regarding which of these diagnostics to compute\ncan impact inferences regarding MCMC convergence and mixing. Our results show\nthe importance of running multiple replicate analyses and of carefully\nassessing topological convergence using the output of these replicate analyses.\nTo this end, we illustrate different ways of assessing and visualizing the\ntopological convergence of these replicates. Given the major importance of\ndetecting convergence and mixing issues in Bayesian phylogenetic analyses, the\nlack of a unified approach to this problem warrants further action, especially\nnow that additional tools are becoming available to researchers."
                },
                "authors": [
                    {
                        "name": "Marius Brusselmans"
                    },
                    {
                        "name": "Luiz Max Carvalho"
                    },
                    {
                        "name": "Samuel L. Hong"
                    },
                    {
                        "name": "Jiansi Gao"
                    },
                    {
                        "name": "Frederick A. Matsen IV"
                    },
                    {
                        "name": "Andrew Rambaut"
                    },
                    {
                        "name": "Philippe Lemey"
                    },
                    {
                        "name": "Marc A. Suchard"
                    },
                    {
                        "name": "Gytis Dudas"
                    },
                    {
                        "name": "Guy Baele"
                    }
                ],
                "author_detail": {
                    "name": "Guy Baele"
                },
                "author": "Guy Baele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17631v2",
                "updated": "2024-08-19T13:59:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    59,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-24T20:44:36Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    20,
                    44,
                    36,
                    2,
                    206,
                    0
                ],
                "title": "BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning"
                },
                "summary": "Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement."
                },
                "authors": [
                    {
                        "name": "Partha Chakraborty"
                    },
                    {
                        "name": "Mahmoud Alfadel"
                    },
                    {
                        "name": "Meiyappan Nagappan"
                    }
                ],
                "author_detail": {
                    "name": "Meiyappan Nagappan"
                },
                "author": "Meiyappan Nagappan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2204.00180v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2204.00180v4",
                "updated": "2024-08-19T13:55:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    55,
                    51,
                    0,
                    232,
                    0
                ],
                "published": "2022-04-01T03:15:25Z",
                "published_parsed": [
                    2022,
                    4,
                    1,
                    3,
                    15,
                    25,
                    4,
                    91,
                    0
                ],
                "title": "Measuring Diagnostic Test Performance Using Imperfect Reference Tests: A\n  Partial Identification Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Diagnostic Test Performance Using Imperfect Reference Tests: A\n  Partial Identification Approach"
                },
                "summary": "Diagnostic tests are almost never perfect. Studies quantifying their\nperformance use knowledge of the true health status, measured with a reference\ndiagnostic test. Researchers commonly assume that the reference test is\nperfect, which is often not the case in practice. When the assumption fails,\nconventional studies identify \"apparent\" performance or performance with\nrespect to the reference, but not true performance. This paper provides the\nsmallest possible bounds on the measures of true performance - sensitivity\n(true positive rate) and specificity (true negative rate), or equivalently\nfalse positive and negative rates, in standard settings. Implied bounds on\npolicy-relevant parameters are derived: 1) Prevalence in screened populations;\n2) Predictive values. Methods for inference based on moment inequalities are\nused to construct uniformly consistent confidence sets in level over a relevant\nfamily of data distributions. Emergency Use Authorization (EUA) and independent\nstudy data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds\ncan be very informative. Analysis reveals that the estimated false negative\nrates for symptomatic and asymptomatic patients are up to 3.17 and 4.59 times\nhigher than the frequently cited \"apparent\" false negative rate. Further\napplicability of the results in the context of imperfect proxies such as survey\nresponses and imputed protected classes is indicated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic tests are almost never perfect. Studies quantifying their\nperformance use knowledge of the true health status, measured with a reference\ndiagnostic test. Researchers commonly assume that the reference test is\nperfect, which is often not the case in practice. When the assumption fails,\nconventional studies identify \"apparent\" performance or performance with\nrespect to the reference, but not true performance. This paper provides the\nsmallest possible bounds on the measures of true performance - sensitivity\n(true positive rate) and specificity (true negative rate), or equivalently\nfalse positive and negative rates, in standard settings. Implied bounds on\npolicy-relevant parameters are derived: 1) Prevalence in screened populations;\n2) Predictive values. Methods for inference based on moment inequalities are\nused to construct uniformly consistent confidence sets in level over a relevant\nfamily of data distributions. Emergency Use Authorization (EUA) and independent\nstudy data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds\ncan be very informative. Analysis reveals that the estimated false negative\nrates for symptomatic and asymptomatic patients are up to 3.17 and 4.59 times\nhigher than the frequently cited \"apparent\" false negative rate. Further\napplicability of the results in the context of imperfect proxies such as survey\nresponses and imputed protected classes is indicated."
                },
                "authors": [
                    {
                        "name": "Filip Obradović"
                    }
                ],
                "author_detail": {
                    "name": "Filip Obradović"
                },
                "author": "Filip Obradović",
                "arxiv_comment": "For associated code, see:\n  https://github.com/obradovicfilip/bounding_test_performance",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2204.00180v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2204.00180v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09937v2",
                "updated": "2024-08-19T13:55:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    55,
                    42,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-15T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    3,
                    41,
                    0,
                    106,
                    0
                ],
                "title": "Compression Represents Intelligence Linearly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Represents Intelligence Linearly"
                },
                "summary": "There is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 31 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs' intelligence -- reflected by average benchmark\nscores -- almost linearly correlates with their ability to compress external\ntext corpora. These results provide concrete evidence supporting the belief\nthat superior compression indicates greater intelligence. Furthermore, our\nfindings suggest that compression efficiency, as an unsupervised metric derived\nfrom raw text corpora, serves as a reliable evaluation measure that is linearly\nassociated with the model capabilities. We open-source our compression datasets\nas well as our data collection pipelines to facilitate future researchers to\nassess compression properly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 31 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs' intelligence -- reflected by average benchmark\nscores -- almost linearly correlates with their ability to compress external\ntext corpora. These results provide concrete evidence supporting the belief\nthat superior compression indicates greater intelligence. Furthermore, our\nfindings suggest that compression efficiency, as an unsupervised metric derived\nfrom raw text corpora, serves as a reliable evaluation measure that is linearly\nassociated with the model capabilities. We open-source our compression datasets\nas well as our data collection pipelines to facilitate future researchers to\nassess compression properly."
                },
                "authors": [
                    {
                        "name": "Yuzhen Huang"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "arxiv_comment": "COLM 2024. Data and code are available at\n  https://github.com/hkust-nlp/llm-compression-intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.10230v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.10230v4",
                "updated": "2024-08-19T13:53:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    53,
                    12,
                    0,
                    232,
                    0
                ],
                "published": "2023-07-15T11:49:43Z",
                "published_parsed": [
                    2023,
                    7,
                    15,
                    11,
                    49,
                    43,
                    5,
                    196,
                    0
                ],
                "title": "Prompt Tuning on Graph-augmented Low-resource Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Tuning on Graph-augmented Low-resource Text Classification"
                },
                "summary": "Text classification is a fundamental problem in information retrieval with\nmany real-world applications, such as predicting the topics of online articles\nand the categories of e-commerce product descriptions. However, low-resource\ntext classification, with no or few labeled samples, presents a serious concern\nfor supervised learning. Meanwhile, many text data are inherently grounded on a\nnetwork structure, such as a hyperlink/citation network for online articles,\nand a user-item purchase network for e-commerce products. These graph\nstructures capture rich semantic relationships, which can potentially augment\nlow-resource text classification. In this paper, we propose a novel model\ncalled Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource\ntext classification in a two-pronged approach. During pre-training, we propose\nthree graph interaction-based contrastive strategies to jointly pre-train a\ngraph-text model; during downstream classification, we explore handcrafted\ndiscrete prompts and continuous prompt tuning for the jointly pre-trained model\nto achieve zero- and few-shot classification, respectively. Moreover, we\nexplore the possibility of employing continuous prompt tuning for zero-shot\ninference. Specifically, we aim to generalize continuous prompts to unseen\nclasses while leveraging a set of base classes. To this end, we extend G2P2\ninto G2P2$^*$, hinging on a new architecture of conditional prompt tuning.\nExtensive experiments on four real-world datasets demonstrate the strength of\nG2P2 in zero- and few-shot low-resource text classification tasks, and\nillustrate the advantage of G2P2$^*$ in dealing with unseen classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification is a fundamental problem in information retrieval with\nmany real-world applications, such as predicting the topics of online articles\nand the categories of e-commerce product descriptions. However, low-resource\ntext classification, with no or few labeled samples, presents a serious concern\nfor supervised learning. Meanwhile, many text data are inherently grounded on a\nnetwork structure, such as a hyperlink/citation network for online articles,\nand a user-item purchase network for e-commerce products. These graph\nstructures capture rich semantic relationships, which can potentially augment\nlow-resource text classification. In this paper, we propose a novel model\ncalled Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource\ntext classification in a two-pronged approach. During pre-training, we propose\nthree graph interaction-based contrastive strategies to jointly pre-train a\ngraph-text model; during downstream classification, we explore handcrafted\ndiscrete prompts and continuous prompt tuning for the jointly pre-trained model\nto achieve zero- and few-shot classification, respectively. Moreover, we\nexplore the possibility of employing continuous prompt tuning for zero-shot\ninference. Specifically, we aim to generalize continuous prompts to unseen\nclasses while leveraging a set of base classes. To this end, we extend G2P2\ninto G2P2$^*$, hinging on a new architecture of conditional prompt tuning.\nExtensive experiments on four real-world datasets demonstrate the strength of\nG2P2 in zero- and few-shot low-resource text classification tasks, and\nillustrate the advantage of G2P2$^*$ in dealing with unseen classes."
                },
                "authors": [
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Yuan Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Fang"
                },
                "author": "Yuan Fang",
                "arxiv_comment": "15 pages, accepted by TKDE (IEEE Transactions on Knowledge and Data\n  Engineering). arXiv admin note: substantial text overlap with\n  arXiv:2305.03324",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.10230v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.10230v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09992v1",
                "updated": "2024-08-19T13:43:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    43,
                    48,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T13:43:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    43,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Efficient Inference of Sub-Item Id-based Sequential Recommendation\n  Models with Millions of Items",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Sub-Item Id-based Sequential Recommendation\n  Models with Millions of Items"
                },
                "summary": "Transformer-based recommender systems, such as BERT4Rec or SASRec, achieve\nstate-of-the-art results in sequential recommendation. However, it is\nchallenging to use these models in production environments with catalogues of\nmillions of items: scaling Transformers beyond a few thousand items is\nproblematic for several reasons, including high model memory consumption and\nslow inference. In this respect, RecJPQ is a state-of-the-art method of\nreducing the models' memory consumption; RecJPQ compresses item catalogues by\ndecomposing item IDs into a small number of shared sub-item IDs. Despite\nreporting the reduction of memory consumption by a factor of up to 50x, the\noriginal RecJPQ paper did not report inference efficiency improvements over the\nbaseline Transformer-based models. Upon analysing RecJPQ's scoring algorithm,\nwe find that its efficiency is limited by its use of score accumulators for\neach item, which prevents parallelisation. In contrast, LightRec (a\nnon-sequential method that uses a similar idea of sub-ids) reported large\ninference efficiency improvements using an algorithm we call PQTopK. We show\nthat it is also possible to improve RecJPQ-based models' inference efficiency\nusing the PQTopK algorithm. In particular, we speed up RecJPQ-enhanced SASRec\nby a factor of 4.5 x compared to the original SASRec's inference method and by\na factor of 1.56 x compared to the method implemented in RecJPQ code on a\nlarge-scale Gowalla dataset with more than a million items. Further, using\nsimulated data, we show that PQTopK remains efficient with catalogues of up to\ntens of millions of items, removing one of the last obstacles to using\nTransformer-based models in production environments with large catalogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based recommender systems, such as BERT4Rec or SASRec, achieve\nstate-of-the-art results in sequential recommendation. However, it is\nchallenging to use these models in production environments with catalogues of\nmillions of items: scaling Transformers beyond a few thousand items is\nproblematic for several reasons, including high model memory consumption and\nslow inference. In this respect, RecJPQ is a state-of-the-art method of\nreducing the models' memory consumption; RecJPQ compresses item catalogues by\ndecomposing item IDs into a small number of shared sub-item IDs. Despite\nreporting the reduction of memory consumption by a factor of up to 50x, the\noriginal RecJPQ paper did not report inference efficiency improvements over the\nbaseline Transformer-based models. Upon analysing RecJPQ's scoring algorithm,\nwe find that its efficiency is limited by its use of score accumulators for\neach item, which prevents parallelisation. In contrast, LightRec (a\nnon-sequential method that uses a similar idea of sub-ids) reported large\ninference efficiency improvements using an algorithm we call PQTopK. We show\nthat it is also possible to improve RecJPQ-based models' inference efficiency\nusing the PQTopK algorithm. In particular, we speed up RecJPQ-enhanced SASRec\nby a factor of 4.5 x compared to the original SASRec's inference method and by\na factor of 1.56 x compared to the method implemented in RecJPQ code on a\nlarge-scale Gowalla dataset with more than a million items. Further, using\nsimulated data, we show that PQTopK remains efficient with catalogues of up to\ntens of millions of items, removing one of the last obstacles to using\nTransformer-based models in production environments with large catalogues."
                },
                "authors": [
                    {
                        "name": "Aleksandr V. Petrov"
                    },
                    {
                        "name": "Craig Macdonald"
                    },
                    {
                        "name": "Nicola Tonellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Tonellotto"
                },
                "author": "Nicola Tonellotto",
                "arxiv_doi": "10.1145/3640457.3688168",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688168",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.09992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by RecSys 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18208v3",
                "updated": "2024-08-19T13:40:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    40,
                    47,
                    0,
                    232,
                    0
                ],
                "published": "2023-10-27T15:31:22Z",
                "published_parsed": [
                    2023,
                    10,
                    27,
                    15,
                    31,
                    22,
                    4,
                    300,
                    0
                ],
                "title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation\n  using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArcheType: A Novel Framework for Open-Source Column Type Annotation\n  using Large Language Models"
                },
                "summary": "Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType."
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Yurong Liu"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "arxiv_doi": "10.14778/3665844.3665857",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3665844.3665857",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.18208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "VLDB 2024",
                "arxiv_journal_ref": "Proceedings of the VLDB Endowment, Volume 17, Issue 9, Pages 2279\n  - 2292, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.3; I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09982v2",
                "updated": "2024-08-20T02:41:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    41,
                    13,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T13:32:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    32,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "Application of Large Language Models in Automated Question Generation: A\n  Case Study on ChatGLM's Structured Questions for National Teacher\n  Certification Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of Large Language Models in Automated Question Generation: A\n  Case Study on ChatGLM's Structured Questions for National Teacher\n  Certification Exams"
                },
                "summary": "This study delves into the application potential of the large language models\n(LLMs) ChatGLM in the automatic generation of structured questions for National\nTeacher Certification Exams (NTCE). Through meticulously designed prompt\nengineering, we guided ChatGLM to generate a series of simulated questions and\nconducted a comprehensive comparison with questions recollected from past\nexaminees. To ensure the objectivity and professionalism of the evaluation, we\ninvited experts in the field of education to assess these questions and their\nscoring criteria. The research results indicate that the questions generated by\nChatGLM exhibit a high level of rationality, scientificity, and practicality\nsimilar to those of the real exam questions across most evaluation criteria,\ndemonstrating the model's accuracy and reliability in question generation.\nNevertheless, the study also reveals limitations in the model's consideration\nof various rating criteria when generating questions, suggesting the need for\nfurther optimization and adjustment. This research not only validates the\napplication potential of ChatGLM in the field of educational assessment but\nalso provides crucial empirical support for the development of more efficient\nand intelligent educational automated generation systems in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study delves into the application potential of the large language models\n(LLMs) ChatGLM in the automatic generation of structured questions for National\nTeacher Certification Exams (NTCE). Through meticulously designed prompt\nengineering, we guided ChatGLM to generate a series of simulated questions and\nconducted a comprehensive comparison with questions recollected from past\nexaminees. To ensure the objectivity and professionalism of the evaluation, we\ninvited experts in the field of education to assess these questions and their\nscoring criteria. The research results indicate that the questions generated by\nChatGLM exhibit a high level of rationality, scientificity, and practicality\nsimilar to those of the real exam questions across most evaluation criteria,\ndemonstrating the model's accuracy and reliability in question generation.\nNevertheless, the study also reveals limitations in the model's consideration\nof various rating criteria when generating questions, suggesting the need for\nfurther optimization and adjustment. This research not only validates the\napplication potential of ChatGLM in the field of educational assessment but\nalso provides crucial empirical support for the development of more efficient\nand intelligent educational automated generation systems in the future."
                },
                "authors": [
                    {
                        "name": "Ling He"
                    },
                    {
                        "name": "Yanxin Chen"
                    },
                    {
                        "name": "Xiaoqiang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqiang Hu"
                },
                "author": "Xiaoqiang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13602v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13602v4",
                "updated": "2024-08-19T13:27:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    27,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-02-21T08:09:05Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    8,
                    9,
                    5,
                    2,
                    52,
                    0
                ],
                "title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving"
                },
                "summary": "Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems."
                },
                "authors": [
                    {
                        "name": "Mehdi Azarafza"
                    },
                    {
                        "name": "Mojtaba Nayyeri"
                    },
                    {
                        "name": "Charles Steinmetz"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Achim Rettberg"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettberg"
                },
                "author": "Achim Rettberg",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13602v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13602v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09972v1",
                "updated": "2024-08-19T13:19:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    19,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T13:19:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    19,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with\n  Large Language Models"
                },
                "summary": "Integrating large language models (LLMs) into autonomous driving enhances\npersonalization and adaptability in open-world scenarios. However, traditional\nedge computing models still face significant challenges in processing complex\ndriving data, particularly regarding real-time performance and system\nefficiency. To address these challenges, this study introduces EC-Drive, a\nnovel edge-cloud collaborative autonomous driving system with data drift\ndetection capabilities. EC-Drive utilizes drift detection algorithms to\nselectively upload critical data, including new obstacles and traffic pattern\nchanges, to the cloud for processing by GPT-4, while routine data is\nefficiently managed by smaller LLMs on edge devices. This approach not only\nreduces inference latency but also improves system efficiency by optimizing\ncommunication resource use. Experimental validation confirms the system's\nrobust processing capabilities and practical applicability in real-world\ndriving conditions, demonstrating the effectiveness of this edge-cloud\ncollaboration framework. Our data and system demonstration will be released at\nhttps://sites.google.com/view/ec-drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) into autonomous driving enhances\npersonalization and adaptability in open-world scenarios. However, traditional\nedge computing models still face significant challenges in processing complex\ndriving data, particularly regarding real-time performance and system\nefficiency. To address these challenges, this study introduces EC-Drive, a\nnovel edge-cloud collaborative autonomous driving system with data drift\ndetection capabilities. EC-Drive utilizes drift detection algorithms to\nselectively upload critical data, including new obstacles and traffic pattern\nchanges, to the cloud for processing by GPT-4, while routine data is\nefficiently managed by smaller LLMs on edge devices. This approach not only\nreduces inference latency but also improves system efficiency by optimizing\ncommunication resource use. Experimental validation confirms the system's\nrobust processing capabilities and practical applicability in real-world\ndriving conditions, demonstrating the effectiveness of this edge-cloud\ncollaboration framework. Our data and system demonstration will be released at\nhttps://sites.google.com/view/ec-drive."
                },
                "authors": [
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Suyan Dai"
                    },
                    {
                        "name": "Fangfang Chen"
                    },
                    {
                        "name": "Zuohong Lv"
                    },
                    {
                        "name": "Jianhua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tang"
                },
                "author": "Jianhua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09816v3",
                "updated": "2024-08-19T13:16:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    16,
                    16,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-13T09:22:33Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    9,
                    22,
                    33,
                    5,
                    195,
                    0
                ],
                "title": "MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts"
                },
                "summary": "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Xue Bai"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Guangyuan Ma"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09966v1",
                "updated": "2024-08-19T13:14:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    14,
                    2,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T13:14:02Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    14,
                    2,
                    0,
                    232,
                    0
                ],
                "title": "Mask in the Mirror: Implicit Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask in the Mirror: Implicit Sparsification"
                },
                "summary": "Sparsifying deep neural networks to reduce their inference cost is an NP-hard\nproblem and difficult to optimize due to its mixed discrete and continuous\nnature. Yet, as we prove, continuous sparsification has already an implicit\nbias towards sparsity that would not require common projections of relaxed mask\nvariables. While implicit rather than explicit regularization induces benefits,\nit usually does not provide enough flexibility in practice, as only a specific\ntarget sparsity is obtainable. To exploit its potential for continuous\nsparsification, we propose a way to control the strength of the implicit bias.\nBased on the mirror flow framework, we derive resulting convergence and\noptimality guarantees in the context of underdetermined linear regression and\ndemonstrate the utility of our insights in more general neural network\nsparsification experiments, achieving significant performance gains,\nparticularly in the high-sparsity regime. Our theoretical contribution might be\nof independent interest, as we highlight a way to enter the rich regime and\nshow that implicit bias is controllable by a time-dependent Bregman potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsifying deep neural networks to reduce their inference cost is an NP-hard\nproblem and difficult to optimize due to its mixed discrete and continuous\nnature. Yet, as we prove, continuous sparsification has already an implicit\nbias towards sparsity that would not require common projections of relaxed mask\nvariables. While implicit rather than explicit regularization induces benefits,\nit usually does not provide enough flexibility in practice, as only a specific\ntarget sparsity is obtainable. To exploit its potential for continuous\nsparsification, we propose a way to control the strength of the implicit bias.\nBased on the mirror flow framework, we derive resulting convergence and\noptimality guarantees in the context of underdetermined linear regression and\ndemonstrate the utility of our insights in more general neural network\nsparsification experiments, achieving significant performance gains,\nparticularly in the high-sparsity regime. Our theoretical contribution might be\nof independent interest, as we highlight a way to enter the rich regime and\nshow that implicit bias is controllable by a time-dependent Bregman potential."
                },
                "authors": [
                    {
                        "name": "Tom Jacobs"
                    },
                    {
                        "name": "Rebekka Burkholz"
                    }
                ],
                "author_detail": {
                    "name": "Rebekka Burkholz"
                },
                "author": "Rebekka Burkholz",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15760v2",
                "updated": "2024-08-19T12:59:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    59,
                    53,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-23T08:24:09Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    8,
                    24,
                    9,
                    5,
                    83,
                    0
                ],
                "title": "An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side\n  Pre-trained Generator to Clients in Heterogeneous Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side\n  Pre-trained Generator to Clients in Heterogeneous Federated Learning"
                },
                "summary": "Heterogeneous Federated Learning (HtFL) enables task-specific knowledge\nsharing among clients with different model architectures while preserving\nprivacy. Despite recent research progress, transferring knowledge in HtFL is\nstill difficult due to data and model heterogeneity. To tackle this, we\nintroduce a public pre-trained generator (e.g., StyleGAN or Stable Diffusion)\nas the bridge and propose a new upload-efficient knowledge transfer scheme\ncalled Federated Knowledge-Transfer-Loop (FedKTL). It can produce task-related\nprototypical image-vector pairs via the generator's inference on the server.\nWith these pairs, each client can transfer common knowledge from the generator\nto its local model through an additional supervised local task. We conduct\nextensive experiments on four datasets under two types of data heterogeneity\nwith 14 heterogeneous models, including CNNs and ViTs. Results show that our\nFedKTL surpasses seven state-of-the-art methods by up to 7.31%. Moreover, our\nknowledge transfer scheme is applicable in cloud-edge scenarios with only one\nedge client. Code: https://github.com/TsingZ0/FedKTL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Federated Learning (HtFL) enables task-specific knowledge\nsharing among clients with different model architectures while preserving\nprivacy. Despite recent research progress, transferring knowledge in HtFL is\nstill difficult due to data and model heterogeneity. To tackle this, we\nintroduce a public pre-trained generator (e.g., StyleGAN or Stable Diffusion)\nas the bridge and propose a new upload-efficient knowledge transfer scheme\ncalled Federated Knowledge-Transfer-Loop (FedKTL). It can produce task-related\nprototypical image-vector pairs via the generator's inference on the server.\nWith these pairs, each client can transfer common knowledge from the generator\nto its local model through an additional supervised local task. We conduct\nextensive experiments on four datasets under two types of data heterogeneity\nwith 14 heterogeneous models, including CNNs and ViTs. Results show that our\nFedKTL surpasses seven state-of-the-art methods by up to 7.31%. Moreover, our\nknowledge transfer scheme is applicable in cloud-edge scenarios with only one\nedge client. Code: https://github.com/TsingZ0/FedKTL"
                },
                "authors": [
                    {
                        "name": "Jianqing Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yang Hua"
                    },
                    {
                        "name": "Jian Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cao"
                },
                "author": "Jian Cao",
                "arxiv_comment": "Accepted by CVPR2024. We have incorporated additional analysis for\n  the Stable Diffusion experiments in Appendix A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09955v2",
                "updated": "2024-08-20T05:51:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    51,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T12:55:16Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    55,
                    16,
                    0,
                    232,
                    0
                ],
                "title": "MegaAgent: A Practical Framework for Autonomous Cooperation in\n  Large-Scale LLM Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaAgent: A Practical Framework for Autonomous Cooperation in\n  Large-Scale LLM Agent Systems"
                },
                "summary": "With the emergence of large language models (LLMs), LLM-powered multi-agent\nsystems (LLM-MA systems) have been proposed to tackle real-world tasks.\nHowever, their agents mostly follow predefined Standard Operating Procedures\n(SOPs) that remain unchanged across the whole interaction, lacking autonomy and\nscalability. Additionally, current solutions often overlook the necessity for\neffective agent cooperation. To address the above limitations, we propose\nMegaAgent, a practical framework designed for autonomous cooperation in\nlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents to\ndynamically generate agents based on task requirements, incorporating features\nsuch as automatically dividing tasks, systematic planning and monitoring of\nagent activities, and managing concurrent operations. In addition, MegaAgent is\ndesigned with a hierarchical structure and employs system-level parallelism to\nenhance performance and boost communication. We demonstrate the effectiveness\nof MegaAgent through Gobang game development, showing that it outperforms\npopular LLM-MA systems; and national policy simulation, demonstrating its high\nautonomy and potential to rapidly scale up to 590 agents while ensuring\neffective cooperation among them. Our results indicate that MegaAgent is the\nfirst autonomous large-scale LLM-MA system with no pre-defined SOPs, high\neffectiveness and scalability, paving the way for further research in this\nfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of large language models (LLMs), LLM-powered multi-agent\nsystems (LLM-MA systems) have been proposed to tackle real-world tasks.\nHowever, their agents mostly follow predefined Standard Operating Procedures\n(SOPs) that remain unchanged across the whole interaction, lacking autonomy and\nscalability. Additionally, current solutions often overlook the necessity for\neffective agent cooperation. To address the above limitations, we propose\nMegaAgent, a practical framework designed for autonomous cooperation in\nlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents to\ndynamically generate agents based on task requirements, incorporating features\nsuch as automatically dividing tasks, systematic planning and monitoring of\nagent activities, and managing concurrent operations. In addition, MegaAgent is\ndesigned with a hierarchical structure and employs system-level parallelism to\nenhance performance and boost communication. We demonstrate the effectiveness\nof MegaAgent through Gobang game development, showing that it outperforms\npopular LLM-MA systems; and national policy simulation, demonstrating its high\nautonomy and potential to rapidly scale up to 590 agents while ensuring\neffective cooperation among them. Our results indicate that MegaAgent is the\nfirst autonomous large-scale LLM-MA system with no pre-defined SOPs, high\neffectiveness and scalability, paving the way for further research in this\nfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Jingsheng Liang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11897v3",
                "updated": "2024-08-19T12:47:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    47,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2023-12-19T06:42:47Z",
                "published_parsed": [
                    2023,
                    12,
                    19,
                    6,
                    42,
                    47,
                    1,
                    353,
                    0
                ],
                "title": "Text-Conditioned Resampler For Long Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Conditioned Resampler For Long Form Video Understanding"
                },
                "summary": "In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge."
                },
                "authors": [
                    {
                        "name": "Bruno Korbar"
                    },
                    {
                        "name": "Yongqin Xian"
                    },
                    {
                        "name": "Alessio Tonioni"
                    },
                    {
                        "name": "Andrew Zisserman"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "arxiv_comment": "Accepted to the ECCV24 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08164v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08164v5",
                "updated": "2024-08-19T12:44:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    44,
                    27,
                    0,
                    232,
                    0
                ],
                "published": "2023-10-12T09:36:03Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    9,
                    36,
                    3,
                    3,
                    285,
                    0
                ],
                "title": "Interpreting Learned Feedback Patterns in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Learned Feedback Patterns in Large Language Models"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs."
                },
                "authors": [
                    {
                        "name": "Luke Marks"
                    },
                    {
                        "name": "Amir Abdullah"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Rauno Arike"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08164v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08164v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09950v1",
                "updated": "2024-08-19T12:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    42,
                    46,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T12:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    42,
                    46,
                    0,
                    232,
                    0
                ],
                "title": "Non-ergodic inference for stationary-increment harmonizable stable\n  processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-ergodic inference for stationary-increment harmonizable stable\n  processes"
                },
                "summary": "We consider the class of stationary-increment harmonizable stable processes\nwith infinite control measure, which most notably includes real harmonizable\nfractional stable motions. We give conditions for the integrability of the\npaths of such processes with respect to a finite, absolutely continuous measure\nand derive the distributional characteristics of the path integral with respect\nto said measure. The convolution of the path of a stationary-increment\nharmonizable stable process with a suitable measure yields a real stationary\nharmonizable stable process with finite control measure. This allows us to\nconstruct consistent estimators for the index of stability as well as the\nkernel function in the integral representation of a stationary increment\nharmonizable stable process (up to a constant factor). For real harmonizable\nfractional stable motions consistent estimators for the index of stability and\nits Hurst parameter are given. These are computed directly from the periodogram\nfrequency estimates of the smoothed process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the class of stationary-increment harmonizable stable processes\nwith infinite control measure, which most notably includes real harmonizable\nfractional stable motions. We give conditions for the integrability of the\npaths of such processes with respect to a finite, absolutely continuous measure\nand derive the distributional characteristics of the path integral with respect\nto said measure. The convolution of the path of a stationary-increment\nharmonizable stable process with a suitable measure yields a real stationary\nharmonizable stable process with finite control measure. This allows us to\nconstruct consistent estimators for the index of stability as well as the\nkernel function in the integral representation of a stationary increment\nharmonizable stable process (up to a constant factor). For real harmonizable\nfractional stable motions consistent estimators for the index of stability and\nits Hurst parameter are given. These are computed directly from the periodogram\nfrequency estimates of the smoothed process."
                },
                "authors": [
                    {
                        "name": "Ly Viet Hoang"
                    },
                    {
                        "name": "Evgeny Spodarev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Spodarev"
                },
                "author": "Evgeny Spodarev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09946v1",
                "updated": "2024-08-19T12:35:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    35,
                    23,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T12:35:23Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    35,
                    23,
                    0,
                    232,
                    0
                ],
                "title": "Microscopic Analysis on LLM players via Social Deduction Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microscopic Analysis on LLM players via Social Deduction Game"
                },
                "summary": "Recent studies have begun developing autonomous game players for social\ndeduction games using large language models (LLMs). When building LLM players,\nfine-grained evaluations are crucial for addressing weaknesses in game-playing\nabilities. However, existing studies have often overlooked such assessments.\nSpecifically, we point out two issues with the evaluation methods employed.\nFirst, game-playing abilities have typically been assessed through game-level\noutcomes rather than specific event-level skills; Second, error analyses have\nlacked structured methodologies. To address these issues, we propose an\napproach utilizing a variant of the SpyFall game, named SpyGame. We conducted\nan experiment with four LLMs, analyzing their gameplay behavior in SpyGame both\nquantitatively and qualitatively. For the quantitative analysis, we introduced\neight metrics to resolve the first issue, revealing that these metrics are more\neffective than existing ones for evaluating the two critical skills: intent\nidentification and camouflage. In the qualitative analysis, we performed\nthematic analysis to resolve the second issue. This analysis identifies four\nmajor categories that affect gameplay of LLMs. Additionally, we demonstrate how\nthese categories complement and support the findings from the quantitative\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have begun developing autonomous game players for social\ndeduction games using large language models (LLMs). When building LLM players,\nfine-grained evaluations are crucial for addressing weaknesses in game-playing\nabilities. However, existing studies have often overlooked such assessments.\nSpecifically, we point out two issues with the evaluation methods employed.\nFirst, game-playing abilities have typically been assessed through game-level\noutcomes rather than specific event-level skills; Second, error analyses have\nlacked structured methodologies. To address these issues, we propose an\napproach utilizing a variant of the SpyFall game, named SpyGame. We conducted\nan experiment with four LLMs, analyzing their gameplay behavior in SpyGame both\nquantitatively and qualitatively. For the quantitative analysis, we introduced\neight metrics to resolve the first issue, revealing that these metrics are more\neffective than existing ones for evaluating the two critical skills: intent\nidentification and camouflage. In the qualitative analysis, we performed\nthematic analysis to resolve the second issue. This analysis identifies four\nmajor categories that affect gameplay of LLMs. Additionally, we demonstrate how\nthese categories complement and support the findings from the quantitative\nanalysis."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Under review, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09945v1",
                "updated": "2024-08-19T12:34:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T12:34:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Lianzhang Lou"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09943v1",
                "updated": "2024-08-19T12:32:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    32,
                    50,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T12:32:50Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    32,
                    50,
                    0,
                    232,
                    0
                ],
                "title": "Calibrating Noise for Group Privacy in Subsampled Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Noise for Group Privacy in Subsampled Mechanisms"
                },
                "summary": "Given a group size m and a sensitive dataset D, group privacy (GP) releases\ninformation about D with the guarantee that the adversary cannot infer with\nhigh confidence whether the underlying data is D or a neighboring dataset D'\nthat differs from D by m records. GP generalizes the well-established notion of\ndifferential privacy (DP) for protecting individuals' privacy; in particular,\nwhen m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the\nsensitive aggregate information of a group of up to m individuals, e.g., the\naverage annual income among members of a yacht club. Despite its longstanding\npresence in the research literature and its promising applications, GP is often\ntreated as an afterthought, with most approaches first developing a DP\nmechanism and then using a generic conversion to adapt it for GP, treating the\nDP solution as a black box. As we point out in the paper, this methodology is\nsuboptimal when the underlying DP solution involves subsampling, e.g., in the\nclassic DP-SGD method for training deep learning models. In this case, the\nDP-to-GP conversion is overly pessimistic in its analysis, leading to low\nutility in the published results under GP.\n  Motivated by this, we propose a novel analysis framework that provides tight\nprivacy accounting for subsampled GP mechanisms. Instead of converting a\nblack-box DP mechanism to GP, our solution carefully analyzes and utilizes the\ninherent randomness in subsampled mechanisms, leading to a substantially\nimproved bound on the privacy loss with respect to GP. The proposed solution\napplies to a wide variety of foundational mechanisms with subsampling.\nExtensive experiments with real datasets demonstrate that compared to the\nbaseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise\nreductions of over an order of magnitude in several practical settings,\nincluding deep neural network training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a group size m and a sensitive dataset D, group privacy (GP) releases\ninformation about D with the guarantee that the adversary cannot infer with\nhigh confidence whether the underlying data is D or a neighboring dataset D'\nthat differs from D by m records. GP generalizes the well-established notion of\ndifferential privacy (DP) for protecting individuals' privacy; in particular,\nwhen m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the\nsensitive aggregate information of a group of up to m individuals, e.g., the\naverage annual income among members of a yacht club. Despite its longstanding\npresence in the research literature and its promising applications, GP is often\ntreated as an afterthought, with most approaches first developing a DP\nmechanism and then using a generic conversion to adapt it for GP, treating the\nDP solution as a black box. As we point out in the paper, this methodology is\nsuboptimal when the underlying DP solution involves subsampling, e.g., in the\nclassic DP-SGD method for training deep learning models. In this case, the\nDP-to-GP conversion is overly pessimistic in its analysis, leading to low\nutility in the published results under GP.\n  Motivated by this, we propose a novel analysis framework that provides tight\nprivacy accounting for subsampled GP mechanisms. Instead of converting a\nblack-box DP mechanism to GP, our solution carefully analyzes and utilizes the\ninherent randomness in subsampled mechanisms, leading to a substantially\nimproved bound on the privacy loss with respect to GP. The proposed solution\napplies to a wide variety of foundational mechanisms with subsampling.\nExtensive experiments with real datasets demonstrate that compared to the\nbaseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise\nreductions of over an order of magnitude in several practical settings,\nincluding deep neural network training."
                },
                "authors": [
                    {
                        "name": "Yangfan Jiang"
                    },
                    {
                        "name": "Xinjian Luo"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "arxiv_comment": "accepted for publication in Proceedings of VLDB Endowment (PVLDB)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14362v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14362v4",
                "updated": "2024-08-19T12:28:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    28,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-21T12:45:01Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    12,
                    45,
                    1,
                    3,
                    81,
                    0
                ],
                "title": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics"
                },
                "summary": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Yue"
                    },
                    {
                        "name": "Jiancheng Zhao"
                    },
                    {
                        "name": "Chunhui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chunhui Zhao"
                },
                "author": "Chunhui Zhao",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14362v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14362v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08664v2",
                "updated": "2024-08-19T12:20:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    20,
                    26,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T11:11:56Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    11,
                    56,
                    4,
                    229,
                    0
                ],
                "title": "A new perspective on Bayesian Operational Modal Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new perspective on Bayesian Operational Modal Analysis"
                },
                "summary": "In the field of operational modal analysis (OMA), obtained modal information\nis frequently used to assess the current state of aerospace, mechanical,\noffshore and civil structures. However, the stochasticity of operational\nsystems and the lack of forcing information can lead to inconsistent results.\nQuantifying the uncertainty of the recovered modal parameters through OMA is\ntherefore of significant value. In this article, a new perspective on Bayesian\nOMA is proposed: a Bayesian stochastic subspace identification (SSI) algorithm.\nDistinct from existing approaches to Bayesian OMA, a hierarchical probabilistic\nmodel is embedded at the core of covariance-driven SSI. Through substitution of\ncanonical correlation analysis with a Bayesian equivalent, posterior\ndistributions over the modal properties are obtained. Two inference schemes are\npresented for the proposed Bayesian formulation: Markov Chain Monte Carlo and\nvariational Bayes. Two case studies are then explored. The first is benchmark\nstudy using data from a simulated, multi degree-of-freedom, linear system.\nFollowing application of Bayesian SSI, it is shown that the same posterior is\ntargeted and recovered by both inference schemes, with good agreement between\nthe posterior mean and the conventional SSI result. The second study applies\nthe variational form to data obtained from an in-service structure: The Z24\nbridge. The results of this study are presented at single model orders, and\nthen using a stabilisation diagram. The recovered posterior uncertainty is\npresented and compared to the classic SSI result. It is observed that the\nposterior distributions with mean values coinciding with the natural\nfrequencies exhibit lower variance than values situated away from the natural\nfrequencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of operational modal analysis (OMA), obtained modal information\nis frequently used to assess the current state of aerospace, mechanical,\noffshore and civil structures. However, the stochasticity of operational\nsystems and the lack of forcing information can lead to inconsistent results.\nQuantifying the uncertainty of the recovered modal parameters through OMA is\ntherefore of significant value. In this article, a new perspective on Bayesian\nOMA is proposed: a Bayesian stochastic subspace identification (SSI) algorithm.\nDistinct from existing approaches to Bayesian OMA, a hierarchical probabilistic\nmodel is embedded at the core of covariance-driven SSI. Through substitution of\ncanonical correlation analysis with a Bayesian equivalent, posterior\ndistributions over the modal properties are obtained. Two inference schemes are\npresented for the proposed Bayesian formulation: Markov Chain Monte Carlo and\nvariational Bayes. Two case studies are then explored. The first is benchmark\nstudy using data from a simulated, multi degree-of-freedom, linear system.\nFollowing application of Bayesian SSI, it is shown that the same posterior is\ntargeted and recovered by both inference schemes, with good agreement between\nthe posterior mean and the conventional SSI result. The second study applies\nthe variational form to data obtained from an in-service structure: The Z24\nbridge. The results of this study are presented at single model orders, and\nthen using a stabilisation diagram. The recovered posterior uncertainty is\npresented and compared to the classic SSI result. It is observed that the\nposterior distributions with mean values coinciding with the natural\nfrequencies exhibit lower variance than values situated away from the natural\nfrequencies."
                },
                "authors": [
                    {
                        "name": "Brandon J. O'Connell"
                    },
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Rogers"
                },
                "author": "Timothy J. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09916v1",
                "updated": "2024-08-19T11:44:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T11:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Tingting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Liu"
                },
                "author": "Tingting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06355v3",
                "updated": "2024-08-19T11:43:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    43,
                    18,
                    0,
                    232,
                    0
                ],
                "published": "2023-12-11T13:03:39Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    13,
                    3,
                    39,
                    0,
                    345,
                    0
                ],
                "title": "Linguistic and Structural Basis of Engineering Design Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic and Structural Basis of Engineering Design Knowledge"
                },
                "summary": "Natural language artefact descriptions are primary carriers of engineering\ndesign knowledge, whose retrieval, representation, and reuse are fundamental to\nsupporting knowledge-intensive tasks in the design process. In this paper, we\nexplicate design knowledge from patented artefact descriptions as knowledge\ngraphs and examine these to understand the linguistic and structural basis. The\npurpose of our work is to advance the traditional and ontological perspectives\nof design knowledge and to guide Large-Language Models (LLMs) on how to\narticulate natural language responses that reflect knowledge that is valuable\nin a design environment. We populate 33,881 knowledge graphs from a sample of\npatents stratified according to technology classes. For linguistic basis, we\nconduct Zipf distribution analyses on the frequencies of unique entities and\nrelationships to identify 64 and 37 generalisable linguistic syntaxes\nrespectively. The relationships largely represent attributes ('of'), structure\n('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification\n('such as'), and behaviour ('to', 'from'). For structural basis, we draw\ninspiration from various studies on biological/ecological networks and discover\nmotifs from patent knowledge graphs. We identify four 3-node and four 4-node\nsubgraph patterns that could be converged and simplified into sequence\n[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these\nresults, we suggest concretisation strategies for entities and relationships\nand explicating hierarchical structures, potentially aiding the construction\nand modularisation of design knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language artefact descriptions are primary carriers of engineering\ndesign knowledge, whose retrieval, representation, and reuse are fundamental to\nsupporting knowledge-intensive tasks in the design process. In this paper, we\nexplicate design knowledge from patented artefact descriptions as knowledge\ngraphs and examine these to understand the linguistic and structural basis. The\npurpose of our work is to advance the traditional and ontological perspectives\nof design knowledge and to guide Large-Language Models (LLMs) on how to\narticulate natural language responses that reflect knowledge that is valuable\nin a design environment. We populate 33,881 knowledge graphs from a sample of\npatents stratified according to technology classes. For linguistic basis, we\nconduct Zipf distribution analyses on the frequencies of unique entities and\nrelationships to identify 64 and 37 generalisable linguistic syntaxes\nrespectively. The relationships largely represent attributes ('of'), structure\n('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification\n('such as'), and behaviour ('to', 'from'). For structural basis, we draw\ninspiration from various studies on biological/ecological networks and discover\nmotifs from patent knowledge graphs. We identify four 3-node and four 4-node\nsubgraph patterns that could be converged and simplified into sequence\n[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these\nresults, we suggest concretisation strategies for entities and relationships\nand explicating hierarchical structures, potentially aiding the construction\nand modularisation of design knowledge."
                },
                "authors": [
                    {
                        "name": "L. Siddharth"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "arxiv_comment": "The data for this research is made available at Zenodo -\n  https://zenodo.org/doi/10.5281/zenodo.13328257",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13213v2",
                "updated": "2024-08-19T11:38:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    38,
                    14,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-19T04:53:48Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    4,
                    53,
                    48,
                    2,
                    171,
                    0
                ],
                "title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\n  Filtering with LLM-Extracted Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\n  Filtering with LLM-Extracted Metadata"
                },
                "summary": "The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG."
                },
                "authors": [
                    {
                        "name": "Mykhailo Poliakov"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Accepted to ICTERI 2024 Posters Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v1",
                "updated": "2024-08-19T11:09:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09889v1",
                "updated": "2024-08-19T11:05:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    5,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T11:05:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    5,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Towards a Field Based Bayesian Evidence Inference from Nested Sampling\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Field Based Bayesian Evidence Inference from Nested Sampling\n  Data"
                },
                "summary": "Nested sampling (NS) is a stochastic method for computing the log-evidence of\na Bayesian problem. It relies on stochastic estimates of prior volumes enclosed\nby likelihood contours, which limits the accuracy of the log-evidence\ncalculation. We propose to transform the prior volume estimation into a\nBayesian inference problem, which allows us to incorporate a smoothness\nassumption for likelihood-prior volume relations. As a result, we aim to\nincrease the accuracy of the volume estimates and thus improve the overall\nlog-evidence calculation using NS. The method presented works as a\npost-processing step for NS and provides posterior samples of the\nlikelihood-prior-volume relation, from which the log-evidence can be\ncalculated. We demonstrate an implementation of the algorithm and compare its\nresults with plain NS on two synthetic datasets for which the underlying\nevidence is known. We find a significant improvement in accuracy for runs with\nless than one hundred active samples in NS, but are prone to numerical problems\nbeyond this point.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested sampling (NS) is a stochastic method for computing the log-evidence of\na Bayesian problem. It relies on stochastic estimates of prior volumes enclosed\nby likelihood contours, which limits the accuracy of the log-evidence\ncalculation. We propose to transform the prior volume estimation into a\nBayesian inference problem, which allows us to incorporate a smoothness\nassumption for likelihood-prior volume relations. As a result, we aim to\nincrease the accuracy of the volume estimates and thus improve the overall\nlog-evidence calculation using NS. The method presented works as a\npost-processing step for NS and provides posterior samples of the\nlikelihood-prior-volume relation, from which the log-evidence can be\ncalculated. We demonstrate an implementation of the algorithm and compare its\nresults with plain NS on two synthetic datasets for which the underlying\nevidence is known. We find a significant improvement in accuracy for runs with\nless than one hundred active samples in NS, but are prone to numerical problems\nbeyond this point."
                },
                "authors": [
                    {
                        "name": "Margret Westerkamp"
                    },
                    {
                        "name": "Jakob Roth"
                    },
                    {
                        "name": "Philipp Frank"
                    },
                    {
                        "name": "Will Handley"
                    },
                    {
                        "name": "Torsten Enßlin"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Enßlin"
                },
                "author": "Torsten Enßlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09881v1",
                "updated": "2024-08-19T10:46:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    46,
                    19,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:46:19Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    46,
                    19,
                    0,
                    232,
                    0
                ],
                "title": "Uncertainty Quantification of Pre-Trained and Fine-Tuned Surrogate\n  Models using Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification of Pre-Trained and Fine-Tuned Surrogate\n  Models using Conformal Prediction"
                },
                "summary": "Data-driven surrogate models have shown immense potential as quick,\ninexpensive approximations to complex numerical and experimental modelling\ntasks. However, most surrogate models characterising physical systems do not\nquantify their uncertainty, rendering their predictions unreliable, and needing\nfurther validation. Though Bayesian approximations offer some solace in\nestimating the error associated with these models, they cannot provide they\ncannot provide guarantees, and the quality of their inferences depends on the\navailability of prior information and good approximations to posteriors for\ncomplex problems. This is particularly pertinent to multi-variable or\nspatio-temporal problems. Our work constructs and formalises a conformal\nprediction framework that satisfies marginal coverage for spatio-temporal\npredictions in a model-agnostic manner, requiring near-zero computational\ncosts. The paper provides an extensive empirical study of the application of\nthe framework to ascertain valid error bars that provide guaranteed coverage\nacross the surrogate model's domain of operation. The application scope of our\nwork extends across a large range of spatio-temporal models, ranging from\nsolving partial differential equations to weather forecasting. Through the\napplications, the paper looks at providing statistically valid error bars for\ndeterministic models, as well as crafting guarantees to the error bars of\nprobabilistic models. The paper concludes with a viable conformal prediction\nformalisation that provides guaranteed coverage of the surrogate model,\nregardless of model architecture, and its training regime and is unbothered by\nthe curse of dimensionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven surrogate models have shown immense potential as quick,\ninexpensive approximations to complex numerical and experimental modelling\ntasks. However, most surrogate models characterising physical systems do not\nquantify their uncertainty, rendering their predictions unreliable, and needing\nfurther validation. Though Bayesian approximations offer some solace in\nestimating the error associated with these models, they cannot provide they\ncannot provide guarantees, and the quality of their inferences depends on the\navailability of prior information and good approximations to posteriors for\ncomplex problems. This is particularly pertinent to multi-variable or\nspatio-temporal problems. Our work constructs and formalises a conformal\nprediction framework that satisfies marginal coverage for spatio-temporal\npredictions in a model-agnostic manner, requiring near-zero computational\ncosts. The paper provides an extensive empirical study of the application of\nthe framework to ascertain valid error bars that provide guaranteed coverage\nacross the surrogate model's domain of operation. The application scope of our\nwork extends across a large range of spatio-temporal models, ranging from\nsolving partial differential equations to weather forecasting. Through the\napplications, the paper looks at providing statistically valid error bars for\ndeterministic models, as well as crafting guarantees to the error bars of\nprobabilistic models. The paper concludes with a viable conformal prediction\nformalisation that provides guaranteed coverage of the surrogate model,\nregardless of model architecture, and its training regime and is unbothered by\nthe curse of dimensionality."
                },
                "authors": [
                    {
                        "name": "Vignesh Gopakumar"
                    },
                    {
                        "name": "Ander Gray"
                    },
                    {
                        "name": "Joel Oskarsson"
                    },
                    {
                        "name": "Lorenzo Zanisi"
                    },
                    {
                        "name": "Stanislas Pamela"
                    },
                    {
                        "name": "Daniel Giles"
                    },
                    {
                        "name": "Matt Kusner"
                    },
                    {
                        "name": "Marc Peter Deisenroth"
                    }
                ],
                "author_detail": {
                    "name": "Marc Peter Deisenroth"
                },
                "author": "Marc Peter Deisenroth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09878v1",
                "updated": "2024-08-19T10:39:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    39,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:39:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    39,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Transferring Backdoors between Large Language Models by Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferring Backdoors between Large Language Models by Knowledge\n  Distillation"
                },
                "summary": "Backdoor Attacks have been a serious vulnerability against Large Language\nModels (LLMs). However, previous methods only reveal such risk in specific\nmodels, or present tasks transferability after attacking the pre-trained phase.\nSo, how risky is the model transferability of a backdoor attack? In this paper,\nwe focus on whether existing mini-LLMs may be unconsciously instructed in\nbackdoor knowledge by poisoned teacher LLMs through knowledge distillation\n(KD). Specifically, we propose ATBA, an adaptive transferable backdoor attack,\nwhich can effectively distill the backdoor of teacher LLMs into small models\nwhen only executing clean-tuning. We first propose the Target Trigger\nGeneration (TTG) module that filters out a set of indicative trigger candidates\nfrom the token list based on cosine similarity distribution. Then, we exploit a\nshadow model to imitate the distilling process and introduce an Adaptive\nTrigger Optimization (ATO) module to realize a gradient-based greedy feedback\nto search optimal triggers. Extensive experiments show that ATBA generates not\nonly positive guidance for student models but also implicitly transfers\nbackdoor knowledge. Our attack is robust and stealthy, with over 80% backdoor\ntransferability, and hopes the attention of security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor Attacks have been a serious vulnerability against Large Language\nModels (LLMs). However, previous methods only reveal such risk in specific\nmodels, or present tasks transferability after attacking the pre-trained phase.\nSo, how risky is the model transferability of a backdoor attack? In this paper,\nwe focus on whether existing mini-LLMs may be unconsciously instructed in\nbackdoor knowledge by poisoned teacher LLMs through knowledge distillation\n(KD). Specifically, we propose ATBA, an adaptive transferable backdoor attack,\nwhich can effectively distill the backdoor of teacher LLMs into small models\nwhen only executing clean-tuning. We first propose the Target Trigger\nGeneration (TTG) module that filters out a set of indicative trigger candidates\nfrom the token list based on cosine similarity distribution. Then, we exploit a\nshadow model to imitate the distilling process and introduce an Adaptive\nTrigger Optimization (ATO) module to realize a gradient-based greedy feedback\nto search optimal triggers. Extensive experiments show that ATBA generates not\nonly positive guidance for student models but also implicitly transfers\nbackdoor knowledge. Our attack is robust and stealthy, with over 80% backdoor\ntransferability, and hopes the attention of security."
                },
                "authors": [
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Zhuosheng Zhang Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuosheng Zhang Gongshen Liu"
                },
                "author": "Zhuosheng Zhang Gongshen Liu",
                "arxiv_comment": "13 pages, 16 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03297v2",
                "updated": "2024-08-19T10:38:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    38,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-06T16:55:54Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    55,
                    54,
                    1,
                    219,
                    0
                ],
                "title": "KnowPO: Knowledge-aware Preference Optimization for Controllable\n  Knowledge Selection in Retrieval-Augmented Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowPO: Knowledge-aware Preference Optimization for Controllable\n  Knowledge Selection in Retrieval-Augmented Language Models"
                },
                "summary": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors such\nas contextual ignorance and contextual overinclusion. To this end, we propose a\nKnowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at\nachieving adaptive knowledge selection based on contextual relevance in real\nretrieval scenarios. Concretely, we proposed a general paradigm for\nconstructing knowledge conflict datasets, which comprehensively cover various\nerror types and learn how to avoid these negative signals through preference\noptimization methods. Simultaneously, we proposed a rewriting strategy and data\nratio optimization strategy to address preference imbalances. Experimental\nresults show that KnowPO outperforms previous methods for handling knowledge\nconflicts by over 37\\%, while also exhibiting robust generalization across\nvarious out-of-distribution datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors such\nas contextual ignorance and contextual overinclusion. To this end, we propose a\nKnowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at\nachieving adaptive knowledge selection based on contextual relevance in real\nretrieval scenarios. Concretely, we proposed a general paradigm for\nconstructing knowledge conflict datasets, which comprehensively cover various\nerror types and learn how to avoid these negative signals through preference\noptimization methods. Simultaneously, we proposed a rewriting strategy and data\nratio optimization strategy to address preference imbalances. Experimental\nresults show that KnowPO outperforms previous methods for handling knowledge\nconflicts by over 37\\%, while also exhibiting robust generalization across\nvarious out-of-distribution datasets."
                },
                "authors": [
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Yuzhen Xiao"
                    },
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09868v1",
                "updated": "2024-08-19T10:18:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    18,
                    34,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:18:34Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    18,
                    34,
                    0,
                    232,
                    0
                ],
                "title": "Weak instruments in multivariable Mendelian randomization: methods and\n  practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak instruments in multivariable Mendelian randomization: methods and\n  practice"
                },
                "summary": "The method of multivariable Mendelian randomization uses genetic variants to\ninstrument multiple exposures, to estimate the effect that a given exposure has\non an outcome conditional on all other exposures included in a linear model.\nUnfortunately, the inclusion of every additional exposure makes a weak\ninstruments problem more likely, because we require conditionally strong\ngenetic predictors of each exposure. This issue is well appreciated in\npractice, with different versions of F-statistics routinely reported as\nmeasures of instument strength. Less transparently, however, these F-statistics\nare sometimes used to guide instrument selection, and even to decide whether to\nreport empirical results. Rather than discarding findings with low\nF-statistics, weak instrument-robust methods can provide valid inference under\nweak instruments. For multivariable Mendelian randomization with two-sample\nsummary data, we encourage use of the inference strategy of Andrews (2018) that\nreports both robust and non-robust confidence sets, along with a statistic that\nmeasures how reliable the non-robust confidence set is in terms of coverage. We\nalso propose a novel adjusted-Kleibergen statistic that corrects for\noverdispersion heterogeneity in genetic associations with the outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The method of multivariable Mendelian randomization uses genetic variants to\ninstrument multiple exposures, to estimate the effect that a given exposure has\non an outcome conditional on all other exposures included in a linear model.\nUnfortunately, the inclusion of every additional exposure makes a weak\ninstruments problem more likely, because we require conditionally strong\ngenetic predictors of each exposure. This issue is well appreciated in\npractice, with different versions of F-statistics routinely reported as\nmeasures of instument strength. Less transparently, however, these F-statistics\nare sometimes used to guide instrument selection, and even to decide whether to\nreport empirical results. Rather than discarding findings with low\nF-statistics, weak instrument-robust methods can provide valid inference under\nweak instruments. For multivariable Mendelian randomization with two-sample\nsummary data, we encourage use of the inference strategy of Andrews (2018) that\nreports both robust and non-robust confidence sets, along with a statistic that\nmeasures how reliable the non-robust confidence set is in terms of coverage. We\nalso propose a novel adjusted-Kleibergen statistic that corrects for\noverdispersion heterogeneity in genetic associations with the outcome."
                },
                "authors": [
                    {
                        "name": "Ashish Patel"
                    },
                    {
                        "name": "James Lane"
                    },
                    {
                        "name": "Stephen Burgess"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Burgess"
                },
                "author": "Stephen Burgess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09865v1",
                "updated": "2024-08-19T10:12:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    12,
                    52,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:12:52Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    12,
                    52,
                    0,
                    232,
                    0
                ],
                "title": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in\n  Explainable Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in\n  Explainable Recommendation"
                },
                "summary": "Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models treat review-generation as a proxy of explainable recommendation.\nAlthough they are able to generate fluent and grammatical sentences, they\nsuffer from generality and hallucination issues. We propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it\nintegrates aspect category as another input dimension to facilitate the\nmemorization of fine-grained aspect terms. Experiments on two real-world review\ndatasets in restaurant domain show that MAPLE outperforms the baseline\nreview-generation models in terms of text and feature diversity while\nmaintaining excellent coherence and factual relevance. We further treat MAPLE\nas a retriever component in the retriever-reader framework and employ a\nLarge-Language Model (LLM) as the reader, showing that MAPLE's explanation\nalong with the LLM's comprehension ability leads to enriched and personalized\nexplanation as a result. We will release the code and data in this http upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models treat review-generation as a proxy of explainable recommendation.\nAlthough they are able to generate fluent and grammatical sentences, they\nsuffer from generality and hallucination issues. We propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it\nintegrates aspect category as another input dimension to facilitate the\nmemorization of fine-grained aspect terms. Experiments on two real-world review\ndatasets in restaurant domain show that MAPLE outperforms the baseline\nreview-generation models in terms of text and feature diversity while\nmaintaining excellent coherence and factual relevance. We further treat MAPLE\nas a retriever component in the retriever-reader framework and employ a\nLarge-Language Model (LLM) as the reader, showing that MAPLE's explanation\nalong with the LLM's comprehension ability leads to enriched and personalized\nexplanation as a result. We will release the code and data in this http upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Ching-Wen Yang"
                    },
                    {
                        "name": "Che Wei Chen"
                    },
                    {
                        "name": "Kun-da Wu"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Jui-Feng Yao"
                    },
                    {
                        "name": "Hung-Yu Kao"
                    }
                ],
                "author_detail": {
                    "name": "Hung-Yu Kao"
                },
                "author": "Hung-Yu Kao",
                "arxiv_comment": "8 main pages, 10 pages for appendix. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09856v1",
                "updated": "2024-08-19T09:58:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    58,
                    53,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    58,
                    53,
                    0,
                    232,
                    0
                ],
                "title": "TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and\n  Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and\n  Competition"
                },
                "summary": "While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have\neffectively addressed GPU memory constraints during fine-tuning, their\nperformance often falls short, especially in multidimensional task scenarios.\nTo address this issue, one straightforward solution is to introduce\ntask-specific LoRA modules as domain experts, leveraging the modeling of\nmultiple experts' capabilities and thus enhancing the general capability of\nmulti-task learning. Despite promising, these additional components often add\ncomplexity to the training and inference process, contravening the efficient\ncharacterization of PEFT designed for. Considering this, we introduce an\ninnovative PEFT method, TeamLoRA, consisting of a collaboration and competition\nmodule for experts, and thus achieving the right balance of effectiveness and\nefficiency: (i) For collaboration, a novel knowledge-sharing and -organizing\nmechanism is devised to appropriately reduce the scale of matrix operations,\nthereby boosting the training and inference speed. (ii) For competition, we\npropose leveraging a game-theoretic interaction mechanism for experts,\nencouraging experts to transfer their domain-specific knowledge while facing\ndiverse downstream tasks, and thus enhancing the performance. By doing so,\nTeamLoRA elegantly connects the experts as a \"Team\" with internal collaboration\nand competition, enabling a faster and more accurate PEFT paradigm for\nmulti-task learning. To validate the superiority of TeamLoRA, we curate a\ncomprehensive multi-task evaluation(CME) benchmark to thoroughly assess the\ncapability of multi-task learning. Experiments conducted on our CME and other\nbenchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project\nis available at https://github.com/Lin-Tianwei/TeamLoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have\neffectively addressed GPU memory constraints during fine-tuning, their\nperformance often falls short, especially in multidimensional task scenarios.\nTo address this issue, one straightforward solution is to introduce\ntask-specific LoRA modules as domain experts, leveraging the modeling of\nmultiple experts' capabilities and thus enhancing the general capability of\nmulti-task learning. Despite promising, these additional components often add\ncomplexity to the training and inference process, contravening the efficient\ncharacterization of PEFT designed for. Considering this, we introduce an\ninnovative PEFT method, TeamLoRA, consisting of a collaboration and competition\nmodule for experts, and thus achieving the right balance of effectiveness and\nefficiency: (i) For collaboration, a novel knowledge-sharing and -organizing\nmechanism is devised to appropriately reduce the scale of matrix operations,\nthereby boosting the training and inference speed. (ii) For competition, we\npropose leveraging a game-theoretic interaction mechanism for experts,\nencouraging experts to transfer their domain-specific knowledge while facing\ndiverse downstream tasks, and thus enhancing the performance. By doing so,\nTeamLoRA elegantly connects the experts as a \"Team\" with internal collaboration\nand competition, enabling a faster and more accurate PEFT paradigm for\nmulti-task learning. To validate the superiority of TeamLoRA, we curate a\ncomprehensive multi-task evaluation(CME) benchmark to thoroughly assess the\ncapability of multi-task learning. Experiments conducted on our CME and other\nbenchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project\nis available at https://github.com/Lin-Tianwei/TeamLoRA."
                },
                "authors": [
                    {
                        "name": "Tianwei Lin"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Zhaocheng Li"
                    },
                    {
                        "name": "Yang Dai"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09853v1",
                "updated": "2024-08-19T09:57:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    57,
                    28,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:57:28Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    57,
                    28,
                    0,
                    232,
                    0
                ],
                "title": "Self-Directed Turing Test for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Directed Turing Test for Large Language Models"
                },
                "summary": "The Turing test examines whether AIs can exhibit human-like behaviour in\nnatural language conversations. Traditional Turing tests adopt a rigid dialogue\nformat where each participant sends only one message each time and require\ncontinuous human involvement to direct the entire interaction with the test\nsubject. This fails to reflect a natural conversational style and hinders the\nevaluation of Large Language Models (LLMs) in complex and prolonged dialogues.\nThis paper proposes the Self-Directed Turing Test, which extends the original\ntest with a burst dialogue format, allowing more dynamic exchanges by multiple\nconsecutive messages. It further efficiently reduces human workload by having\nthe LLM self-direct the majority of the test process, iteratively generating\ndialogues that simulate its interaction with humans. With the pseudo-dialogue\nhistory, the model then engages in a shorter dialogue with a human, which is\npaired with a human-human conversation on the same topic to be judged using\nquestionnaires. We introduce the X-Turn Pass-Rate metric to assess the human\nlikeness of LLMs across varying durations. While LLMs like GPT-4 initially\nperform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10\nturns of dialogues respectively, their performance drops as the dialogue\nprogresses, which underscores the difficulty in maintaining consistency in the\nlong term.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing test examines whether AIs can exhibit human-like behaviour in\nnatural language conversations. Traditional Turing tests adopt a rigid dialogue\nformat where each participant sends only one message each time and require\ncontinuous human involvement to direct the entire interaction with the test\nsubject. This fails to reflect a natural conversational style and hinders the\nevaluation of Large Language Models (LLMs) in complex and prolonged dialogues.\nThis paper proposes the Self-Directed Turing Test, which extends the original\ntest with a burst dialogue format, allowing more dynamic exchanges by multiple\nconsecutive messages. It further efficiently reduces human workload by having\nthe LLM self-direct the majority of the test process, iteratively generating\ndialogues that simulate its interaction with humans. With the pseudo-dialogue\nhistory, the model then engages in a shorter dialogue with a human, which is\npaired with a human-human conversation on the same topic to be judged using\nquestionnaires. We introduce the X-Turn Pass-Rate metric to assess the human\nlikeness of LLMs across varying durations. While LLMs like GPT-4 initially\nperform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10\nturns of dialogues respectively, their performance drops as the dialogue\nprogresses, which underscores the difficulty in maintaining consistency in the\nlong term."
                },
                "authors": [
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09849v1",
                "updated": "2024-08-19T09:51:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    51,
                    2,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:51:02Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    51,
                    2,
                    0,
                    232,
                    0
                ],
                "title": "Importance Weighting Can Help Large Language Models Self-Improve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance Weighting Can Help Large Language Models Self-Improve"
                },
                "summary": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models."
                },
                "authors": [
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Chi-min Chan"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10868v3",
                "updated": "2024-08-20T09:25:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    25,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-16T09:36:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    36,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts"
                },
                "summary": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09834v1",
                "updated": "2024-08-19T09:29:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:29:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Minor DPO reject penalty to increase training robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor DPO reject penalty to increase training robustness"
                },
                "summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    },
                    {
                        "name": "Yingfan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yingfan Hu"
                },
                "author": "Yingfan Hu",
                "arxiv_comment": "8 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09831v1",
                "updated": "2024-08-19T09:27:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    27,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:27:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    27,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions"
                },
                "summary": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Most evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required, such as health, and\nwhere misleading or incorrect answers can have a significant impact on a user's\nhealth. Using human experts to evaluate the quality of LLM answers is generally\nconsidered the gold standard, but expert annotation is costly and slow. We\npresent a method for evaluating LLM answers that uses ranking signals as a\nsubstitute for explicit relevance judgements. Our scoring method correlates\nwith the preferences of human experts. We validate it by investigating the\nwell-known fact that the quality of generated answers improves with the size of\nthe model as well as with more sophisticated prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Most evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required, such as health, and\nwhere misleading or incorrect answers can have a significant impact on a user's\nhealth. Using human experts to evaluate the quality of LLM answers is generally\nconsidered the gold standard, but expert annotation is costly and slow. We\npresent a method for evaluating LLM answers that uses ranking signals as a\nsubstitute for explicit relevance judgements. Our scoring method correlates\nwith the preferences of human experts. We validate it by investigating the\nwell-known fact that the quality of generated answers improves with the size of\nthe model as well as with more sophisticated prompting strategies."
                },
                "authors": [
                    {
                        "name": "Sebastian Heineking"
                    },
                    {
                        "name": "Jonas Probst"
                    },
                    {
                        "name": "Daniel Steinbach"
                    },
                    {
                        "name": "Martin Potthast"
                    },
                    {
                        "name": "Harrisen Scells"
                    }
                ],
                "author_detail": {
                    "name": "Harrisen Scells"
                },
                "author": "Harrisen Scells",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09819v1",
                "updated": "2024-08-19T09:15:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    15,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:15:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    15,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language\n  Models"
                },
                "summary": "What a large language model (LLM) would respond in ethically relevant\ncontext? In this paper, we curate a large benchmark CMoralEval for morality\nevaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a\nChinese TV program discussing Chinese moral norms with stories from the society\nand 2) a collection of Chinese moral anomies from various newspapers and\nacademic papers on morality. With these sources, we aim to create a moral\nevaluation dataset characterized by diversity and authenticity. We develop a\nmorality taxonomy and a set of fundamental moral principles that are not only\nrooted in traditional Chinese culture but also consistent with contemporary\nsocietal norms. To facilitate efficient construction and annotation of\ninstances in CMoralEval, we establish a platform with AI-assisted instance\ngeneration to streamline the annotation process. These help us curate\nCMoralEval that encompasses both explicit moral scenarios (14,964 instances)\nand moral dilemma scenarios (15,424 instances), each with instances from\ndifferent data sources. We conduct extensive experiments with CMoralEval to\nexamine a variety of Chinese LLMs. Experiment results demonstrate that\nCMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly\navailable at \\url{https://github.com/tjunlp-lab/CMoralEval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What a large language model (LLM) would respond in ethically relevant\ncontext? In this paper, we curate a large benchmark CMoralEval for morality\nevaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a\nChinese TV program discussing Chinese moral norms with stories from the society\nand 2) a collection of Chinese moral anomies from various newspapers and\nacademic papers on morality. With these sources, we aim to create a moral\nevaluation dataset characterized by diversity and authenticity. We develop a\nmorality taxonomy and a set of fundamental moral principles that are not only\nrooted in traditional Chinese culture but also consistent with contemporary\nsocietal norms. To facilitate efficient construction and annotation of\ninstances in CMoralEval, we establish a platform with AI-assisted instance\ngeneration to streamline the annotation process. These help us curate\nCMoralEval that encompasses both explicit moral scenarios (14,964 instances)\nand moral dilemma scenarios (15,424 instances), each with instances from\ndifferent data sources. We conduct extensive experiments with CMoralEval to\nexamine a variety of Chinese LLMs. Experiment results demonstrate that\nCMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly\navailable at \\url{https://github.com/tjunlp-lab/CMoralEval}."
                },
                "authors": [
                    {
                        "name": "Linhao Yu"
                    },
                    {
                        "name": "Yongqi Leng"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Shang Wu"
                    },
                    {
                        "name": "Haixin Liu"
                    },
                    {
                        "name": "Xinmeng Ji"
                    },
                    {
                        "name": "Jiahui Zhao"
                    },
                    {
                        "name": "Jinwang Song"
                    },
                    {
                        "name": "Tingting Cui"
                    },
                    {
                        "name": "Xiaoqing Cheng"
                    },
                    {
                        "name": "Tao Liu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "Accepted by ACL 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11441v2",
                "updated": "2024-08-19T08:50:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    50,
                    54,
                    0,
                    232,
                    0
                ],
                "published": "2024-05-19T04:31:54Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    4,
                    31,
                    54,
                    6,
                    140,
                    0
                ],
                "title": "EmbSum: Leveraging the Summarization Capabilities of Large Language\n  Models for Content-Based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbSum: Leveraging the Summarization Capabilities of Large Language\n  Models for Content-Based Recommendations"
                },
                "summary": "Content-based recommendation systems play a crucial role in delivering\npersonalized content to users in the digital world. In this work, we introduce\nEmbSum, a novel framework that enables offline pre-computations of users and\ncandidate items while capturing the interactions within the user engagement\nhistory. By utilizing the pretrained encoder-decoder model and poly-attention\nlayers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding\n(CPE) to calculate relevance scores between users and candidate items. EmbSum\nactively learns the long user engagement histories by generating user-interest\nsummary with supervision from large language model (LLM). The effectiveness of\nEmbSum is validated on two datasets from different domains, surpassing\nstate-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\nAdditionally, the model's ability to generate summaries of user interests\nserves as a valuable by-product, enhancing its usefulness for personalized\ncontent recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-based recommendation systems play a crucial role in delivering\npersonalized content to users in the digital world. In this work, we introduce\nEmbSum, a novel framework that enables offline pre-computations of users and\ncandidate items while capturing the interactions within the user engagement\nhistory. By utilizing the pretrained encoder-decoder model and poly-attention\nlayers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding\n(CPE) to calculate relevance scores between users and candidate items. EmbSum\nactively learns the long user engagement histories by generating user-interest\nsummary with supervision from large language model (LLM). The effectiveness of\nEmbSum is validated on two datasets from different domains, surpassing\nstate-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\nAdditionally, the model's ability to generate summaries of user interests\nserves as a valuable by-product, enhancing its usefulness for personalized\ncontent recommendations."
                },
                "authors": [
                    {
                        "name": "Chiyu Zhang"
                    },
                    {
                        "name": "Yifei Sun"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Jie Lei"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Angli Liu"
                    },
                    {
                        "name": "Ji Zhu"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Ning Yao"
                    },
                    {
                        "name": "Bo Long"
                    }
                ],
                "author_detail": {
                    "name": "Bo Long"
                },
                "author": "Bo Long",
                "arxiv_comment": "Accepted by RecSys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19965v2",
                "updated": "2024-08-19T08:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    48,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-29T12:55:40Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    12,
                    55,
                    40,
                    0,
                    211,
                    0
                ],
                "title": "Simply Trainable Nearest Neighbour Machine Translation with GPU\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simply Trainable Nearest Neighbour Machine Translation with GPU\n  Inference"
                },
                "summary": "Nearest neighbor machine translation is a successful approach for fast domain\nadaption, which interpolates the pre-trained transformers with domain-specific\ntoken-level k-nearest-neighbor (kNN) retrieval without retraining. Despite kNN\nMT's success, searching large reference corpus and fixed interpolation between\nthe kNN and pre-trained model led to computational complexity and translation\nquality challenges. Among other papers, Dai et al. proposed methods to obtain a\nsmall number of reference samples dynamically for which they introduced a\ndistance-aware interpolation method using an equation that includes free\nparameters. This paper proposes a simply trainable nearest neighbor machine\ntranslation and carry out inference experiments on GPU. Similar to Dai et al.,\nwe first adaptively construct a small datastore for each input sentence.\nSecond, we train a single-layer network for the interpolation coefficient\nbetween the knnMT and pre-trained result to automatically interpolate in\ndifferent domains. Experimental results on different domains show that our\nproposed method either improves or sometimes maintain the translation quality\nof methods in Dai et al. while being automatic. In addition, our GPU inference\nresults demonstrate that knnMT can be integrated into GPUs with a drop of only\n5% in terms of speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest neighbor machine translation is a successful approach for fast domain\nadaption, which interpolates the pre-trained transformers with domain-specific\ntoken-level k-nearest-neighbor (kNN) retrieval without retraining. Despite kNN\nMT's success, searching large reference corpus and fixed interpolation between\nthe kNN and pre-trained model led to computational complexity and translation\nquality challenges. Among other papers, Dai et al. proposed methods to obtain a\nsmall number of reference samples dynamically for which they introduced a\ndistance-aware interpolation method using an equation that includes free\nparameters. This paper proposes a simply trainable nearest neighbor machine\ntranslation and carry out inference experiments on GPU. Similar to Dai et al.,\nwe first adaptively construct a small datastore for each input sentence.\nSecond, we train a single-layer network for the interpolation coefficient\nbetween the knnMT and pre-trained result to automatically interpolate in\ndifferent domains. Experimental results on different domains show that our\nproposed method either improves or sometimes maintain the translation quality\nof methods in Dai et al. while being automatic. In addition, our GPU inference\nresults demonstrate that knnMT can be integrated into GPUs with a drop of only\n5% in terms of speed."
                },
                "authors": [
                    {
                        "name": "Hossam Amer"
                    },
                    {
                        "name": "Abdelrahman Abouelenin"
                    },
                    {
                        "name": "Mohamed Maher"
                    },
                    {
                        "name": "Evram Narouz"
                    },
                    {
                        "name": "Mohamed Afify"
                    },
                    {
                        "name": "Hany Awadallah"
                    }
                ],
                "author_detail": {
                    "name": "Hany Awadallah"
                },
                "author": "Hany Awadallah",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09803v1",
                "updated": "2024-08-19T08:47:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    47,
                    51,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:47:51Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    47,
                    51,
                    0,
                    232,
                    0
                ],
                "title": "Non-perturbative phase boundaries in the Gross-Neveu model from a\n  stability analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-perturbative phase boundaries in the Gross-Neveu model from a\n  stability analysis"
                },
                "summary": "Two out of three phase boundaries of the 1+1 dimensional Gross-Neveu model in\nthe chiral limit can be obtained from a standard, perturbative stability\nanalysis of the homogeneous phases. The third one separating the massive\nhomogeneous phase from the kink crystal is non-perturbative and could so far\nonly be inferred from the full solution of the model. We show that this phase\nboundary can also be obtained via a modified stability analysis, based on the\nthermodynamic potential of a single kink or baryon. The same method works for\nthe massive Gross-Neveu model, so that all phase boundaries of the Gross-Neveu\nmodel could have been predicted quantitatively without prior knowledge of the\nfull crystal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two out of three phase boundaries of the 1+1 dimensional Gross-Neveu model in\nthe chiral limit can be obtained from a standard, perturbative stability\nanalysis of the homogeneous phases. The third one separating the massive\nhomogeneous phase from the kink crystal is non-perturbative and could so far\nonly be inferred from the full solution of the model. We show that this phase\nboundary can also be obtained via a modified stability analysis, based on the\nthermodynamic potential of a single kink or baryon. The same method works for\nthe massive Gross-Neveu model, so that all phase boundaries of the Gross-Neveu\nmodel could have been predicted quantitatively without prior knowledge of the\nfull crystal solution."
                },
                "authors": [
                    {
                        "name": "Michael Thies"
                    }
                ],
                "author_detail": {
                    "name": "Michael Thies"
                },
                "author": "Michael Thies",
                "arxiv_comment": "15 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09798v1",
                "updated": "2024-08-19T08:44:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    44,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:44:55Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    44,
                    55,
                    0,
                    232,
                    0
                ],
                "title": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with\n  Adversarial Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with\n  Adversarial Prompting"
                },
                "summary": "Converting different modalities into generalized text, which then serves as\ninput prompts for large language models (LLMs), is a common approach for\naligning multimodal models, particularly when pairwise data is limited.\nText-centric alignment method leverages the unique properties of text as a\nmodality space, transforming diverse inputs into a unified textual\nrepresentation, thereby enabling downstream models to effectively interpret\nvarious modal inputs. This study evaluates the quality and robustness of\nmultimodal representations in the face of noise imperfections, dynamic input\norder permutations, and missing modalities, revealing that current text-centric\nalignment methods can compromise downstream robustness. To address this issue,\nwe propose a new text-centric adversarial training approach that significantly\nenhances robustness compared to traditional robust training methods and\npre-trained multimodal foundation models. Our findings underscore the potential\nof this approach to improve the robustness and adaptability of multimodal\nrepresentations, offering a promising solution for dynamic and real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting different modalities into generalized text, which then serves as\ninput prompts for large language models (LLMs), is a common approach for\naligning multimodal models, particularly when pairwise data is limited.\nText-centric alignment method leverages the unique properties of text as a\nmodality space, transforming diverse inputs into a unified textual\nrepresentation, thereby enabling downstream models to effectively interpret\nvarious modal inputs. This study evaluates the quality and robustness of\nmultimodal representations in the face of noise imperfections, dynamic input\norder permutations, and missing modalities, revealing that current text-centric\nalignment methods can compromise downstream robustness. To address this issue,\nwe propose a new text-centric adversarial training approach that significantly\nenhances robustness compared to traditional robust training methods and\npre-trained multimodal foundation models. Our findings underscore the potential\nof this approach to improve the robustness and adaptability of multimodal\nrepresentations, offering a promising solution for dynamic and real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Yun-Da Tsai"
                    },
                    {
                        "name": "Ting-Yu Yen"
                    },
                    {
                        "name": "Keng-Te Liao"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.05036",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09794v1",
                "updated": "2024-08-19T08:41:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    41,
                    40,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:41:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    41,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "AutoML-guided Fusion of Entity and LLM-based representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoML-guided Fusion of Entity and LLM-based representations"
                },
                "summary": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (embeddings) do not efficiently\nexploit these resources. Dense and robust representations of documents are\nessential for effectively solving downstream classification and retrieval\ntasks. This work demonstrates that injecting embedded information from\nknowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (embeddings) do not efficiently\nexploit these resources. Dense and robust representations of documents are\nessential for effectively solving downstream classification and retrieval\ntasks. This work demonstrates that injecting embedded information from\nknowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets."
                },
                "authors": [
                    {
                        "name": "Boshko Koloski"
                    },
                    {
                        "name": "Senja Pollak"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Blaž Škrlj"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Škrlj"
                },
                "author": "Blaž Škrlj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09785v1",
                "updated": "2024-08-19T08:22:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    22,
                    20,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:22:20Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    22,
                    20,
                    0,
                    232,
                    0
                ],
                "title": "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining\n  Automotive Software Release Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining\n  Automotive Software Release Decision-Making"
                },
                "summary": "Traditional methods for making software deployment decisions in the\nautomotive industry typically rely on manual analysis of tabular software test\ndata. These methods often lead to higher costs and delays in the software\nrelease cycle due to their labor-intensive nature. Large Language Models (LLMs)\npresent a promising solution to these challenges. However, their application\ngenerally demands multiple rounds of human-driven prompt engineering, which\nlimits their practical deployment, particularly for industrial end-users who\nneed reliable and efficient results. In this paper, we propose GoNoGo, an LLM\nagent system designed to streamline automotive software deployment while\nmeeting both functional requirements and practical industrial constraints.\nUnlike previous systems, GoNoGo is specifically tailored to address\ndomain-specific and risk-sensitive systems. We evaluate GoNoGo's performance\nacross different task difficulties using zero-shot and few-shot examples taken\nfrom industrial practice. Our results show that GoNoGo achieves a 100% success\nrate for tasks up to Level 2 difficulty with 3-shot examples, and maintains\nhigh performance even for more complex tasks. We find that GoNoGo effectively\nautomates decision-making for simpler tasks, significantly reducing the need\nfor manual intervention. In summary, GoNoGo represents an efficient and\nuser-friendly LLM-based solution currently employed in our industrial partner's\ncompany to assist with software release decision-making, supporting more\ninformed and timely decisions in the release process for risk-sensitive vehicle\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for making software deployment decisions in the\nautomotive industry typically rely on manual analysis of tabular software test\ndata. These methods often lead to higher costs and delays in the software\nrelease cycle due to their labor-intensive nature. Large Language Models (LLMs)\npresent a promising solution to these challenges. However, their application\ngenerally demands multiple rounds of human-driven prompt engineering, which\nlimits their practical deployment, particularly for industrial end-users who\nneed reliable and efficient results. In this paper, we propose GoNoGo, an LLM\nagent system designed to streamline automotive software deployment while\nmeeting both functional requirements and practical industrial constraints.\nUnlike previous systems, GoNoGo is specifically tailored to address\ndomain-specific and risk-sensitive systems. We evaluate GoNoGo's performance\nacross different task difficulties using zero-shot and few-shot examples taken\nfrom industrial practice. Our results show that GoNoGo achieves a 100% success\nrate for tasks up to Level 2 difficulty with 3-shot examples, and maintains\nhigh performance even for more complex tasks. We find that GoNoGo effectively\nautomates decision-making for simpler tasks, significantly reducing the need\nfor manual intervention. In summary, GoNoGo represents an efficient and\nuser-friendly LLM-based solution currently employed in our industrial partner's\ncompany to assist with software release decision-making, supporting more\ninformed and timely decisions in the release process for risk-sensitive vehicle\nsystems."
                },
                "authors": [
                    {
                        "name": "Arsham Gholamzadeh Khoee"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Andris Freimanis"
                    },
                    {
                        "name": "Patrick Andersson"
                    },
                    {
                        "name": "Dhasarathy Parthasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Dhasarathy Parthasarathy"
                },
                "author": "Dhasarathy Parthasarathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09773v1",
                "updated": "2024-08-19T08:01:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    1,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:01:11Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    1,
                    11,
                    0,
                    232,
                    0
                ],
                "title": "Are Large Language Models More Honest in Their Probabilistic or\n  Verbalized Confidence?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models More Honest in Their Probabilistic or\n  Verbalized Confidence?"
                },
                "summary": "Large language models (LLMs) have been found to produce hallucinations when\nthe question exceeds their internal knowledge boundaries. A reliable model\nshould have a clear perception of its knowledge boundaries, providing correct\nanswers within its scope and refusing to answer when it lacks knowledge.\nExisting research on LLMs' perception of their knowledge boundaries typically\nuses either the probability of the generated tokens or the verbalized\nconfidence as the model's confidence in its response. However, these studies\noverlook the differences and connections between the two. In this paper, we\nconduct a comprehensive analysis and comparison of LLMs' probabilistic\nperception and verbalized perception of their factual knowledge boundaries.\nFirst, we investigate the pros and cons of these two perceptions. Then, we\nstudy how they change under questions of varying frequencies. Finally, we\nmeasure the correlation between LLMs' probabilistic confidence and verbalized\nconfidence. Experimental results show that 1) LLMs' probabilistic perception is\ngenerally more accurate than verbalized perception but requires an in-domain\nvalidation set to adjust the confidence threshold. 2) Both perceptions perform\nbetter on less frequent questions. 3) It is challenging for LLMs to accurately\nexpress their internal confidence in natural language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been found to produce hallucinations when\nthe question exceeds their internal knowledge boundaries. A reliable model\nshould have a clear perception of its knowledge boundaries, providing correct\nanswers within its scope and refusing to answer when it lacks knowledge.\nExisting research on LLMs' perception of their knowledge boundaries typically\nuses either the probability of the generated tokens or the verbalized\nconfidence as the model's confidence in its response. However, these studies\noverlook the differences and connections between the two. In this paper, we\nconduct a comprehensive analysis and comparison of LLMs' probabilistic\nperception and verbalized perception of their factual knowledge boundaries.\nFirst, we investigate the pros and cons of these two perceptions. Then, we\nstudy how they change under questions of varying frequencies. Finally, we\nmeasure the correlation between LLMs' probabilistic confidence and verbalized\nconfidence. Experimental results show that 1) LLMs' probabilistic perception is\ngenerally more accurate than verbalized perception but requires an in-domain\nvalidation set to adjust the confidence threshold. 2) Both perceptions perform\nbetter on less frequent questions. 3) It is challenging for LLMs to accurately\nexpress their internal confidence in natural language."
                },
                "authors": [
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Lulu Yu"
                    },
                    {
                        "name": "Jiafeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jiafeng Guo"
                },
                "author": "Jiafeng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02889v3",
                "updated": "2024-08-19T07:53:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    53,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-05T11:50:01Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    11,
                    50,
                    1,
                    1,
                    65,
                    0
                ],
                "title": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated\n  Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated\n  Answers"
                },
                "summary": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 87% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\nof 81%, all without relying on external knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 87% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\nof 81%, all without relying on external knowledge."
                },
                "authors": [
                    {
                        "name": "Yakir Yehuda"
                    },
                    {
                        "name": "Itzik Malkiel"
                    },
                    {
                        "name": "Oren Barkan"
                    },
                    {
                        "name": "Jonathan Weill"
                    },
                    {
                        "name": "Royi Ronen"
                    },
                    {
                        "name": "Noam Koenigstein"
                    }
                ],
                "author_detail": {
                    "name": "Noam Koenigstein"
                },
                "author": "Noam Koenigstein",
                "arxiv_journal_ref": "https://aclanthology.org/2024.acl-long.506/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06063v2",
                "updated": "2024-08-19T07:50:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    50,
                    54,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-09T07:02:14Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    7,
                    2,
                    14,
                    1,
                    100,
                    0
                ],
                "title": "Heuristic-enhanced Candidates Selection strategy for GPTs tackle\n  Few-Shot Aspect-Based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic-enhanced Candidates Selection strategy for GPTs tackle\n  Few-Shot Aspect-Based Sentiment Analysis"
                },
                "summary": "Few-Shot Aspect-Based Sentiment Analysis (FSABSA) is an indispensable and\nhighly challenging task in natural language processing. However, methods based\non Pre-trained Language Models (PLMs) struggle to accommodate multiple\nsub-tasks, and methods based on Generative Pre-trained Transformers (GPTs)\nperform poorly. To address the above issues, the paper designs a\nHeuristic-enhanced Candidates Selection (HCS) strategy and further proposes All\nin One (AiO) model based on it. The model works in a two-stage, which\nsimultaneously accommodates the accuracy of PLMs and the generalization\ncapability of GPTs. Specifically, in the first stage, a backbone model based on\nPLMs generates rough heuristic candidates for the input sentence. In the second\nstage, AiO leverages LLMs' contextual learning capabilities to generate precise\npredictions. The study conducted comprehensive comparative and ablation\nexperiments on five benchmark datasets. The experimental results demonstrate\nthat the proposed model can better adapt to multiple sub-tasks, and also\noutperforms the methods that directly utilize GPTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Aspect-Based Sentiment Analysis (FSABSA) is an indispensable and\nhighly challenging task in natural language processing. However, methods based\non Pre-trained Language Models (PLMs) struggle to accommodate multiple\nsub-tasks, and methods based on Generative Pre-trained Transformers (GPTs)\nperform poorly. To address the above issues, the paper designs a\nHeuristic-enhanced Candidates Selection (HCS) strategy and further proposes All\nin One (AiO) model based on it. The model works in a two-stage, which\nsimultaneously accommodates the accuracy of PLMs and the generalization\ncapability of GPTs. Specifically, in the first stage, a backbone model based on\nPLMs generates rough heuristic candidates for the input sentence. In the second\nstage, AiO leverages LLMs' contextual learning capabilities to generate precise\npredictions. The study conducted comprehensive comparative and ablation\nexperiments on five benchmark datasets. The experimental results demonstrate\nthat the proposed model can better adapt to multiple sub-tasks, and also\noutperforms the methods that directly utilize GPTs."
                },
                "authors": [
                    {
                        "name": "Baoxing Jiang"
                    },
                    {
                        "name": "Yujie Wan"
                    },
                    {
                        "name": "Shenggen Ju"
                    }
                ],
                "author_detail": {
                    "name": "Shenggen Ju"
                },
                "author": "Shenggen Ju",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09760v1",
                "updated": "2024-08-19T07:41:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    41,
                    29,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:41:29Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    41,
                    29,
                    0,
                    232,
                    0
                ],
                "title": "Regional and spatial dependence of poverty factors in Thailand, and its\n  use into Bayesian hierarchical regression analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regional and spatial dependence of poverty factors in Thailand, and its\n  use into Bayesian hierarchical regression analysis"
                },
                "summary": "Poverty is a serious issue that harms humanity progression. The simplest\nsolution is to use one-shirt-size policy to alleviate it. Nevertheless, each\nregion has its unique issues, which require a unique solution to solve them. In\nthe aspect of spatial analysis, neighbor regions can provide useful information\nto analyze issues of a given region. In this work, we proposed inferred\nboundaries of regions of Thailand that can explain better the poverty dynamics,\ninstead of the usual government administrative regions. The proposed regions\nmaximize a trade-off between poverty-related features and geographical\ncoherence. We use a spatial analysis together with Moran's cluster algorithms\nand Bayesian hierarchical regression models, with the potential of assist the\nimplementation of the right policy to alleviate the poverty phenomenon. We\nfound that all variables considered show a positive spatial autocorrelation.\nThe results of analysis illustrate that 1) Northern, Northeastern Thailand, and\nin less extend Northcentral Thailand are the regions that require more\nattention in the aspect of poverty issues, 2) Northcentral, Northeastern,\nNorthern and Southern Thailand present dramatically low levels of education,\nincome and amount of savings contrasted with large cities such as\nBangkok-Pattaya and Central Thailand, and 3) Bangkok-Pattaya is the only region\nwhose average years of education is above 12 years, which corresponds (approx.)\nwith a complete senior high school.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poverty is a serious issue that harms humanity progression. The simplest\nsolution is to use one-shirt-size policy to alleviate it. Nevertheless, each\nregion has its unique issues, which require a unique solution to solve them. In\nthe aspect of spatial analysis, neighbor regions can provide useful information\nto analyze issues of a given region. In this work, we proposed inferred\nboundaries of regions of Thailand that can explain better the poverty dynamics,\ninstead of the usual government administrative regions. The proposed regions\nmaximize a trade-off between poverty-related features and geographical\ncoherence. We use a spatial analysis together with Moran's cluster algorithms\nand Bayesian hierarchical regression models, with the potential of assist the\nimplementation of the right policy to alleviate the poverty phenomenon. We\nfound that all variables considered show a positive spatial autocorrelation.\nThe results of analysis illustrate that 1) Northern, Northeastern Thailand, and\nin less extend Northcentral Thailand are the regions that require more\nattention in the aspect of poverty issues, 2) Northcentral, Northeastern,\nNorthern and Southern Thailand present dramatically low levels of education,\nincome and amount of savings contrasted with large cities such as\nBangkok-Pattaya and Central Thailand, and 3) Bangkok-Pattaya is the only region\nwhose average years of education is above 12 years, which corresponds (approx.)\nwith a complete senior high school."
                },
                "authors": [
                    {
                        "name": "Irving Gómez-Méndez"
                    },
                    {
                        "name": "Chainarong Amornbunchornvej"
                    }
                ],
                "author_detail": {
                    "name": "Chainarong Amornbunchornvej"
                },
                "author": "Chainarong Amornbunchornvej",
                "arxiv_comment": "Codes to reproduce our results are available in\n  https://github.com/IrvingGomez/SpatialPovertyFactors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.2.6; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09757v1",
                "updated": "2024-08-19T07:34:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    34,
                    43,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:34:43Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    34,
                    43,
                    0,
                    232,
                    0
                ],
                "title": "Strategic Demonstration Selection for Improved Fairness in LLM\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Demonstration Selection for Improved Fairness in LLM\n  In-Context Learning"
                },
                "summary": "Recent studies highlight the effectiveness of using in-context learning (ICL)\nto steer large language models (LLMs) in processing tabular data, a challenging\ntask given the structured nature of such data. Despite advancements in\nperformance, the fairness implications of these methods are less understood.\nThis study investigates how varying demonstrations within ICL prompts influence\nthe fairness outcomes of LLMs. Our findings reveal that deliberately including\nminority group samples in prompts significantly boosts fairness without\nsacrificing predictive accuracy. Further experiments demonstrate that the\nproportion of minority to majority samples in demonstrations affects the\ntrade-off between fairness and prediction accuracy. Based on these insights, we\nintroduce a mitigation technique that employs clustering and evolutionary\nstrategies to curate a diverse and representative sample set from the training\ndata. This approach aims to enhance both predictive performance and fairness in\nICL applications. Experimental results validate that our proposed method\ndramatically improves fairness across various metrics, showing its efficacy in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies highlight the effectiveness of using in-context learning (ICL)\nto steer large language models (LLMs) in processing tabular data, a challenging\ntask given the structured nature of such data. Despite advancements in\nperformance, the fairness implications of these methods are less understood.\nThis study investigates how varying demonstrations within ICL prompts influence\nthe fairness outcomes of LLMs. Our findings reveal that deliberately including\nminority group samples in prompts significantly boosts fairness without\nsacrificing predictive accuracy. Further experiments demonstrate that the\nproportion of minority to majority samples in demonstrations affects the\ntrade-off between fairness and prediction accuracy. Based on these insights, we\nintroduce a mitigation technique that employs clustering and evolutionary\nstrategies to curate a diverse and representative sample set from the training\ndata. This approach aims to enhance both predictive performance and fairness in\nICL applications. Experimental results validate that our proposed method\ndramatically improves fairness across various metrics, showing its efficacy in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jingyu Hu"
                    },
                    {
                        "name": "Weiru Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07930v2",
                "updated": "2024-08-19T07:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    32,
                    25,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-10T07:22:15Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    7,
                    22,
                    15,
                    2,
                    192,
                    0
                ],
                "title": "Token-Mol 1.0: Tokenized drug design with large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Mol 1.0: Tokenized drug design with large language model"
                },
                "summary": "Significant interests have recently risen in leveraging sequence-based large\nlanguage models (LLMs) for drug design. However, most current applications of\nLLMs in drug discovery lack the ability to comprehend three-dimensional (3D)\nstructures, thereby limiting their effectiveness in tasks that explicitly\ninvolve molecular conformations. In this study, we introduced Token-Mol, a\ntoken-only 3D drug design model. This model encodes all molecular information,\nincluding 2D and 3D structures, as well as molecular property data, into\ntokens, which transforms classification and regression tasks in drug discovery\ninto probabilistic prediction problems, thereby enabling learning through a\nunified paradigm. Token-Mol is built on the transformer decoder architecture\nand trained using random causal masking techniques. Additionally, we proposed\nthe Gaussian cross-entropy (GCE) loss function to overcome the challenges in\nregression tasks, significantly enhancing the capacity of LLMs to learn\ncontinuous numerical values. Through a combination of fine-tuning and\nreinforcement learning (RL), Token-Mol achieves performance comparable to or\nsurpassing existing task-specific methods across various downstream tasks,\nincluding pocket-based molecular generation, conformation generation, and\nmolecular property prediction. Compared to existing molecular pre-trained\nmodels, Token-Mol exhibits superior proficiency in handling a wider range of\ndownstream tasks essential for drug design. Notably, our approach improves\nregression task accuracy by approximately 30% compared to similar token-only\nmethods. Token-Mol overcomes the precision limitations of token-only models and\nhas the potential to integrate seamlessly with general models such as ChatGPT,\npaving the way for the development of a universal artificial intelligence drug\ndesign model that facilitates rapid and high-quality drug design by experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant interests have recently risen in leveraging sequence-based large\nlanguage models (LLMs) for drug design. However, most current applications of\nLLMs in drug discovery lack the ability to comprehend three-dimensional (3D)\nstructures, thereby limiting their effectiveness in tasks that explicitly\ninvolve molecular conformations. In this study, we introduced Token-Mol, a\ntoken-only 3D drug design model. This model encodes all molecular information,\nincluding 2D and 3D structures, as well as molecular property data, into\ntokens, which transforms classification and regression tasks in drug discovery\ninto probabilistic prediction problems, thereby enabling learning through a\nunified paradigm. Token-Mol is built on the transformer decoder architecture\nand trained using random causal masking techniques. Additionally, we proposed\nthe Gaussian cross-entropy (GCE) loss function to overcome the challenges in\nregression tasks, significantly enhancing the capacity of LLMs to learn\ncontinuous numerical values. Through a combination of fine-tuning and\nreinforcement learning (RL), Token-Mol achieves performance comparable to or\nsurpassing existing task-specific methods across various downstream tasks,\nincluding pocket-based molecular generation, conformation generation, and\nmolecular property prediction. Compared to existing molecular pre-trained\nmodels, Token-Mol exhibits superior proficiency in handling a wider range of\ndownstream tasks essential for drug design. Notably, our approach improves\nregression task accuracy by approximately 30% compared to similar token-only\nmethods. Token-Mol overcomes the precision limitations of token-only models and\nhas the potential to integrate seamlessly with general models such as ChatGPT,\npaving the way for the development of a universal artificial intelligence drug\ndesign model that facilitates rapid and high-quality drug design by experts."
                },
                "authors": [
                    {
                        "name": "Jike Wang"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Meijing Fang"
                    },
                    {
                        "name": "Yangyang Zhang"
                    },
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Qun Su"
                    },
                    {
                        "name": "Qiaolin Gou"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Odin Zhang"
                    },
                    {
                        "name": "Zhenxing Wu"
                    },
                    {
                        "name": "Dejun Jiang"
                    },
                    {
                        "name": "Xujun Zhang"
                    },
                    {
                        "name": "Huifeng Zhao"
                    },
                    {
                        "name": "Xiaozhe Wan"
                    },
                    {
                        "name": "Zhourui Wu"
                    },
                    {
                        "name": "Liwei Liu"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Chang-Yu Hsieh"
                    },
                    {
                        "name": "Tingjun Hou"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Hou"
                },
                "author": "Tingjun Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09754v1",
                "updated": "2024-08-19T07:30:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    30,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:30:59Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    30,
                    59,
                    0,
                    232,
                    0
                ],
                "title": "Efficient onboard multi-task AI architecture based on self-supervised\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient onboard multi-task AI architecture based on self-supervised\n  learning"
                },
                "summary": "There is growing interest towards the use of AI directly onboard satellites\nfor quick analysis and rapid response to critical events such as natural\ndisasters. This paper presents a blueprint to the mission designer for the\ndevelopment of a modular and efficient deep learning payload to address\nmultiple onboard inference tasks. In particular, we design a self-supervised\nlightweight backbone that provides features to efficient task-specific heads.\nThe latter can be developed independently and with reduced data labeling\nrequirements thanks to the frozen backbone. Experiments on three sample tasks\nof cloud segmentation, flood detection, and marine debris classification on a\n7W embedded system show competitive results with inference quality close to\nhigh-complexity state-of-the-art models and high throughput in excess of 8\nMpx/s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest towards the use of AI directly onboard satellites\nfor quick analysis and rapid response to critical events such as natural\ndisasters. This paper presents a blueprint to the mission designer for the\ndevelopment of a modular and efficient deep learning payload to address\nmultiple onboard inference tasks. In particular, we design a self-supervised\nlightweight backbone that provides features to efficient task-specific heads.\nThe latter can be developed independently and with reduced data labeling\nrequirements thanks to the frozen backbone. Experiments on three sample tasks\nof cloud segmentation, flood detection, and marine debris classification on a\n7W embedded system show competitive results with inference quality close to\nhigh-complexity state-of-the-art models and high throughput in excess of 8\nMpx/s."
                },
                "authors": [
                    {
                        "name": "Gabriele Inzerillo"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08835v2",
                "updated": "2024-08-19T07:29:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-13T05:57:54Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    5,
                    57,
                    54,
                    3,
                    165,
                    0
                ],
                "title": "EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech\n  Recognition Architecture with High Accuracy and Inference Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech\n  Recognition Architecture with High Accuracy and Inference Speed"
                },
                "summary": "Non-autoregressive (NAR) automatic speech recognition (ASR) models predict\ntokens independently and simultaneously, bringing high inference speed.\nHowever, there is still a gap in the accuracy of the NAR models compared to the\nautoregressive (AR) models. In this paper, we propose a single-step NAR ASR\narchitecture with high accuracy and inference speed, called EffectiveASR. It\nuses an Index Mapping Vector (IMV) based alignment generator to generate\nalignments during training, and an alignment predictor to learn the alignments\nfor inference. It can be trained end-to-end (E2E) with cross-entropy loss\ncombined with alignment loss. The proposed EffectiveASR achieves competitive\nresults on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the\nleading models. Specifically, it achieves character error rates (CER) of\n4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR\nConformer with about 30x inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-autoregressive (NAR) automatic speech recognition (ASR) models predict\ntokens independently and simultaneously, bringing high inference speed.\nHowever, there is still a gap in the accuracy of the NAR models compared to the\nautoregressive (AR) models. In this paper, we propose a single-step NAR ASR\narchitecture with high accuracy and inference speed, called EffectiveASR. It\nuses an Index Mapping Vector (IMV) based alignment generator to generate\nalignments during training, and an alignment predictor to learn the alignments\nfor inference. It can be trained end-to-end (E2E) with cross-entropy loss\ncombined with alignment loss. The proposed EffectiveASR achieves competitive\nresults on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the\nleading models. Specifically, it achieves character error rates (CER) of\n4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR\nConformer with about 30x inference speedup."
                },
                "authors": [
                    {
                        "name": "Ziyang Zhuang"
                    },
                    {
                        "name": "Chenfeng Miao"
                    },
                    {
                        "name": "Kun Zou"
                    },
                    {
                        "name": "Shuai Gong"
                    },
                    {
                        "name": "Ming Fang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Shaojun Wang"
                    },
                    {
                        "name": "Jing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xiao"
                },
                "author": "Jing Xiao",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09744v1",
                "updated": "2024-08-19T07:15:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    44,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:15:44Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    44,
                    0,
                    232,
                    0
                ],
                "title": "RealCustom++: Representing Images as Real-Word for Real-Time\n  Customization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealCustom++: Representing Images as Real-Word for Real-Time\n  Customization"
                },
                "summary": "Text-to-image customization, which takes given texts and images depicting\ngiven subjects as inputs, aims to synthesize new images that align with both\ntext semantics and subject appearance. This task provides precise control over\ndetails that text alone cannot capture and is fundamental for various\nreal-world applications, garnering significant interest from academia and\nindustry. Existing works follow the pseudo-word paradigm, which involves\nrepresenting given subjects as pseudo-words and combining them with given texts\nto collectively guide the generation. However, the inherent conflict and\nentanglement between the pseudo-words and texts result in a dual-optimum\nparadox, where subject similarity and text controllability cannot be optimal\nsimultaneously. We propose a novel real-words paradigm termed RealCustom++ that\ninstead represents subjects as non-conflict real words, thereby disentangling\nsubject similarity from text controllability and allowing both to be optimized\nsimultaneously. Specifically, RealCustom++ introduces a novel \"train-inference\"\ndecoupled framework: (1) During training, RealCustom++ learns the alignment\nbetween vision conditions and all real words in the text, ensuring high\nsubject-similarity generation in open domains. This is achieved by the\ncross-layer cross-scale projector to robustly and finely extract subject\nfeatures, and a curriculum training recipe that adapts the generated subject to\ndiverse poses and sizes. (2) During inference, leveraging the learned general\nalignment, an adaptive mask guidance is proposed to only customize the\ngeneration of the specific target real word, keeping other subject-irrelevant\nregions uncontaminated to ensure high text-controllability in real-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image customization, which takes given texts and images depicting\ngiven subjects as inputs, aims to synthesize new images that align with both\ntext semantics and subject appearance. This task provides precise control over\ndetails that text alone cannot capture and is fundamental for various\nreal-world applications, garnering significant interest from academia and\nindustry. Existing works follow the pseudo-word paradigm, which involves\nrepresenting given subjects as pseudo-words and combining them with given texts\nto collectively guide the generation. However, the inherent conflict and\nentanglement between the pseudo-words and texts result in a dual-optimum\nparadox, where subject similarity and text controllability cannot be optimal\nsimultaneously. We propose a novel real-words paradigm termed RealCustom++ that\ninstead represents subjects as non-conflict real words, thereby disentangling\nsubject similarity from text controllability and allowing both to be optimized\nsimultaneously. Specifically, RealCustom++ introduces a novel \"train-inference\"\ndecoupled framework: (1) During training, RealCustom++ learns the alignment\nbetween vision conditions and all real words in the text, ensuring high\nsubject-similarity generation in open domains. This is achieved by the\ncross-layer cross-scale projector to robustly and finely extract subject\nfeatures, and a curriculum training recipe that adapts the generated subject to\ndiverse poses and sizes. (2) During inference, leveraging the learned general\nalignment, an adaptive mask guidance is proposed to only customize the\ngeneration of the specific target real word, keeping other subject-irrelevant\nregions uncontaminated to ensure high text-controllability in real-time."
                },
                "authors": [
                    {
                        "name": "Zhendong Mao"
                    },
                    {
                        "name": "Mengqi Huang"
                    },
                    {
                        "name": "Fei Ding"
                    },
                    {
                        "name": "Mingcong Liu"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09743v1",
                "updated": "2024-08-19T07:15:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:15:11Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    11,
                    0,
                    232,
                    0
                ],
                "title": "R2GenCSR: Retrieving Context Samples for Large Language Model based\n  X-ray Medical Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2GenCSR: Retrieving Context Samples for Large Language Model based\n  X-ray Medical Report Generation"
                },
                "summary": "Inspired by the tremendous success of Large Language Models (LLMs), existing\nX-ray medical report generation methods attempt to leverage large models to\nachieve better performance. They usually adopt a Transformer to extract the\nvisual features of a given X-ray image, and then, feed them into the LLM for\ntext generation. How to extract more effective information for the LLMs to help\nthem improve final results is an urgent problem that needs to be solved.\nAdditionally, the use of visual Transformer models also brings high\ncomputational complexity. To address these issues, this paper proposes a novel\ncontext-guided efficient X-ray medical report generation framework.\nSpecifically, we introduce the Mamba as the vision backbone with linear\ncomplexity, and the performance obtained is comparable to that of the strong\nTransformer model. More importantly, we perform context retrieval from the\ntraining set for samples within each mini-batch during the training phase,\nutilizing both positively and negatively related samples to enhance feature\nrepresentation and discriminative learning. Subsequently, we feed the vision\ntokens, context information, and prompt statements to invoke the LLM for\ngenerating high-quality medical reports. Extensive experiments on three X-ray\nreport generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully\nvalidated the effectiveness of our proposed model. The source code of this work\nwill be released on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the tremendous success of Large Language Models (LLMs), existing\nX-ray medical report generation methods attempt to leverage large models to\nachieve better performance. They usually adopt a Transformer to extract the\nvisual features of a given X-ray image, and then, feed them into the LLM for\ntext generation. How to extract more effective information for the LLMs to help\nthem improve final results is an urgent problem that needs to be solved.\nAdditionally, the use of visual Transformer models also brings high\ncomputational complexity. To address these issues, this paper proposes a novel\ncontext-guided efficient X-ray medical report generation framework.\nSpecifically, we introduce the Mamba as the vision backbone with linear\ncomplexity, and the performance obtained is comparable to that of the strong\nTransformer model. More importantly, we perform context retrieval from the\ntraining set for samples within each mini-batch during the training phase,\nutilizing both positively and negatively related samples to enhance feature\nrepresentation and discriminative learning. Subsequently, we feed the vision\ntokens, context information, and prompt statements to invoke the LLM for\ngenerating high-quality medical reports. Extensive experiments on three X-ray\nreport generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully\nvalidated the effectiveness of our proposed model. The source code of this work\nwill be released on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Yuehang Li"
                    },
                    {
                        "name": "Fuling Wang"
                    },
                    {
                        "name": "Shiao Wang"
                    },
                    {
                        "name": "Chuanfu Li"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "In Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09742v1",
                "updated": "2024-08-19T07:14:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    14,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:14:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    14,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "Paired Completion: Flexible Quantification of Issue-framing at Scale\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paired Completion: Flexible Quantification of Issue-framing at Scale\n  with LLMs"
                },
                "summary": "Detecting and quantifying issue framing in textual discourse - the\nperspective one takes to a given topic (e.g. climate science vs. denialism,\nmisogyny vs. gender equality) - is highly valuable to a range of end-users from\nsocial and political scientists to program evaluators and policy analysts.\nHowever, conceptual framing is notoriously challenging for automated natural\nlanguage processing (NLP) methods since the words and phrases used by either\n`side' of an issue are often held in common, with only subtle stylistic\nflourishes separating their use. Here we develop and rigorously evaluate new\ndetection methods for issue framing and narrative analysis within large text\ndatasets. By introducing a novel application of next-token log probabilities\nderived from generative large language models (LLMs) we show that issue framing\ncan be reliably and efficiently detected in large corpora with only a few\nexamples of either perspective on a given issue, a method we call `paired\ncompletion'. Through 192 independent experiments over three novel, synthetic\ndatasets, we evaluate paired completion against prompt-based LLM methods and\nlabelled methods using traditional NLP and recent LLM contextual embeddings. We\nadditionally conduct a cost-based analysis to mark out the feasible set of\nperformant methods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable, accurate and\nlow-bias issue-framing in large corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and quantifying issue framing in textual discourse - the\nperspective one takes to a given topic (e.g. climate science vs. denialism,\nmisogyny vs. gender equality) - is highly valuable to a range of end-users from\nsocial and political scientists to program evaluators and policy analysts.\nHowever, conceptual framing is notoriously challenging for automated natural\nlanguage processing (NLP) methods since the words and phrases used by either\n`side' of an issue are often held in common, with only subtle stylistic\nflourishes separating their use. Here we develop and rigorously evaluate new\ndetection methods for issue framing and narrative analysis within large text\ndatasets. By introducing a novel application of next-token log probabilities\nderived from generative large language models (LLMs) we show that issue framing\ncan be reliably and efficiently detected in large corpora with only a few\nexamples of either perspective on a given issue, a method we call `paired\ncompletion'. Through 192 independent experiments over three novel, synthetic\ndatasets, we evaluate paired completion against prompt-based LLM methods and\nlabelled methods using traditional NLP and recent LLM contextual embeddings. We\nadditionally conduct a cost-based analysis to mark out the feasible set of\nperformant methods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable, accurate and\nlow-bias issue-framing in large corpora."
                },
                "authors": [
                    {
                        "name": "Simon D Angus"
                    },
                    {
                        "name": "Lachlan O'Neill"
                    }
                ],
                "author_detail": {
                    "name": "Lachlan O'Neill"
                },
                "author": "Lachlan O'Neill",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09735v1",
                "updated": "2024-08-19T06:49:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    49,
                    4,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T06:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    49,
                    4,
                    0,
                    232,
                    0
                ],
                "title": "Icing on the Cake: Automatic Code Summarization at Ericsson",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Icing on the Cake: Automatic Code Summarization at Ericsson"
                },
                "summary": "This paper presents our findings on the automatic summarization of Java\nmethods within Ericsson, a global telecommunications company. We evaluate the\nperformance of an approach called Automatic Semantic Augmentation of Prompts\n(ASAP), which uses a Large Language Model (LLM) to generate leading summary\ncomments for Java methods. ASAP enhances the $LLM's$ prompt context by\nintegrating static program analysis and information retrieval techniques to\nidentify similar exemplar methods along with their developer-written Javadocs,\nand serves as the baseline in our study. In contrast, we explore and compare\nthe performance of four simpler approaches that do not require static program\nanalysis, information retrieval, or the presence of exemplars as in the ASAP\nmethod. Our methods rely solely on the Java method body as input, making them\nlightweight and more suitable for rapid deployment in commercial software\ndevelopment environments. We conducted experiments on an Ericsson software\nproject and replicated the study using two widely-used open-source Java\nprojects, Guava and Elasticsearch, to ensure the reliability of our results.\nPerformance was measured across eight metrics that capture various aspects of\nsimilarity. Notably, one of our simpler approaches performed as well as or\nbetter than the ASAP method on both the Ericsson project and the open-source\nprojects. Additionally, we performed an ablation study to examine the impact of\nmethod names on Javadoc summary generation across our four proposed approaches\nand the ASAP method. By masking the method names and observing the generated\nsummaries, we found that our approaches were statistically significantly less\ninfluenced by the absence of method names compared to the baseline. This\nsuggests that our methods are more robust to variations in method names and may\nderive summaries more comprehensively from the method body than the ASAP\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our findings on the automatic summarization of Java\nmethods within Ericsson, a global telecommunications company. We evaluate the\nperformance of an approach called Automatic Semantic Augmentation of Prompts\n(ASAP), which uses a Large Language Model (LLM) to generate leading summary\ncomments for Java methods. ASAP enhances the $LLM's$ prompt context by\nintegrating static program analysis and information retrieval techniques to\nidentify similar exemplar methods along with their developer-written Javadocs,\nand serves as the baseline in our study. In contrast, we explore and compare\nthe performance of four simpler approaches that do not require static program\nanalysis, information retrieval, or the presence of exemplars as in the ASAP\nmethod. Our methods rely solely on the Java method body as input, making them\nlightweight and more suitable for rapid deployment in commercial software\ndevelopment environments. We conducted experiments on an Ericsson software\nproject and replicated the study using two widely-used open-source Java\nprojects, Guava and Elasticsearch, to ensure the reliability of our results.\nPerformance was measured across eight metrics that capture various aspects of\nsimilarity. Notably, one of our simpler approaches performed as well as or\nbetter than the ASAP method on both the Ericsson project and the open-source\nprojects. Additionally, we performed an ablation study to examine the impact of\nmethod names on Javadoc summary generation across our four proposed approaches\nand the ASAP method. By masking the method names and observing the generated\nsummaries, we found that our approaches were statistically significantly less\ninfluenced by the absence of method names compared to the baseline. This\nsuggests that our methods are more robust to variations in method names and may\nderive summaries more comprehensively from the method body than the ASAP\napproach."
                },
                "authors": [
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Sumit Soman"
                    },
                    {
                        "name": "Ranjani H G"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "arxiv_comment": "16 pages, 6 tables, 4 figures. Accepted at the 2024 International\n  Conference on Software Maintenance and Evolution (ICSME) 2024 - Industry\n  Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09720v1",
                "updated": "2024-08-19T06:19:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    19,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T06:19:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    19,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large\n  Language Model Augmented Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large\n  Language Model Augmented Framework"
                },
                "summary": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in\nhuman-centered research. However, existing datasets neglect different domains\n(e.g., environments, times, populations, and data sources), only conducting\nsimple random splits, and the performance of these datasets has already\napproached saturation. In the past five years, no large-scale dataset has been\nopened to the public. To address this issue, this paper proposes a new\nlarge-scale, cross-domain pedestrian attribute recognition dataset to fill the\ndata gap, termed MSP60K. It consists of 60,122 images and 57 attribute\nannotations across eight scenarios. Synthetic degradation is also conducted to\nfurther narrow the gap between the dataset and real-world challenging\nscenarios. To establish a more rigorous benchmark, we evaluate 17\nrepresentative PAR models under both random and cross-domain split protocols on\nour dataset. Additionally, we propose an innovative Large Language Model (LLM)\naugmented PAR framework, named LLM-PAR. This framework processes pedestrian\nimages through a Vision Transformer (ViT) backbone to extract features and\nintroduces a multi-embedding query Transformer to learn partial-aware features\nfor attribute classification. Significantly, we enhance this framework with LLM\nfor ensemble learning and visual feature augmentation. Comprehensive\nexperiments across multiple PAR benchmark datasets have thoroughly validated\nthe efficacy of our proposed framework. The dataset and source code\naccompanying this paper will be made publicly available at\n\\url{https://github.com/Event-AHU/OpenPAR}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in\nhuman-centered research. However, existing datasets neglect different domains\n(e.g., environments, times, populations, and data sources), only conducting\nsimple random splits, and the performance of these datasets has already\napproached saturation. In the past five years, no large-scale dataset has been\nopened to the public. To address this issue, this paper proposes a new\nlarge-scale, cross-domain pedestrian attribute recognition dataset to fill the\ndata gap, termed MSP60K. It consists of 60,122 images and 57 attribute\nannotations across eight scenarios. Synthetic degradation is also conducted to\nfurther narrow the gap between the dataset and real-world challenging\nscenarios. To establish a more rigorous benchmark, we evaluate 17\nrepresentative PAR models under both random and cross-domain split protocols on\nour dataset. Additionally, we propose an innovative Large Language Model (LLM)\naugmented PAR framework, named LLM-PAR. This framework processes pedestrian\nimages through a Vision Transformer (ViT) backbone to extract features and\nintroduces a multi-embedding query Transformer to learn partial-aware features\nfor attribute classification. Significantly, we enhance this framework with LLM\nfor ensemble learning and visual feature augmentation. Comprehensive\nexperiments across multiple PAR benchmark datasets have thoroughly validated\nthe efficacy of our proposed framework. The dataset and source code\naccompanying this paper will be made publicly available at\n\\url{https://github.com/Event-AHU/OpenPAR}."
                },
                "authors": [
                    {
                        "name": "Jiandong Jin"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qian Zhu"
                    },
                    {
                        "name": "Haiyang Wang"
                    },
                    {
                        "name": "Chenglong Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenglong Li"
                },
                "author": "Chenglong Li",
                "arxiv_comment": "MSP60K PAR Benchmark Dataset, LLM based PAR model, In Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07970v2",
                "updated": "2024-08-19T06:08:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    8,
                    46,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-12T07:49:36Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    49,
                    36,
                    2,
                    164,
                    0
                ],
                "title": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation"
                },
                "summary": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50."
                },
                "authors": [
                    {
                        "name": "Javad Pourmostafa Roshan Sharami"
                    },
                    {
                        "name": "Dimitar Shterionov"
                    },
                    {
                        "name": "Pieter Spronck"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Spronck"
                },
                "author": "Pieter Spronck",
                "arxiv_comment": "Camera-ready version of the Association for Machine Translation in\n  the Americas (AMTA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09713v2",
                "updated": "2024-08-20T12:22:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    22,
                    16,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T06:05:24Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    5,
                    24,
                    0,
                    232,
                    0
                ],
                "title": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation"
                },
                "summary": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices."
                },
                "authors": [
                    {
                        "name": "Haijin Wang"
                    },
                    {
                        "name": "Mianrong Zhang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Nan Shang"
                    },
                    {
                        "name": "Shangheng Yao"
                    },
                    {
                        "name": "Fushuan Wen"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09710v1",
                "updated": "2024-08-19T05:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    55,
                    22,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T05:55:22Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    55,
                    22,
                    0,
                    232,
                    0
                ],
                "title": "Likelihood inference of the non-stationary Hawkes process with\n  non-exponential kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood inference of the non-stationary Hawkes process with\n  non-exponential kernel"
                },
                "summary": "The Hawkes process is a popular point process model for event sequences that\nexhibit temporal clustering. The intensity process of a Hawkes process consists\nof two components, the baseline intensity and the accumulated excitation effect\ndue to past events, with the latter specified via an excitation kernel. The\nclassical Hawkes process assumes a constant baseline intensity and an\nexponential excitation kernel. This results in an intensity process that is\nMarkovian, a fact that has been used extensively to establish the strong\nconsistency and asymtpotic normality of maximum likelihood estimators or\nsimilar. However, these assumptions can be overly restrictive and unrealistic\nfor modelling the many applications which require the baseline intensity to\nvary with time and the excitation kernel to have non-exponential decay.\nHowever, asymptotic properties of maximum likelihood inference for the\nparameters specifying the baseline intensity and the self-exciting decay under\nthis setup are substantially more difficult since the resulting intensity\nprocess is non-Markovian. To overcome this challenge, we develop an\napproximation procedure to show the intensity process is asymptotically ergodic\nin a suitably defined sense. This allows for the identification of an ergodic\nlimit to the likelihood function and its derivatives, as required for obtaining\nlarge sample inference under minimal regularity conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hawkes process is a popular point process model for event sequences that\nexhibit temporal clustering. The intensity process of a Hawkes process consists\nof two components, the baseline intensity and the accumulated excitation effect\ndue to past events, with the latter specified via an excitation kernel. The\nclassical Hawkes process assumes a constant baseline intensity and an\nexponential excitation kernel. This results in an intensity process that is\nMarkovian, a fact that has been used extensively to establish the strong\nconsistency and asymtpotic normality of maximum likelihood estimators or\nsimilar. However, these assumptions can be overly restrictive and unrealistic\nfor modelling the many applications which require the baseline intensity to\nvary with time and the excitation kernel to have non-exponential decay.\nHowever, asymptotic properties of maximum likelihood inference for the\nparameters specifying the baseline intensity and the self-exciting decay under\nthis setup are substantially more difficult since the resulting intensity\nprocess is non-Markovian. To overcome this challenge, we develop an\napproximation procedure to show the intensity process is asymptotically ergodic\nin a suitably defined sense. This allows for the identification of an ergodic\nlimit to the likelihood function and its derivatives, as required for obtaining\nlarge sample inference under minimal regularity conditions."
                },
                "authors": [
                    {
                        "name": "Tsz-Kit Jeffrey Kwan"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "William Dunsmuir"
                    }
                ],
                "author_detail": {
                    "name": "William Dunsmuir"
                },
                "author": "William Dunsmuir",
                "arxiv_comment": "41 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary: 60G55. Secondary: 62F12, 62M15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07922v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07922v3",
                "updated": "2024-08-19T05:46:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    46,
                    56,
                    0,
                    232,
                    0
                ],
                "published": "2023-08-15T17:59:18Z",
                "published_parsed": [
                    2023,
                    8,
                    15,
                    17,
                    59,
                    18,
                    1,
                    227,
                    0
                ],
                "title": "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder\n  Language Models"
                },
                "summary": "In this paper, we investigate the in-context learning ability of\nretrieval-augmented encoder-decoder language models. We first conduct a\ncomprehensive analysis of existing models and identify their limitations in\nin-context learning, primarily due to a mismatch between pretraining and\ninference, as well as a restricted context length. To address these issues, we\npropose RAVEN, a model that combines retrieval-augmented masked language\nmodeling and prefix language modeling. We further introduce Fusion-in-Context\nLearning to enhance the few-shot performance by enabling the model to leverage\nmore in-context examples without requiring additional training. Through\nextensive experiments, we demonstrate that our simple yet effective design\nsignificantly improves performance, achieving results comparable to the most\nadvanced language models in certain scenarios, despite having substantially\nfewer parameters. Our work underscores the potential of retrieval-augmented\nencoder-decoder language models for in-context learning and encourages further\nresearch in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the in-context learning ability of\nretrieval-augmented encoder-decoder language models. We first conduct a\ncomprehensive analysis of existing models and identify their limitations in\nin-context learning, primarily due to a mismatch between pretraining and\ninference, as well as a restricted context length. To address these issues, we\npropose RAVEN, a model that combines retrieval-augmented masked language\nmodeling and prefix language modeling. We further introduce Fusion-in-Context\nLearning to enhance the few-shot performance by enabling the model to leverage\nmore in-context examples without requiring additional training. Through\nextensive experiments, we demonstrate that our simple yet effective design\nsignificantly improves performance, achieving results comparable to the most\nadvanced language models in certain scenarios, despite having substantially\nfewer parameters. Our work underscores the potential of retrieval-augmented\nencoder-decoder language models for in-context learning and encourages further\nresearch in this direction."
                },
                "authors": [
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Kevin Chen-Chuan Chang"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07922v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07922v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09703v1",
                "updated": "2024-08-19T05:18:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    18,
                    50,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T05:18:50Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    18,
                    50,
                    0,
                    232,
                    0
                ],
                "title": "Partial-Multivariate Model for Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial-Multivariate Model for Forecasting"
                },
                "summary": "When solving forecasting problems including multiple time-series features,\nexisting approaches often fall into two extreme categories, depending on\nwhether to utilize inter-feature information: univariate and\ncomplete-multivariate models. Unlike univariate cases which ignore the\ninformation, complete-multivariate models compute relationships among a\ncomplete set of features. However, despite the potential advantage of\nleveraging the additional information, complete-multivariate models sometimes\nunderperform univariate ones. Therefore, our research aims to explore a middle\nground between these two by introducing what we term Partial-Multivariate\nmodels where a neural network captures only partial relationships, that is,\ndependencies within subsets of all features. To this end, we propose PMformer,\na Transformer-based partial-multivariate model, with its training algorithm. We\ndemonstrate that PMformer outperforms various univariate and\ncomplete-multivariate models, providing a theoretical rationale and empirical\nanalysis for its superiority. Additionally, by proposing an inference technique\nfor PMformer, the forecasting accuracy is further enhanced. Finally, we\nhighlight other advantages of PMformer: efficiency and robustness under missing\nfeatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When solving forecasting problems including multiple time-series features,\nexisting approaches often fall into two extreme categories, depending on\nwhether to utilize inter-feature information: univariate and\ncomplete-multivariate models. Unlike univariate cases which ignore the\ninformation, complete-multivariate models compute relationships among a\ncomplete set of features. However, despite the potential advantage of\nleveraging the additional information, complete-multivariate models sometimes\nunderperform univariate ones. Therefore, our research aims to explore a middle\nground between these two by introducing what we term Partial-Multivariate\nmodels where a neural network captures only partial relationships, that is,\ndependencies within subsets of all features. To this end, we propose PMformer,\na Transformer-based partial-multivariate model, with its training algorithm. We\ndemonstrate that PMformer outperforms various univariate and\ncomplete-multivariate models, providing a theoretical rationale and empirical\nanalysis for its superiority. Additionally, by proposing an inference technique\nfor PMformer, the forecasting accuracy is further enhanced. Finally, we\nhighlight other advantages of PMformer: efficiency and robustness under missing\nfeatures."
                },
                "authors": [
                    {
                        "name": "Jaehoon Lee"
                    },
                    {
                        "name": "Hankook Lee"
                    },
                    {
                        "name": "Sungik Choi"
                    },
                    {
                        "name": "Sungjun Cho"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09701v1",
                "updated": "2024-08-19T05:11:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    11,
                    46,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T05:11:46Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    11,
                    46,
                    0,
                    232,
                    0
                ],
                "title": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code\n  Generation in LLMs via Zero-Shot Cross-Lingual Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code\n  Generation in LLMs via Zero-Shot Cross-Lingual Transfer"
                },
                "summary": "The use of Large Language Models (LLMs) for program code generation has\ngained substantial attention, but their biases and limitations with non-English\nprompts challenge global inclusivity. This paper investigates the complexities\nof multilingual prompt-based code generation. Our evaluations of LLMs,\nincluding CodeLLaMa and CodeGemma, reveal significant disparities in code\nquality for non-English prompts; we also demonstrate the inadequacy of simple\napproaches like prompt translation, bootstrapped data augmentation, and\nfine-tuning. To address this, we propose a zero-shot cross-lingual approach\nusing a neural projection technique, integrating a cross-lingual encoder like\nLASER artetxe2019massively to map multilingual embeddings from it into the\nLLM's token space. This method requires training only on English data and\nscales effectively to other languages. Results on a translated and\nquality-checked MBPP dataset show substantial improvements in code quality.\nThis research promotes a more inclusive code generation landscape by empowering\nLLMs with multilingual capabilities to support the diverse linguistic spectrum\nin programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for program code generation has\ngained substantial attention, but their biases and limitations with non-English\nprompts challenge global inclusivity. This paper investigates the complexities\nof multilingual prompt-based code generation. Our evaluations of LLMs,\nincluding CodeLLaMa and CodeGemma, reveal significant disparities in code\nquality for non-English prompts; we also demonstrate the inadequacy of simple\napproaches like prompt translation, bootstrapped data augmentation, and\nfine-tuning. To address this, we propose a zero-shot cross-lingual approach\nusing a neural projection technique, integrating a cross-lingual encoder like\nLASER artetxe2019massively to map multilingual embeddings from it into the\nLLM's token space. This method requires training only on English data and\nscales effectively to other languages. Results on a translated and\nquality-checked MBPP dataset show substantial improvements in code quality.\nThis research promotes a more inclusive code generation landscape by empowering\nLLMs with multilingual capabilities to support the diverse linguistic spectrum\nin programming."
                },
                "authors": [
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Abhijit Mishra"
                    },
                    {
                        "name": "Utkarsh Mujumdar"
                    }
                ],
                "author_detail": {
                    "name": "Utkarsh Mujumdar"
                },
                "author": "Utkarsh Mujumdar",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08694v3",
                "updated": "2024-08-19T04:54:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    54,
                    36,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-13T16:57:57Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    16,
                    57,
                    57,
                    2,
                    73,
                    0
                ],
                "title": "TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning"
                },
                "summary": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of the strong baseline's total), along with enhanced capabilities of\nLLMs in crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of the strong baseline's total), along with enhanced capabilities of\nLLMs in crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL"
                },
                "authors": [
                    {
                        "name": "Shangding Gu"
                    },
                    {
                        "name": "Alois Knoll"
                    },
                    {
                        "name": "Ming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jin"
                },
                "author": "Ming Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v2",
                "updated": "2024-08-20T16:09:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    9,
                    33,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08686v2",
                "updated": "2024-08-19T04:31:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    31,
                    51,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T11:59:01Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    59,
                    1,
                    4,
                    229,
                    0
                ],
                "title": "SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking\n  for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking\n  for Sequential Recommendation"
                },
                "summary": "Language Models (LMs) are increasingly employed in recommendation systems due\nto their advanced language understanding and generation capabilities. Recent\nrecommender systems based on generative retrieval have leveraged the\ninferential abilities of LMs to directly generate the index tokens of the next\nitem, based on item sequences within the user's interaction history. Previous\nstudies have mostly focused on item indices based solely on textual semantic or\ncollaborative information. However, although the standalone effectiveness of\nthese aspects has been demonstrated, the integration of this information has\nremained unexplored. Our in-depth analysis finds that there is a significant\ndifference in the knowledge captured by the model from heterogeneous item\nindices and diverse input prompts, which can have a high potential for\ncomplementarity. In this paper, we propose SC-Rec, a unified recommender system\nthat learns diverse preference knowledge from two distinct item indices and\nmultiple prompt templates. Furthermore, SC-Rec adopts a novel reranking\nstrategy that aggregates a set of ranking results, inferred based on different\nindices and prompts, to achieve the self-consistency of the model. Our\nempirical evaluation on three real-world datasets demonstrates that SC-Rec\nconsiderably outperforms the state-of-the-art methods for sequential\nrecommendation, effectively incorporating complementary knowledge from varied\noutputs of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are increasingly employed in recommendation systems due\nto their advanced language understanding and generation capabilities. Recent\nrecommender systems based on generative retrieval have leveraged the\ninferential abilities of LMs to directly generate the index tokens of the next\nitem, based on item sequences within the user's interaction history. Previous\nstudies have mostly focused on item indices based solely on textual semantic or\ncollaborative information. However, although the standalone effectiveness of\nthese aspects has been demonstrated, the integration of this information has\nremained unexplored. Our in-depth analysis finds that there is a significant\ndifference in the knowledge captured by the model from heterogeneous item\nindices and diverse input prompts, which can have a high potential for\ncomplementarity. In this paper, we propose SC-Rec, a unified recommender system\nthat learns diverse preference knowledge from two distinct item indices and\nmultiple prompt templates. Furthermore, SC-Rec adopts a novel reranking\nstrategy that aggregates a set of ranking results, inferred based on different\nindices and prompts, to achieve the self-consistency of the model. Our\nempirical evaluation on three real-world datasets demonstrates that SC-Rec\nconsiderably outperforms the state-of-the-art methods for sequential\nrecommendation, effectively incorporating complementary knowledge from varied\noutputs of the model."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Seongku Kang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08869v2",
                "updated": "2024-08-19T04:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    29,
                    34,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T17:54:09Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    54,
                    9,
                    4,
                    229,
                    0
                ],
                "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars"
                },
                "summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches."
                },
                "authors": [
                    {
                        "name": "Sumanth Prabhu"
                    }
                ],
                "author_detail": {
                    "name": "Sumanth Prabhu"
                },
                "author": "Sumanth Prabhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v3",
                "updated": "2024-08-19T04:02:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    2,
                    44,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Universal Approximation Theory: The Basic Theory for Transformer-based\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation Theory: The Basic Theory for Transformer-based\n  Large Language Models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09688v1",
                "updated": "2024-08-19T03:53:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:53:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts"
                },
                "summary": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task."
                },
                "authors": [
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14192v2",
                "updated": "2024-08-19T03:47:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    47,
                    16,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-20T10:52:14Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    52,
                    14,
                    3,
                    172,
                    0
                ],
                "title": "Timo: Towards Better Temporal Reasoning for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timo: Towards Better Temporal Reasoning for Language Models"
                },
                "summary": "Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo."
                },
                "authors": [
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "This paper has been accepted to the COLM 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09682v1",
                "updated": "2024-08-19T03:41:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    41,
                    43,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:41:43Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    41,
                    43,
                    0,
                    232,
                    0
                ],
                "title": "Simulating Field Experiments with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Field Experiments with Large Language Models"
                },
                "summary": "Prevailing large language models (LLMs) are capable of human responses\nsimulation through its unprecedented content generation and reasoning\nabilities. However, it is not clear whether and how to leverage LLMs to\nsimulate field experiments. In this paper, we propose and evaluate two\nprompting strategies: the observer mode that allows a direct prediction on main\nconclusions and the participant mode that simulates distributions of responses\nfrom participants. Using this approach, we examine fifteen well cited field\nexperimental papers published in INFORMS and MISQ, finding encouraging\nalignments between simulated experimental results and the actual results in\ncertain scenarios. We further identify topics of which LLMs underperform,\nincluding gender difference and social norms related research. Additionally,\nthe automatic and standardized workflow proposed in this paper enables the\npossibility of a large-scale screening of more papers with field experiments.\nThis paper pioneers the utilization of large language models (LLMs) for\nsimulating field experiments, presenting a significant extension to previous\nwork which focused solely on lab environments. By introducing two novel\nprompting strategies, observer and participant modes, we demonstrate the\nability of LLMs to both predict outcomes and replicate participant responses\nwithin complex field settings. Our findings indicate a promising alignment with\nactual experimental results in certain scenarios, achieving a stimulation\naccuracy of 66% in observer mode. This study expands the scope of potential\napplications for LLMs and illustrates their utility in assisting researchers\nprior to engaging in expensive field experiments. Moreover, it sheds light on\nthe boundaries of LLMs when used in simulating field experiments, serving as a\ncautionary note for researchers considering the integration of LLMs into their\nexperimental toolkit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevailing large language models (LLMs) are capable of human responses\nsimulation through its unprecedented content generation and reasoning\nabilities. However, it is not clear whether and how to leverage LLMs to\nsimulate field experiments. In this paper, we propose and evaluate two\nprompting strategies: the observer mode that allows a direct prediction on main\nconclusions and the participant mode that simulates distributions of responses\nfrom participants. Using this approach, we examine fifteen well cited field\nexperimental papers published in INFORMS and MISQ, finding encouraging\nalignments between simulated experimental results and the actual results in\ncertain scenarios. We further identify topics of which LLMs underperform,\nincluding gender difference and social norms related research. Additionally,\nthe automatic and standardized workflow proposed in this paper enables the\npossibility of a large-scale screening of more papers with field experiments.\nThis paper pioneers the utilization of large language models (LLMs) for\nsimulating field experiments, presenting a significant extension to previous\nwork which focused solely on lab environments. By introducing two novel\nprompting strategies, observer and participant modes, we demonstrate the\nability of LLMs to both predict outcomes and replicate participant responses\nwithin complex field settings. Our findings indicate a promising alignment with\nactual experimental results in certain scenarios, achieving a stimulation\naccuracy of 66% in observer mode. This study expands the scope of potential\napplications for LLMs and illustrates their utility in assisting researchers\nprior to engaging in expensive field experiments. Moreover, it sheds light on\nthe boundaries of LLMs when used in simulating field experiments, serving as a\ncautionary note for researchers considering the integration of LLMs into their\nexperimental toolkit."
                },
                "authors": [
                    {
                        "name": "Yaoyu Chen"
                    },
                    {
                        "name": "Yuheng Hu"
                    },
                    {
                        "name": "Yingda Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Lu"
                },
                "author": "Yingda Lu",
                "arxiv_comment": "17 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13269v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13269v3",
                "updated": "2024-08-19T03:31:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    31,
                    19,
                    0,
                    232,
                    0
                ],
                "published": "2023-07-25T05:39:21Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    5,
                    39,
                    21,
                    1,
                    206,
                    0
                ],
                "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA\n  Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA\n  Composition"
                },
                "summary": "Low-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a simple framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a\nfew examples from a new task, LoraHub can fluidly combine multiple LoRA\nmodules, eliminating the need for human expertise and assumptions. Notably, the\ncomposition requires neither additional model parameters nor gradients.\nEmpirical results on the Big-Bench Hard benchmark suggest that LoraHub, while\nnot surpassing the performance of in-context learning, offers a notable\nperformance-efficiency trade-off in few-shot scenarios by employing a\nsignificantly reduced number of tokens per example during inference. Notably,\nLoraHub establishes a better upper bound compared to in-context learning when\npaired with different demonstration examples, demonstrating its potential for\nfuture development. Our vision is to establish a platform for LoRA modules,\nempowering users to share their trained LoRA modules. This collaborative\napproach facilitates the seamless application of LoRA modules to novel tasks,\ncontributing to an adaptive ecosystem. Our code is available at\nhttps://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are\nreleased at https://huggingface.co/lorahub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a simple framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a\nfew examples from a new task, LoraHub can fluidly combine multiple LoRA\nmodules, eliminating the need for human expertise and assumptions. Notably, the\ncomposition requires neither additional model parameters nor gradients.\nEmpirical results on the Big-Bench Hard benchmark suggest that LoraHub, while\nnot surpassing the performance of in-context learning, offers a notable\nperformance-efficiency trade-off in few-shot scenarios by employing a\nsignificantly reduced number of tokens per example during inference. Notably,\nLoraHub establishes a better upper bound compared to in-context learning when\npaired with different demonstration examples, demonstrating its potential for\nfuture development. Our vision is to establish a platform for LoRA modules,\nempowering users to share their trained LoRA modules. This collaborative\napproach facilitates the seamless application of LoRA modules to novel tasks,\ncontributing to an adaptive ecosystem. Our code is available at\nhttps://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are\nreleased at https://huggingface.co/lorahub."
                },
                "authors": [
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.13269v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13269v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09674v1",
                "updated": "2024-08-19T03:30:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    30,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:30:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    30,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "Implicit Grid Convolution for Multi-Scale Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Grid Convolution for Multi-Scale Image Super-Resolution"
                },
                "summary": "Recently, Super-Resolution (SR) achieved significant performance improvement\nby employing neural networks. Most SR methods conventionally train a single\nmodel for each targeted scale, which increases redundancy in training and\ndeployment in proportion to the number of scales targeted. This paper\nchallenges this conventional fixed-scale approach. Our preliminary analysis\nreveals that, surprisingly, encoders trained at different scales extract\nsimilar features from images. Furthermore, the commonly used scale-specific\nupsampler, Sub-Pixel Convolution (SPConv), exhibits significant inter-scale\ncorrelations. Based on these observations, we propose a framework for training\nmultiple integer scales simultaneously with a single model. We use a single\nencoder to extract features and introduce a novel upsampler, Implicit Grid\nConvolution~(IGConv), which integrates SPConv at all scales within a single\nmodule to predict multiple scales. Our extensive experiments demonstrate that\ntraining multiple scales with a single model reduces the training budget and\nstored parameters by one-third while achieving equivalent inference latency and\ncomparable performance. Furthermore, we propose IGConv$^{+}$, which addresses\nspectral bias and input-independent upsampling and uses ensemble prediction to\nimprove performance. As a result, SRFormer-IGConv$^{+}$ achieves a remarkable\n0.25dB improvement in PSNR at Urban100$\\times$4 while reducing the training\nbudget, stored parameters, and inference cost compared to the existing\nSRFormer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Super-Resolution (SR) achieved significant performance improvement\nby employing neural networks. Most SR methods conventionally train a single\nmodel for each targeted scale, which increases redundancy in training and\ndeployment in proportion to the number of scales targeted. This paper\nchallenges this conventional fixed-scale approach. Our preliminary analysis\nreveals that, surprisingly, encoders trained at different scales extract\nsimilar features from images. Furthermore, the commonly used scale-specific\nupsampler, Sub-Pixel Convolution (SPConv), exhibits significant inter-scale\ncorrelations. Based on these observations, we propose a framework for training\nmultiple integer scales simultaneously with a single model. We use a single\nencoder to extract features and introduce a novel upsampler, Implicit Grid\nConvolution~(IGConv), which integrates SPConv at all scales within a single\nmodule to predict multiple scales. Our extensive experiments demonstrate that\ntraining multiple scales with a single model reduces the training budget and\nstored parameters by one-third while achieving equivalent inference latency and\ncomparable performance. Furthermore, we propose IGConv$^{+}$, which addresses\nspectral bias and input-independent upsampling and uses ensemble prediction to\nimprove performance. As a result, SRFormer-IGConv$^{+}$ achieves a remarkable\n0.25dB improvement in PSNR at Urban100$\\times$4 while reducing the training\nbudget, stored parameters, and inference cost compared to the existing\nSRFormer."
                },
                "authors": [
                    {
                        "name": "Dongheon Lee"
                    },
                    {
                        "name": "Seokju Yun"
                    },
                    {
                        "name": "Youngmin Ro"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Ro"
                },
                "author": "Youngmin Ro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.10197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10197v1",
                "updated": "2024-08-19T17:54:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    54,
                    29,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:54:29Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    54,
                    29,
                    0,
                    232,
                    0
                ],
                "title": "Demystifying the Communication Characteristics for Distributed\n  Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Communication Characteristics for Distributed\n  Transformer Models"
                },
                "summary": "Deep learning (DL) models based on the transformer architecture have\nrevolutionized many DL applications such as large language models (LLMs),\nvision transformers, audio generation, and time series prediction. Much of this\nprogress has been fueled by distributed training, yet distributed communication\nremains a substantial bottleneck to training progress. This paper examines the\ncommunication behavior of transformer models - that is, how different\nparallelism schemes used in multi-node/multi-GPU DL Training communicate data\nin the context of transformers. We use GPT-based language models as a case\nstudy of the transformer architecture due to their ubiquity. We validate the\nempirical results obtained from our communication logs using analytical models.\nAt a high level, our analysis reveals a need to optimize small message\npoint-to-point communication further, correlations between sequence length,\nper-GPU throughput, model size, and optimizations used, and where to\npotentially guide further optimizations in framework and HPC middleware design\nand optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) models based on the transformer architecture have\nrevolutionized many DL applications such as large language models (LLMs),\nvision transformers, audio generation, and time series prediction. Much of this\nprogress has been fueled by distributed training, yet distributed communication\nremains a substantial bottleneck to training progress. This paper examines the\ncommunication behavior of transformer models - that is, how different\nparallelism schemes used in multi-node/multi-GPU DL Training communicate data\nin the context of transformers. We use GPT-based language models as a case\nstudy of the transformer architecture due to their ubiquity. We validate the\nempirical results obtained from our communication logs using analytical models.\nAt a high level, our analysis reveals a need to optimize small message\npoint-to-point communication further, correlations between sequence length,\nper-GPU throughput, model size, and optimizations used, and where to\npotentially guide further optimizations in framework and HPC middleware design\nand optimization."
                },
                "authors": [
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Benjamin Michalowicz"
                    },
                    {
                        "name": "Jacob Hatef"
                    },
                    {
                        "name": "Lang Xu"
                    },
                    {
                        "name": "Mustafa Abduljabbar"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar Panda"
                },
                "author": "Dhabaleswar Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10184v1",
                "updated": "2024-08-19T17:42:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    42,
                    25,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:42:25Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    42,
                    25,
                    0,
                    232,
                    0
                ],
                "title": "Participatory Mapping of Local Green Hydrogen Cost-Potentials in\n  Sub-Saharan Africa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Participatory Mapping of Local Green Hydrogen Cost-Potentials in\n  Sub-Saharan Africa"
                },
                "summary": "Green hydrogen is a promising solution within carbon free energy systems with\nSub-Saharan Africa being a possibly well-suited candidate for its production.\nHowever, green hydrogen in Sub-Saharan Africa is not yet investigated in\ndetail. This work determines the green hydrogen cost-potential for green\nhydrogen within this region. Therefore, a potential analysis for PV, wind and\nhydropower, groundwater analysis, and energy systems optimization are\nconducted. The results are evaluated under local socio-economic factors.\nResults show that hydrogen costs start at 1.6 EUR/kg in Mauritania with a total\npotential of ~259 TWh/a under 2 EUR/kg in 2050. Two third of the regions\nexperience groundwater limitations and need desalination at surplus costs of\n~1% of hydrogen costs. Socio-economic analysis show, that green hydrogen\ndeployment can be hindered along the Upper Guinea Coast and the African Great\nLakes, driven by limited energy access, low labor costs in West Africa, and\nhigh labor potential in other regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green hydrogen is a promising solution within carbon free energy systems with\nSub-Saharan Africa being a possibly well-suited candidate for its production.\nHowever, green hydrogen in Sub-Saharan Africa is not yet investigated in\ndetail. This work determines the green hydrogen cost-potential for green\nhydrogen within this region. Therefore, a potential analysis for PV, wind and\nhydropower, groundwater analysis, and energy systems optimization are\nconducted. The results are evaluated under local socio-economic factors.\nResults show that hydrogen costs start at 1.6 EUR/kg in Mauritania with a total\npotential of ~259 TWh/a under 2 EUR/kg in 2050. Two third of the regions\nexperience groundwater limitations and need desalination at surplus costs of\n~1% of hydrogen costs. Socio-economic analysis show, that green hydrogen\ndeployment can be hindered along the Upper Guinea Coast and the African Great\nLakes, driven by limited energy access, low labor costs in West Africa, and\nhigh labor potential in other regions."
                },
                "authors": [
                    {
                        "name": "C. Winkler"
                    },
                    {
                        "name": "H. Heinrichs"
                    },
                    {
                        "name": "S. Ishmam"
                    },
                    {
                        "name": "B. Bayat"
                    },
                    {
                        "name": "A. Lahnaoui"
                    },
                    {
                        "name": "S. Agbo"
                    },
                    {
                        "name": "E. U. Peña Sanchez"
                    },
                    {
                        "name": "D. Franzmann"
                    },
                    {
                        "name": "N. Oijeabou"
                    },
                    {
                        "name": "C. Koerner"
                    },
                    {
                        "name": "Y. Michael"
                    },
                    {
                        "name": "B. Oloruntoba"
                    },
                    {
                        "name": "C. Montzka"
                    },
                    {
                        "name": "H. Vereecken"
                    },
                    {
                        "name": "H. Hendricks Franssen"
                    },
                    {
                        "name": "J. Brendt"
                    },
                    {
                        "name": "S. Brauner"
                    },
                    {
                        "name": "W. Kuckshinrichs"
                    },
                    {
                        "name": "S. Venghaus"
                    },
                    {
                        "name": "D. Kone"
                    },
                    {
                        "name": "B. Korgo"
                    },
                    {
                        "name": "K. Ogunjobi"
                    },
                    {
                        "name": "J. Olwoch"
                    },
                    {
                        "name": "V. Chiteculo"
                    },
                    {
                        "name": "Z. Getenga"
                    },
                    {
                        "name": "J. Linßen"
                    },
                    {
                        "name": "D. Stolten"
                    }
                ],
                "author_detail": {
                    "name": "D. Stolten"
                },
                "author": "D. Stolten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02138v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02138v3",
                "updated": "2024-08-19T17:16:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    16,
                    8,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-02T17:49:40Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    17,
                    49,
                    40,
                    1,
                    93,
                    0
                ],
                "title": "Topic-Based Watermarks for LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-Based Watermarks for LLM-Generated Text"
                },
                "summary": "The indistinguishability of text generated by large language models (LLMs)\nfrom human-generated text poses significant challenges. Watermarking algorithms\nare potential solutions by embedding detectable signatures within LLM-generated\noutputs. However, current watermarking schemes lack robustness to a range of\nattacks such as text substitution or manipulation, undermining their\nreliability. This paper proposes a novel topic-based watermarking algorithm for\nLLMs, designed to enhance the robustness of watermarking in LLMs. Our approach\nleverages the topics extracted from input prompts or outputs of non-watermarked\nLLMs in the generation process of watermarked text. We dynamically utilize\ntoken lists on identified topics and adjust token sampling weights accordingly.\nBy using these topic-specific token biases, we embed a topic-sensitive\nwatermarking into the generated text. We outline the theoretical framework of\nour topic-based watermarking algorithm and discuss its potential advantages in\nvarious scenarios. Additionally, we explore a comprehensive range of attacks\nagainst watermarking algorithms, including discrete alterations, paraphrasing,\nand tokenizations. We demonstrate that our proposed watermarking scheme\nclassifies various watermarked text topics with 99.99% confidence and\noutperforms existing algorithms in terms of z-score robustness and the\nfeasibility of modeling text degradation by potential attackers, while\nconsidering the trade-offs between the benefits and losses of watermarking\nLLM-generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The indistinguishability of text generated by large language models (LLMs)\nfrom human-generated text poses significant challenges. Watermarking algorithms\nare potential solutions by embedding detectable signatures within LLM-generated\noutputs. However, current watermarking schemes lack robustness to a range of\nattacks such as text substitution or manipulation, undermining their\nreliability. This paper proposes a novel topic-based watermarking algorithm for\nLLMs, designed to enhance the robustness of watermarking in LLMs. Our approach\nleverages the topics extracted from input prompts or outputs of non-watermarked\nLLMs in the generation process of watermarked text. We dynamically utilize\ntoken lists on identified topics and adjust token sampling weights accordingly.\nBy using these topic-specific token biases, we embed a topic-sensitive\nwatermarking into the generated text. We outline the theoretical framework of\nour topic-based watermarking algorithm and discuss its potential advantages in\nvarious scenarios. Additionally, we explore a comprehensive range of attacks\nagainst watermarking algorithms, including discrete alterations, paraphrasing,\nand tokenizations. We demonstrate that our proposed watermarking scheme\nclassifies various watermarked text topics with 99.99% confidence and\noutperforms existing algorithms in terms of z-score robustness and the\nfeasibility of modeling text degradation by potential attackers, while\nconsidering the trade-offs between the benefits and losses of watermarking\nLLM-generated text."
                },
                "authors": [
                    {
                        "name": "Alexander Nemecek"
                    },
                    {
                        "name": "Yuzhou Jiang"
                    },
                    {
                        "name": "Erman Ayday"
                    }
                ],
                "author_detail": {
                    "name": "Erman Ayday"
                },
                "author": "Erman Ayday",
                "arxiv_comment": "Results for proposed scheme, additional/removal of content (figures\n  and equations), 12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02138v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02138v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10159v1",
                "updated": "2024-08-19T17:09:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:09:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems predict a user's next item of interest by\nanalyzing past interactions, aligning recommendations with individual\npreferences. Leveraging the strengths of Large Language Models (LLMs) in\nknowledge comprehension and reasoning, recent approaches have applied LLMs to\nsequential recommendation through language generation paradigms. These methods\nconvert user behavior sequences into prompts for LLM fine-tuning, utilizing\nLow-Rank Adaptation (LoRA) modules to refine recommendations. However, the\nuniform application of LoRA across diverse user behaviors sometimes fails to\ncapture individual variability, leading to suboptimal performance and negative\ntransfer between disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)\nframework. iLoRA creates a diverse array of experts, each capturing specific\naspects of user preferences, and introduces a sequence representation guided\ngate function. This gate function processes historical interaction sequences to\ngenerate enriched representations, guiding the gating network to output\ncustomized expert participation weights. This tailored approach mitigates\nnegative transfer and dynamically adjusts to diverse behavior patterns.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness\nof iLoRA, highlighting its superior performance compared to existing methods in\ncapturing user-specific preferences and improving recommendation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems predict a user's next item of interest by\nanalyzing past interactions, aligning recommendations with individual\npreferences. Leveraging the strengths of Large Language Models (LLMs) in\nknowledge comprehension and reasoning, recent approaches have applied LLMs to\nsequential recommendation through language generation paradigms. These methods\nconvert user behavior sequences into prompts for LLM fine-tuning, utilizing\nLow-Rank Adaptation (LoRA) modules to refine recommendations. However, the\nuniform application of LoRA across diverse user behaviors sometimes fails to\ncapture individual variability, leading to suboptimal performance and negative\ntransfer between disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)\nframework. iLoRA creates a diverse array of experts, each capturing specific\naspects of user preferences, and introduces a sequence representation guided\ngate function. This gate function processes historical interaction sequences to\ngenerate enriched representations, guiding the gating network to output\ncustomized expert participation weights. This tailored approach mitigates\nnegative transfer and dynamically adjusts to diverse behavior patterns.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness\nof iLoRA, highlighting its superior performance compared to existing methods in\ncapturing user-specific preferences and improving recommendation accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10151v1",
                "updated": "2024-08-19T17:02:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    2,
                    6,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T17:02:06Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    2,
                    6,
                    0,
                    232,
                    0
                ],
                "title": "Multilingual Needle in a Haystack: Investigating Long-Context Behavior\n  of Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Needle in a Haystack: Investigating Long-Context Behavior\n  of Multilingual Large Language Models"
                },
                "summary": "While recent large language models (LLMs) demonstrate remarkable abilities in\nresponding to queries in diverse languages, their ability to handle long\nmultilingual contexts is unexplored. As such, a systematic evaluation of the\nlong-context capabilities of LLMs in multilingual settings is crucial,\nspecifically in the context of information retrieval. To address this gap, we\nintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to\nassess a model's ability to retrieve relevant information (the needle) from a\ncollection of multilingual distractor texts (the haystack). This test serves as\nan extension of the multilingual question-answering task, encompassing both\nmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs\non MLNeedle. Our findings reveal that model performance can vary significantly\nwith language and needle position. Specifically, we observe that model\nperformance is the lowest when the needle is (i) in a language outside the\nEnglish language family and (ii) located in the middle of the input context.\nFurthermore, although some models claim a context size of $8k$ tokens or\ngreater, none demonstrate satisfactory cross-lingual retrieval performance as\nthe context length increases. Our analysis provides key insights into the\nlong-context behavior of LLMs in multilingual settings to guide future\nevaluation protocols. To our knowledge, this is the first study to investigate\nthe multilingual long-context behavior of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent large language models (LLMs) demonstrate remarkable abilities in\nresponding to queries in diverse languages, their ability to handle long\nmultilingual contexts is unexplored. As such, a systematic evaluation of the\nlong-context capabilities of LLMs in multilingual settings is crucial,\nspecifically in the context of information retrieval. To address this gap, we\nintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to\nassess a model's ability to retrieve relevant information (the needle) from a\ncollection of multilingual distractor texts (the haystack). This test serves as\nan extension of the multilingual question-answering task, encompassing both\nmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs\non MLNeedle. Our findings reveal that model performance can vary significantly\nwith language and needle position. Specifically, we observe that model\nperformance is the lowest when the needle is (i) in a language outside the\nEnglish language family and (ii) located in the middle of the input context.\nFurthermore, although some models claim a context size of $8k$ tokens or\ngreater, none demonstrate satisfactory cross-lingual retrieval performance as\nthe context length increases. Our analysis provides key insights into the\nlong-context behavior of LLMs in multilingual settings to guide future\nevaluation protocols. To our knowledge, this is the first study to investigate\nthe multilingual long-context behavior of LLMs."
                },
                "authors": [
                    {
                        "name": "Amey Hengle"
                    },
                    {
                        "name": "Prasoon Bajpai"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10146v1",
                "updated": "2024-08-19T16:45:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    45,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:45:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    45,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Epitaxial Films and Devices of Transparent Conducting Oxides:\n  La:BaSnO$_3$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epitaxial Films and Devices of Transparent Conducting Oxides:\n  La:BaSnO$_3$"
                },
                "summary": "This paper reviews recent developments in material science and device physics\nof high-quality epitaxial films of the transparent perovskite La-doped barium\nstannate, La:BaSnO$_3$. It presents current efforts in the synthesis science of\nepitaxial La:BaSnO$_3$ films for achieving reduced defect densities and high\nelectron mobility at room temperature. We discuss the scattering mechanisms and\nthe route towards engineering defect-free epitaxial La:BaSnO$_3$\nheterostructures. By combining chemical surface characterization and electronic\ntransport studies, a special emphasis is laid on the proper correlation between\nthe transport properties and the electronic band structure of La:BaSnO$_3$\nfilms and heterostructures. For application purposes, interesting optical\nproperties of La:BaSnO$_3$ films are discussed. Finally, for their potential\napplication in oxide electronics, an overview of current progress in the\nfabrication of La:BaSnO$_3$-based thin-film field-effect transistors is\npresented together with recent progress in the the fundamental realization of\ntwo-dimensional electron gases with high electron mobility in\nLa:BaSnO$_3$-based heterostructures. Future experimental studies to reveal the\npotential deployment of La:BaSnO$_3$ films in optoelectronic and transparent\nelectronics are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reviews recent developments in material science and device physics\nof high-quality epitaxial films of the transparent perovskite La-doped barium\nstannate, La:BaSnO$_3$. It presents current efforts in the synthesis science of\nepitaxial La:BaSnO$_3$ films for achieving reduced defect densities and high\nelectron mobility at room temperature. We discuss the scattering mechanisms and\nthe route towards engineering defect-free epitaxial La:BaSnO$_3$\nheterostructures. By combining chemical surface characterization and electronic\ntransport studies, a special emphasis is laid on the proper correlation between\nthe transport properties and the electronic band structure of La:BaSnO$_3$\nfilms and heterostructures. For application purposes, interesting optical\nproperties of La:BaSnO$_3$ films are discussed. Finally, for their potential\napplication in oxide electronics, an overview of current progress in the\nfabrication of La:BaSnO$_3$-based thin-film field-effect transistors is\npresented together with recent progress in the the fundamental realization of\ntwo-dimensional electron gases with high electron mobility in\nLa:BaSnO$_3$-based heterostructures. Future experimental studies to reveal the\npotential deployment of La:BaSnO$_3$ films in optoelectronic and transparent\nelectronics are also discussed."
                },
                "authors": [
                    {
                        "name": "Prosper Ngabonziza"
                    },
                    {
                        "name": "Arnaud P. Nono Tchiomo"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud P. Nono Tchiomo"
                },
                "author": "Arnaud P. Nono Tchiomo",
                "arxiv_comment": "45 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08808v3",
                "updated": "2024-08-20T02:32:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    32,
                    58,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-16T15:41:43Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    41,
                    43,
                    4,
                    229,
                    0
                ],
                "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Ravi Raju"
                    },
                    {
                        "name": "Swayambhoo Jain"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "arxiv_comment": "14 pages, 8 figures, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10141v1",
                "updated": "2024-08-19T16:41:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    41,
                    7,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:41:07Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    41,
                    7,
                    0,
                    232,
                    0
                ],
                "title": "Instruction Finetuning for Leaderboard Generation from Empirical AI\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Finetuning for Leaderboard Generation from Empirical AI\n  Research"
                },
                "summary": "This study demonstrates the application of instruction finetuning of\npretrained Large Language Models (LLMs) to automate the generation of AI\nresearch leaderboards, extracting (Task, Dataset, Metric, Score) quadruples\nfrom articles. It aims to streamline the dissemination of advancements in AI\nresearch by transitioning from traditional, manual community curation, or\notherwise taxonomy-constrained natural language inference (NLI) models, to an\nautomated, generative LLM-based approach. Utilizing the FLAN-T5 model, this\nresearch enhances LLMs' adaptability and reliability in information extraction,\noffering a novel method for structured knowledge representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the application of instruction finetuning of\npretrained Large Language Models (LLMs) to automate the generation of AI\nresearch leaderboards, extracting (Task, Dataset, Metric, Score) quadruples\nfrom articles. It aims to streamline the dissemination of advancements in AI\nresearch by transitioning from traditional, manual community curation, or\notherwise taxonomy-constrained natural language inference (NLI) models, to an\nautomated, generative LLM-based approach. Utilizing the FLAN-T5 model, this\nresearch enhances LLMs' adaptability and reliability in information extraction,\noffering a novel method for structured knowledge representation."
                },
                "authors": [
                    {
                        "name": "Salomon Kabongo"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer D'Souza"
                },
                "author": "Jennifer D'Souza",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.02409",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10128v1",
                "updated": "2024-08-19T16:15:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    15,
                    9,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:15:09Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    15,
                    9,
                    0,
                    232,
                    0
                ],
                "title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language"
                },
                "summary": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data."
                },
                "authors": [
                    {
                        "name": "Manjil Karki"
                    },
                    {
                        "name": "Pratik Shakya"
                    },
                    {
                        "name": "Sandesh Acharya"
                    },
                    {
                        "name": "Ravi Pandit"
                    },
                    {
                        "name": "Dinesh Gothe"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Gothe"
                },
                "author": "Dinesh Gothe",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10124v1",
                "updated": "2024-08-19T16:11:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    59,
                    0,
                    232,
                    0
                ],
                "title": "Molecular Graph Representation Learning Integrating Large Language\n  Models with Domain-specific Small Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Graph Representation Learning Integrating Large Language\n  Models with Domain-specific Small Models"
                },
                "summary": "Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Yuxiang Ren"
                    },
                    {
                        "name": "Chengbin Hou"
                    },
                    {
                        "name": "Hairong Lv"
                    },
                    {
                        "name": "Xuegong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuegong Zhang"
                },
                "author": "Xuegong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10123v1",
                "updated": "2024-08-19T16:11:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    47,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T16:11:47Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    11,
                    47,
                    0,
                    232,
                    0
                ],
                "title": "Learning Precise Affordances from Egocentric Videos for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Precise Affordances from Egocentric Videos for Robotic\n  Manipulation"
                },
                "summary": "Affordance, defined as the potential actions that an object offers, is\ncrucial for robotic manipulation tasks. A deep understanding of affordance can\nlead to more intelligent AI systems. For example, such knowledge directs an\nagent to grasp a knife by the handle for cutting and by the blade when passing\nit to someone. In this paper, we present a streamlined affordance learning\nsystem that encompasses data collection, effective model training, and robot\ndeployment. First, we collect training data from egocentric videos in an\nautomatic manner. Different from previous methods that focus only on the object\ngraspable affordance and represent it as coarse heatmaps, we cover both\ngraspable (e.g., object handles) and functional affordances (e.g., knife\nblades, hammer heads) and extract data with precise segmentation masks. We then\npropose an effective model, termed Geometry-guided Affordance Transformer\n(GKT), to train on the collected data. GKT integrates an innovative Depth\nFeature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing\nthe model's understanding of affordances. To enable affordance-oriented\nmanipulation, we further introduce Aff-Grasp, a framework that combines GKT\nwith a grasp generation model. For comprehensive evaluation, we create an\naffordance evaluation dataset with pixel-wise annotations, and design\nreal-world tasks for robot experiments. The results show that GKT surpasses the\nstate-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of\n95.5% in affordance prediction and 77.1% in successful grasping among 179\ntrials, including evaluations with seen, unseen objects, and cluttered scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance, defined as the potential actions that an object offers, is\ncrucial for robotic manipulation tasks. A deep understanding of affordance can\nlead to more intelligent AI systems. For example, such knowledge directs an\nagent to grasp a knife by the handle for cutting and by the blade when passing\nit to someone. In this paper, we present a streamlined affordance learning\nsystem that encompasses data collection, effective model training, and robot\ndeployment. First, we collect training data from egocentric videos in an\nautomatic manner. Different from previous methods that focus only on the object\ngraspable affordance and represent it as coarse heatmaps, we cover both\ngraspable (e.g., object handles) and functional affordances (e.g., knife\nblades, hammer heads) and extract data with precise segmentation masks. We then\npropose an effective model, termed Geometry-guided Affordance Transformer\n(GKT), to train on the collected data. GKT integrates an innovative Depth\nFeature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing\nthe model's understanding of affordances. To enable affordance-oriented\nmanipulation, we further introduce Aff-Grasp, a framework that combines GKT\nwith a grasp generation model. For comprehensive evaluation, we create an\naffordance evaluation dataset with pixel-wise annotations, and design\nreal-world tasks for robot experiments. The results show that GKT surpasses the\nstate-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of\n95.5% in affordance prediction and 77.1% in successful grasping among 179\ntrials, including evaluations with seen, unseen objects, and cluttered scenes."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Nikolaos Tsagkas"
                    },
                    {
                        "name": "Jifei Song"
                    },
                    {
                        "name": "Ruaridh Mon-Williams"
                    },
                    {
                        "name": "Sethu Vijayakumar"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Laura Sevilla-Lara"
                    }
                ],
                "author_detail": {
                    "name": "Laura Sevilla-Lara"
                },
                "author": "Laura Sevilla-Lara",
                "arxiv_comment": "Project page: https://reagan1311.github.io/affgrasp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12021v2",
                "updated": "2024-08-19T15:28:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    28,
                    37,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-27T22:20:39Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    22,
                    20,
                    39,
                    3,
                    179,
                    0
                ],
                "title": "Adaptive Draft-Verification for Efficient Large Language Model Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Draft-Verification for Efficient Large Language Model Decoding"
                },
                "summary": "Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications."
                },
                "authors": [
                    {
                        "name": "Xukun Liu"
                    },
                    {
                        "name": "Bowen Lei"
                    },
                    {
                        "name": "Ruqi Zhang"
                    },
                    {
                        "name": "Dongkuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dongkuan Xu"
                },
                "author": "Dongkuan Xu",
                "arxiv_comment": "Under review of Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10086v1",
                "updated": "2024-08-19T15:27:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    27,
                    25,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:27:25Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    27,
                    25,
                    0,
                    232,
                    0
                ],
                "title": "ARMADA: Attribute-Based Multimodal Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARMADA: Attribute-Based Multimodal Data Augmentation"
                },
                "summary": "In Multimodal Language Models (MLMs), the cost of manually annotating\nhigh-quality image-text pair data for fine-tuning and alignment is extremely\nhigh. While existing multimodal data augmentation frameworks propose ways to\naugment image-text pairs, they either suffer from semantic inconsistency\nbetween texts and images, or generate unrealistic images, causing knowledge gap\nwith real world examples. To address these issues, we propose Attribute-based\nMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentation\nmethod via knowledge-guided manipulation of visual attributes of the mentioned\nentities. Specifically, we extract entities and their visual attributes from\nthe original text data, then search for alternative values for the visual\nattributes under the guidance of knowledge bases (KBs) and large language\nmodels (LLMs). We then utilize an image-editing model to edit the images with\nthe extracted attributes. ARMADA is a novel multimodal data generation\nframework that: (i) extracts knowledge-grounded attributes from symbolic KBs\nfor semantically consistent yet distinctive image-text pair generation, (ii)\ngenerates visually similar images of disparate categories using neighboring\nentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs\nto modulate auxiliary visual attributes such as backgrounds for more robust\nrepresentation of original entities. Our empirical results over four downstream\ntasks demonstrate the efficacy of our framework to produce high-quality data\nand enhance the model performance. This also highlights the need to leverage\nexternal knowledge proxies for enhanced interpretability and real-world\ngrounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Multimodal Language Models (MLMs), the cost of manually annotating\nhigh-quality image-text pair data for fine-tuning and alignment is extremely\nhigh. While existing multimodal data augmentation frameworks propose ways to\naugment image-text pairs, they either suffer from semantic inconsistency\nbetween texts and images, or generate unrealistic images, causing knowledge gap\nwith real world examples. To address these issues, we propose Attribute-based\nMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentation\nmethod via knowledge-guided manipulation of visual attributes of the mentioned\nentities. Specifically, we extract entities and their visual attributes from\nthe original text data, then search for alternative values for the visual\nattributes under the guidance of knowledge bases (KBs) and large language\nmodels (LLMs). We then utilize an image-editing model to edit the images with\nthe extracted attributes. ARMADA is a novel multimodal data generation\nframework that: (i) extracts knowledge-grounded attributes from symbolic KBs\nfor semantically consistent yet distinctive image-text pair generation, (ii)\ngenerates visually similar images of disparate categories using neighboring\nentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs\nto modulate auxiliary visual attributes such as backgrounds for more robust\nrepresentation of original entities. Our empirical results over four downstream\ntasks demonstrate the efficacy of our framework to produce high-quality data\nand enhance the model performance. This also highlights the need to leverage\nexternal knowledge proxies for enhanced interpretability and real-world\ngrounding."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Jin"
                    },
                    {
                        "name": "Jeonghwan Kim"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Kuan-Hao Huang"
                    },
                    {
                        "name": "Te-Lin Wu"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07952v2",
                "updated": "2024-08-19T14:56:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    56,
                    5,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-12T07:22:05Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    22,
                    5,
                    2,
                    164,
                    0
                ],
                "title": "Spatial-Frequency Dual Progressive Attention Network For Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-Frequency Dual Progressive Attention Network For Medical Image\n  Segmentation"
                },
                "summary": "In medical images, various types of lesions often manifest significant\ndifferences in their shape and texture. Accurate medical image segmentation\ndemands deep learning models with robust capabilities in multi-scale and\nboundary feature learning. However, previous networks still have limitations in\naddressing the above issues. Firstly, previous networks simultaneously fuse\nmulti-level features or employ deep supervision to enhance multi-scale\nlearning. However, this may lead to feature redundancy and excessive\ncomputational overhead, which is not conducive to network training and clinical\ndeployment. Secondly, the majority of medical image segmentation networks\nexclusively learn features in the spatial domain, disregarding the abundant\nglobal information in the frequency domain. This results in a bias towards\nlow-frequency components, neglecting crucial high-frequency information. To\naddress these problems, we introduce SF-UNet, a spatial-frequency dual-domain\nattention network. It comprises two main components: the Multi-scale\nProgressive Channel Attention (MPCA) block, which progressively extract\nmulti-scale features across adjacent encoder layers, and the lightweight\nFrequency-Spatial Attention (FSA) block, with only 0.05M parameters, enabling\nconcurrent learning of texture and boundary features from both spatial and\nfrequency domains. We validate the effectiveness of the proposed SF-UNet on\nthree public datasets. Experimental results show that compared to previous\nstate-of-the-art (SOTA) medical image segmentation networks, SF-UNet achieves\nthe best performance, and achieves up to 9.4\\% and 10.78\\% improvement in DSC\nand IOU. Codes will be released at https://github.com/nkicsl/SF-UNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical images, various types of lesions often manifest significant\ndifferences in their shape and texture. Accurate medical image segmentation\ndemands deep learning models with robust capabilities in multi-scale and\nboundary feature learning. However, previous networks still have limitations in\naddressing the above issues. Firstly, previous networks simultaneously fuse\nmulti-level features or employ deep supervision to enhance multi-scale\nlearning. However, this may lead to feature redundancy and excessive\ncomputational overhead, which is not conducive to network training and clinical\ndeployment. Secondly, the majority of medical image segmentation networks\nexclusively learn features in the spatial domain, disregarding the abundant\nglobal information in the frequency domain. This results in a bias towards\nlow-frequency components, neglecting crucial high-frequency information. To\naddress these problems, we introduce SF-UNet, a spatial-frequency dual-domain\nattention network. It comprises two main components: the Multi-scale\nProgressive Channel Attention (MPCA) block, which progressively extract\nmulti-scale features across adjacent encoder layers, and the lightweight\nFrequency-Spatial Attention (FSA) block, with only 0.05M parameters, enabling\nconcurrent learning of texture and boundary features from both spatial and\nfrequency domains. We validate the effectiveness of the proposed SF-UNet on\nthree public datasets. Experimental results show that compared to previous\nstate-of-the-art (SOTA) medical image segmentation networks, SF-UNet achieves\nthe best performance, and achieves up to 9.4\\% and 10.78\\% improvement in DSC\nand IOU. Codes will be released at https://github.com/nkicsl/SF-UNet."
                },
                "authors": [
                    {
                        "name": "Zhenhuan Zhou"
                    },
                    {
                        "name": "Along He"
                    },
                    {
                        "name": "Yanlin Wu"
                    },
                    {
                        "name": "Rui Yao"
                    },
                    {
                        "name": "Xueshuo Xie"
                    },
                    {
                        "name": "Tao Li"
                    }
                ],
                "author_detail": {
                    "name": "Tao Li"
                },
                "author": "Tao Li",
                "arxiv_comment": "6 pages accepted by 2024 IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10053v1",
                "updated": "2024-08-19T14:48:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    48,
                    4,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T14:48:04Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    48,
                    4,
                    0,
                    232,
                    0
                ],
                "title": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory"
                },
                "summary": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Xuebing Zhou"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03009v2",
                "updated": "2024-08-19T14:47:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    47,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-02-05T13:47:53Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    13,
                    47,
                    53,
                    0,
                    36,
                    0
                ],
                "title": "UniMem: Towards a Unified View of Long-Context Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMem: Towards a Unified View of Long-Context Large Language Models"
                },
                "summary": "Long-context processing is a critical ability that constrains the\napplicability of large language models (LLMs). Although there exist various\nmethods devoted to enhancing the long-context processing ability of LLMs, they\nare developed in an isolated manner and lack systematic analysis and\nintegration of their strengths, hindering further developments. In this paper,\nwe introduce UniMem, a Unified framework that reformulates existing\nlong-context methods from the view of Memory augmentation of LLMs.\nDistinguished by its four core dimensions-Memory Management, Memory Writing,\nMemory Reading, and Memory Injection, UniMem empowers researchers to conduct\nsystematic exploration of long-context methods. We re-formulate 16 existing\nmethods based on UniMem and analyze four representative methods:\nTransformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent\nUniMem forms to reveal their design principles and strengths. Based on these\nanalyses, we propose UniMix, an innovative approach that integrates the\nstrengths of these algorithms. Experimental results show that UniMix achieves\nsuperior performance in handling long contexts with significantly lower\nperplexity than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context processing is a critical ability that constrains the\napplicability of large language models (LLMs). Although there exist various\nmethods devoted to enhancing the long-context processing ability of LLMs, they\nare developed in an isolated manner and lack systematic analysis and\nintegration of their strengths, hindering further developments. In this paper,\nwe introduce UniMem, a Unified framework that reformulates existing\nlong-context methods from the view of Memory augmentation of LLMs.\nDistinguished by its four core dimensions-Memory Management, Memory Writing,\nMemory Reading, and Memory Injection, UniMem empowers researchers to conduct\nsystematic exploration of long-context methods. We re-formulate 16 existing\nmethods based on UniMem and analyze four representative methods:\nTransformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent\nUniMem forms to reveal their design principles and strengths. Based on these\nanalyses, we propose UniMix, an innovative approach that integrates the\nstrengths of these algorithms. Experimental results show that UniMix achieves\nsuperior performance in handling long contexts with significantly lower\nperplexity than baselines."
                },
                "authors": [
                    {
                        "name": "Junjie Fang"
                    },
                    {
                        "name": "Likai Tang"
                    },
                    {
                        "name": "Hongzhe Bi"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Si Sun"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Haolun Li"
                    },
                    {
                        "name": "Yongjian Li"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Xiaodong Shi"
                    },
                    {
                        "name": "Sen Song"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10039v1",
                "updated": "2024-08-19T14:31:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    31,
                    57,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T14:31:57Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    31,
                    57,
                    0,
                    232,
                    0
                ],
                "title": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis"
                },
                "summary": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task."
                },
                "authors": [
                    {
                        "name": "Ruihui Hou"
                    },
                    {
                        "name": "Shencheng Chen"
                    },
                    {
                        "name": "Yongqi Fan"
                    },
                    {
                        "name": "Lifeng Zhu"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Tong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Tong Ruan"
                },
                "author": "Tong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06009v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06009v3",
                "updated": "2024-08-19T14:24:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    24,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-09T21:07:16Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    21,
                    7,
                    16,
                    5,
                    69,
                    0
                ],
                "title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and\n  Limitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and\n  Limitations"
                },
                "summary": "Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope."
                },
                "authors": [
                    {
                        "name": "Swapnaja Achintalwar"
                    },
                    {
                        "name": "Adriana Alvarado Garcia"
                    },
                    {
                        "name": "Ateret Anaby-Tavor"
                    },
                    {
                        "name": "Ioana Baldini"
                    },
                    {
                        "name": "Sara E. Berger"
                    },
                    {
                        "name": "Bishwaranjan Bhattacharjee"
                    },
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Lamogha Chiazor"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Kirushikesh DB"
                    },
                    {
                        "name": "Rogério Abreu de Paula"
                    },
                    {
                        "name": "Pierre Dognin"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Soumya Ghosh"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "George Kour"
                    },
                    {
                        "name": "Ja Young Lee"
                    },
                    {
                        "name": "Nishtha Madaan"
                    },
                    {
                        "name": "Sameep Mehta"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Manish Nagireddy"
                    },
                    {
                        "name": "Inkit Padhi"
                    },
                    {
                        "name": "David Piorkowski"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    },
                    {
                        "name": "Hendrik Strobelt"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Christoph Tillmann"
                    },
                    {
                        "name": "Aashka Trivedi"
                    },
                    {
                        "name": "Kush R. Varshney"
                    },
                    {
                        "name": "Dennis Wei"
                    },
                    {
                        "name": "Shalisha Witherspooon"
                    },
                    {
                        "name": "Marcel Zalmanovici"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Zalmanovici"
                },
                "author": "Marcel Zalmanovici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06009v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06009v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10013v1",
                "updated": "2024-08-19T14:09:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    9,
                    48,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T14:09:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    9,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "TBA: Faster Large Language Model Training Using SSD-Based Activation\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TBA: Faster Large Language Model Training Using SSD-Based Activation\n  Offloading"
                },
                "summary": "The growth rate of the GPU memory capacity has not been able to keep up with\nthat of the size of large language models (LLMs), hindering the model training\nprocess. In particular, activations -- the intermediate tensors produced during\nforward propagation and reused in backward propagation -- dominate the GPU\nmemory use. To address this challenge, we propose TBA to efficiently offload\nactivations to high-capacity NVMe SSDs. This approach reduces GPU memory usage\nwithout impacting performance by adaptively overlapping data transfers with\ncomputation. TBA is compatible with popular deep learning frameworks like\nPyTorch, Megatron, and DeepSpeed, and it employs techniques such as tensor\ndeduplication, forwarding, and adaptive offloading to further enhance\nefficiency. We conduct extensive experiments on GPT, BERT, and T5. Results\ndemonstrate that TBA effectively reduces 47% of the activation peak memory\nusage. At the same time, TBA perfectly overlaps the I/O with the computation\nand incurs negligible performance overhead. We introduce the\nrecompute-offload-keep (ROK) curve to compare the TBA offloading with other two\ntensor placement strategies, keeping activations in memory and layerwise full\nrecomputation. We find that TBA achieves better memory savings than layerwise\nfull recomputation while retaining the performance of keeping the activations\nin memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth rate of the GPU memory capacity has not been able to keep up with\nthat of the size of large language models (LLMs), hindering the model training\nprocess. In particular, activations -- the intermediate tensors produced during\nforward propagation and reused in backward propagation -- dominate the GPU\nmemory use. To address this challenge, we propose TBA to efficiently offload\nactivations to high-capacity NVMe SSDs. This approach reduces GPU memory usage\nwithout impacting performance by adaptively overlapping data transfers with\ncomputation. TBA is compatible with popular deep learning frameworks like\nPyTorch, Megatron, and DeepSpeed, and it employs techniques such as tensor\ndeduplication, forwarding, and adaptive offloading to further enhance\nefficiency. We conduct extensive experiments on GPT, BERT, and T5. Results\ndemonstrate that TBA effectively reduces 47% of the activation peak memory\nusage. At the same time, TBA perfectly overlaps the I/O with the computation\nand incurs negligible performance overhead. We introduce the\nrecompute-offload-keep (ROK) curve to compare the TBA offloading with other two\ntensor placement strategies, keeping activations in memory and layerwise full\nrecomputation. We find that TBA achieves better memory savings than layerwise\nfull recomputation while retaining the performance of keeping the activations\nin memory."
                },
                "authors": [
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Mert Hidayetoğlu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Steven Sam Lumetta"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17631v2",
                "updated": "2024-08-19T13:59:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    59,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-24T20:44:36Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    20,
                    44,
                    36,
                    2,
                    206,
                    0
                ],
                "title": "BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning"
                },
                "summary": "Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement."
                },
                "authors": [
                    {
                        "name": "Partha Chakraborty"
                    },
                    {
                        "name": "Mahmoud Alfadel"
                    },
                    {
                        "name": "Meiyappan Nagappan"
                    }
                ],
                "author_detail": {
                    "name": "Meiyappan Nagappan"
                },
                "author": "Meiyappan Nagappan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09937v2",
                "updated": "2024-08-19T13:55:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    55,
                    42,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-15T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    3,
                    41,
                    0,
                    106,
                    0
                ],
                "title": "Compression Represents Intelligence Linearly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Represents Intelligence Linearly"
                },
                "summary": "There is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 31 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs' intelligence -- reflected by average benchmark\nscores -- almost linearly correlates with their ability to compress external\ntext corpora. These results provide concrete evidence supporting the belief\nthat superior compression indicates greater intelligence. Furthermore, our\nfindings suggest that compression efficiency, as an unsupervised metric derived\nfrom raw text corpora, serves as a reliable evaluation measure that is linearly\nassociated with the model capabilities. We open-source our compression datasets\nas well as our data collection pipelines to facilitate future researchers to\nassess compression properly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 31 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs' intelligence -- reflected by average benchmark\nscores -- almost linearly correlates with their ability to compress external\ntext corpora. These results provide concrete evidence supporting the belief\nthat superior compression indicates greater intelligence. Furthermore, our\nfindings suggest that compression efficiency, as an unsupervised metric derived\nfrom raw text corpora, serves as a reliable evaluation measure that is linearly\nassociated with the model capabilities. We open-source our compression datasets\nas well as our data collection pipelines to facilitate future researchers to\nassess compression properly."
                },
                "authors": [
                    {
                        "name": "Yuzhen Huang"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "arxiv_comment": "COLM 2024. Data and code are available at\n  https://github.com/hkust-nlp/llm-compression-intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03416v2",
                "updated": "2024-08-19T13:42:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    42,
                    9,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-06T19:30:49Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    19,
                    30,
                    49,
                    1,
                    219,
                    0
                ],
                "title": "The AI-Native Software Development Lifecycle: A Theoretical and\n  Practical New Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI-Native Software Development Lifecycle: A Theoretical and\n  Practical New Methodology"
                },
                "summary": "As AI continues to advance and impact every phase of the software development\nlifecycle (SDLC), a need for a new way of building software will emerge. By\nanalyzing the factors that influence the current state of the SDLC and how\nthose will change with AI we propose a new model of development. This white\npaper proposes the emergence of a fully AI-native SDLC, where AI is integrated\nseamlessly into every phase of development, from planning to deployment. We\nintroduce the V-Bounce model, an adaptation of the traditional V-model that\nincorporates AI from end to end. The V-Bounce model leverages AI to\ndramatically reduce time spent in implementation phases, shifting emphasis\ntowards requirements gathering, architecture design, and continuous validation.\nThis model redefines the role of humans from primary implementers to primarily\nvalidators and verifiers with AI acting as an implementation engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI continues to advance and impact every phase of the software development\nlifecycle (SDLC), a need for a new way of building software will emerge. By\nanalyzing the factors that influence the current state of the SDLC and how\nthose will change with AI we propose a new model of development. This white\npaper proposes the emergence of a fully AI-native SDLC, where AI is integrated\nseamlessly into every phase of development, from planning to deployment. We\nintroduce the V-Bounce model, an adaptation of the traditional V-model that\nincorporates AI from end to end. The V-Bounce model leverages AI to\ndramatically reduce time spent in implementation phases, shifting emphasis\ntowards requirements gathering, architecture design, and continuous validation.\nThis model redefines the role of humans from primary implementers to primarily\nvalidators and verifiers with AI acting as an implementation engine."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    }
                ],
                "author_detail": {
                    "name": "Cory Hymel"
                },
                "author": "Cory Hymel",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09982v2",
                "updated": "2024-08-20T02:41:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    41,
                    13,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T13:32:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    32,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "Application of Large Language Models in Automated Question Generation: A\n  Case Study on ChatGLM's Structured Questions for National Teacher\n  Certification Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of Large Language Models in Automated Question Generation: A\n  Case Study on ChatGLM's Structured Questions for National Teacher\n  Certification Exams"
                },
                "summary": "This study delves into the application potential of the large language models\n(LLMs) ChatGLM in the automatic generation of structured questions for National\nTeacher Certification Exams (NTCE). Through meticulously designed prompt\nengineering, we guided ChatGLM to generate a series of simulated questions and\nconducted a comprehensive comparison with questions recollected from past\nexaminees. To ensure the objectivity and professionalism of the evaluation, we\ninvited experts in the field of education to assess these questions and their\nscoring criteria. The research results indicate that the questions generated by\nChatGLM exhibit a high level of rationality, scientificity, and practicality\nsimilar to those of the real exam questions across most evaluation criteria,\ndemonstrating the model's accuracy and reliability in question generation.\nNevertheless, the study also reveals limitations in the model's consideration\nof various rating criteria when generating questions, suggesting the need for\nfurther optimization and adjustment. This research not only validates the\napplication potential of ChatGLM in the field of educational assessment but\nalso provides crucial empirical support for the development of more efficient\nand intelligent educational automated generation systems in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study delves into the application potential of the large language models\n(LLMs) ChatGLM in the automatic generation of structured questions for National\nTeacher Certification Exams (NTCE). Through meticulously designed prompt\nengineering, we guided ChatGLM to generate a series of simulated questions and\nconducted a comprehensive comparison with questions recollected from past\nexaminees. To ensure the objectivity and professionalism of the evaluation, we\ninvited experts in the field of education to assess these questions and their\nscoring criteria. The research results indicate that the questions generated by\nChatGLM exhibit a high level of rationality, scientificity, and practicality\nsimilar to those of the real exam questions across most evaluation criteria,\ndemonstrating the model's accuracy and reliability in question generation.\nNevertheless, the study also reveals limitations in the model's consideration\nof various rating criteria when generating questions, suggesting the need for\nfurther optimization and adjustment. This research not only validates the\napplication potential of ChatGLM in the field of educational assessment but\nalso provides crucial empirical support for the development of more efficient\nand intelligent educational automated generation systems in the future."
                },
                "authors": [
                    {
                        "name": "Ling He"
                    },
                    {
                        "name": "Yanxin Chen"
                    },
                    {
                        "name": "Xiaoqiang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqiang Hu"
                },
                "author": "Xiaoqiang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13602v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13602v4",
                "updated": "2024-08-19T13:27:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    27,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-02-21T08:09:05Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    8,
                    9,
                    5,
                    2,
                    52,
                    0
                ],
                "title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving"
                },
                "summary": "Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems."
                },
                "authors": [
                    {
                        "name": "Mehdi Azarafza"
                    },
                    {
                        "name": "Mojtaba Nayyeri"
                    },
                    {
                        "name": "Charles Steinmetz"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Achim Rettberg"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettberg"
                },
                "author": "Achim Rettberg",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13602v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13602v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09972v1",
                "updated": "2024-08-19T13:19:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    19,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T13:19:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    19,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with\n  Large Language Models"
                },
                "summary": "Integrating large language models (LLMs) into autonomous driving enhances\npersonalization and adaptability in open-world scenarios. However, traditional\nedge computing models still face significant challenges in processing complex\ndriving data, particularly regarding real-time performance and system\nefficiency. To address these challenges, this study introduces EC-Drive, a\nnovel edge-cloud collaborative autonomous driving system with data drift\ndetection capabilities. EC-Drive utilizes drift detection algorithms to\nselectively upload critical data, including new obstacles and traffic pattern\nchanges, to the cloud for processing by GPT-4, while routine data is\nefficiently managed by smaller LLMs on edge devices. This approach not only\nreduces inference latency but also improves system efficiency by optimizing\ncommunication resource use. Experimental validation confirms the system's\nrobust processing capabilities and practical applicability in real-world\ndriving conditions, demonstrating the effectiveness of this edge-cloud\ncollaboration framework. Our data and system demonstration will be released at\nhttps://sites.google.com/view/ec-drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) into autonomous driving enhances\npersonalization and adaptability in open-world scenarios. However, traditional\nedge computing models still face significant challenges in processing complex\ndriving data, particularly regarding real-time performance and system\nefficiency. To address these challenges, this study introduces EC-Drive, a\nnovel edge-cloud collaborative autonomous driving system with data drift\ndetection capabilities. EC-Drive utilizes drift detection algorithms to\nselectively upload critical data, including new obstacles and traffic pattern\nchanges, to the cloud for processing by GPT-4, while routine data is\nefficiently managed by smaller LLMs on edge devices. This approach not only\nreduces inference latency but also improves system efficiency by optimizing\ncommunication resource use. Experimental validation confirms the system's\nrobust processing capabilities and practical applicability in real-world\ndriving conditions, demonstrating the effectiveness of this edge-cloud\ncollaboration framework. Our data and system demonstration will be released at\nhttps://sites.google.com/view/ec-drive."
                },
                "authors": [
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Suyan Dai"
                    },
                    {
                        "name": "Fangfang Chen"
                    },
                    {
                        "name": "Zuohong Lv"
                    },
                    {
                        "name": "Jianhua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tang"
                },
                "author": "Jianhua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09955v2",
                "updated": "2024-08-20T05:51:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    51,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T12:55:16Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    55,
                    16,
                    0,
                    232,
                    0
                ],
                "title": "MegaAgent: A Practical Framework for Autonomous Cooperation in\n  Large-Scale LLM Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaAgent: A Practical Framework for Autonomous Cooperation in\n  Large-Scale LLM Agent Systems"
                },
                "summary": "With the emergence of large language models (LLMs), LLM-powered multi-agent\nsystems (LLM-MA systems) have been proposed to tackle real-world tasks.\nHowever, their agents mostly follow predefined Standard Operating Procedures\n(SOPs) that remain unchanged across the whole interaction, lacking autonomy and\nscalability. Additionally, current solutions often overlook the necessity for\neffective agent cooperation. To address the above limitations, we propose\nMegaAgent, a practical framework designed for autonomous cooperation in\nlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents to\ndynamically generate agents based on task requirements, incorporating features\nsuch as automatically dividing tasks, systematic planning and monitoring of\nagent activities, and managing concurrent operations. In addition, MegaAgent is\ndesigned with a hierarchical structure and employs system-level parallelism to\nenhance performance and boost communication. We demonstrate the effectiveness\nof MegaAgent through Gobang game development, showing that it outperforms\npopular LLM-MA systems; and national policy simulation, demonstrating its high\nautonomy and potential to rapidly scale up to 590 agents while ensuring\neffective cooperation among them. Our results indicate that MegaAgent is the\nfirst autonomous large-scale LLM-MA system with no pre-defined SOPs, high\neffectiveness and scalability, paving the way for further research in this\nfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of large language models (LLMs), LLM-powered multi-agent\nsystems (LLM-MA systems) have been proposed to tackle real-world tasks.\nHowever, their agents mostly follow predefined Standard Operating Procedures\n(SOPs) that remain unchanged across the whole interaction, lacking autonomy and\nscalability. Additionally, current solutions often overlook the necessity for\neffective agent cooperation. To address the above limitations, we propose\nMegaAgent, a practical framework designed for autonomous cooperation in\nlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents to\ndynamically generate agents based on task requirements, incorporating features\nsuch as automatically dividing tasks, systematic planning and monitoring of\nagent activities, and managing concurrent operations. In addition, MegaAgent is\ndesigned with a hierarchical structure and employs system-level parallelism to\nenhance performance and boost communication. We demonstrate the effectiveness\nof MegaAgent through Gobang game development, showing that it outperforms\npopular LLM-MA systems; and national policy simulation, demonstrating its high\nautonomy and potential to rapidly scale up to 590 agents while ensuring\neffective cooperation among them. Our results indicate that MegaAgent is the\nfirst autonomous large-scale LLM-MA system with no pre-defined SOPs, high\neffectiveness and scalability, paving the way for further research in this\nfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Jingsheng Liang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11897v3",
                "updated": "2024-08-19T12:47:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    47,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2023-12-19T06:42:47Z",
                "published_parsed": [
                    2023,
                    12,
                    19,
                    6,
                    42,
                    47,
                    1,
                    353,
                    0
                ],
                "title": "Text-Conditioned Resampler For Long Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Conditioned Resampler For Long Form Video Understanding"
                },
                "summary": "In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge."
                },
                "authors": [
                    {
                        "name": "Bruno Korbar"
                    },
                    {
                        "name": "Yongqin Xian"
                    },
                    {
                        "name": "Alessio Tonioni"
                    },
                    {
                        "name": "Andrew Zisserman"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "arxiv_comment": "Accepted to the ECCV24 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08164v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08164v5",
                "updated": "2024-08-19T12:44:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    44,
                    27,
                    0,
                    232,
                    0
                ],
                "published": "2023-10-12T09:36:03Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    9,
                    36,
                    3,
                    3,
                    285,
                    0
                ],
                "title": "Interpreting Learned Feedback Patterns in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Learned Feedback Patterns in Large Language Models"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs."
                },
                "authors": [
                    {
                        "name": "Luke Marks"
                    },
                    {
                        "name": "Amir Abdullah"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Rauno Arike"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08164v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08164v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09946v1",
                "updated": "2024-08-19T12:35:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    35,
                    23,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T12:35:23Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    35,
                    23,
                    0,
                    232,
                    0
                ],
                "title": "Microscopic Analysis on LLM players via Social Deduction Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microscopic Analysis on LLM players via Social Deduction Game"
                },
                "summary": "Recent studies have begun developing autonomous game players for social\ndeduction games using large language models (LLMs). When building LLM players,\nfine-grained evaluations are crucial for addressing weaknesses in game-playing\nabilities. However, existing studies have often overlooked such assessments.\nSpecifically, we point out two issues with the evaluation methods employed.\nFirst, game-playing abilities have typically been assessed through game-level\noutcomes rather than specific event-level skills; Second, error analyses have\nlacked structured methodologies. To address these issues, we propose an\napproach utilizing a variant of the SpyFall game, named SpyGame. We conducted\nan experiment with four LLMs, analyzing their gameplay behavior in SpyGame both\nquantitatively and qualitatively. For the quantitative analysis, we introduced\neight metrics to resolve the first issue, revealing that these metrics are more\neffective than existing ones for evaluating the two critical skills: intent\nidentification and camouflage. In the qualitative analysis, we performed\nthematic analysis to resolve the second issue. This analysis identifies four\nmajor categories that affect gameplay of LLMs. Additionally, we demonstrate how\nthese categories complement and support the findings from the quantitative\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have begun developing autonomous game players for social\ndeduction games using large language models (LLMs). When building LLM players,\nfine-grained evaluations are crucial for addressing weaknesses in game-playing\nabilities. However, existing studies have often overlooked such assessments.\nSpecifically, we point out two issues with the evaluation methods employed.\nFirst, game-playing abilities have typically been assessed through game-level\noutcomes rather than specific event-level skills; Second, error analyses have\nlacked structured methodologies. To address these issues, we propose an\napproach utilizing a variant of the SpyFall game, named SpyGame. We conducted\nan experiment with four LLMs, analyzing their gameplay behavior in SpyGame both\nquantitatively and qualitatively. For the quantitative analysis, we introduced\neight metrics to resolve the first issue, revealing that these metrics are more\neffective than existing ones for evaluating the two critical skills: intent\nidentification and camouflage. In the qualitative analysis, we performed\nthematic analysis to resolve the second issue. This analysis identifies four\nmajor categories that affect gameplay of LLMs. Additionally, we demonstrate how\nthese categories complement and support the findings from the quantitative\nanalysis."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Under review, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09945v1",
                "updated": "2024-08-19T12:34:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T12:34:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Lianzhang Lou"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14362v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14362v4",
                "updated": "2024-08-19T12:28:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    28,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-21T12:45:01Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    12,
                    45,
                    1,
                    3,
                    81,
                    0
                ],
                "title": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics"
                },
                "summary": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Yue"
                    },
                    {
                        "name": "Jiancheng Zhao"
                    },
                    {
                        "name": "Chunhui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chunhui Zhao"
                },
                "author": "Chunhui Zhao",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14362v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14362v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09916v1",
                "updated": "2024-08-19T11:44:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T11:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Tingting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Liu"
                },
                "author": "Tingting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06355v3",
                "updated": "2024-08-19T11:43:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    43,
                    18,
                    0,
                    232,
                    0
                ],
                "published": "2023-12-11T13:03:39Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    13,
                    3,
                    39,
                    0,
                    345,
                    0
                ],
                "title": "Linguistic and Structural Basis of Engineering Design Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic and Structural Basis of Engineering Design Knowledge"
                },
                "summary": "Natural language artefact descriptions are primary carriers of engineering\ndesign knowledge, whose retrieval, representation, and reuse are fundamental to\nsupporting knowledge-intensive tasks in the design process. In this paper, we\nexplicate design knowledge from patented artefact descriptions as knowledge\ngraphs and examine these to understand the linguistic and structural basis. The\npurpose of our work is to advance the traditional and ontological perspectives\nof design knowledge and to guide Large-Language Models (LLMs) on how to\narticulate natural language responses that reflect knowledge that is valuable\nin a design environment. We populate 33,881 knowledge graphs from a sample of\npatents stratified according to technology classes. For linguistic basis, we\nconduct Zipf distribution analyses on the frequencies of unique entities and\nrelationships to identify 64 and 37 generalisable linguistic syntaxes\nrespectively. The relationships largely represent attributes ('of'), structure\n('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification\n('such as'), and behaviour ('to', 'from'). For structural basis, we draw\ninspiration from various studies on biological/ecological networks and discover\nmotifs from patent knowledge graphs. We identify four 3-node and four 4-node\nsubgraph patterns that could be converged and simplified into sequence\n[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these\nresults, we suggest concretisation strategies for entities and relationships\nand explicating hierarchical structures, potentially aiding the construction\nand modularisation of design knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language artefact descriptions are primary carriers of engineering\ndesign knowledge, whose retrieval, representation, and reuse are fundamental to\nsupporting knowledge-intensive tasks in the design process. In this paper, we\nexplicate design knowledge from patented artefact descriptions as knowledge\ngraphs and examine these to understand the linguistic and structural basis. The\npurpose of our work is to advance the traditional and ontological perspectives\nof design knowledge and to guide Large-Language Models (LLMs) on how to\narticulate natural language responses that reflect knowledge that is valuable\nin a design environment. We populate 33,881 knowledge graphs from a sample of\npatents stratified according to technology classes. For linguistic basis, we\nconduct Zipf distribution analyses on the frequencies of unique entities and\nrelationships to identify 64 and 37 generalisable linguistic syntaxes\nrespectively. The relationships largely represent attributes ('of'), structure\n('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification\n('such as'), and behaviour ('to', 'from'). For structural basis, we draw\ninspiration from various studies on biological/ecological networks and discover\nmotifs from patent knowledge graphs. We identify four 3-node and four 4-node\nsubgraph patterns that could be converged and simplified into sequence\n[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these\nresults, we suggest concretisation strategies for entities and relationships\nand explicating hierarchical structures, potentially aiding the construction\nand modularisation of design knowledge."
                },
                "authors": [
                    {
                        "name": "L. Siddharth"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "arxiv_comment": "The data for this research is made available at Zenodo -\n  https://zenodo.org/doi/10.5281/zenodo.13328257",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13213v2",
                "updated": "2024-08-19T11:38:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    38,
                    14,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-19T04:53:48Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    4,
                    53,
                    48,
                    2,
                    171,
                    0
                ],
                "title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\n  Filtering with LLM-Extracted Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\n  Filtering with LLM-Extracted Metadata"
                },
                "summary": "The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG."
                },
                "authors": [
                    {
                        "name": "Mykhailo Poliakov"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Accepted to ICTERI 2024 Posters Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v1",
                "updated": "2024-08-19T11:09:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09878v1",
                "updated": "2024-08-19T10:39:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    39,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:39:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    39,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Transferring Backdoors between Large Language Models by Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferring Backdoors between Large Language Models by Knowledge\n  Distillation"
                },
                "summary": "Backdoor Attacks have been a serious vulnerability against Large Language\nModels (LLMs). However, previous methods only reveal such risk in specific\nmodels, or present tasks transferability after attacking the pre-trained phase.\nSo, how risky is the model transferability of a backdoor attack? In this paper,\nwe focus on whether existing mini-LLMs may be unconsciously instructed in\nbackdoor knowledge by poisoned teacher LLMs through knowledge distillation\n(KD). Specifically, we propose ATBA, an adaptive transferable backdoor attack,\nwhich can effectively distill the backdoor of teacher LLMs into small models\nwhen only executing clean-tuning. We first propose the Target Trigger\nGeneration (TTG) module that filters out a set of indicative trigger candidates\nfrom the token list based on cosine similarity distribution. Then, we exploit a\nshadow model to imitate the distilling process and introduce an Adaptive\nTrigger Optimization (ATO) module to realize a gradient-based greedy feedback\nto search optimal triggers. Extensive experiments show that ATBA generates not\nonly positive guidance for student models but also implicitly transfers\nbackdoor knowledge. Our attack is robust and stealthy, with over 80% backdoor\ntransferability, and hopes the attention of security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor Attacks have been a serious vulnerability against Large Language\nModels (LLMs). However, previous methods only reveal such risk in specific\nmodels, or present tasks transferability after attacking the pre-trained phase.\nSo, how risky is the model transferability of a backdoor attack? In this paper,\nwe focus on whether existing mini-LLMs may be unconsciously instructed in\nbackdoor knowledge by poisoned teacher LLMs through knowledge distillation\n(KD). Specifically, we propose ATBA, an adaptive transferable backdoor attack,\nwhich can effectively distill the backdoor of teacher LLMs into small models\nwhen only executing clean-tuning. We first propose the Target Trigger\nGeneration (TTG) module that filters out a set of indicative trigger candidates\nfrom the token list based on cosine similarity distribution. Then, we exploit a\nshadow model to imitate the distilling process and introduce an Adaptive\nTrigger Optimization (ATO) module to realize a gradient-based greedy feedback\nto search optimal triggers. Extensive experiments show that ATBA generates not\nonly positive guidance for student models but also implicitly transfers\nbackdoor knowledge. Our attack is robust and stealthy, with over 80% backdoor\ntransferability, and hopes the attention of security."
                },
                "authors": [
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Zhuosheng Zhang Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuosheng Zhang Gongshen Liu"
                },
                "author": "Zhuosheng Zhang Gongshen Liu",
                "arxiv_comment": "13 pages, 16 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03297v2",
                "updated": "2024-08-19T10:38:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    38,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-06T16:55:54Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    55,
                    54,
                    1,
                    219,
                    0
                ],
                "title": "KnowPO: Knowledge-aware Preference Optimization for Controllable\n  Knowledge Selection in Retrieval-Augmented Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowPO: Knowledge-aware Preference Optimization for Controllable\n  Knowledge Selection in Retrieval-Augmented Language Models"
                },
                "summary": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors such\nas contextual ignorance and contextual overinclusion. To this end, we propose a\nKnowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at\nachieving adaptive knowledge selection based on contextual relevance in real\nretrieval scenarios. Concretely, we proposed a general paradigm for\nconstructing knowledge conflict datasets, which comprehensively cover various\nerror types and learn how to avoid these negative signals through preference\noptimization methods. Simultaneously, we proposed a rewriting strategy and data\nratio optimization strategy to address preference imbalances. Experimental\nresults show that KnowPO outperforms previous methods for handling knowledge\nconflicts by over 37\\%, while also exhibiting robust generalization across\nvarious out-of-distribution datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors such\nas contextual ignorance and contextual overinclusion. To this end, we propose a\nKnowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at\nachieving adaptive knowledge selection based on contextual relevance in real\nretrieval scenarios. Concretely, we proposed a general paradigm for\nconstructing knowledge conflict datasets, which comprehensively cover various\nerror types and learn how to avoid these negative signals through preference\noptimization methods. Simultaneously, we proposed a rewriting strategy and data\nratio optimization strategy to address preference imbalances. Experimental\nresults show that KnowPO outperforms previous methods for handling knowledge\nconflicts by over 37\\%, while also exhibiting robust generalization across\nvarious out-of-distribution datasets."
                },
                "authors": [
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Yuzhen Xiao"
                    },
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09865v1",
                "updated": "2024-08-19T10:12:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    12,
                    52,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:12:52Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    12,
                    52,
                    0,
                    232,
                    0
                ],
                "title": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in\n  Explainable Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in\n  Explainable Recommendation"
                },
                "summary": "Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models treat review-generation as a proxy of explainable recommendation.\nAlthough they are able to generate fluent and grammatical sentences, they\nsuffer from generality and hallucination issues. We propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it\nintegrates aspect category as another input dimension to facilitate the\nmemorization of fine-grained aspect terms. Experiments on two real-world review\ndatasets in restaurant domain show that MAPLE outperforms the baseline\nreview-generation models in terms of text and feature diversity while\nmaintaining excellent coherence and factual relevance. We further treat MAPLE\nas a retriever component in the retriever-reader framework and employ a\nLarge-Language Model (LLM) as the reader, showing that MAPLE's explanation\nalong with the LLM's comprehension ability leads to enriched and personalized\nexplanation as a result. We will release the code and data in this http upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models treat review-generation as a proxy of explainable recommendation.\nAlthough they are able to generate fluent and grammatical sentences, they\nsuffer from generality and hallucination issues. We propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it\nintegrates aspect category as another input dimension to facilitate the\nmemorization of fine-grained aspect terms. Experiments on two real-world review\ndatasets in restaurant domain show that MAPLE outperforms the baseline\nreview-generation models in terms of text and feature diversity while\nmaintaining excellent coherence and factual relevance. We further treat MAPLE\nas a retriever component in the retriever-reader framework and employ a\nLarge-Language Model (LLM) as the reader, showing that MAPLE's explanation\nalong with the LLM's comprehension ability leads to enriched and personalized\nexplanation as a result. We will release the code and data in this http upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Ching-Wen Yang"
                    },
                    {
                        "name": "Che Wei Chen"
                    },
                    {
                        "name": "Kun-da Wu"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Jui-Feng Yao"
                    },
                    {
                        "name": "Hung-Yu Kao"
                    }
                ],
                "author_detail": {
                    "name": "Hung-Yu Kao"
                },
                "author": "Hung-Yu Kao",
                "arxiv_comment": "8 main pages, 10 pages for appendix. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09859v1",
                "updated": "2024-08-19T10:07:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    7,
                    0,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T10:07:00Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    7,
                    0,
                    0,
                    232,
                    0
                ],
                "title": "OccMamba: Semantic Occupancy Prediction with State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OccMamba: Semantic Occupancy Prediction with State Space Models"
                },
                "summary": "Training deep learning models for semantic occupancy prediction is\nchallenging due to factors such as a large number of occupancy cells, severe\nocclusion, limited visual cues, complicated driving scenarios, etc. Recent\nmethods often adopt transformer-based architectures given their strong\ncapability in learning input-conditioned weights and long-range relationships.\nHowever, transformer-based networks are notorious for their quadratic\ncomputation complexity, seriously undermining their efficacy and deployment in\nsemantic occupancy prediction. Inspired by the global modeling and linear\ncomputation complexity of the Mamba architecture, we present the first\nMamba-based network for semantic occupancy prediction, termed OccMamba.\nHowever, directly applying the Mamba architecture to the occupancy prediction\ntask yields unsatisfactory performance due to the inherent domain gap between\nthe linguistic and 3D domains. To relieve this problem, we present a simple yet\neffective 3D-to-1D reordering operation, i.e., height-prioritized 2D Hilbert\nexpansion. It can maximally retain the spatial structure of point clouds as\nwell as facilitate the processing of Mamba blocks. Our OccMamba achieves\nstate-of-the-art performance on three prevalent occupancy prediction\nbenchmarks, including OpenOccupancy, SemanticKITTI and SemanticPOSS. Notably,\non OpenOccupancy, our OccMamba outperforms the previous state-of-the-art Co-Occ\nby 3.1% IoU and 3.2% mIoU, respectively. Codes will be released upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training deep learning models for semantic occupancy prediction is\nchallenging due to factors such as a large number of occupancy cells, severe\nocclusion, limited visual cues, complicated driving scenarios, etc. Recent\nmethods often adopt transformer-based architectures given their strong\ncapability in learning input-conditioned weights and long-range relationships.\nHowever, transformer-based networks are notorious for their quadratic\ncomputation complexity, seriously undermining their efficacy and deployment in\nsemantic occupancy prediction. Inspired by the global modeling and linear\ncomputation complexity of the Mamba architecture, we present the first\nMamba-based network for semantic occupancy prediction, termed OccMamba.\nHowever, directly applying the Mamba architecture to the occupancy prediction\ntask yields unsatisfactory performance due to the inherent domain gap between\nthe linguistic and 3D domains. To relieve this problem, we present a simple yet\neffective 3D-to-1D reordering operation, i.e., height-prioritized 2D Hilbert\nexpansion. It can maximally retain the spatial structure of point clouds as\nwell as facilitate the processing of Mamba blocks. Our OccMamba achieves\nstate-of-the-art performance on three prevalent occupancy prediction\nbenchmarks, including OpenOccupancy, SemanticKITTI and SemanticPOSS. Notably,\non OpenOccupancy, our OccMamba outperforms the previous state-of-the-art Co-Occ\nby 3.1% IoU and 3.2% mIoU, respectively. Codes will be released upon\npublication."
                },
                "authors": [
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Yuenan Hou"
                    },
                    {
                        "name": "Xiaohan Xing"
                    },
                    {
                        "name": "Xiao Sun"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09853v1",
                "updated": "2024-08-19T09:57:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    57,
                    28,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:57:28Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    57,
                    28,
                    0,
                    232,
                    0
                ],
                "title": "Self-Directed Turing Test for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Directed Turing Test for Large Language Models"
                },
                "summary": "The Turing test examines whether AIs can exhibit human-like behaviour in\nnatural language conversations. Traditional Turing tests adopt a rigid dialogue\nformat where each participant sends only one message each time and require\ncontinuous human involvement to direct the entire interaction with the test\nsubject. This fails to reflect a natural conversational style and hinders the\nevaluation of Large Language Models (LLMs) in complex and prolonged dialogues.\nThis paper proposes the Self-Directed Turing Test, which extends the original\ntest with a burst dialogue format, allowing more dynamic exchanges by multiple\nconsecutive messages. It further efficiently reduces human workload by having\nthe LLM self-direct the majority of the test process, iteratively generating\ndialogues that simulate its interaction with humans. With the pseudo-dialogue\nhistory, the model then engages in a shorter dialogue with a human, which is\npaired with a human-human conversation on the same topic to be judged using\nquestionnaires. We introduce the X-Turn Pass-Rate metric to assess the human\nlikeness of LLMs across varying durations. While LLMs like GPT-4 initially\nperform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10\nturns of dialogues respectively, their performance drops as the dialogue\nprogresses, which underscores the difficulty in maintaining consistency in the\nlong term.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing test examines whether AIs can exhibit human-like behaviour in\nnatural language conversations. Traditional Turing tests adopt a rigid dialogue\nformat where each participant sends only one message each time and require\ncontinuous human involvement to direct the entire interaction with the test\nsubject. This fails to reflect a natural conversational style and hinders the\nevaluation of Large Language Models (LLMs) in complex and prolonged dialogues.\nThis paper proposes the Self-Directed Turing Test, which extends the original\ntest with a burst dialogue format, allowing more dynamic exchanges by multiple\nconsecutive messages. It further efficiently reduces human workload by having\nthe LLM self-direct the majority of the test process, iteratively generating\ndialogues that simulate its interaction with humans. With the pseudo-dialogue\nhistory, the model then engages in a shorter dialogue with a human, which is\npaired with a human-human conversation on the same topic to be judged using\nquestionnaires. We introduce the X-Turn Pass-Rate metric to assess the human\nlikeness of LLMs across varying durations. While LLMs like GPT-4 initially\nperform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10\nturns of dialogues respectively, their performance drops as the dialogue\nprogresses, which underscores the difficulty in maintaining consistency in the\nlong term."
                },
                "authors": [
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09851v1",
                "updated": "2024-08-19T09:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    55,
                    22,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:55:22Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    55,
                    22,
                    0,
                    232,
                    0
                ],
                "title": "ISAC-Fi: Enabling Full-fledged Monostatic Sensing over Wi-Fi\n  Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISAC-Fi: Enabling Full-fledged Monostatic Sensing over Wi-Fi\n  Communication"
                },
                "summary": "Whereas Wi-Fi communications have been exploited for sensing purpose for over\na decade, the bistatic or multistatic nature of Wi-Fi still poses multiple\nchallenges, hampering real-life deployment of integrated sensing and\ncommunication (ISAC) within Wi-Fi framework. In this paper, we aim to re-design\nWiFi so that monostatic sensing (mimicking radar) can be achieved over the\nmultistatic communication infrastructure. Specifically, we propose, design, and\nimplement ISAC-Fi as an ISAC-ready Wi-Fi prototype. We first present a novel\nself-interference cancellation scheme, in order to extract reflected (radio\nfrequency) signals for sensing purpose in the face of transmissions. We then\nsubtly revise existing Wi-Fi framework so as to seamlessly operate monostatic\nsensing under Wi-Fi communication standard. Finally, we offer two ISAC-Fi\ndesigns: while a USRP-based one emulates a totally re-designed ISAC-Fi device,\nanother plug-andplay design allows for backward compatibility by attaching an\nextra module to an arbitrary Wi-Fi device. We perform extensive experiments to\nvalidate the efficacy of ISAC-Fi and also to demonstrate its superiority over\nexisting Wi-Fi sensing proposals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whereas Wi-Fi communications have been exploited for sensing purpose for over\na decade, the bistatic or multistatic nature of Wi-Fi still poses multiple\nchallenges, hampering real-life deployment of integrated sensing and\ncommunication (ISAC) within Wi-Fi framework. In this paper, we aim to re-design\nWiFi so that monostatic sensing (mimicking radar) can be achieved over the\nmultistatic communication infrastructure. Specifically, we propose, design, and\nimplement ISAC-Fi as an ISAC-ready Wi-Fi prototype. We first present a novel\nself-interference cancellation scheme, in order to extract reflected (radio\nfrequency) signals for sensing purpose in the face of transmissions. We then\nsubtly revise existing Wi-Fi framework so as to seamlessly operate monostatic\nsensing under Wi-Fi communication standard. Finally, we offer two ISAC-Fi\ndesigns: while a USRP-based one emulates a totally re-designed ISAC-Fi device,\nanother plug-andplay design allows for backward compatibility by attaching an\nextra module to an arbitrary Wi-Fi device. We perform extensive experiments to\nvalidate the efficacy of ISAC-Fi and also to demonstrate its superiority over\nexisting Wi-Fi sensing proposals."
                },
                "authors": [
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Chao Hu"
                    },
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Hangcheng Cao"
                    },
                    {
                        "name": "Yanbing Yang"
                    },
                    {
                        "name": "Yen Chu"
                    },
                    {
                        "name": "Hongbo Jiang"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "14 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09849v1",
                "updated": "2024-08-19T09:51:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    51,
                    2,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:51:02Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    51,
                    2,
                    0,
                    232,
                    0
                ],
                "title": "Importance Weighting Can Help Large Language Models Self-Improve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance Weighting Can Help Large Language Models Self-Improve"
                },
                "summary": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models."
                },
                "authors": [
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Chi-min Chan"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10868v3",
                "updated": "2024-08-20T09:25:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    25,
                    23,
                    1,
                    233,
                    0
                ],
                "published": "2024-06-16T09:36:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    36,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts"
                },
                "summary": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ncontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can we\naddress the challenge of long-form text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15610v2",
                "updated": "2024-08-19T09:46:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    46,
                    4,
                    0,
                    232,
                    0
                ],
                "published": "2024-05-24T14:53:33Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    14,
                    53,
                    33,
                    4,
                    145,
                    0
                ],
                "title": "Increasing Efficiency and Result Reliability of Continuous Benchmarking\n  for FaaS Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing Efficiency and Result Reliability of Continuous Benchmarking\n  for FaaS Applications"
                },
                "summary": "In a continuous deployment setting, Function-as-a-Service (FaaS) applications\nfrequently receive updated releases, each of which can cause a performance\nregression. While continuous benchmarking, i.e., comparing benchmark results of\nthe updated and the previous version, can detect such regressions, performance\nvariability of FaaS platforms necessitates thousands of function calls, thus,\nmaking continuous benchmarking time-intensive and expensive.\n  In this paper, we propose DuetFaaS, an approach which adapts duet\nbenchmarking to FaaS applications. With DuetFaaS, we deploy two versions of\nFaaS function in a single cloud function instance and execute them in parallel\nto reduce the impact of platform variability. We evaluate our approach against\nstate-of-the-art approaches, running on AWS Lambda.\n  Overall, DuetFaaS requires fewer invocations to accurately detect performance\nregressions than other state-of-the-art approaches. In 98.41% of evaluated\ncases, our approach provides equal or smaller confidence interval size.\nDuetFaaS achieves an interval size reduction in 59.06% of all evaluated sample\nsizes when compared to the competitive approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a continuous deployment setting, Function-as-a-Service (FaaS) applications\nfrequently receive updated releases, each of which can cause a performance\nregression. While continuous benchmarking, i.e., comparing benchmark results of\nthe updated and the previous version, can detect such regressions, performance\nvariability of FaaS platforms necessitates thousands of function calls, thus,\nmaking continuous benchmarking time-intensive and expensive.\n  In this paper, we propose DuetFaaS, an approach which adapts duet\nbenchmarking to FaaS applications. With DuetFaaS, we deploy two versions of\nFaaS function in a single cloud function instance and execute them in parallel\nto reduce the impact of platform variability. We evaluate our approach against\nstate-of-the-art approaches, running on AWS Lambda.\n  Overall, DuetFaaS requires fewer invocations to accurately detect performance\nregressions than other state-of-the-art approaches. In 98.41% of evaluated\ncases, our approach provides equal or smaller confidence interval size.\nDuetFaaS achieves an interval size reduction in 59.06% of all evaluated sample\nsizes when compared to the competitive approaches."
                },
                "authors": [
                    {
                        "name": "Tim C. Rese"
                    },
                    {
                        "name": "Nils Japke"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_comment": "Accepted for publication in 12th IEEE International Conference on\n  Cloud Engineering (IC2E 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09834v1",
                "updated": "2024-08-19T09:29:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:29:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Minor DPO reject penalty to increase training robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor DPO reject penalty to increase training robustness"
                },
                "summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    },
                    {
                        "name": "Yingfan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yingfan Hu"
                },
                "author": "Yingfan Hu",
                "arxiv_comment": "8 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09831v1",
                "updated": "2024-08-19T09:27:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    27,
                    45,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:27:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    27,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Generated Answers: On the Agreement of Retrieval Models with\n  Humans on Consumer Health Questions"
                },
                "summary": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Most evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required, such as health, and\nwhere misleading or incorrect answers can have a significant impact on a user's\nhealth. Using human experts to evaluate the quality of LLM answers is generally\nconsidered the gold standard, but expert annotation is costly and slow. We\npresent a method for evaluating LLM answers that uses ranking signals as a\nsubstitute for explicit relevance judgements. Our scoring method correlates\nwith the preferences of human experts. We validate it by investigating the\nwell-known fact that the quality of generated answers improves with the size of\nthe model as well as with more sophisticated prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the output of generative large language models (LLMs) is\nchallenging and difficult to scale. Most evaluations of LLMs focus on tasks\nsuch as single-choice question-answering or text classification. These tasks\nare not suitable for assessing open-ended question-answering capabilities,\nwhich are critical in domains where expertise is required, such as health, and\nwhere misleading or incorrect answers can have a significant impact on a user's\nhealth. Using human experts to evaluate the quality of LLM answers is generally\nconsidered the gold standard, but expert annotation is costly and slow. We\npresent a method for evaluating LLM answers that uses ranking signals as a\nsubstitute for explicit relevance judgements. Our scoring method correlates\nwith the preferences of human experts. We validate it by investigating the\nwell-known fact that the quality of generated answers improves with the size of\nthe model as well as with more sophisticated prompting strategies."
                },
                "authors": [
                    {
                        "name": "Sebastian Heineking"
                    },
                    {
                        "name": "Jonas Probst"
                    },
                    {
                        "name": "Daniel Steinbach"
                    },
                    {
                        "name": "Martin Potthast"
                    },
                    {
                        "name": "Harrisen Scells"
                    }
                ],
                "author_detail": {
                    "name": "Harrisen Scells"
                },
                "author": "Harrisen Scells",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09819v1",
                "updated": "2024-08-19T09:15:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    15,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:15:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    15,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language\n  Models"
                },
                "summary": "What a large language model (LLM) would respond in ethically relevant\ncontext? In this paper, we curate a large benchmark CMoralEval for morality\nevaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a\nChinese TV program discussing Chinese moral norms with stories from the society\nand 2) a collection of Chinese moral anomies from various newspapers and\nacademic papers on morality. With these sources, we aim to create a moral\nevaluation dataset characterized by diversity and authenticity. We develop a\nmorality taxonomy and a set of fundamental moral principles that are not only\nrooted in traditional Chinese culture but also consistent with contemporary\nsocietal norms. To facilitate efficient construction and annotation of\ninstances in CMoralEval, we establish a platform with AI-assisted instance\ngeneration to streamline the annotation process. These help us curate\nCMoralEval that encompasses both explicit moral scenarios (14,964 instances)\nand moral dilemma scenarios (15,424 instances), each with instances from\ndifferent data sources. We conduct extensive experiments with CMoralEval to\nexamine a variety of Chinese LLMs. Experiment results demonstrate that\nCMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly\navailable at \\url{https://github.com/tjunlp-lab/CMoralEval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What a large language model (LLM) would respond in ethically relevant\ncontext? In this paper, we curate a large benchmark CMoralEval for morality\nevaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a\nChinese TV program discussing Chinese moral norms with stories from the society\nand 2) a collection of Chinese moral anomies from various newspapers and\nacademic papers on morality. With these sources, we aim to create a moral\nevaluation dataset characterized by diversity and authenticity. We develop a\nmorality taxonomy and a set of fundamental moral principles that are not only\nrooted in traditional Chinese culture but also consistent with contemporary\nsocietal norms. To facilitate efficient construction and annotation of\ninstances in CMoralEval, we establish a platform with AI-assisted instance\ngeneration to streamline the annotation process. These help us curate\nCMoralEval that encompasses both explicit moral scenarios (14,964 instances)\nand moral dilemma scenarios (15,424 instances), each with instances from\ndifferent data sources. We conduct extensive experiments with CMoralEval to\nexamine a variety of Chinese LLMs. Experiment results demonstrate that\nCMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly\navailable at \\url{https://github.com/tjunlp-lab/CMoralEval}."
                },
                "authors": [
                    {
                        "name": "Linhao Yu"
                    },
                    {
                        "name": "Yongqi Leng"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Shang Wu"
                    },
                    {
                        "name": "Haixin Liu"
                    },
                    {
                        "name": "Xinmeng Ji"
                    },
                    {
                        "name": "Jiahui Zhao"
                    },
                    {
                        "name": "Jinwang Song"
                    },
                    {
                        "name": "Tingting Cui"
                    },
                    {
                        "name": "Xiaoqing Cheng"
                    },
                    {
                        "name": "Tao Liu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "Accepted by ACL 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11441v2",
                "updated": "2024-08-19T08:50:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    50,
                    54,
                    0,
                    232,
                    0
                ],
                "published": "2024-05-19T04:31:54Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    4,
                    31,
                    54,
                    6,
                    140,
                    0
                ],
                "title": "EmbSum: Leveraging the Summarization Capabilities of Large Language\n  Models for Content-Based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbSum: Leveraging the Summarization Capabilities of Large Language\n  Models for Content-Based Recommendations"
                },
                "summary": "Content-based recommendation systems play a crucial role in delivering\npersonalized content to users in the digital world. In this work, we introduce\nEmbSum, a novel framework that enables offline pre-computations of users and\ncandidate items while capturing the interactions within the user engagement\nhistory. By utilizing the pretrained encoder-decoder model and poly-attention\nlayers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding\n(CPE) to calculate relevance scores between users and candidate items. EmbSum\nactively learns the long user engagement histories by generating user-interest\nsummary with supervision from large language model (LLM). The effectiveness of\nEmbSum is validated on two datasets from different domains, surpassing\nstate-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\nAdditionally, the model's ability to generate summaries of user interests\nserves as a valuable by-product, enhancing its usefulness for personalized\ncontent recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-based recommendation systems play a crucial role in delivering\npersonalized content to users in the digital world. In this work, we introduce\nEmbSum, a novel framework that enables offline pre-computations of users and\ncandidate items while capturing the interactions within the user engagement\nhistory. By utilizing the pretrained encoder-decoder model and poly-attention\nlayers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding\n(CPE) to calculate relevance scores between users and candidate items. EmbSum\nactively learns the long user engagement histories by generating user-interest\nsummary with supervision from large language model (LLM). The effectiveness of\nEmbSum is validated on two datasets from different domains, surpassing\nstate-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\nAdditionally, the model's ability to generate summaries of user interests\nserves as a valuable by-product, enhancing its usefulness for personalized\ncontent recommendations."
                },
                "authors": [
                    {
                        "name": "Chiyu Zhang"
                    },
                    {
                        "name": "Yifei Sun"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Jie Lei"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Angli Liu"
                    },
                    {
                        "name": "Ji Zhu"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Ning Yao"
                    },
                    {
                        "name": "Bo Long"
                    }
                ],
                "author_detail": {
                    "name": "Bo Long"
                },
                "author": "Bo Long",
                "arxiv_comment": "Accepted by RecSys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09798v1",
                "updated": "2024-08-19T08:44:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    44,
                    55,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:44:55Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    44,
                    55,
                    0,
                    232,
                    0
                ],
                "title": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with\n  Adversarial Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with\n  Adversarial Prompting"
                },
                "summary": "Converting different modalities into generalized text, which then serves as\ninput prompts for large language models (LLMs), is a common approach for\naligning multimodal models, particularly when pairwise data is limited.\nText-centric alignment method leverages the unique properties of text as a\nmodality space, transforming diverse inputs into a unified textual\nrepresentation, thereby enabling downstream models to effectively interpret\nvarious modal inputs. This study evaluates the quality and robustness of\nmultimodal representations in the face of noise imperfections, dynamic input\norder permutations, and missing modalities, revealing that current text-centric\nalignment methods can compromise downstream robustness. To address this issue,\nwe propose a new text-centric adversarial training approach that significantly\nenhances robustness compared to traditional robust training methods and\npre-trained multimodal foundation models. Our findings underscore the potential\nof this approach to improve the robustness and adaptability of multimodal\nrepresentations, offering a promising solution for dynamic and real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting different modalities into generalized text, which then serves as\ninput prompts for large language models (LLMs), is a common approach for\naligning multimodal models, particularly when pairwise data is limited.\nText-centric alignment method leverages the unique properties of text as a\nmodality space, transforming diverse inputs into a unified textual\nrepresentation, thereby enabling downstream models to effectively interpret\nvarious modal inputs. This study evaluates the quality and robustness of\nmultimodal representations in the face of noise imperfections, dynamic input\norder permutations, and missing modalities, revealing that current text-centric\nalignment methods can compromise downstream robustness. To address this issue,\nwe propose a new text-centric adversarial training approach that significantly\nenhances robustness compared to traditional robust training methods and\npre-trained multimodal foundation models. Our findings underscore the potential\nof this approach to improve the robustness and adaptability of multimodal\nrepresentations, offering a promising solution for dynamic and real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Yun-Da Tsai"
                    },
                    {
                        "name": "Ting-Yu Yen"
                    },
                    {
                        "name": "Keng-Te Liao"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.05036",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09794v1",
                "updated": "2024-08-19T08:41:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    41,
                    40,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:41:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    41,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "AutoML-guided Fusion of Entity and LLM-based representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoML-guided Fusion of Entity and LLM-based representations"
                },
                "summary": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (embeddings) do not efficiently\nexploit these resources. Dense and robust representations of documents are\nessential for effectively solving downstream classification and retrieval\ntasks. This work demonstrates that injecting embedded information from\nknowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large semantic knowledge bases are grounded in factual knowledge. However,\nrecent approaches to dense text representations (embeddings) do not efficiently\nexploit these resources. Dense and robust representations of documents are\nessential for effectively solving downstream classification and retrieval\ntasks. This work demonstrates that injecting embedded information from\nknowledge bases can augment the performance of contemporary Large Language\nModel (LLM)-based representations for the task of text classification. Further,\nby considering automated machine learning (AutoML) with the fused\nrepresentation space, we demonstrate it is possible to improve classification\naccuracy even if we use low-dimensional projections of the original\nrepresentation space obtained via efficient matrix factorization. This result\nshows that significantly faster classifiers can be achieved with minimal or no\nloss in predictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets."
                },
                "authors": [
                    {
                        "name": "Boshko Koloski"
                    },
                    {
                        "name": "Senja Pollak"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Blaž Škrlj"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Škrlj"
                },
                "author": "Blaž Škrlj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09785v1",
                "updated": "2024-08-19T08:22:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    22,
                    20,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:22:20Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    22,
                    20,
                    0,
                    232,
                    0
                ],
                "title": "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining\n  Automotive Software Release Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining\n  Automotive Software Release Decision-Making"
                },
                "summary": "Traditional methods for making software deployment decisions in the\nautomotive industry typically rely on manual analysis of tabular software test\ndata. These methods often lead to higher costs and delays in the software\nrelease cycle due to their labor-intensive nature. Large Language Models (LLMs)\npresent a promising solution to these challenges. However, their application\ngenerally demands multiple rounds of human-driven prompt engineering, which\nlimits their practical deployment, particularly for industrial end-users who\nneed reliable and efficient results. In this paper, we propose GoNoGo, an LLM\nagent system designed to streamline automotive software deployment while\nmeeting both functional requirements and practical industrial constraints.\nUnlike previous systems, GoNoGo is specifically tailored to address\ndomain-specific and risk-sensitive systems. We evaluate GoNoGo's performance\nacross different task difficulties using zero-shot and few-shot examples taken\nfrom industrial practice. Our results show that GoNoGo achieves a 100% success\nrate for tasks up to Level 2 difficulty with 3-shot examples, and maintains\nhigh performance even for more complex tasks. We find that GoNoGo effectively\nautomates decision-making for simpler tasks, significantly reducing the need\nfor manual intervention. In summary, GoNoGo represents an efficient and\nuser-friendly LLM-based solution currently employed in our industrial partner's\ncompany to assist with software release decision-making, supporting more\ninformed and timely decisions in the release process for risk-sensitive vehicle\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for making software deployment decisions in the\nautomotive industry typically rely on manual analysis of tabular software test\ndata. These methods often lead to higher costs and delays in the software\nrelease cycle due to their labor-intensive nature. Large Language Models (LLMs)\npresent a promising solution to these challenges. However, their application\ngenerally demands multiple rounds of human-driven prompt engineering, which\nlimits their practical deployment, particularly for industrial end-users who\nneed reliable and efficient results. In this paper, we propose GoNoGo, an LLM\nagent system designed to streamline automotive software deployment while\nmeeting both functional requirements and practical industrial constraints.\nUnlike previous systems, GoNoGo is specifically tailored to address\ndomain-specific and risk-sensitive systems. We evaluate GoNoGo's performance\nacross different task difficulties using zero-shot and few-shot examples taken\nfrom industrial practice. Our results show that GoNoGo achieves a 100% success\nrate for tasks up to Level 2 difficulty with 3-shot examples, and maintains\nhigh performance even for more complex tasks. We find that GoNoGo effectively\nautomates decision-making for simpler tasks, significantly reducing the need\nfor manual intervention. In summary, GoNoGo represents an efficient and\nuser-friendly LLM-based solution currently employed in our industrial partner's\ncompany to assist with software release decision-making, supporting more\ninformed and timely decisions in the release process for risk-sensitive vehicle\nsystems."
                },
                "authors": [
                    {
                        "name": "Arsham Gholamzadeh Khoee"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Andris Freimanis"
                    },
                    {
                        "name": "Patrick Andersson"
                    },
                    {
                        "name": "Dhasarathy Parthasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Dhasarathy Parthasarathy"
                },
                "author": "Dhasarathy Parthasarathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09773v1",
                "updated": "2024-08-19T08:01:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    1,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T08:01:11Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    8,
                    1,
                    11,
                    0,
                    232,
                    0
                ],
                "title": "Are Large Language Models More Honest in Their Probabilistic or\n  Verbalized Confidence?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models More Honest in Their Probabilistic or\n  Verbalized Confidence?"
                },
                "summary": "Large language models (LLMs) have been found to produce hallucinations when\nthe question exceeds their internal knowledge boundaries. A reliable model\nshould have a clear perception of its knowledge boundaries, providing correct\nanswers within its scope and refusing to answer when it lacks knowledge.\nExisting research on LLMs' perception of their knowledge boundaries typically\nuses either the probability of the generated tokens or the verbalized\nconfidence as the model's confidence in its response. However, these studies\noverlook the differences and connections between the two. In this paper, we\nconduct a comprehensive analysis and comparison of LLMs' probabilistic\nperception and verbalized perception of their factual knowledge boundaries.\nFirst, we investigate the pros and cons of these two perceptions. Then, we\nstudy how they change under questions of varying frequencies. Finally, we\nmeasure the correlation between LLMs' probabilistic confidence and verbalized\nconfidence. Experimental results show that 1) LLMs' probabilistic perception is\ngenerally more accurate than verbalized perception but requires an in-domain\nvalidation set to adjust the confidence threshold. 2) Both perceptions perform\nbetter on less frequent questions. 3) It is challenging for LLMs to accurately\nexpress their internal confidence in natural language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been found to produce hallucinations when\nthe question exceeds their internal knowledge boundaries. A reliable model\nshould have a clear perception of its knowledge boundaries, providing correct\nanswers within its scope and refusing to answer when it lacks knowledge.\nExisting research on LLMs' perception of their knowledge boundaries typically\nuses either the probability of the generated tokens or the verbalized\nconfidence as the model's confidence in its response. However, these studies\noverlook the differences and connections between the two. In this paper, we\nconduct a comprehensive analysis and comparison of LLMs' probabilistic\nperception and verbalized perception of their factual knowledge boundaries.\nFirst, we investigate the pros and cons of these two perceptions. Then, we\nstudy how they change under questions of varying frequencies. Finally, we\nmeasure the correlation between LLMs' probabilistic confidence and verbalized\nconfidence. Experimental results show that 1) LLMs' probabilistic perception is\ngenerally more accurate than verbalized perception but requires an in-domain\nvalidation set to adjust the confidence threshold. 2) Both perceptions perform\nbetter on less frequent questions. 3) It is challenging for LLMs to accurately\nexpress their internal confidence in natural language."
                },
                "authors": [
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Lulu Yu"
                    },
                    {
                        "name": "Jiafeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jiafeng Guo"
                },
                "author": "Jiafeng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02889v3",
                "updated": "2024-08-19T07:53:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    53,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-05T11:50:01Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    11,
                    50,
                    1,
                    1,
                    65,
                    0
                ],
                "title": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated\n  Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated\n  Answers"
                },
                "summary": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 87% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\nof 81%, all without relying on external knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 87% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\nof 81%, all without relying on external knowledge."
                },
                "authors": [
                    {
                        "name": "Yakir Yehuda"
                    },
                    {
                        "name": "Itzik Malkiel"
                    },
                    {
                        "name": "Oren Barkan"
                    },
                    {
                        "name": "Jonathan Weill"
                    },
                    {
                        "name": "Royi Ronen"
                    },
                    {
                        "name": "Noam Koenigstein"
                    }
                ],
                "author_detail": {
                    "name": "Noam Koenigstein"
                },
                "author": "Noam Koenigstein",
                "arxiv_journal_ref": "https://aclanthology.org/2024.acl-long.506/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06063v2",
                "updated": "2024-08-19T07:50:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    50,
                    54,
                    0,
                    232,
                    0
                ],
                "published": "2024-04-09T07:02:14Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    7,
                    2,
                    14,
                    1,
                    100,
                    0
                ],
                "title": "Heuristic-enhanced Candidates Selection strategy for GPTs tackle\n  Few-Shot Aspect-Based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic-enhanced Candidates Selection strategy for GPTs tackle\n  Few-Shot Aspect-Based Sentiment Analysis"
                },
                "summary": "Few-Shot Aspect-Based Sentiment Analysis (FSABSA) is an indispensable and\nhighly challenging task in natural language processing. However, methods based\non Pre-trained Language Models (PLMs) struggle to accommodate multiple\nsub-tasks, and methods based on Generative Pre-trained Transformers (GPTs)\nperform poorly. To address the above issues, the paper designs a\nHeuristic-enhanced Candidates Selection (HCS) strategy and further proposes All\nin One (AiO) model based on it. The model works in a two-stage, which\nsimultaneously accommodates the accuracy of PLMs and the generalization\ncapability of GPTs. Specifically, in the first stage, a backbone model based on\nPLMs generates rough heuristic candidates for the input sentence. In the second\nstage, AiO leverages LLMs' contextual learning capabilities to generate precise\npredictions. The study conducted comprehensive comparative and ablation\nexperiments on five benchmark datasets. The experimental results demonstrate\nthat the proposed model can better adapt to multiple sub-tasks, and also\noutperforms the methods that directly utilize GPTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Aspect-Based Sentiment Analysis (FSABSA) is an indispensable and\nhighly challenging task in natural language processing. However, methods based\non Pre-trained Language Models (PLMs) struggle to accommodate multiple\nsub-tasks, and methods based on Generative Pre-trained Transformers (GPTs)\nperform poorly. To address the above issues, the paper designs a\nHeuristic-enhanced Candidates Selection (HCS) strategy and further proposes All\nin One (AiO) model based on it. The model works in a two-stage, which\nsimultaneously accommodates the accuracy of PLMs and the generalization\ncapability of GPTs. Specifically, in the first stage, a backbone model based on\nPLMs generates rough heuristic candidates for the input sentence. In the second\nstage, AiO leverages LLMs' contextual learning capabilities to generate precise\npredictions. The study conducted comprehensive comparative and ablation\nexperiments on five benchmark datasets. The experimental results demonstrate\nthat the proposed model can better adapt to multiple sub-tasks, and also\noutperforms the methods that directly utilize GPTs."
                },
                "authors": [
                    {
                        "name": "Baoxing Jiang"
                    },
                    {
                        "name": "Yujie Wan"
                    },
                    {
                        "name": "Shenggen Ju"
                    }
                ],
                "author_detail": {
                    "name": "Shenggen Ju"
                },
                "author": "Shenggen Ju",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09757v1",
                "updated": "2024-08-19T07:34:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    34,
                    43,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:34:43Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    34,
                    43,
                    0,
                    232,
                    0
                ],
                "title": "Strategic Demonstration Selection for Improved Fairness in LLM\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Demonstration Selection for Improved Fairness in LLM\n  In-Context Learning"
                },
                "summary": "Recent studies highlight the effectiveness of using in-context learning (ICL)\nto steer large language models (LLMs) in processing tabular data, a challenging\ntask given the structured nature of such data. Despite advancements in\nperformance, the fairness implications of these methods are less understood.\nThis study investigates how varying demonstrations within ICL prompts influence\nthe fairness outcomes of LLMs. Our findings reveal that deliberately including\nminority group samples in prompts significantly boosts fairness without\nsacrificing predictive accuracy. Further experiments demonstrate that the\nproportion of minority to majority samples in demonstrations affects the\ntrade-off between fairness and prediction accuracy. Based on these insights, we\nintroduce a mitigation technique that employs clustering and evolutionary\nstrategies to curate a diverse and representative sample set from the training\ndata. This approach aims to enhance both predictive performance and fairness in\nICL applications. Experimental results validate that our proposed method\ndramatically improves fairness across various metrics, showing its efficacy in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies highlight the effectiveness of using in-context learning (ICL)\nto steer large language models (LLMs) in processing tabular data, a challenging\ntask given the structured nature of such data. Despite advancements in\nperformance, the fairness implications of these methods are less understood.\nThis study investigates how varying demonstrations within ICL prompts influence\nthe fairness outcomes of LLMs. Our findings reveal that deliberately including\nminority group samples in prompts significantly boosts fairness without\nsacrificing predictive accuracy. Further experiments demonstrate that the\nproportion of minority to majority samples in demonstrations affects the\ntrade-off between fairness and prediction accuracy. Based on these insights, we\nintroduce a mitigation technique that employs clustering and evolutionary\nstrategies to curate a diverse and representative sample set from the training\ndata. This approach aims to enhance both predictive performance and fairness in\nICL applications. Experimental results validate that our proposed method\ndramatically improves fairness across various metrics, showing its efficacy in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jingyu Hu"
                    },
                    {
                        "name": "Weiru Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07930v2",
                "updated": "2024-08-19T07:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    32,
                    25,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-10T07:22:15Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    7,
                    22,
                    15,
                    2,
                    192,
                    0
                ],
                "title": "Token-Mol 1.0: Tokenized drug design with large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Mol 1.0: Tokenized drug design with large language model"
                },
                "summary": "Significant interests have recently risen in leveraging sequence-based large\nlanguage models (LLMs) for drug design. However, most current applications of\nLLMs in drug discovery lack the ability to comprehend three-dimensional (3D)\nstructures, thereby limiting their effectiveness in tasks that explicitly\ninvolve molecular conformations. In this study, we introduced Token-Mol, a\ntoken-only 3D drug design model. This model encodes all molecular information,\nincluding 2D and 3D structures, as well as molecular property data, into\ntokens, which transforms classification and regression tasks in drug discovery\ninto probabilistic prediction problems, thereby enabling learning through a\nunified paradigm. Token-Mol is built on the transformer decoder architecture\nand trained using random causal masking techniques. Additionally, we proposed\nthe Gaussian cross-entropy (GCE) loss function to overcome the challenges in\nregression tasks, significantly enhancing the capacity of LLMs to learn\ncontinuous numerical values. Through a combination of fine-tuning and\nreinforcement learning (RL), Token-Mol achieves performance comparable to or\nsurpassing existing task-specific methods across various downstream tasks,\nincluding pocket-based molecular generation, conformation generation, and\nmolecular property prediction. Compared to existing molecular pre-trained\nmodels, Token-Mol exhibits superior proficiency in handling a wider range of\ndownstream tasks essential for drug design. Notably, our approach improves\nregression task accuracy by approximately 30% compared to similar token-only\nmethods. Token-Mol overcomes the precision limitations of token-only models and\nhas the potential to integrate seamlessly with general models such as ChatGPT,\npaving the way for the development of a universal artificial intelligence drug\ndesign model that facilitates rapid and high-quality drug design by experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant interests have recently risen in leveraging sequence-based large\nlanguage models (LLMs) for drug design. However, most current applications of\nLLMs in drug discovery lack the ability to comprehend three-dimensional (3D)\nstructures, thereby limiting their effectiveness in tasks that explicitly\ninvolve molecular conformations. In this study, we introduced Token-Mol, a\ntoken-only 3D drug design model. This model encodes all molecular information,\nincluding 2D and 3D structures, as well as molecular property data, into\ntokens, which transforms classification and regression tasks in drug discovery\ninto probabilistic prediction problems, thereby enabling learning through a\nunified paradigm. Token-Mol is built on the transformer decoder architecture\nand trained using random causal masking techniques. Additionally, we proposed\nthe Gaussian cross-entropy (GCE) loss function to overcome the challenges in\nregression tasks, significantly enhancing the capacity of LLMs to learn\ncontinuous numerical values. Through a combination of fine-tuning and\nreinforcement learning (RL), Token-Mol achieves performance comparable to or\nsurpassing existing task-specific methods across various downstream tasks,\nincluding pocket-based molecular generation, conformation generation, and\nmolecular property prediction. Compared to existing molecular pre-trained\nmodels, Token-Mol exhibits superior proficiency in handling a wider range of\ndownstream tasks essential for drug design. Notably, our approach improves\nregression task accuracy by approximately 30% compared to similar token-only\nmethods. Token-Mol overcomes the precision limitations of token-only models and\nhas the potential to integrate seamlessly with general models such as ChatGPT,\npaving the way for the development of a universal artificial intelligence drug\ndesign model that facilitates rapid and high-quality drug design by experts."
                },
                "authors": [
                    {
                        "name": "Jike Wang"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Meijing Fang"
                    },
                    {
                        "name": "Yangyang Zhang"
                    },
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Qun Su"
                    },
                    {
                        "name": "Qiaolin Gou"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Odin Zhang"
                    },
                    {
                        "name": "Zhenxing Wu"
                    },
                    {
                        "name": "Dejun Jiang"
                    },
                    {
                        "name": "Xujun Zhang"
                    },
                    {
                        "name": "Huifeng Zhao"
                    },
                    {
                        "name": "Xiaozhe Wan"
                    },
                    {
                        "name": "Zhourui Wu"
                    },
                    {
                        "name": "Liwei Liu"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Chang-Yu Hsieh"
                    },
                    {
                        "name": "Tingjun Hou"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Hou"
                },
                "author": "Tingjun Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09743v1",
                "updated": "2024-08-19T07:15:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    11,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:15:11Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    11,
                    0,
                    232,
                    0
                ],
                "title": "R2GenCSR: Retrieving Context Samples for Large Language Model based\n  X-ray Medical Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2GenCSR: Retrieving Context Samples for Large Language Model based\n  X-ray Medical Report Generation"
                },
                "summary": "Inspired by the tremendous success of Large Language Models (LLMs), existing\nX-ray medical report generation methods attempt to leverage large models to\nachieve better performance. They usually adopt a Transformer to extract the\nvisual features of a given X-ray image, and then, feed them into the LLM for\ntext generation. How to extract more effective information for the LLMs to help\nthem improve final results is an urgent problem that needs to be solved.\nAdditionally, the use of visual Transformer models also brings high\ncomputational complexity. To address these issues, this paper proposes a novel\ncontext-guided efficient X-ray medical report generation framework.\nSpecifically, we introduce the Mamba as the vision backbone with linear\ncomplexity, and the performance obtained is comparable to that of the strong\nTransformer model. More importantly, we perform context retrieval from the\ntraining set for samples within each mini-batch during the training phase,\nutilizing both positively and negatively related samples to enhance feature\nrepresentation and discriminative learning. Subsequently, we feed the vision\ntokens, context information, and prompt statements to invoke the LLM for\ngenerating high-quality medical reports. Extensive experiments on three X-ray\nreport generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully\nvalidated the effectiveness of our proposed model. The source code of this work\nwill be released on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the tremendous success of Large Language Models (LLMs), existing\nX-ray medical report generation methods attempt to leverage large models to\nachieve better performance. They usually adopt a Transformer to extract the\nvisual features of a given X-ray image, and then, feed them into the LLM for\ntext generation. How to extract more effective information for the LLMs to help\nthem improve final results is an urgent problem that needs to be solved.\nAdditionally, the use of visual Transformer models also brings high\ncomputational complexity. To address these issues, this paper proposes a novel\ncontext-guided efficient X-ray medical report generation framework.\nSpecifically, we introduce the Mamba as the vision backbone with linear\ncomplexity, and the performance obtained is comparable to that of the strong\nTransformer model. More importantly, we perform context retrieval from the\ntraining set for samples within each mini-batch during the training phase,\nutilizing both positively and negatively related samples to enhance feature\nrepresentation and discriminative learning. Subsequently, we feed the vision\ntokens, context information, and prompt statements to invoke the LLM for\ngenerating high-quality medical reports. Extensive experiments on three X-ray\nreport generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully\nvalidated the effectiveness of our proposed model. The source code of this work\nwill be released on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Yuehang Li"
                    },
                    {
                        "name": "Fuling Wang"
                    },
                    {
                        "name": "Shiao Wang"
                    },
                    {
                        "name": "Chuanfu Li"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "In Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09742v1",
                "updated": "2024-08-19T07:14:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    14,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T07:14:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    14,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "Paired Completion: Flexible Quantification of Issue-framing at Scale\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paired Completion: Flexible Quantification of Issue-framing at Scale\n  with LLMs"
                },
                "summary": "Detecting and quantifying issue framing in textual discourse - the\nperspective one takes to a given topic (e.g. climate science vs. denialism,\nmisogyny vs. gender equality) - is highly valuable to a range of end-users from\nsocial and political scientists to program evaluators and policy analysts.\nHowever, conceptual framing is notoriously challenging for automated natural\nlanguage processing (NLP) methods since the words and phrases used by either\n`side' of an issue are often held in common, with only subtle stylistic\nflourishes separating their use. Here we develop and rigorously evaluate new\ndetection methods for issue framing and narrative analysis within large text\ndatasets. By introducing a novel application of next-token log probabilities\nderived from generative large language models (LLMs) we show that issue framing\ncan be reliably and efficiently detected in large corpora with only a few\nexamples of either perspective on a given issue, a method we call `paired\ncompletion'. Through 192 independent experiments over three novel, synthetic\ndatasets, we evaluate paired completion against prompt-based LLM methods and\nlabelled methods using traditional NLP and recent LLM contextual embeddings. We\nadditionally conduct a cost-based analysis to mark out the feasible set of\nperformant methods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable, accurate and\nlow-bias issue-framing in large corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and quantifying issue framing in textual discourse - the\nperspective one takes to a given topic (e.g. climate science vs. denialism,\nmisogyny vs. gender equality) - is highly valuable to a range of end-users from\nsocial and political scientists to program evaluators and policy analysts.\nHowever, conceptual framing is notoriously challenging for automated natural\nlanguage processing (NLP) methods since the words and phrases used by either\n`side' of an issue are often held in common, with only subtle stylistic\nflourishes separating their use. Here we develop and rigorously evaluate new\ndetection methods for issue framing and narrative analysis within large text\ndatasets. By introducing a novel application of next-token log probabilities\nderived from generative large language models (LLMs) we show that issue framing\ncan be reliably and efficiently detected in large corpora with only a few\nexamples of either perspective on a given issue, a method we call `paired\ncompletion'. Through 192 independent experiments over three novel, synthetic\ndatasets, we evaluate paired completion against prompt-based LLM methods and\nlabelled methods using traditional NLP and recent LLM contextual embeddings. We\nadditionally conduct a cost-based analysis to mark out the feasible set of\nperformant methods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable, accurate and\nlow-bias issue-framing in large corpora."
                },
                "authors": [
                    {
                        "name": "Simon D Angus"
                    },
                    {
                        "name": "Lachlan O'Neill"
                    }
                ],
                "author_detail": {
                    "name": "Lachlan O'Neill"
                },
                "author": "Lachlan O'Neill",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09735v1",
                "updated": "2024-08-19T06:49:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    49,
                    4,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T06:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    49,
                    4,
                    0,
                    232,
                    0
                ],
                "title": "Icing on the Cake: Automatic Code Summarization at Ericsson",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Icing on the Cake: Automatic Code Summarization at Ericsson"
                },
                "summary": "This paper presents our findings on the automatic summarization of Java\nmethods within Ericsson, a global telecommunications company. We evaluate the\nperformance of an approach called Automatic Semantic Augmentation of Prompts\n(ASAP), which uses a Large Language Model (LLM) to generate leading summary\ncomments for Java methods. ASAP enhances the $LLM's$ prompt context by\nintegrating static program analysis and information retrieval techniques to\nidentify similar exemplar methods along with their developer-written Javadocs,\nand serves as the baseline in our study. In contrast, we explore and compare\nthe performance of four simpler approaches that do not require static program\nanalysis, information retrieval, or the presence of exemplars as in the ASAP\nmethod. Our methods rely solely on the Java method body as input, making them\nlightweight and more suitable for rapid deployment in commercial software\ndevelopment environments. We conducted experiments on an Ericsson software\nproject and replicated the study using two widely-used open-source Java\nprojects, Guava and Elasticsearch, to ensure the reliability of our results.\nPerformance was measured across eight metrics that capture various aspects of\nsimilarity. Notably, one of our simpler approaches performed as well as or\nbetter than the ASAP method on both the Ericsson project and the open-source\nprojects. Additionally, we performed an ablation study to examine the impact of\nmethod names on Javadoc summary generation across our four proposed approaches\nand the ASAP method. By masking the method names and observing the generated\nsummaries, we found that our approaches were statistically significantly less\ninfluenced by the absence of method names compared to the baseline. This\nsuggests that our methods are more robust to variations in method names and may\nderive summaries more comprehensively from the method body than the ASAP\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our findings on the automatic summarization of Java\nmethods within Ericsson, a global telecommunications company. We evaluate the\nperformance of an approach called Automatic Semantic Augmentation of Prompts\n(ASAP), which uses a Large Language Model (LLM) to generate leading summary\ncomments for Java methods. ASAP enhances the $LLM's$ prompt context by\nintegrating static program analysis and information retrieval techniques to\nidentify similar exemplar methods along with their developer-written Javadocs,\nand serves as the baseline in our study. In contrast, we explore and compare\nthe performance of four simpler approaches that do not require static program\nanalysis, information retrieval, or the presence of exemplars as in the ASAP\nmethod. Our methods rely solely on the Java method body as input, making them\nlightweight and more suitable for rapid deployment in commercial software\ndevelopment environments. We conducted experiments on an Ericsson software\nproject and replicated the study using two widely-used open-source Java\nprojects, Guava and Elasticsearch, to ensure the reliability of our results.\nPerformance was measured across eight metrics that capture various aspects of\nsimilarity. Notably, one of our simpler approaches performed as well as or\nbetter than the ASAP method on both the Ericsson project and the open-source\nprojects. Additionally, we performed an ablation study to examine the impact of\nmethod names on Javadoc summary generation across our four proposed approaches\nand the ASAP method. By masking the method names and observing the generated\nsummaries, we found that our approaches were statistically significantly less\ninfluenced by the absence of method names compared to the baseline. This\nsuggests that our methods are more robust to variations in method names and may\nderive summaries more comprehensively from the method body than the ASAP\napproach."
                },
                "authors": [
                    {
                        "name": "Giriprasad Sridhara"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Sumit Soman"
                    },
                    {
                        "name": "Ranjani H G"
                    },
                    {
                        "name": "Ricardo Britto"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Britto"
                },
                "author": "Ricardo Britto",
                "arxiv_comment": "16 pages, 6 tables, 4 figures. Accepted at the 2024 International\n  Conference on Software Maintenance and Evolution (ICSME) 2024 - Industry\n  Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09720v1",
                "updated": "2024-08-19T06:19:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    19,
                    31,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T06:19:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    19,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large\n  Language Model Augmented Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large\n  Language Model Augmented Framework"
                },
                "summary": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in\nhuman-centered research. However, existing datasets neglect different domains\n(e.g., environments, times, populations, and data sources), only conducting\nsimple random splits, and the performance of these datasets has already\napproached saturation. In the past five years, no large-scale dataset has been\nopened to the public. To address this issue, this paper proposes a new\nlarge-scale, cross-domain pedestrian attribute recognition dataset to fill the\ndata gap, termed MSP60K. It consists of 60,122 images and 57 attribute\nannotations across eight scenarios. Synthetic degradation is also conducted to\nfurther narrow the gap between the dataset and real-world challenging\nscenarios. To establish a more rigorous benchmark, we evaluate 17\nrepresentative PAR models under both random and cross-domain split protocols on\nour dataset. Additionally, we propose an innovative Large Language Model (LLM)\naugmented PAR framework, named LLM-PAR. This framework processes pedestrian\nimages through a Vision Transformer (ViT) backbone to extract features and\nintroduces a multi-embedding query Transformer to learn partial-aware features\nfor attribute classification. Significantly, we enhance this framework with LLM\nfor ensemble learning and visual feature augmentation. Comprehensive\nexperiments across multiple PAR benchmark datasets have thoroughly validated\nthe efficacy of our proposed framework. The dataset and source code\naccompanying this paper will be made publicly available at\n\\url{https://github.com/Event-AHU/OpenPAR}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in\nhuman-centered research. However, existing datasets neglect different domains\n(e.g., environments, times, populations, and data sources), only conducting\nsimple random splits, and the performance of these datasets has already\napproached saturation. In the past five years, no large-scale dataset has been\nopened to the public. To address this issue, this paper proposes a new\nlarge-scale, cross-domain pedestrian attribute recognition dataset to fill the\ndata gap, termed MSP60K. It consists of 60,122 images and 57 attribute\nannotations across eight scenarios. Synthetic degradation is also conducted to\nfurther narrow the gap between the dataset and real-world challenging\nscenarios. To establish a more rigorous benchmark, we evaluate 17\nrepresentative PAR models under both random and cross-domain split protocols on\nour dataset. Additionally, we propose an innovative Large Language Model (LLM)\naugmented PAR framework, named LLM-PAR. This framework processes pedestrian\nimages through a Vision Transformer (ViT) backbone to extract features and\nintroduces a multi-embedding query Transformer to learn partial-aware features\nfor attribute classification. Significantly, we enhance this framework with LLM\nfor ensemble learning and visual feature augmentation. Comprehensive\nexperiments across multiple PAR benchmark datasets have thoroughly validated\nthe efficacy of our proposed framework. The dataset and source code\naccompanying this paper will be made publicly available at\n\\url{https://github.com/Event-AHU/OpenPAR}."
                },
                "authors": [
                    {
                        "name": "Jiandong Jin"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qian Zhu"
                    },
                    {
                        "name": "Haiyang Wang"
                    },
                    {
                        "name": "Chenglong Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenglong Li"
                },
                "author": "Chenglong Li",
                "arxiv_comment": "MSP60K PAR Benchmark Dataset, LLM based PAR model, In Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07970v2",
                "updated": "2024-08-19T06:08:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    8,
                    46,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-12T07:49:36Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    49,
                    36,
                    2,
                    164,
                    0
                ],
                "title": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation"
                },
                "summary": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50."
                },
                "authors": [
                    {
                        "name": "Javad Pourmostafa Roshan Sharami"
                    },
                    {
                        "name": "Dimitar Shterionov"
                    },
                    {
                        "name": "Pieter Spronck"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Spronck"
                },
                "author": "Pieter Spronck",
                "arxiv_comment": "Camera-ready version of the Association for Machine Translation in\n  the Americas (AMTA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09713v2",
                "updated": "2024-08-20T12:22:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    22,
                    16,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T06:05:24Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    6,
                    5,
                    24,
                    0,
                    232,
                    0
                ],
                "title": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Footprint Accounting Driven by Large Language Models and\n  Retrieval-augmented Generation"
                },
                "summary": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon footprint accounting is crucial for quantifying greenhouse gas\nemissions and achieving carbon neutrality.The dynamic nature of processes,\naccounting rules, carbon-related policies, and energy supply structures\nnecessitates real-time updates of CFA. Traditional life cycle assessment\nmethods rely heavily on human expertise, making near-real-time updates\nchallenging. This paper introduces a novel approach integrating large language\nmodels (LLMs) with retrieval-augmented generation technology to enhance the\nreal-time, professional, and economical aspects of carbon footprint information\nretrieval and analysis. By leveraging LLMs' logical and language understanding\nabilities and RAG's efficient retrieval capabilities, the proposed method\nLLMs-RAG-CFA can retrieve more relevant professional information to assist\nLLMs, enhancing the model's generative abilities. This method offers broad\nprofessional coverage, efficient real-time carbon footprint information\nacquisition and accounting, and cost-effective automation without frequent\nLLMs' parameter updates. Experimental results across five industries(primary\naluminum, lithium battery, photovoltaic, new energy vehicles, and\ntransformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional\nmethods and other LLMs, achieving higher information retrieval rates and\nsignificantly lower information deviations and carbon footprint accounting\ndeviations. The economically viable design utilizes RAG technology to balance\nreal-time updates with cost-effectiveness, providing an efficient, reliable,\nand cost-saving solution for real-time carbon emission management, thereby\nenhancing environmental sustainability practices."
                },
                "authors": [
                    {
                        "name": "Haijin Wang"
                    },
                    {
                        "name": "Mianrong Zhang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Nan Shang"
                    },
                    {
                        "name": "Shangheng Yao"
                    },
                    {
                        "name": "Fushuan Wen"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09701v1",
                "updated": "2024-08-19T05:11:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    11,
                    46,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T05:11:46Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    5,
                    11,
                    46,
                    0,
                    232,
                    0
                ],
                "title": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code\n  Generation in LLMs via Zero-Shot Cross-Lingual Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code\n  Generation in LLMs via Zero-Shot Cross-Lingual Transfer"
                },
                "summary": "The use of Large Language Models (LLMs) for program code generation has\ngained substantial attention, but their biases and limitations with non-English\nprompts challenge global inclusivity. This paper investigates the complexities\nof multilingual prompt-based code generation. Our evaluations of LLMs,\nincluding CodeLLaMa and CodeGemma, reveal significant disparities in code\nquality for non-English prompts; we also demonstrate the inadequacy of simple\napproaches like prompt translation, bootstrapped data augmentation, and\nfine-tuning. To address this, we propose a zero-shot cross-lingual approach\nusing a neural projection technique, integrating a cross-lingual encoder like\nLASER artetxe2019massively to map multilingual embeddings from it into the\nLLM's token space. This method requires training only on English data and\nscales effectively to other languages. Results on a translated and\nquality-checked MBPP dataset show substantial improvements in code quality.\nThis research promotes a more inclusive code generation landscape by empowering\nLLMs with multilingual capabilities to support the diverse linguistic spectrum\nin programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for program code generation has\ngained substantial attention, but their biases and limitations with non-English\nprompts challenge global inclusivity. This paper investigates the complexities\nof multilingual prompt-based code generation. Our evaluations of LLMs,\nincluding CodeLLaMa and CodeGemma, reveal significant disparities in code\nquality for non-English prompts; we also demonstrate the inadequacy of simple\napproaches like prompt translation, bootstrapped data augmentation, and\nfine-tuning. To address this, we propose a zero-shot cross-lingual approach\nusing a neural projection technique, integrating a cross-lingual encoder like\nLASER artetxe2019massively to map multilingual embeddings from it into the\nLLM's token space. This method requires training only on English data and\nscales effectively to other languages. Results on a translated and\nquality-checked MBPP dataset show substantial improvements in code quality.\nThis research promotes a more inclusive code generation landscape by empowering\nLLMs with multilingual capabilities to support the diverse linguistic spectrum\nin programming."
                },
                "authors": [
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Abhijit Mishra"
                    },
                    {
                        "name": "Utkarsh Mujumdar"
                    }
                ],
                "author_detail": {
                    "name": "Utkarsh Mujumdar"
                },
                "author": "Utkarsh Mujumdar",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08694v3",
                "updated": "2024-08-19T04:54:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    54,
                    36,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-13T16:57:57Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    16,
                    57,
                    57,
                    2,
                    73,
                    0
                ],
                "title": "TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning"
                },
                "summary": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of the strong baseline's total), along with enhanced capabilities of\nLLMs in crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of the strong baseline's total), along with enhanced capabilities of\nLLMs in crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL"
                },
                "authors": [
                    {
                        "name": "Shangding Gu"
                    },
                    {
                        "name": "Alois Knoll"
                    },
                    {
                        "name": "Ming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jin"
                },
                "author": "Ming Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v2",
                "updated": "2024-08-20T16:09:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    9,
                    33,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08869v2",
                "updated": "2024-08-19T04:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    29,
                    34,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T17:54:09Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    54,
                    9,
                    4,
                    229,
                    0
                ],
                "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars"
                },
                "summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches."
                },
                "authors": [
                    {
                        "name": "Sumanth Prabhu"
                    }
                ],
                "author_detail": {
                    "name": "Sumanth Prabhu"
                },
                "author": "Sumanth Prabhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v3",
                "updated": "2024-08-19T04:02:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    2,
                    44,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Universal Approximation Theory: The Basic Theory for Transformer-based\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation Theory: The Basic Theory for Transformer-based\n  Large Language Models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05180v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05180v3",
                "updated": "2024-08-19T03:56:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    56,
                    26,
                    0,
                    232,
                    0
                ],
                "published": "2023-10-08T14:32:25Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    32,
                    25,
                    6,
                    281,
                    0
                ],
                "title": "Blockchain-Envisioned UAV-Aided Disaster Relief Networks: Challenges and\n  Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-Envisioned UAV-Aided Disaster Relief Networks: Challenges and\n  Solutions"
                },
                "summary": "Natural or man-made disasters pose significant challenges for delivering\ncritical relief to affected populations due to disruptions in critical\ninfrastructures and logistics networks. Unmanned aerial vehicles (UAVs)-aided\ndisaster relief networks (UDRNs) leverage UAVs to assist existing ground relief\nnetworks by swiftly assessing affected areas and timely delivering lifesaving\nsupplies. To meet the growing demands for collaborative, trust-free, and\ntransparent UDRN services, blockchain-based UDRNs emerge as a promising\napproach through immutable ledgers and distributed smart contracts. However,\nseveral efficiency and security challenges hinder the deployment of\nblockchain-based UDRNs, including the lack of cooperation between smart\ncontracts, lack of dynamic audit for smart contract vulnerabilities, and low\nforensics robustness against transaction malleability attacks. Towards\nefficient and secure blockchain-based UDRNs, this paper presents potential\nsolutions: (i) a series of collaborative smart contracts for coordinated relief\nmanagement, (ii) a dynamic contract audit mechanism to prevent known/unknown\ncontract vulnerabilities; and (iii) a robust transaction forensics strategy\nwith on/off-chain cooperation to resist transaction malleability attacks. Our\nprototype implementation and experimental results demonstrate the feasibility\nand effectiveness of our approach. Lastly, we outline key open research issues\ncrucial to advancing this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural or man-made disasters pose significant challenges for delivering\ncritical relief to affected populations due to disruptions in critical\ninfrastructures and logistics networks. Unmanned aerial vehicles (UAVs)-aided\ndisaster relief networks (UDRNs) leverage UAVs to assist existing ground relief\nnetworks by swiftly assessing affected areas and timely delivering lifesaving\nsupplies. To meet the growing demands for collaborative, trust-free, and\ntransparent UDRN services, blockchain-based UDRNs emerge as a promising\napproach through immutable ledgers and distributed smart contracts. However,\nseveral efficiency and security challenges hinder the deployment of\nblockchain-based UDRNs, including the lack of cooperation between smart\ncontracts, lack of dynamic audit for smart contract vulnerabilities, and low\nforensics robustness against transaction malleability attacks. Towards\nefficient and secure blockchain-based UDRNs, this paper presents potential\nsolutions: (i) a series of collaborative smart contracts for coordinated relief\nmanagement, (ii) a dynamic contract audit mechanism to prevent known/unknown\ncontract vulnerabilities; and (iii) a robust transaction forensics strategy\nwith on/off-chain cooperation to resist transaction malleability attacks. Our\nprototype implementation and experimental results demonstrate the feasibility\nand effectiveness of our approach. Lastly, we outline key open research issues\ncrucial to advancing this emerging field."
                },
                "authors": [
                    {
                        "name": "Yuntao Wang"
                    },
                    {
                        "name": "Qinnan Hu"
                    },
                    {
                        "name": "Zhendong Li"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Ruidong Li"
                    },
                    {
                        "name": "Xiang Zou"
                    },
                    {
                        "name": "Jian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhou"
                },
                "author": "Jian Zhou",
                "arxiv_comment": "7 pages, accepted by IEEE Communications Magazine in August 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05180v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05180v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09688v1",
                "updated": "2024-08-19T03:53:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:53:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts"
                },
                "summary": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task."
                },
                "authors": [
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14192v2",
                "updated": "2024-08-19T03:47:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    47,
                    16,
                    0,
                    232,
                    0
                ],
                "published": "2024-06-20T10:52:14Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    52,
                    14,
                    3,
                    172,
                    0
                ],
                "title": "Timo: Towards Better Temporal Reasoning for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timo: Towards Better Temporal Reasoning for Language Models"
                },
                "summary": "Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo."
                },
                "authors": [
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "This paper has been accepted to the COLM 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09682v1",
                "updated": "2024-08-19T03:41:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    41,
                    43,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:41:43Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    41,
                    43,
                    0,
                    232,
                    0
                ],
                "title": "Simulating Field Experiments with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Field Experiments with Large Language Models"
                },
                "summary": "Prevailing large language models (LLMs) are capable of human responses\nsimulation through its unprecedented content generation and reasoning\nabilities. However, it is not clear whether and how to leverage LLMs to\nsimulate field experiments. In this paper, we propose and evaluate two\nprompting strategies: the observer mode that allows a direct prediction on main\nconclusions and the participant mode that simulates distributions of responses\nfrom participants. Using this approach, we examine fifteen well cited field\nexperimental papers published in INFORMS and MISQ, finding encouraging\nalignments between simulated experimental results and the actual results in\ncertain scenarios. We further identify topics of which LLMs underperform,\nincluding gender difference and social norms related research. Additionally,\nthe automatic and standardized workflow proposed in this paper enables the\npossibility of a large-scale screening of more papers with field experiments.\nThis paper pioneers the utilization of large language models (LLMs) for\nsimulating field experiments, presenting a significant extension to previous\nwork which focused solely on lab environments. By introducing two novel\nprompting strategies, observer and participant modes, we demonstrate the\nability of LLMs to both predict outcomes and replicate participant responses\nwithin complex field settings. Our findings indicate a promising alignment with\nactual experimental results in certain scenarios, achieving a stimulation\naccuracy of 66% in observer mode. This study expands the scope of potential\napplications for LLMs and illustrates their utility in assisting researchers\nprior to engaging in expensive field experiments. Moreover, it sheds light on\nthe boundaries of LLMs when used in simulating field experiments, serving as a\ncautionary note for researchers considering the integration of LLMs into their\nexperimental toolkit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevailing large language models (LLMs) are capable of human responses\nsimulation through its unprecedented content generation and reasoning\nabilities. However, it is not clear whether and how to leverage LLMs to\nsimulate field experiments. In this paper, we propose and evaluate two\nprompting strategies: the observer mode that allows a direct prediction on main\nconclusions and the participant mode that simulates distributions of responses\nfrom participants. Using this approach, we examine fifteen well cited field\nexperimental papers published in INFORMS and MISQ, finding encouraging\nalignments between simulated experimental results and the actual results in\ncertain scenarios. We further identify topics of which LLMs underperform,\nincluding gender difference and social norms related research. Additionally,\nthe automatic and standardized workflow proposed in this paper enables the\npossibility of a large-scale screening of more papers with field experiments.\nThis paper pioneers the utilization of large language models (LLMs) for\nsimulating field experiments, presenting a significant extension to previous\nwork which focused solely on lab environments. By introducing two novel\nprompting strategies, observer and participant modes, we demonstrate the\nability of LLMs to both predict outcomes and replicate participant responses\nwithin complex field settings. Our findings indicate a promising alignment with\nactual experimental results in certain scenarios, achieving a stimulation\naccuracy of 66% in observer mode. This study expands the scope of potential\napplications for LLMs and illustrates their utility in assisting researchers\nprior to engaging in expensive field experiments. Moreover, it sheds light on\nthe boundaries of LLMs when used in simulating field experiments, serving as a\ncautionary note for researchers considering the integration of LLMs into their\nexperimental toolkit."
                },
                "authors": [
                    {
                        "name": "Yaoyu Chen"
                    },
                    {
                        "name": "Yuheng Hu"
                    },
                    {
                        "name": "Yingda Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Lu"
                },
                "author": "Yingda Lu",
                "arxiv_comment": "17 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13269v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13269v3",
                "updated": "2024-08-19T03:31:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    31,
                    19,
                    0,
                    232,
                    0
                ],
                "published": "2023-07-25T05:39:21Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    5,
                    39,
                    21,
                    1,
                    206,
                    0
                ],
                "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA\n  Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA\n  Composition"
                },
                "summary": "Low-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a simple framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a\nfew examples from a new task, LoraHub can fluidly combine multiple LoRA\nmodules, eliminating the need for human expertise and assumptions. Notably, the\ncomposition requires neither additional model parameters nor gradients.\nEmpirical results on the Big-Bench Hard benchmark suggest that LoraHub, while\nnot surpassing the performance of in-context learning, offers a notable\nperformance-efficiency trade-off in few-shot scenarios by employing a\nsignificantly reduced number of tokens per example during inference. Notably,\nLoraHub establishes a better upper bound compared to in-context learning when\npaired with different demonstration examples, demonstrating its potential for\nfuture development. Our vision is to establish a platform for LoRA modules,\nempowering users to share their trained LoRA modules. This collaborative\napproach facilitates the seamless application of LoRA modules to novel tasks,\ncontributing to an adaptive ecosystem. Our code is available at\nhttps://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are\nreleased at https://huggingface.co/lorahub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a simple framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a\nfew examples from a new task, LoraHub can fluidly combine multiple LoRA\nmodules, eliminating the need for human expertise and assumptions. Notably, the\ncomposition requires neither additional model parameters nor gradients.\nEmpirical results on the Big-Bench Hard benchmark suggest that LoraHub, while\nnot surpassing the performance of in-context learning, offers a notable\nperformance-efficiency trade-off in few-shot scenarios by employing a\nsignificantly reduced number of tokens per example during inference. Notably,\nLoraHub establishes a better upper bound compared to in-context learning when\npaired with different demonstration examples, demonstrating its potential for\nfuture development. Our vision is to establish a platform for LoRA modules,\nempowering users to share their trained LoRA modules. This collaborative\napproach facilitates the seamless application of LoRA modules to novel tasks,\ncontributing to an adaptive ecosystem. Our code is available at\nhttps://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are\nreleased at https://huggingface.co/lorahub."
                },
                "authors": [
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.13269v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13269v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09674v1",
                "updated": "2024-08-19T03:30:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    30,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:30:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    30,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "Implicit Grid Convolution for Multi-Scale Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Grid Convolution for Multi-Scale Image Super-Resolution"
                },
                "summary": "Recently, Super-Resolution (SR) achieved significant performance improvement\nby employing neural networks. Most SR methods conventionally train a single\nmodel for each targeted scale, which increases redundancy in training and\ndeployment in proportion to the number of scales targeted. This paper\nchallenges this conventional fixed-scale approach. Our preliminary analysis\nreveals that, surprisingly, encoders trained at different scales extract\nsimilar features from images. Furthermore, the commonly used scale-specific\nupsampler, Sub-Pixel Convolution (SPConv), exhibits significant inter-scale\ncorrelations. Based on these observations, we propose a framework for training\nmultiple integer scales simultaneously with a single model. We use a single\nencoder to extract features and introduce a novel upsampler, Implicit Grid\nConvolution~(IGConv), which integrates SPConv at all scales within a single\nmodule to predict multiple scales. Our extensive experiments demonstrate that\ntraining multiple scales with a single model reduces the training budget and\nstored parameters by one-third while achieving equivalent inference latency and\ncomparable performance. Furthermore, we propose IGConv$^{+}$, which addresses\nspectral bias and input-independent upsampling and uses ensemble prediction to\nimprove performance. As a result, SRFormer-IGConv$^{+}$ achieves a remarkable\n0.25dB improvement in PSNR at Urban100$\\times$4 while reducing the training\nbudget, stored parameters, and inference cost compared to the existing\nSRFormer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Super-Resolution (SR) achieved significant performance improvement\nby employing neural networks. Most SR methods conventionally train a single\nmodel for each targeted scale, which increases redundancy in training and\ndeployment in proportion to the number of scales targeted. This paper\nchallenges this conventional fixed-scale approach. Our preliminary analysis\nreveals that, surprisingly, encoders trained at different scales extract\nsimilar features from images. Furthermore, the commonly used scale-specific\nupsampler, Sub-Pixel Convolution (SPConv), exhibits significant inter-scale\ncorrelations. Based on these observations, we propose a framework for training\nmultiple integer scales simultaneously with a single model. We use a single\nencoder to extract features and introduce a novel upsampler, Implicit Grid\nConvolution~(IGConv), which integrates SPConv at all scales within a single\nmodule to predict multiple scales. Our extensive experiments demonstrate that\ntraining multiple scales with a single model reduces the training budget and\nstored parameters by one-third while achieving equivalent inference latency and\ncomparable performance. Furthermore, we propose IGConv$^{+}$, which addresses\nspectral bias and input-independent upsampling and uses ensemble prediction to\nimprove performance. As a result, SRFormer-IGConv$^{+}$ achieves a remarkable\n0.25dB improvement in PSNR at Urban100$\\times$4 while reducing the training\nbudget, stored parameters, and inference cost compared to the existing\nSRFormer."
                },
                "authors": [
                    {
                        "name": "Dongheon Lee"
                    },
                    {
                        "name": "Seokju Yun"
                    },
                    {
                        "name": "Youngmin Ro"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Ro"
                },
                "author": "Youngmin Ro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13444v2",
                "updated": "2024-08-19T03:25:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    25,
                    52,
                    0,
                    232,
                    0
                ],
                "published": "2024-01-24T13:36:50Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    13,
                    36,
                    50,
                    2,
                    24,
                    0
                ],
                "title": "Clue-Guided Path Exploration: Optimizing Knowledge Graph Retrieval with\n  Large Language Models to Address the Information Black Box Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clue-Guided Path Exploration: Optimizing Knowledge Graph Retrieval with\n  Large Language Models to Address the Information Black Box Challenge"
                },
                "summary": "In recent times, large language models (LLMs) have showcased remarkable\ncapabilities. However, updating their knowledge poses challenges, potentially\nleading to inaccuracies when confronted with unfamiliar queries. To address\nthis issue, integrating external knowledge bases such as knowledge graphs with\nlarge language models is a viable approach. The key challenge lies in\nextracting the required knowledge from knowledge graphs based on natural\nlanguage, demanding high semantic understanding. Therefore, researchers are\nconsidering leveraging large language models directly for knowledge retrieval\nfrom these graphs. Current efforts typically rely on the comprehensive\nproblem-solving capabilities of large language models. We argue that a problem\nwe term the 'information black box' can significantly impact the practical\neffectiveness of such methods. Moreover, this kind of methods is less effective\nfor scenarios where the questions are unfamiliar to the large language models.\nIn this paper, we propose a Clue-Guided Path Exploration (CGPE) framework to\noptimize knowledge retrieval based on large language models. By addressing the\n'information black box' issue and employing single-task approaches instead of\ncomplex tasks, we have enhanced the accuracy and efficiency of using large\nlanguage models for retrieving knowledge graphs. Experiments on open-source\ndatasets reveal that CGPE outperforms previous methods and is highly applicable\nto LLMs with fewer parameters. In some instances, even ChatGLM3, with its 6\nbillion parameters, can rival the performance of GPT-4. Furthermore, the\nresults indicate a minimal invocation frequency of CGPE on LLMs, suggesting\nreduced computational overhead. For organizations and individuals facing\nconstraints in computational resources, our research offers significant\npractical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent times, large language models (LLMs) have showcased remarkable\ncapabilities. However, updating their knowledge poses challenges, potentially\nleading to inaccuracies when confronted with unfamiliar queries. To address\nthis issue, integrating external knowledge bases such as knowledge graphs with\nlarge language models is a viable approach. The key challenge lies in\nextracting the required knowledge from knowledge graphs based on natural\nlanguage, demanding high semantic understanding. Therefore, researchers are\nconsidering leveraging large language models directly for knowledge retrieval\nfrom these graphs. Current efforts typically rely on the comprehensive\nproblem-solving capabilities of large language models. We argue that a problem\nwe term the 'information black box' can significantly impact the practical\neffectiveness of such methods. Moreover, this kind of methods is less effective\nfor scenarios where the questions are unfamiliar to the large language models.\nIn this paper, we propose a Clue-Guided Path Exploration (CGPE) framework to\noptimize knowledge retrieval based on large language models. By addressing the\n'information black box' issue and employing single-task approaches instead of\ncomplex tasks, we have enhanced the accuracy and efficiency of using large\nlanguage models for retrieving knowledge graphs. Experiments on open-source\ndatasets reveal that CGPE outperforms previous methods and is highly applicable\nto LLMs with fewer parameters. In some instances, even ChatGLM3, with its 6\nbillion parameters, can rival the performance of GPT-4. Furthermore, the\nresults indicate a minimal invocation frequency of CGPE on LLMs, suggesting\nreduced computational overhead. For organizations and individuals facing\nconstraints in computational resources, our research offers significant\npractical value."
                },
                "authors": [
                    {
                        "name": "Dehao Tao"
                    },
                    {
                        "name": "Feng Huang"
                    },
                    {
                        "name": "Congqi Wang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Minghu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Minghu Jiang"
                },
                "author": "Minghu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05517v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05517v3",
                "updated": "2024-08-19T03:21:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    21,
                    21,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-10T11:00:13Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    11,
                    0,
                    13,
                    5,
                    223,
                    0
                ],
                "title": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning"
                },
                "summary": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the most\ncomprehensive support for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the most\ncomprehensive support for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%."
                },
                "authors": [
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Jintao Huang"
                    },
                    {
                        "name": "Jinghan Hu"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Daoze Zhang"
                    },
                    {
                        "name": "Zeyinzi Jiang"
                    },
                    {
                        "name": "Zhikai Wu"
                    },
                    {
                        "name": "Baole Ai"
                    },
                    {
                        "name": "Ang Wang"
                    },
                    {
                        "name": "Wenmeng Zhou"
                    },
                    {
                        "name": "Yingda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Chen"
                },
                "author": "Yingda Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05517v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05517v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v2",
                "updated": "2024-08-19T03:18:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    18,
                    59,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "HERA: High-efficiency Matrix Compression via Element Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERA: High-efficiency Matrix Compression via Element Replacement"
                },
                "summary": "Matrix quantization involves encoding matrix elements in a more\nspace-efficient manner to minimize storage requirements, with dequantization\nused to reconstruct the original matrix for practical use. We define the\nQuantization Error Minimization (QEM) problem as minimizing the difference\nbetween a matrix before and after quantization while ensuring that the\nquantized matrix occupies the same amount of memory. Matrix quantization is\nessential in various fields, including weight quantization in Large Language\nModels (LLMs), vector databases, KV cache quantization, graph compression, and\nimage compression. The growing scale of LLMs, such as GPT-4 and BERT,\nunderscores the need for matrix compression due to the large size of parameters\nand KV caches, which are stored as matrices.\n  To address the QEM problem, we introduce HETA, an algorithm that leverages\nthe local orderliness of matrix elements by iteratively swapping elements to\ncreate a locally ordered matrix. This matrix is then grouped and quantized by\ncolumns. To further improve HETA, we present two optimizations: additional\nquantization of residuals to reduce mean squared error (MSE) and the\napplication of masking and batch processing to accelerate the algorithm.\n  Our experiments show that HETA effectively reduces MSE to 12.3% of its\noriginal value at the same compression ratio, outperforming leading baseline\nalgorithms. Our contributions include formalizing the QEM problem, developing\nthe HETA algorithm, and proposing two optimizations to enhance both accuracy\nand processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization involves encoding matrix elements in a more\nspace-efficient manner to minimize storage requirements, with dequantization\nused to reconstruct the original matrix for practical use. We define the\nQuantization Error Minimization (QEM) problem as minimizing the difference\nbetween a matrix before and after quantization while ensuring that the\nquantized matrix occupies the same amount of memory. Matrix quantization is\nessential in various fields, including weight quantization in Large Language\nModels (LLMs), vector databases, KV cache quantization, graph compression, and\nimage compression. The growing scale of LLMs, such as GPT-4 and BERT,\nunderscores the need for matrix compression due to the large size of parameters\nand KV caches, which are stored as matrices.\n  To address the QEM problem, we introduce HETA, an algorithm that leverages\nthe local orderliness of matrix elements by iteratively swapping elements to\ncreate a locally ordered matrix. This matrix is then grouped and quantized by\ncolumns. To further improve HETA, we present two optimizations: additional\nquantization of residuals to reduce mean squared error (MSE) and the\napplication of masking and batch processing to accelerate the algorithm.\n  Our experiments show that HETA effectively reduces MSE to 12.3% of its\noriginal value at the same compression ratio, outperforming leading baseline\nalgorithms. Our contributions include formalizing the QEM problem, developing\nthe HETA algorithm, and proposing two optimizations to enhance both accuracy\nand processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09671v1",
                "updated": "2024-08-19T03:13:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    13,
                    20,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:13:20Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    13,
                    20,
                    0,
                    232,
                    0
                ],
                "title": "GANPrompt: Enhancing Robustness in LLM-Based Recommendations with\n  GAN-Enhanced Diversity Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANPrompt: Enhancing Robustness in LLM-Based Recommendations with\n  GAN-Enhanced Diversity Prompts"
                },
                "summary": "In recent years, LLM has demonstrated remarkable proficiency in comprehending\nand generating natural language, with a growing prevalence in the domain of\nrecommender systems. However, LLM continues to face a significant challenge in\nthat it is highly susceptible to the influence of prompt words. This\ninconsistency in response to minor alterations in prompt input may compromise\nthe accuracy and resilience of recommendation models. To address this issue,\nthis paper proposes GANPrompt, a multi-dimensional large language model prompt\ndiversity framework based on Generative Adversarial Networks (GANs). The\nframework enhances the model's adaptability and stability to diverse prompts by\nintegrating GAN generation techniques with the deep semantic understanding\ncapabilities of LLMs. GANPrompt first trains a generator capable of producing\ndiverse prompts by analysing multidimensional user behavioural data. These\ndiverse prompts are then used to train the LLM to improve its performance in\nthe face of unseen prompts. Furthermore, to ensure a high degree of diversity\nand relevance of the prompts, this study introduces a mathematical theory-based\ndiversity constraint mechanism that optimises the generated prompts to ensure\nthat they are not only superficially distinct, but also semantically cover a\nwide range of user intentions. Through extensive experiments on multiple\ndatasets, we demonstrate the effectiveness of the proposed framework,\nespecially in improving the adaptability and robustness of recommender systems\nin complex and dynamic environments. The experimental results demonstrate that\nGANPrompt yields substantial enhancements in accuracy and robustness relative\nto existing state-of-the-art methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, LLM has demonstrated remarkable proficiency in comprehending\nand generating natural language, with a growing prevalence in the domain of\nrecommender systems. However, LLM continues to face a significant challenge in\nthat it is highly susceptible to the influence of prompt words. This\ninconsistency in response to minor alterations in prompt input may compromise\nthe accuracy and resilience of recommendation models. To address this issue,\nthis paper proposes GANPrompt, a multi-dimensional large language model prompt\ndiversity framework based on Generative Adversarial Networks (GANs). The\nframework enhances the model's adaptability and stability to diverse prompts by\nintegrating GAN generation techniques with the deep semantic understanding\ncapabilities of LLMs. GANPrompt first trains a generator capable of producing\ndiverse prompts by analysing multidimensional user behavioural data. These\ndiverse prompts are then used to train the LLM to improve its performance in\nthe face of unseen prompts. Furthermore, to ensure a high degree of diversity\nand relevance of the prompts, this study introduces a mathematical theory-based\ndiversity constraint mechanism that optimises the generated prompts to ensure\nthat they are not only superficially distinct, but also semantically cover a\nwide range of user intentions. Through extensive experiments on multiple\ndatasets, we demonstrate the effectiveness of the proposed framework,\nespecially in improving the adaptability and robustness of recommender systems\nin complex and dynamic environments. The experimental results demonstrate that\nGANPrompt yields substantial enhancements in accuracy and robustness relative\nto existing state-of-the-art methodologies."
                },
                "authors": [
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Chuang Zhao"
                    },
                    {
                        "name": "Hongke Zhao"
                    },
                    {
                        "name": "Likang Wu"
                    },
                    {
                        "name": "Ming HE"
                    }
                ],
                "author_detail": {
                    "name": "Ming HE"
                },
                "author": "Ming HE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20329v2",
                "updated": "2024-08-19T03:06:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    6,
                    24,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-29T17:59:06Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    17,
                    59,
                    6,
                    4,
                    89,
                    0
                ],
                "title": "ReALM: Reference Resolution As Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReALM: Reference Resolution As Language Modeling"
                },
                "summary": "Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it."
                },
                "authors": [
                    {
                        "name": "Joel Ruben Antony Moniz"
                    },
                    {
                        "name": "Soundarya Krishnan"
                    },
                    {
                        "name": "Melis Ozyildirim"
                    },
                    {
                        "name": "Prathamesh Saraf"
                    },
                    {
                        "name": "Halim Cagri Ates"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "Accepted at SIGDIAL 2024 (Oral presentation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08512v2",
                "updated": "2024-08-19T02:46:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    2,
                    46,
                    26,
                    0,
                    232,
                    0
                ],
                "published": "2024-03-13T13:23:05Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    13,
                    23,
                    5,
                    2,
                    73,
                    0
                ],
                "title": "MergeOcc: Bridge the Domain Gap between Different LiDARs for Robust\n  Occupancy Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeOcc: Bridge the Domain Gap between Different LiDARs for Robust\n  Occupancy Prediction"
                },
                "summary": "LiDAR-based 3D occupancy prediction evolved rapidly alongside the emergence\nof large datasets. Nevertheless, the potential of existing diverse datasets\nremains underutilized as they kick in individually. Models trained on a\nspecific dataset often suffer considerable performance degradation when\ndeployed to real-world scenarios or datasets involving disparate LiDARs. This\npaper aims to develop a generalized model called MergeOcc, to simultaneously\nhandle different LiDARs by leveraging multiple datasets. The gaps among LiDAR\ndatasets primarily manifest in geometric disparities and semantic\ninconsistencies. Thus, MergeOcc incorporates a novel model featuring a\ngeometric realignment module and a semantic label mapping module to enable\nmultiple datasets training (MDT). The effectiveness of MergeOcc is validated\nthrough experiments on two prominent datasets for autonomous vehicles:\nOpenOccupancy-nuScenes and SemanticKITTI. The results demonstrate its enhanced\nrobustness and remarkable performance across both types of LiDARs,\noutperforming several SOTA multi-modality methods. Notably, despite using an\nidentical model architecture and hyper-parameter set, MergeOcc can\nsignificantly surpass the baseline due to its exposure to more diverse data.\nMergeOcc is considered the first cross-dataset 3D occupancy prediction pipeline\nthat effectively bridges the domain gap for seamless deployment across\nheterogeneous platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-based 3D occupancy prediction evolved rapidly alongside the emergence\nof large datasets. Nevertheless, the potential of existing diverse datasets\nremains underutilized as they kick in individually. Models trained on a\nspecific dataset often suffer considerable performance degradation when\ndeployed to real-world scenarios or datasets involving disparate LiDARs. This\npaper aims to develop a generalized model called MergeOcc, to simultaneously\nhandle different LiDARs by leveraging multiple datasets. The gaps among LiDAR\ndatasets primarily manifest in geometric disparities and semantic\ninconsistencies. Thus, MergeOcc incorporates a novel model featuring a\ngeometric realignment module and a semantic label mapping module to enable\nmultiple datasets training (MDT). The effectiveness of MergeOcc is validated\nthrough experiments on two prominent datasets for autonomous vehicles:\nOpenOccupancy-nuScenes and SemanticKITTI. The results demonstrate its enhanced\nrobustness and remarkable performance across both types of LiDARs,\noutperforming several SOTA multi-modality methods. Notably, despite using an\nidentical model architecture and hyper-parameter set, MergeOcc can\nsignificantly surpass the baseline due to its exposure to more diverse data.\nMergeOcc is considered the first cross-dataset 3D occupancy prediction pipeline\nthat effectively bridges the domain gap for seamless deployment across\nheterogeneous platforms."
                },
                "authors": [
                    {
                        "name": "Zikun Xu"
                    },
                    {
                        "name": "Jianqiang Wang"
                    },
                    {
                        "name": "Shaobing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shaobing Xu"
                },
                "author": "Shaobing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09656v2",
                "updated": "2024-08-20T02:05:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    5,
                    46,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T02:34:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    2,
                    34,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "A Comparison of Large Language Model and Human Performance on Random\n  Number Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparison of Large Language Model and Human Performance on Random\n  Number Generation Tasks"
                },
                "summary": "Random Number Generation Tasks (RNGTs) are used in psychology for examining\nhow humans generate sequences devoid of predictable patterns. By adapting an\nexisting human RNGT for an LLM-compatible environment, this preliminary study\ntests whether ChatGPT-3.5, a large language model (LLM) trained on\nhuman-generated text, exhibits human-like cognitive biases when generating\nrandom number sequences. Initial findings indicate that ChatGPT-3.5 more\neffectively avoids repetitive and sequential patterns compared to humans, with\nnotably lower repeat frequencies and adjacent number frequencies. Continued\nresearch into different models, parameters, and prompting methodologies will\ndeepen our understanding of how LLMs can more closely mimic human random\ngeneration behaviors, while also broadening their applications in cognitive and\nbehavioral science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Number Generation Tasks (RNGTs) are used in psychology for examining\nhow humans generate sequences devoid of predictable patterns. By adapting an\nexisting human RNGT for an LLM-compatible environment, this preliminary study\ntests whether ChatGPT-3.5, a large language model (LLM) trained on\nhuman-generated text, exhibits human-like cognitive biases when generating\nrandom number sequences. Initial findings indicate that ChatGPT-3.5 more\neffectively avoids repetitive and sequential patterns compared to humans, with\nnotably lower repeat frequencies and adjacent number frequencies. Continued\nresearch into different models, parameters, and prompting methodologies will\ndeepen our understanding of how LLMs can more closely mimic human random\ngeneration behaviors, while also broadening their applications in cognitive and\nbehavioral science research."
                },
                "authors": [
                    {
                        "name": "Rachel M. Harrison"
                    }
                ],
                "author_detail": {
                    "name": "Rachel M. Harrison"
                },
                "author": "Rachel M. Harrison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07314v2",
                "updated": "2024-08-19T02:21:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    2,
                    21,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-05-12T15:49:38Z",
                "published_parsed": [
                    2024,
                    5,
                    12,
                    15,
                    49,
                    38,
                    6,
                    133,
                    0
                ],
                "title": "Learnable Item Tokenization for Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Item Tokenization for Generative Recommendation"
                },
                "summary": "Utilizing powerful Large Language Models (LLMs) for generative recommendation\nhas attracted much attention. Nevertheless, a crucial challenge is transforming\nrecommendation data into the language space of LLMs through effective item\ntokenization. Current approaches, such as ID, textual, and codebook-based\nidentifiers, exhibit shortcomings in encoding semantic information,\nincorporating collaborative signals, or handling code assignment bias. To\naddress these limitations, we propose LETTER (a LEarnable Tokenizer for\ngeneraTivE Recommendation), which integrates hierarchical semantics,\ncollaborative signals, and code assignment diversity to satisfy the essential\nrequirements of identifiers. LETTER incorporates Residual Quantized VAE for\nsemantic regularization, a contrastive alignment loss for collaborative\nregularization, and a diversity loss to mitigate code assignment bias. We\ninstantiate LETTER on two models and propose a ranking-guided generation loss\nto augment their ranking ability theoretically. Experiments on three datasets\nvalidate the superiority of LETTER, advancing the state-of-the-art in the field\nof LLM-based generative recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing powerful Large Language Models (LLMs) for generative recommendation\nhas attracted much attention. Nevertheless, a crucial challenge is transforming\nrecommendation data into the language space of LLMs through effective item\ntokenization. Current approaches, such as ID, textual, and codebook-based\nidentifiers, exhibit shortcomings in encoding semantic information,\nincorporating collaborative signals, or handling code assignment bias. To\naddress these limitations, we propose LETTER (a LEarnable Tokenizer for\ngeneraTivE Recommendation), which integrates hierarchical semantics,\ncollaborative signals, and code assignment diversity to satisfy the essential\nrequirements of identifiers. LETTER incorporates Residual Quantized VAE for\nsemantic regularization, a contrastive alignment loss for collaborative\nregularization, and a diversity loss to mitigate code assignment bias. We\ninstantiate LETTER on two models and propose a ranking-guided generation loss\nto augment their ranking ability theoretically. Experiments on three datasets\nvalidate the superiority of LETTER, advancing the state-of-the-art in the field\nof LLM-based generative recommendation."
                },
                "authors": [
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Honghui Bao"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21369v2",
                "updated": "2024-08-19T01:54:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    54,
                    38,
                    0,
                    232,
                    0
                ],
                "published": "2024-07-31T06:35:15Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    6,
                    35,
                    15,
                    2,
                    213,
                    0
                ],
                "title": "An LLM-based Readability Measurement for Unit Tests' Context-aware\n  Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Readability Measurement for Unit Tests' Context-aware\n  Inputs"
                },
                "summary": "Automated test techniques usually generate unit tests with higher code\ncoverage than manual tests. However, the readability of automated tests is\ncrucial for code comprehension and maintenance. The readability of unit tests\ninvolves many aspects. In this paper, we focus on test inputs. The central\nlimitation of existing studies on input readability is that they focus on test\ncodes alone without taking the tested source codes into consideration, making\nthem either ignore different source codes' different readability requirements\nor require manual efforts to write readable inputs. However, we observe that\nthe source codes specify the contexts that test inputs must satisfy. Based on\nsuch observation, we introduce the \\underline{C}ontext \\underline{C}onsistency\n\\underline{C}riterion (a.k.a, C3), which is a readability measurement tool that\nleverages Large Language Models to extract primitive-type (including\nstring-type) parameters' readability contexts from the source codes and checks\nwhether test inputs are consistent with those contexts. We have also proposed\nEvoSuiteC3. It leverages C3's extracted contexts to help EvoSuite generate\nreadable test inputs. We have evaluated C3's performance on $409$ \\java{}\nclasses and compared manual and automated tests' readability under C3\nmeasurement. The results are two-fold. First, The Precision, Recall, and\nF1-Score of C3's mined readability contexts are \\precision{}, \\recall{}, and\n\\fone{}, respectively. Second, under C3's measurement, the string-type input\nreadability scores of EvoSuiteC3, ChatUniTest (an LLM-based test generation\ntool), manual tests, and two traditional tools (EvoSuite and Randoop) are\n$90\\%$, $83\\%$, $68\\%$, $8\\%$, and $8\\%$, showing the traditional tools'\ninability in generating readable string-type inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test techniques usually generate unit tests with higher code\ncoverage than manual tests. However, the readability of automated tests is\ncrucial for code comprehension and maintenance. The readability of unit tests\ninvolves many aspects. In this paper, we focus on test inputs. The central\nlimitation of existing studies on input readability is that they focus on test\ncodes alone without taking the tested source codes into consideration, making\nthem either ignore different source codes' different readability requirements\nor require manual efforts to write readable inputs. However, we observe that\nthe source codes specify the contexts that test inputs must satisfy. Based on\nsuch observation, we introduce the \\underline{C}ontext \\underline{C}onsistency\n\\underline{C}riterion (a.k.a, C3), which is a readability measurement tool that\nleverages Large Language Models to extract primitive-type (including\nstring-type) parameters' readability contexts from the source codes and checks\nwhether test inputs are consistent with those contexts. We have also proposed\nEvoSuiteC3. It leverages C3's extracted contexts to help EvoSuite generate\nreadable test inputs. We have evaluated C3's performance on $409$ \\java{}\nclasses and compared manual and automated tests' readability under C3\nmeasurement. The results are two-fold. First, The Precision, Recall, and\nF1-Score of C3's mined readability contexts are \\precision{}, \\recall{}, and\n\\fone{}, respectively. Second, under C3's measurement, the string-type input\nreadability scores of EvoSuiteC3, ChatUniTest (an LLM-based test generation\ntool), manual tests, and two traditional tools (EvoSuite and Randoop) are\n$90\\%$, $83\\%$, $68\\%$, $8\\%$, and $8\\%$, showing the traditional tools'\ninability in generating readable string-type inputs."
                },
                "authors": [
                    {
                        "name": "Zhichao Zhou"
                    },
                    {
                        "name": "Yutian Tang"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Jingzhu He"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhu He"
                },
                "author": "Jingzhu He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09639v1",
                "updated": "2024-08-19T01:53:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    53,
                    47,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T01:53:47Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    53,
                    47,
                    0,
                    232,
                    0
                ],
                "title": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments"
                },
                "summary": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where LMs are presented with a pair of\nacceptable and unacceptable sentences and required to judge which is\nacceptable. The existing dominant approach, however, naively calculates and\ncompares the probabilities of paired sentences using LMs. Additionally, large\nlanguage models (LLMs) have yet to be thoroughly examined in this field. We\nthus investigate how to make the most of LLMs' grammatical knowledge to\ncomprehensively evaluate it. Through extensive experiments of nine judgment\nmethods in English and Chinese, we demonstrate that a probability readout\nmethod, in-template LP, and a prompting-based method, Yes/No probability\ncomputing, achieve particularly high performance, surpassing the conventional\napproach. Our analysis reveals their different strengths, e.g., Yes/No\nprobability computing is robust against token-length bias, suggesting that they\nharness different aspects of LLMs' grammatical knowledge. Consequently, we\nrecommend using diverse judgment methods to evaluate LLMs comprehensively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where LMs are presented with a pair of\nacceptable and unacceptable sentences and required to judge which is\nacceptable. The existing dominant approach, however, naively calculates and\ncompares the probabilities of paired sentences using LMs. Additionally, large\nlanguage models (LLMs) have yet to be thoroughly examined in this field. We\nthus investigate how to make the most of LLMs' grammatical knowledge to\ncomprehensively evaluate it. Through extensive experiments of nine judgment\nmethods in English and Chinese, we demonstrate that a probability readout\nmethod, in-template LP, and a prompting-based method, Yes/No probability\ncomputing, achieve particularly high performance, surpassing the conventional\napproach. Our analysis reveals their different strengths, e.g., Yes/No\nprobability computing is robust against token-length bias, suggesting that they\nharness different aspects of LLMs' grammatical knowledge. Consequently, we\nrecommend using diverse judgment methods to evaluate LLMs comprehensively."
                },
                "authors": [
                    {
                        "name": "Yusuke Ide"
                    },
                    {
                        "name": "Yuto Nishida"
                    },
                    {
                        "name": "Miyu Oba"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Justin Vasselli"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09632v2",
                "updated": "2024-08-20T05:28:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    5,
                    28,
                    27,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T01:30:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    30,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDeGPT: Modular Decomposition for Large Language Model Compression"
                },
                "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."
                },
                "authors": [
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Abhishek Patel"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A23 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09629v1",
                "updated": "2024-08-19T01:22:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    22,
                    21,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T01:22:21Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    22,
                    21,
                    0,
                    232,
                    0
                ],
                "title": "A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic\n  Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic\n  Text Classification"
                },
                "summary": "Transformer models have achieved state-of-the-art results, with Large\nLanguage Models (LLMs), an evolution of first-generation transformers (1stTR),\nbeing considered the cutting edge in several NLP tasks. However, the literature\nhas yet to conclusively demonstrate that LLMs consistently outperform 1stTRs\nacross all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and\nBART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis\ndatasets. The results indicate that open LLMs may moderately outperform or\nmatch 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this\nsubstantial cost for only moderate gains, the practical applicability of these\nmodels in cost-sensitive scenarios is questionable. In this context, a\nconfidence-based strategy that seamlessly integrates 1stTRs with open LLMs\nbased on prediction certainty is proposed. High-confidence documents are\nclassified by the more cost-effective 1stTRs, while uncertain cases are handled\nby LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned\nversions. Experiments in sentiment analysis demonstrate that our solution not\nonly outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely\nwith fine-tuned LLMs at a fraction of the cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have achieved state-of-the-art results, with Large\nLanguage Models (LLMs), an evolution of first-generation transformers (1stTR),\nbeing considered the cutting edge in several NLP tasks. However, the literature\nhas yet to conclusively demonstrate that LLMs consistently outperform 1stTRs\nacross all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and\nBART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis\ndatasets. The results indicate that open LLMs may moderately outperform or\nmatch 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this\nsubstantial cost for only moderate gains, the practical applicability of these\nmodels in cost-sensitive scenarios is questionable. In this context, a\nconfidence-based strategy that seamlessly integrates 1stTRs with open LLMs\nbased on prediction certainty is proposed. High-confidence documents are\nclassified by the more cost-effective 1stTRs, while uncertain cases are handled\nby LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned\nversions. Experiments in sentiment analysis demonstrate that our solution not\nonly outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely\nwith fine-tuned LLMs at a fraction of the cost."
                },
                "authors": [
                    {
                        "name": "Claudio M. V. de Andrade"
                    },
                    {
                        "name": "Washington Cunha"
                    },
                    {
                        "name": "Davi Reis"
                    },
                    {
                        "name": "Adriana Silvina Pagano"
                    },
                    {
                        "name": "Leonardo Rocha"
                    },
                    {
                        "name": "Marcos André Gonçalves"
                    }
                ],
                "author_detail": {
                    "name": "Marcos André Gonçalves"
                },
                "author": "Marcos André Gonçalves",
                "arxiv_comment": "13 pages, 3 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06824v3",
                "updated": "2024-08-19T01:18:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    18,
                    41,
                    0,
                    232,
                    0
                ],
                "published": "2023-10-10T17:54:39Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    17,
                    54,
                    39,
                    1,
                    283,
                    0
                ],
                "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model\n  Representations of True/False Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Truth: Emergent Linear Structure in Large Language Model\n  Representations of True/False Datasets"
                },
                "summary": "Large Language Models (LLMs) have impressive capabilities, but are prone to\noutputting falsehoods. Recent work has developed techniques for inferring\nwhether a LLM is telling the truth by training probes on the LLM's internal\nactivations. However, this line of work is controversial, with some authors\npointing out failures of these probes to generalize in basic ways, among other\nconceptual issues. In this work, we use high-quality datasets of simple\ntrue/false statements to study in detail the structure of LLM representations\nof truth, drawing on three lines of evidence: 1. Visualizations of LLM\ntrue/false statement representations, which reveal clear linear structure. 2.\nTransfer experiments in which probes trained on one dataset generalize to\ndifferent datasets. 3. Causal evidence obtained by surgically intervening in a\nLLM's forward pass, causing it to treat false statements as true and vice\nversa. Overall, we present evidence that at sufficient scale, LLMs linearly\nrepresent the truth or falsehood of factual statements. We also show that\nsimple difference-in-mean probes generalize as well as other probing techniques\nwhile identifying directions which are more causally implicated in model\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have impressive capabilities, but are prone to\noutputting falsehoods. Recent work has developed techniques for inferring\nwhether a LLM is telling the truth by training probes on the LLM's internal\nactivations. However, this line of work is controversial, with some authors\npointing out failures of these probes to generalize in basic ways, among other\nconceptual issues. In this work, we use high-quality datasets of simple\ntrue/false statements to study in detail the structure of LLM representations\nof truth, drawing on three lines of evidence: 1. Visualizations of LLM\ntrue/false statement representations, which reveal clear linear structure. 2.\nTransfer experiments in which probes trained on one dataset generalize to\ndifferent datasets. 3. Causal evidence obtained by surgically intervening in a\nLLM's forward pass, causing it to treat false statements as true and vice\nversa. Overall, we present evidence that at sufficient scale, LLMs linearly\nrepresent the truth or falsehood of factual statements. We also show that\nsimple difference-in-mean probes generalize as well as other probing techniques\nwhile identifying directions which are more causally implicated in model\noutputs."
                },
                "authors": [
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "Conference on Language Modeling, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08475v2",
                "updated": "2024-08-19T01:04:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    4,
                    7,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T01:21:57Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    21,
                    57,
                    4,
                    229,
                    0
                ],
                "title": "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy"
                },
                "summary": "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust."
                },
                "authors": [
                    {
                        "name": "Mary Anne Smart"
                    },
                    {
                        "name": "Priyanka Nanayakkara"
                    },
                    {
                        "name": "Rachel Cummings"
                    },
                    {
                        "name": "Gabriel Kaptchuk"
                    },
                    {
                        "name": "Elissa Redmiles"
                    }
                ],
                "author_detail": {
                    "name": "Elissa Redmiles"
                },
                "author": "Elissa Redmiles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05374v3",
                "updated": "2024-08-19T00:52:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    0,
                    52,
                    51,
                    0,
                    232,
                    0
                ],
                "published": "2024-02-08T03:12:25Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    3,
                    12,
                    25,
                    3,
                    39,
                    0
                ],
                "title": "CIC: A framework for Culturally-aware Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIC: A framework for Culturally-aware Image Captioning"
                },
                "summary": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic.."
                },
                "authors": [
                    {
                        "name": "Youngsik Yun"
                    },
                    {
                        "name": "Jihie Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihie Kim"
                },
                "author": "Jihie Kim",
                "arxiv_doi": "10.24963/ijcai.2024/180",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/180",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in IJCAI 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09622v1",
                "updated": "2024-08-19T00:29:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    0,
                    29,
                    42,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T00:29:42Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    0,
                    29,
                    42,
                    0,
                    232,
                    0
                ],
                "title": "Global BGP Attacks that Evade Route Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global BGP Attacks that Evade Route Monitoring"
                },
                "summary": "As the deployment of comprehensive Border Gateway Protocol (BGP) security\nmeasures is still in progress, BGP monitoring continues to play a critical role\nin protecting the Internet from routing attacks. Fundamentally, monitoring\ninvolves observing BGP feeds to detect suspicious announcements and taking\ndefensive action. However, BGP monitoring relies on seeing the malicious BGP\nannouncement in the first place! In this paper, we develop a novel attack that\ncan hide itself from all state-of-the-art BGP monitoring systems we tested\nwhile affecting the entire Internet. The attack involves launching a sub-prefix\nhijack with the RFC-specified NO_EXPORT community attached to prevent networks\nwith the malicious route installed from sending the route to BGP monitoring\nsystems. We study the viability of this attack at four tier-1 networks and find\nall networks we studied were vulnerable to the attack. Finally, we propose a\nmitigation that significantly improves the robustness of the BGP monitoring\necosystem. Our paper aims to raise awareness of this issue and offer guidance\nto providers to protect against such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the deployment of comprehensive Border Gateway Protocol (BGP) security\nmeasures is still in progress, BGP monitoring continues to play a critical role\nin protecting the Internet from routing attacks. Fundamentally, monitoring\ninvolves observing BGP feeds to detect suspicious announcements and taking\ndefensive action. However, BGP monitoring relies on seeing the malicious BGP\nannouncement in the first place! In this paper, we develop a novel attack that\ncan hide itself from all state-of-the-art BGP monitoring systems we tested\nwhile affecting the entire Internet. The attack involves launching a sub-prefix\nhijack with the RFC-specified NO_EXPORT community attached to prevent networks\nwith the malicious route installed from sending the route to BGP monitoring\nsystems. We study the viability of this attack at four tier-1 networks and find\nall networks we studied were vulnerable to the attack. Finally, we propose a\nmitigation that significantly improves the robustness of the BGP monitoring\necosystem. Our paper aims to raise awareness of this issue and offer guidance\nto providers to protect against such attacks."
                },
                "authors": [
                    {
                        "name": "Henry Birge-Lee"
                    },
                    {
                        "name": "Maria Apostolaki"
                    },
                    {
                        "name": "Jennifer Rexford"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Rexford"
                },
                "author": "Jennifer Rexford",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09615v1",
                "updated": "2024-08-18T23:53:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    23,
                    53,
                    34,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T23:53:34Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    23,
                    53,
                    34,
                    6,
                    231,
                    0
                ],
                "title": "The First Competition on Resource-Limited Infrared Small Target\n  Detection Challenge: Methods and Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The First Competition on Resource-Limited Infrared Small Target\n  Detection Challenge: Methods and Results"
                },
                "summary": "In this paper, we briefly summarize the first competition on resource-limited\ninfrared small target detection (namely, LimitIRSTD). This competition has two\ntracks, including weakly-supervised infrared small target detection (Track 1)\nand lightweight infrared small target detection (Track 2). 46 and 60 teams\nsuccessfully registered and took part in Tracks 1 and Track 2, respectively.\nThe top-performing methods and their results in each track are described with\ndetails. This competition inspires the community to explore the tough problems\nin the application of infrared small target detection, and ultimately promote\nthe deployment of this technology under limited resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we briefly summarize the first competition on resource-limited\ninfrared small target detection (namely, LimitIRSTD). This competition has two\ntracks, including weakly-supervised infrared small target detection (Track 1)\nand lightweight infrared small target detection (Track 2). 46 and 60 teams\nsuccessfully registered and took part in Tracks 1 and Track 2, respectively.\nThe top-performing methods and their results in each track are described with\ndetails. This competition inspires the community to explore the tough problems\nin the application of infrared small target detection, and ultimately promote\nthe deployment of this technology under limited resource."
                },
                "authors": [
                    {
                        "name": "Boyang Li"
                    },
                    {
                        "name": "Xinyi Ying"
                    },
                    {
                        "name": "Ruojing Li"
                    },
                    {
                        "name": "Yongxian Liu"
                    },
                    {
                        "name": "Yangsi Shi"
                    },
                    {
                        "name": "Miao Li"
                    }
                ],
                "author_detail": {
                    "name": "Miao Li"
                },
                "author": "Miao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00029v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00029v3",
                "updated": "2024-08-18T22:26:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    22,
                    26,
                    13,
                    6,
                    231,
                    0
                ],
                "published": "2023-11-16T07:31:18Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    7,
                    31,
                    18,
                    3,
                    320,
                    0
                ],
                "title": "Bergeron: Combating Adversarial Attacks through a Conscience-Based\n  Alignment Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bergeron: Combating Adversarial Attacks through a Conscience-Based\n  Alignment Framework"
                },
                "summary": "Research into AI alignment has grown considerably since the recent\nintroduction of increasingly capable Large Language Models (LLMs).\nUnfortunately, modern methods of alignment still fail to fully prevent harmful\nresponses when models are deliberately attacked. Such vulnerabilities can lead\nto LLMs being manipulated into generating hazardous content: from instructions\nfor creating dangerous materials to inciting violence or endorsing unethical\nbehaviors. To help mitigate this issue, we introduce Bergeron: a framework\ndesigned to improve the robustness of LLMs against attacks without any\nadditional parameter fine-tuning. Bergeron is organized into two tiers; with a\nsecondary LLM acting as a guardian to the primary LLM. This framework better\nsafeguards the primary model against incoming attacks while monitoring its\noutput for any harmful content. Empirical analysis reviews that by using\nBergeron to complement models with existing alignment training, we can\nsignificantly improve the robustness and safety of multiple, commonly used\ncommercial and open-source LLMs. Specifically, we found that models integrated\nwith Bergeron are, on average, nearly seven times more resistant to attacks\ncompared to models without such support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research into AI alignment has grown considerably since the recent\nintroduction of increasingly capable Large Language Models (LLMs).\nUnfortunately, modern methods of alignment still fail to fully prevent harmful\nresponses when models are deliberately attacked. Such vulnerabilities can lead\nto LLMs being manipulated into generating hazardous content: from instructions\nfor creating dangerous materials to inciting violence or endorsing unethical\nbehaviors. To help mitigate this issue, we introduce Bergeron: a framework\ndesigned to improve the robustness of LLMs against attacks without any\nadditional parameter fine-tuning. Bergeron is organized into two tiers; with a\nsecondary LLM acting as a guardian to the primary LLM. This framework better\nsafeguards the primary model against incoming attacks while monitoring its\noutput for any harmful content. Empirical analysis reviews that by using\nBergeron to complement models with existing alignment training, we can\nsignificantly improve the robustness and safety of multiple, commonly used\ncommercial and open-source LLMs. Specifically, we found that models integrated\nwith Bergeron are, on average, nearly seven times more resistant to attacks\ncompared to models without such support."
                },
                "authors": [
                    {
                        "name": "Matthew Pisano"
                    },
                    {
                        "name": "Peter Ly"
                    },
                    {
                        "name": "Abraham Sanders"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Tomek Strzalkowski"
                    },
                    {
                        "name": "Mei Si"
                    }
                ],
                "author_detail": {
                    "name": "Mei Si"
                },
                "author": "Mei Si",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00029v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00029v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09600v1",
                "updated": "2024-08-18T21:45:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    45,
                    3,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T21:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    45,
                    3,
                    6,
                    231,
                    0
                ],
                "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning"
                },
                "summary": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Gautam Bhattacharya"
                    },
                    {
                        "name": "Pratik Joshi"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02885v3",
                "updated": "2024-08-18T21:23:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    23,
                    42,
                    6,
                    231,
                    0
                ],
                "published": "2024-07-03T07:59:52Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    7,
                    59,
                    52,
                    2,
                    185,
                    0
                ],
                "title": "CogErgLLM: Exploring Large Language Model Systems Design Perspective\n  Using Cognitive Ergonomics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogErgLLM: Exploring Large Language Model Systems Design Perspective\n  Using Cognitive Ergonomics"
                },
                "summary": "Integrating cognitive ergonomics with LLMs is essential for enhancing safety,\nreliability, and user satisfaction in human-AI interactions. Current LLM design\noften lacks this integration, leading to systems that may not fully align with\nhuman cognitive capabilities and limitations. Insufficient focus on\nincorporating cognitive science methods exacerbates biases in LLM outputs,\nwhile inconsistent application of user-centered design principles results in\nsub-optimal user experiences. To address these challenges, our position paper\nexplores the critical integration of cognitive ergonomics principles into LLM\ndesign, aiming to provide a comprehensive framework and practical guidelines\nfor ethical LLM development. Through our contributions, we seek to advance\nunderstanding and practice in integrating cognitive ergonomics into LLM\nsystems, fostering safer, more reliable, and ethically sound human-AI\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating cognitive ergonomics with LLMs is essential for enhancing safety,\nreliability, and user satisfaction in human-AI interactions. Current LLM design\noften lacks this integration, leading to systems that may not fully align with\nhuman cognitive capabilities and limitations. Insufficient focus on\nincorporating cognitive science methods exacerbates biases in LLM outputs,\nwhile inconsistent application of user-centered design principles results in\nsub-optimal user experiences. To address these challenges, our position paper\nexplores the critical integration of cognitive ergonomics principles into LLM\ndesign, aiming to provide a comprehensive framework and practical guidelines\nfor ethical LLM development. Through our contributions, we seek to advance\nunderstanding and practice in integrating cognitive ergonomics into LLM\nsystems, fostering safer, more reliable, and ethically sound human-AI\ninteractions."
                },
                "authors": [
                    {
                        "name": "Azmine Toushik Wasi"
                    }
                ],
                "author_detail": {
                    "name": "Azmine Toushik Wasi"
                },
                "author": "Azmine Toushik Wasi",
                "arxiv_comment": "8 Page, 3 Figures. Accepted to Large Language Models and Cognition @\n  ICML 2024 (https://llm-cognition.github.io/#:~:text=CogErgLLM); Read in\n  OpenReview: https://openreview.net/forum?id=63C9YSc77p",
                "arxiv_journal_ref": "ICML 2024 Workshop on LLMs and Cognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09594v1",
                "updated": "2024-08-18T20:59:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    59,
                    59,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T20:59:59Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    59,
                    59,
                    6,
                    231,
                    0
                ],
                "title": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models"
                },
                "summary": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels."
                },
                "authors": [
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Tim Merino"
                    },
                    {
                        "name": "Nidhushan Kanagaraja"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Zhan Zhuang"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09585v1",
                "updated": "2024-08-18T20:08:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    8,
                    42,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T20:08:42Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    8,
                    42,
                    6,
                    231,
                    0
                ],
                "title": "On the Necessity of World Knowledge for Mitigating Missing Labels in\n  Extreme Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Necessity of World Knowledge for Mitigating Missing Labels in\n  Extreme Classification"
                },
                "summary": "Extreme Classification (XC) aims to map a query to the most relevant\ndocuments from a very large document set. XC algorithms used in real-world\napplications learn this mapping from datasets curated from implicit feedback,\nsuch as user clicks. However, these datasets inevitably suffer from missing\nlabels. In this work, we observe that systematic missing labels lead to missing\nknowledge, which is critical for accurately modelling relevance between queries\nand documents. We formally show that this absence of knowledge cannot be\nrecovered using existing methods such as propensity weighting and data\nimputation strategies that solely rely on the training dataset. While LLMs\nprovide an attractive solution to augment the missing knowledge, leveraging\nthem in applications with low latency requirements and large document sets is\nchallenging. To incorporate missing knowledge at scale, we propose SKIM\n(Scalable Knowledge Infusion for Missing Labels), an algorithm that leverages a\ncombination of small LM and abundant unstructured meta-data to effectively\nmitigate the missing label problem. We show the efficacy of our method on\nlarge-scale public datasets through exhaustive unbiased evaluation ranging from\nhuman annotations to simulations inspired from industrial settings. SKIM\noutperforms existing methods on Recall@100 by more than 10 absolute points.\nAdditionally, SKIM scales to proprietary query-ad retrieval datasets containing\n10 million documents, outperforming contemporary methods by 12% in offline\nevaluation and increased ad click-yield by 1.23% in an online A/B test\nconducted on a popular search engine. We release our code, prompts, trained XC\nmodels and finetuned SLMs at: https://github.com/bicycleman15/skim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Classification (XC) aims to map a query to the most relevant\ndocuments from a very large document set. XC algorithms used in real-world\napplications learn this mapping from datasets curated from implicit feedback,\nsuch as user clicks. However, these datasets inevitably suffer from missing\nlabels. In this work, we observe that systematic missing labels lead to missing\nknowledge, which is critical for accurately modelling relevance between queries\nand documents. We formally show that this absence of knowledge cannot be\nrecovered using existing methods such as propensity weighting and data\nimputation strategies that solely rely on the training dataset. While LLMs\nprovide an attractive solution to augment the missing knowledge, leveraging\nthem in applications with low latency requirements and large document sets is\nchallenging. To incorporate missing knowledge at scale, we propose SKIM\n(Scalable Knowledge Infusion for Missing Labels), an algorithm that leverages a\ncombination of small LM and abundant unstructured meta-data to effectively\nmitigate the missing label problem. We show the efficacy of our method on\nlarge-scale public datasets through exhaustive unbiased evaluation ranging from\nhuman annotations to simulations inspired from industrial settings. SKIM\noutperforms existing methods on Recall@100 by more than 10 absolute points.\nAdditionally, SKIM scales to proprietary query-ad retrieval datasets containing\n10 million documents, outperforming contemporary methods by 12% in offline\nevaluation and increased ad click-yield by 1.23% in an online A/B test\nconducted on a popular search engine. We release our code, prompts, trained XC\nmodels and finetuned SLMs at: https://github.com/bicycleman15/skim"
                },
                "authors": [
                    {
                        "name": "Jatin Prakash"
                    },
                    {
                        "name": "Anirudh Buvanesh"
                    },
                    {
                        "name": "Bishal Santra"
                    },
                    {
                        "name": "Deepak Saini"
                    },
                    {
                        "name": "Sachin Yadav"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Yashoteja Prabhu"
                    },
                    {
                        "name": "Amit Sharma"
                    },
                    {
                        "name": "Manik Varma"
                    }
                ],
                "author_detail": {
                    "name": "Manik Varma"
                },
                "author": "Manik Varma",
                "arxiv_comment": "Preprint, 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11068v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11068v3",
                "updated": "2024-08-18T19:44:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    19,
                    44,
                    35,
                    6,
                    231,
                    0
                ],
                "published": "2024-07-12T14:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    14,
                    17,
                    26,
                    4,
                    194,
                    0
                ],
                "title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual\n  Understanding with ChildPlay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual\n  Understanding with ChildPlay"
                },
                "summary": "We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess\nbroader cognitive functions, particularly in non-linguistic domains. Our\napproach extends beyond standard linguistic benchmarks by incorporating games\nlike Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess\nstrategic thinking and decision-making. To evaluate the models' ability to\ngeneralize beyond their training data, we introduce two additional games. The\nfirst game, LEGO Connect Language (LCL), tests the models' capacity to\nunderstand spatial logic and follow assembly instructions. The second game, the\ngame of shapes, challenges the models to identify shapes represented by 1s\nwithin a matrix of zeros, further testing their spatial reasoning skills. This\n\"show, don't tell\" strategy uses games instead of simply querying the models.\nOur results show that despite their proficiency on standard benchmarks, GPT-3.5\nand GPT-4's abilities to play and reason about fully observable games without\npre-training is mediocre. Both models fail to anticipate losing moves in\nTic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly.\nWhile GPT-4 shows some success in the game of shapes, both models fail at the\nassembly tasks presented in the LCL game. These results suggest that while GPT\nmodels can emulate conversational proficiency and basic rule comprehension,\ntheir performance in strategic gameplay and spatial reasoning tasks is very\nlimited. Importantly, this reveals a blind spot in current LLM benchmarks that\nwe highlight with our gameplay benchmark suite ChildPlay\n(https://github.com/child-play-neurips/child-play). Our findings provide a\ncautionary tale about claims of emergent intelligence and reasoning\ncapabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess\nbroader cognitive functions, particularly in non-linguistic domains. Our\napproach extends beyond standard linguistic benchmarks by incorporating games\nlike Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess\nstrategic thinking and decision-making. To evaluate the models' ability to\ngeneralize beyond their training data, we introduce two additional games. The\nfirst game, LEGO Connect Language (LCL), tests the models' capacity to\nunderstand spatial logic and follow assembly instructions. The second game, the\ngame of shapes, challenges the models to identify shapes represented by 1s\nwithin a matrix of zeros, further testing their spatial reasoning skills. This\n\"show, don't tell\" strategy uses games instead of simply querying the models.\nOur results show that despite their proficiency on standard benchmarks, GPT-3.5\nand GPT-4's abilities to play and reason about fully observable games without\npre-training is mediocre. Both models fail to anticipate losing moves in\nTic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly.\nWhile GPT-4 shows some success in the game of shapes, both models fail at the\nassembly tasks presented in the LCL game. These results suggest that while GPT\nmodels can emulate conversational proficiency and basic rule comprehension,\ntheir performance in strategic gameplay and spatial reasoning tasks is very\nlimited. Importantly, this reveals a blind spot in current LLM benchmarks that\nwe highlight with our gameplay benchmark suite ChildPlay\n(https://github.com/child-play-neurips/child-play). Our findings provide a\ncautionary tale about claims of emergent intelligence and reasoning\ncapabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4."
                },
                "authors": [
                    {
                        "name": "Gonçalo Hora de Carvalho"
                    },
                    {
                        "name": "Oscar Knap"
                    },
                    {
                        "name": "Robert Pollice"
                    }
                ],
                "author_detail": {
                    "name": "Robert Pollice"
                },
                "author": "Robert Pollice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11068v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11068v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07702v2",
                "updated": "2024-08-18T19:06:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    19,
                    6,
                    4,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-14T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "title": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models"
                },
                "summary": "Schema linking is a crucial step in Text-to-SQL pipelines. Its goal is to\nretrieve the relevant tables and columns of a target database for a user's\nquery while disregarding irrelevant ones. However, imperfect schema linking can\noften exclude required columns needed for accurate query generation. In this\nwork, we revisit schema linking when using the latest generation of large\nlanguage models (LLMs). We find empirically that newer models are adept at\nutilizing relevant schema elements during generation even in the presence of\nlarge numbers of irrelevant ones. As such, our Text-to-SQL pipeline entirely\nforgoes schema linking in cases where the schema fits within the model's\ncontext window in order to minimize issues due to filtering required schema\nelements. Furthermore, instead of filtering contextual information, we\nhighlight techniques such as augmentation, selection, and correction, and adopt\nthem to improve the accuracy of our Text-to-SQL pipeline. Our approach ranks\nfirst on the BIRD benchmark achieving an accuracy of 71.83%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a crucial step in Text-to-SQL pipelines. Its goal is to\nretrieve the relevant tables and columns of a target database for a user's\nquery while disregarding irrelevant ones. However, imperfect schema linking can\noften exclude required columns needed for accurate query generation. In this\nwork, we revisit schema linking when using the latest generation of large\nlanguage models (LLMs). We find empirically that newer models are adept at\nutilizing relevant schema elements during generation even in the presence of\nlarge numbers of irrelevant ones. As such, our Text-to-SQL pipeline entirely\nforgoes schema linking in cases where the schema fits within the model's\ncontext window in order to minimize issues due to filtering required schema\nelements. Furthermore, instead of filtering contextual information, we\nhighlight techniques such as augmentation, selection, and correction, and adopt\nthem to improve the accuracy of our Text-to-SQL pipeline. Our approach ranks\nfirst on the BIRD benchmark achieving an accuracy of 71.83%."
                },
                "authors": [
                    {
                        "name": "Karime Maamari"
                    },
                    {
                        "name": "Fadhil Abubaker"
                    },
                    {
                        "name": "Daniel Jaroslawicz"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09568v1",
                "updated": "2024-08-18T18:45:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    45,
                    48,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T18:45:48Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    45,
                    48,
                    6,
                    231,
                    0
                ],
                "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in\n  Code LLMs for Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in\n  Code LLMs for Automated Program Repair"
                },
                "summary": "[Context] Large Language Models (LLMs) have shown good performance in several\nsoftware development-related tasks such as program repair, documentation, code\nrefactoring, debugging, and testing. Adapters are specialized, small modules\ndesigned for parameter efficient fine-tuning of LLMs for specific tasks,\ndomains, or applications without requiring extensive retraining of the entire\nmodel. These adapters offer a more efficient way to customize LLMs for\nparticular needs, leveraging the pre-existing capabilities of the large model.\nMerging LLMs and adapters has shown promising results for various natural\nlanguage domains and tasks, enabling the use of the learned models and adapters\nwithout additional training for a new task. [Objective] This research proposes\ncontinual merging and empirically studies the capabilities of merged adapters\nin Code LLMs, specially for the Automated Program Repair (APR) task. The goal\nis to gain insights into whether and how merging task-specific adapters can\naffect the performance of APR. [Method] In our framework, MergeRepair, we plan\nto merge multiple task-specific adapters using three different merging methods\nand evaluate the performance of the merged adapter for the APR task.\nParticularly, we will employ two main merging scenarios for all three\ntechniques, (i) merging using equal-weight averaging applied on parameters of\ndifferent adapters, where all adapters are of equal importance; and (ii) our\nproposed approach, continual merging, in which we sequentially merge the\ntask-specific adapters and the order and weight of merged adapters matter. By\nexploratory study of merging techniques, we will investigate the improvement\nand generalizability of merged adapters for APR. Through continual merging, we\nwill explore the capability of merged adapters and the effect of task order, as\nit occurs in real-world software projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context] Large Language Models (LLMs) have shown good performance in several\nsoftware development-related tasks such as program repair, documentation, code\nrefactoring, debugging, and testing. Adapters are specialized, small modules\ndesigned for parameter efficient fine-tuning of LLMs for specific tasks,\ndomains, or applications without requiring extensive retraining of the entire\nmodel. These adapters offer a more efficient way to customize LLMs for\nparticular needs, leveraging the pre-existing capabilities of the large model.\nMerging LLMs and adapters has shown promising results for various natural\nlanguage domains and tasks, enabling the use of the learned models and adapters\nwithout additional training for a new task. [Objective] This research proposes\ncontinual merging and empirically studies the capabilities of merged adapters\nin Code LLMs, specially for the Automated Program Repair (APR) task. The goal\nis to gain insights into whether and how merging task-specific adapters can\naffect the performance of APR. [Method] In our framework, MergeRepair, we plan\nto merge multiple task-specific adapters using three different merging methods\nand evaluate the performance of the merged adapter for the APR task.\nParticularly, we will employ two main merging scenarios for all three\ntechniques, (i) merging using equal-weight averaging applied on parameters of\ndifferent adapters, where all adapters are of equal importance; and (ii) our\nproposed approach, continual merging, in which we sequentially merge the\ntask-specific adapters and the order and weight of merged adapters matter. By\nexploratory study of merging techniques, we will investigate the improvement\nand generalizability of merged adapters for APR. Through continual merging, we\nwill explore the capability of merged adapters and the effect of task order, as\nit occurs in real-world software projects."
                },
                "authors": [
                    {
                        "name": "Meghdad Dehghan"
                    },
                    {
                        "name": "Jie JW Wu"
                    },
                    {
                        "name": "Fatemeh H. Fard"
                    },
                    {
                        "name": "Ali Ouni"
                    }
                ],
                "author_detail": {
                    "name": "Ali Ouni"
                },
                "author": "Ali Ouni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09565v1",
                "updated": "2024-08-18T18:31:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    31,
                    55,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T18:31:55Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    31,
                    55,
                    6,
                    231,
                    0
                ],
                "title": "Grammatical Error Feedback: An Implicit Evaluation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammatical Error Feedback: An Implicit Evaluation Approach"
                },
                "summary": "Grammatical feedback is crucial for consolidating second language (L2)\nlearning. Most research in computer-assisted language learning has focused on\nfeedback through grammatical error correction (GEC) systems, rather than\nexamining more holistic feedback that may be more useful for learners. This\nholistic feedback will be referred to as grammatical error feedback (GEF). In\nthis paper, we present a novel implicit evaluation approach to GEF that\neliminates the need for manual feedback annotations. Our method adopts a\ngrammatical lineup approach where the task is to pair feedback and essay\nrepresentations from a set of possible alternatives. This matching process can\nbe performed by appropriately prompting a large language model (LLM). An\nimportant aspect of this process, explored here, is the form of the lineup,\ni.e., the selection of foils. This paper exploits this framework to examine the\nquality and need for GEC to generate feedback, as well as the system used to\ngenerate feedback, using essays from the Cambridge Learner Corpus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammatical feedback is crucial for consolidating second language (L2)\nlearning. Most research in computer-assisted language learning has focused on\nfeedback through grammatical error correction (GEC) systems, rather than\nexamining more holistic feedback that may be more useful for learners. This\nholistic feedback will be referred to as grammatical error feedback (GEF). In\nthis paper, we present a novel implicit evaluation approach to GEF that\neliminates the need for manual feedback annotations. Our method adopts a\ngrammatical lineup approach where the task is to pair feedback and essay\nrepresentations from a set of possible alternatives. This matching process can\nbe performed by appropriately prompting a large language model (LLM). An\nimportant aspect of this process, explored here, is the form of the lineup,\ni.e., the selection of foils. This paper exploits this framework to examine the\nquality and need for GEC to generate feedback, as well as the system used to\ngenerate feedback, using essays from the Cambridge Learner Corpus."
                },
                "authors": [
                    {
                        "name": "Stefano Bannò"
                    },
                    {
                        "name": "Kate Knill"
                    },
                    {
                        "name": "Mark J. F. Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. F. Gales"
                },
                "author": "Mark J. F. Gales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09562v1",
                "updated": "2024-08-18T18:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    21,
                    24,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T18:21:24Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    21,
                    24,
                    6,
                    231,
                    0
                ],
                "title": "Security Concerns in Quantum Machine Learning as a Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Concerns in Quantum Machine Learning as a Service"
                },
                "summary": "Quantum machine learning (QML) is a category of algorithms that employ\nvariational quantum circuits (VQCs) to tackle machine learning tasks. Recent\ndiscoveries have shown that QML models can effectively generalize from limited\ntraining data samples. This capability has sparked increased interest in\ndeploying these models to address practical, real-world challenges, resulting\nin the emergence of Quantum Machine Learning as a Service (QMLaaS). QMLaaS\nrepresents a hybrid model that utilizes both classical and quantum computing\nresources. Classical computers play a crucial role in this setup, handling\ninitial pre-processing and subsequent post-processing of data to compensate for\nthe current limitations of quantum hardware. Since this is a new area, very\nlittle work exists to paint the whole picture of QMLaaS in the context of known\nsecurity threats in the domain of classical and quantum machine learning. This\nSoK paper is aimed to bridge this gap by outlining the complete QMLaaS\nworkflow, which encompasses both the training and inference phases and\nhighlighting significant security concerns involving untrusted classical or\nquantum providers. QML models contain several sensitive assets, such as the\nmodel architecture, training/testing data, encoding techniques, and trained\nparameters. Unauthorized access to these components could compromise the\nmodel's integrity and lead to intellectual property (IP) theft. We pinpoint the\ncritical security issues that must be considered to pave the way for a secure\nQMLaaS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum machine learning (QML) is a category of algorithms that employ\nvariational quantum circuits (VQCs) to tackle machine learning tasks. Recent\ndiscoveries have shown that QML models can effectively generalize from limited\ntraining data samples. This capability has sparked increased interest in\ndeploying these models to address practical, real-world challenges, resulting\nin the emergence of Quantum Machine Learning as a Service (QMLaaS). QMLaaS\nrepresents a hybrid model that utilizes both classical and quantum computing\nresources. Classical computers play a crucial role in this setup, handling\ninitial pre-processing and subsequent post-processing of data to compensate for\nthe current limitations of quantum hardware. Since this is a new area, very\nlittle work exists to paint the whole picture of QMLaaS in the context of known\nsecurity threats in the domain of classical and quantum machine learning. This\nSoK paper is aimed to bridge this gap by outlining the complete QMLaaS\nworkflow, which encompasses both the training and inference phases and\nhighlighting significant security concerns involving untrusted classical or\nquantum providers. QML models contain several sensitive assets, such as the\nmodel architecture, training/testing data, encoding techniques, and trained\nparameters. Unauthorized access to these components could compromise the\nmodel's integrity and lead to intellectual property (IP) theft. We pinpoint the\ncritical security issues that must be considered to pave the way for a secure\nQMLaaS deployment."
                },
                "authors": [
                    {
                        "name": "Satwik Kundu"
                    },
                    {
                        "name": "Swaroop Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Ghosh"
                },
                "author": "Swaroop Ghosh",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]