[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v1",
                "updated": "2024-11-05T15:22:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v2",
                "updated": "2024-11-05T04:14:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    4,
                    14,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie She"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao She"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v1",
                "updated": "2024-11-04T18:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v1",
                "updated": "2024-11-04T04:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v1",
                "updated": "2024-11-03T04:25:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v1",
                "updated": "2024-11-02T15:45:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "$\\texttt{HEXA-MoE}$: Efficient and Heterogeneous-aware MoE Acceleration\n  with ZERO Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{HEXA-MoE}$: Efficient and Heterogeneous-aware MoE Acceleration\n  with ZERO Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, \\textit{i.e.}, reducing\n$10\\%\\sim48\\%$ memory consumption and achieving $0.5\\sim4.3\\times$ speed up\ncompared to current state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at\n\\href{https://github.com/UNITES-Lab/HEXA-MoE}{\\underline{here}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, \\textit{i.e.}, reducing\n$10\\%\\sim48\\%$ memory consumption and achieving $0.5\\sim4.3\\times$ speed up\ncompared to current state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at\n\\href{https://github.com/UNITES-Lab/HEXA-MoE}{\\underline{here}}."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03312v1",
                "updated": "2024-11-05T18:54:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    54,
                    21,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T18:54:21Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    54,
                    21,
                    1,
                    310,
                    0
                ],
                "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Optimal VLMs Need Only One Visual Token but Larger Models"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression."
                },
                "authors": [
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Joao D. Semedo"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03307v1",
                "updated": "2024-11-05T18:01:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    1,
                    12,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T18:01:12Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    1,
                    12,
                    1,
                    310,
                    0
                ],
                "title": "LLMs for Domain Generation Algorithm Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Domain Generation Algorithm Detection"
                },
                "summary": "This work analyzes the use of large language models (LLMs) for detecting\ndomain generation algorithms (DGAs). We perform a detailed evaluation of two\nimportant techniques: In-Context Learning (ICL) and Supervised Fine-Tuning\n(SFT), showing how they can improve detection. SFT increases performance by\nusing domain-specific data, whereas ICL helps the detection model to quickly\nadapt to new threats without requiring much retraining. We use Meta's Llama3 8B\nmodel, on a custom dataset with 68 malware families and normal domains,\ncovering several hard-to-detect schemes, including recent word-based DGAs.\nResults proved that LLM-based methods can achieve competitive results in DGA\ndetection. In particular, the SFT-based LLM DGA detector outperforms\nstate-of-the-art models using attention layers, achieving 94% accuracy with a\n4% false positive rate (FPR) and excelling at detecting word-based DGA domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work analyzes the use of large language models (LLMs) for detecting\ndomain generation algorithms (DGAs). We perform a detailed evaluation of two\nimportant techniques: In-Context Learning (ICL) and Supervised Fine-Tuning\n(SFT), showing how they can improve detection. SFT increases performance by\nusing domain-specific data, whereas ICL helps the detection model to quickly\nadapt to new threats without requiring much retraining. We use Meta's Llama3 8B\nmodel, on a custom dataset with 68 malware families and normal domains,\ncovering several hard-to-detect schemes, including recent word-based DGAs.\nResults proved that LLM-based methods can achieve competitive results in DGA\ndetection. In particular, the SFT-based LLM DGA detector outperforms\nstate-of-the-art models using attention layers, achieving 94% accuracy with a\n4% false positive rate (FPR) and excelling at detecting word-based DGA domains."
                },
                "authors": [
                    {
                        "name": "Reynier Leyva La O"
                    },
                    {
                        "name": "Carlos A. Catania"
                    },
                    {
                        "name": "Tatiana Parlanti"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Parlanti"
                },
                "author": "Tatiana Parlanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03300v1",
                "updated": "2024-11-05T17:53:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    53,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:53:25Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    53,
                    25,
                    1,
                    310,
                    0
                ],
                "title": "VERITAS: A Unified Approach to Reliability Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERITAS: A Unified Approach to Reliability Evaluation"
                },
                "summary": "Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting."
                },
                "authors": [
                    {
                        "name": "Rajkumar Ramamurthy"
                    },
                    {
                        "name": "Meghana Arakkal Rajeev"
                    },
                    {
                        "name": "Oliver Molenschot"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Nazneen Rajani"
                    }
                ],
                "author_detail": {
                    "name": "Nazneen Rajani"
                },
                "author": "Nazneen Rajani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00318v2",
                "updated": "2024-11-05T17:51:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    51,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-03-30T10:54:59Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    10,
                    54,
                    59,
                    5,
                    90,
                    0
                ],
                "title": "Cognitive Planning for Object Goal Navigation using Generative AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Planning for Object Goal Navigation using Generative AI Models"
                },
                "summary": "Recent advancements in Generative AI, particularly in Large Language Models\n(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for\nintegrating cognitive planning into robotic systems. In this work, we present a\nnovel framework for solving the object goal navigation problem that generates\nefficient exploration strategies. Our approach enables a robot to navigate\nunfamiliar environments by leveraging LLMs and LVLMs to understand the semantic\nstructure of the scene. To address the challenge of representing complex\nenvironments without overwhelming the system, we propose a 3D modular scene\nrepresentation, enriched with semantic descriptions. This representation is\ndynamically pruned using an LLM-based mechanism, which filters irrelevant\ninformation and focuses on task-specific data. By combining these elements, our\nsystem generates high-level sub-goals that guide the exploration of the robot\ntoward the target object. We validate our approach in simulated environments,\ndemonstrating its ability to enhance object search efficiency while maintaining\nscalability in complex settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI, particularly in Large Language Models\n(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for\nintegrating cognitive planning into robotic systems. In this work, we present a\nnovel framework for solving the object goal navigation problem that generates\nefficient exploration strategies. Our approach enables a robot to navigate\nunfamiliar environments by leveraging LLMs and LVLMs to understand the semantic\nstructure of the scene. To address the challenge of representing complex\nenvironments without overwhelming the system, we propose a 3D modular scene\nrepresentation, enriched with semantic descriptions. This representation is\ndynamically pruned using an LLM-based mechanism, which filters irrelevant\ninformation and focuses on task-specific data. By combining these elements, our\nsystem generates high-level sub-goals that guide the exploration of the robot\ntoward the target object. We validate our approach in simulated environments,\ndemonstrating its ability to enhance object search efficiency while maintaining\nscalability in complex settings."
                },
                "authors": [
                    {
                        "name": "Arjun P S"
                    },
                    {
                        "name": "Andrew Melnik"
                    },
                    {
                        "name": "Gora Chand Nandi"
                    }
                ],
                "author_detail": {
                    "name": "Gora Chand Nandi"
                },
                "author": "Gora Chand Nandi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03295v1",
                "updated": "2024-11-05T17:42:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    42,
                    43,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:42:43Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    42,
                    43,
                    1,
                    310,
                    0
                ],
                "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online"
                },
                "summary": "This paper examines how large language models (LLMs) can help people write\nconstructive comments in online debates on divisive social issues and whether\nthe notions of constructiveness vary across cultures. Through controlled\nexperiments with 600 participants from India and the US, who reviewed and wrote\nconstructive comments on online threads on Islamophobia and homophobia, we\nfound potential misalignment in how LLMs and humans perceive constructiveness\nin online comments. While the LLM was more likely to view dialectical comments\nas more constructive, participants favored comments that emphasized logic and\nfacts more than the LLM did. Despite these differences, participants rated\nLLM-generated and human-AI co-written comments as significantly more\nconstructive than those written independently by humans. Our analysis also\nrevealed that LLM-generated and human-AI co-written comments exhibited more\nlinguistic features associated with constructiveness compared to human-written\ncomments on divisive topics. When participants used LLMs to refine their\ncomments, the resulting comments were longer, more polite, positive, less\ntoxic, and more readable, with added argumentative features that retained the\noriginal intent but occasionally lost nuances. Based on these findings, we\ndiscuss ethical and design considerations in using LLMs to facilitate\nconstructive discourse online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines how large language models (LLMs) can help people write\nconstructive comments in online debates on divisive social issues and whether\nthe notions of constructiveness vary across cultures. Through controlled\nexperiments with 600 participants from India and the US, who reviewed and wrote\nconstructive comments on online threads on Islamophobia and homophobia, we\nfound potential misalignment in how LLMs and humans perceive constructiveness\nin online comments. While the LLM was more likely to view dialectical comments\nas more constructive, participants favored comments that emphasized logic and\nfacts more than the LLM did. Despite these differences, participants rated\nLLM-generated and human-AI co-written comments as significantly more\nconstructive than those written independently by humans. Our analysis also\nrevealed that LLM-generated and human-AI co-written comments exhibited more\nlinguistic features associated with constructiveness compared to human-written\ncomments on divisive topics. When participants used LLMs to refine their\ncomments, the resulting comments were longer, more polite, positive, less\ntoxic, and more readable, with added argumentative features that retained the\noriginal intent but occasionally lost nuances. Based on these findings, we\ndiscuss ethical and design considerations in using LLMs to facilitate\nconstructive discourse online."
                },
                "authors": [
                    {
                        "name": "Farhana Shahid"
                    },
                    {
                        "name": "Maximilian Dittgen"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03294v1",
                "updated": "2024-11-05T17:41:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    41,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:41:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    41,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy\n  For Visuomotor Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy\n  For Visuomotor Imitation Learning"
                },
                "summary": "We propose an object-centric recovery policy framework to address the\nchallenges of out-of-distribution (OOD) scenarios in visuomotor policy\nlearning. Previous behavior cloning (BC) methods rely heavily on a large amount\nof labeled data coverage, failing in unfamiliar spatial states. Without relying\non extra data collection, our approach learns a recovery policy constructed by\nan inverse policy inferred from object keypoint manifold gradient in the\noriginal training data. The recovery policy serves as a simple add-on to any\nbase visuomotor BC policy, agnostic to a specific method, guiding the system\nback towards the training distribution to ensure task success even in OOD\nsituations. We demonstrate the effectiveness of our object-centric framework in\nboth simulation and real robot experiments, achieving an improvement of\n$\\textbf{77.7\\%}$ over the base policy in OOD. Project Website:\nhttps://sites.google.com/view/ocr-penn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an object-centric recovery policy framework to address the\nchallenges of out-of-distribution (OOD) scenarios in visuomotor policy\nlearning. Previous behavior cloning (BC) methods rely heavily on a large amount\nof labeled data coverage, failing in unfamiliar spatial states. Without relying\non extra data collection, our approach learns a recovery policy constructed by\nan inverse policy inferred from object keypoint manifold gradient in the\noriginal training data. The recovery policy serves as a simple add-on to any\nbase visuomotor BC policy, agnostic to a specific method, guiding the system\nback towards the training distribution to ensure task success even in OOD\nsituations. We demonstrate the effectiveness of our object-centric framework in\nboth simulation and real robot experiments, achieving an improvement of\n$\\textbf{77.7\\%}$ over the base policy in OOD. Project Website:\nhttps://sites.google.com/view/ocr-penn"
                },
                "authors": [
                    {
                        "name": "George Jiayuan Gao"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Nadia Figueroa"
                    }
                ],
                "author_detail": {
                    "name": "Nadia Figueroa"
                },
                "author": "Nadia Figueroa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03287v1",
                "updated": "2024-11-05T17:36:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    36,
                    32,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:36:32Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    36,
                    32,
                    1,
                    310,
                    0
                ],
                "title": "The Future of Intelligent Healthcare: A Systematic Analysis and\n  Discussion on the Integration and Impact of Robots Using Large Language\n  Models for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of Intelligent Healthcare: A Systematic Analysis and\n  Discussion on the Integration and Impact of Robots Using Large Language\n  Models for Healthcare"
                },
                "summary": "The potential use of large language models (LLMs) in healthcare robotics can\nhelp address the significant demand put on healthcare systems around the world\nwith respect to an aging demographic and a shortage of healthcare\nprofessionals. Even though LLMs have already been integrated into medicine to\nassist both clinicians and patients, the integration of LLMs within healthcare\nrobots has not yet been explored for clinical settings. In this perspective\npaper, we investigate the groundbreaking developments in robotics and LLMs to\nuniquely identify the needed system requirements for designing health specific\nLLM based robots in terms of multi modal communication through human robot\ninteractions (HRIs), semantic reasoning, and task planning. Furthermore, we\ndiscuss the ethical issues, open challenges, and potential future research\ndirections for this emerging innovative field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential use of large language models (LLMs) in healthcare robotics can\nhelp address the significant demand put on healthcare systems around the world\nwith respect to an aging demographic and a shortage of healthcare\nprofessionals. Even though LLMs have already been integrated into medicine to\nassist both clinicians and patients, the integration of LLMs within healthcare\nrobots has not yet been explored for clinical settings. In this perspective\npaper, we investigate the groundbreaking developments in robotics and LLMs to\nuniquely identify the needed system requirements for designing health specific\nLLM based robots in terms of multi modal communication through human robot\ninteractions (HRIs), semantic reasoning, and task planning. Furthermore, we\ndiscuss the ethical issues, open challenges, and potential future research\ndirections for this emerging innovative field."
                },
                "authors": [
                    {
                        "name": "Souren Pashangpour"
                    },
                    {
                        "name": "Goldie Nejat"
                    }
                ],
                "author_detail": {
                    "name": "Goldie Nejat"
                },
                "author": "Goldie Nejat",
                "arxiv_doi": "10.3390/robotics13080112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/robotics13080112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "MDPI Robotics 2024, 13(8)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03284v1",
                "updated": "2024-11-05T17:33:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    33,
                    39,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:33:39Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    33,
                    39,
                    1,
                    310,
                    0
                ],
                "title": "SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents"
                },
                "summary": "While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Peijia Qian"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Kumar Satvik Chaudhary"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Jiayi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Shen"
                },
                "author": "Jiayi Shen",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16299v2",
                "updated": "2024-11-05T17:22:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    22,
                    10,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-09T19:35:34Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    19,
                    35,
                    34,
                    0,
                    253,
                    0
                ],
                "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale"
                },
                "summary": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Huy Nhat Phan"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03276v1",
                "updated": "2024-11-05T17:17:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    17,
                    47,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:17:47Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    17,
                    47,
                    1,
                    310,
                    0
                ],
                "title": "Line of Sight Bias in Dark Matter Inferences from Galaxy Cluster Mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Line of Sight Bias in Dark Matter Inferences from Galaxy Cluster Mergers"
                },
                "summary": "In collisions of galaxy clusters, the lack of displacement between dark\nmatter and galaxies suggests that the dark matter scattering depth is small.\nThis yields an upper limit on the dark matter cross section if the dark matter\ncolumn density is known. We investigate a bias in such constraints: the\nmeasured column density (along the line of sight, using gravitational lensing)\nis lower than that experienced by a dark matter particle, as follows. Dark\nmatter halos are triaxial and generally collide along their major axes,\nyielding a high scattering column density -- but the merger is obvious only to\nobservers whose line of sight is nearly perpendicular to that axis, yielding a\nlow observed column density. We trace lines of sight through merging halos from\nthe BigMDPL n-body simulation, both with and without mock observational\neffects. We find that a hypothetical skewer through the halo along the merger\naxis (more precisely, along the current separation vector of the two halos) has\ntwice the column density of a typical line of sight. With weak lensing\nmeasurements, which involve some spatial averaging, this ratio is reduced to\n1.25, suggesting that existing constraints on the scattering cross section are\nbiased high by about 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In collisions of galaxy clusters, the lack of displacement between dark\nmatter and galaxies suggests that the dark matter scattering depth is small.\nThis yields an upper limit on the dark matter cross section if the dark matter\ncolumn density is known. We investigate a bias in such constraints: the\nmeasured column density (along the line of sight, using gravitational lensing)\nis lower than that experienced by a dark matter particle, as follows. Dark\nmatter halos are triaxial and generally collide along their major axes,\nyielding a high scattering column density -- but the merger is obvious only to\nobservers whose line of sight is nearly perpendicular to that axis, yielding a\nlow observed column density. We trace lines of sight through merging halos from\nthe BigMDPL n-body simulation, both with and without mock observational\neffects. We find that a hypothetical skewer through the halo along the merger\naxis (more precisely, along the current separation vector of the two halos) has\ntwice the column density of a typical line of sight. With weak lensing\nmeasurements, which involve some spatial averaging, this ratio is reduced to\n1.25, suggesting that existing constraints on the scattering cross section are\nbiased high by about 25%."
                },
                "authors": [
                    {
                        "name": "David Wittman"
                    },
                    {
                        "name": "Scott Adler"
                    }
                ],
                "author_detail": {
                    "name": "Scott Adler"
                },
                "arxiv_affiliation": "UC Davis",
                "author": "Scott Adler",
                "arxiv_comment": "submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03262v1",
                "updated": "2024-11-05T17:01:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    1,
                    16,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:01:16Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    1,
                    16,
                    1,
                    310,
                    0
                ],
                "title": "Spatio-temporal topology of plasmonic spin meron pairs revealed by\n  polarimetric photo-emission microscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal topology of plasmonic spin meron pairs revealed by\n  polarimetric photo-emission microscopy"
                },
                "summary": "Topology is the study of geometrical properties and spatial relations\nunaffected by continuous changes, and has become an important tool for\nunderstanding complex physical systems. Although recent optical experiments\nhave inferred the existence of vector fields with the topologies of merons, the\ninability to extract the full three dimensional vectors misses a richer set of\ntopologies that have not yet been fully explored. In our work, we extend the\nstudy of the topology of electromagnetic fields on surfaces to a spin\nquasi-particle with the topology of a meron pair, formed by interfering surface\nplasmon polaritons, and show that the in-plane vectors are constrained by the\nembedding topology of the space as dictated by the Poincare-Hopf theorem. In\naddition we explore the time evolution of the three dimensional topology of the\nspin field formed by femtosecond laser pulses. These experiments are possible\nusing our here developed method called polarimetric photoemission electron\nmicroscopy (polarimetric PEEM) that combines an optical pump-probe technique\nand polarimetry with photo-emission electron microscopy. This method allows for\nthe accurate generation of surface plasmon polariton fields and their\nsubsequent measurement, revealing both the spatial distribution of the full\nthree-dimensional electromagnetic fields at deep sub-wavelength resolution and\ntheir time evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology is the study of geometrical properties and spatial relations\nunaffected by continuous changes, and has become an important tool for\nunderstanding complex physical systems. Although recent optical experiments\nhave inferred the existence of vector fields with the topologies of merons, the\ninability to extract the full three dimensional vectors misses a richer set of\ntopologies that have not yet been fully explored. In our work, we extend the\nstudy of the topology of electromagnetic fields on surfaces to a spin\nquasi-particle with the topology of a meron pair, formed by interfering surface\nplasmon polaritons, and show that the in-plane vectors are constrained by the\nembedding topology of the space as dictated by the Poincare-Hopf theorem. In\naddition we explore the time evolution of the three dimensional topology of the\nspin field formed by femtosecond laser pulses. These experiments are possible\nusing our here developed method called polarimetric photoemission electron\nmicroscopy (polarimetric PEEM) that combines an optical pump-probe technique\nand polarimetry with photo-emission electron microscopy. This method allows for\nthe accurate generation of surface plasmon polariton fields and their\nsubsequent measurement, revealing both the spatial distribution of the full\nthree-dimensional electromagnetic fields at deep sub-wavelength resolution and\ntheir time evolution."
                },
                "authors": [
                    {
                        "name": "Pascal Dreher"
                    },
                    {
                        "name": "Alexander Neuhaus"
                    },
                    {
                        "name": "David Janoschka"
                    },
                    {
                        "name": "Alexandra Roedl"
                    },
                    {
                        "name": "Tim Meiler"
                    },
                    {
                        "name": "Bettina Frank"
                    },
                    {
                        "name": "Timothy J. Davis"
                    },
                    {
                        "name": "Harald Giessen"
                    },
                    {
                        "name": "Frank Meyer zu Heringdorf"
                    }
                ],
                "author_detail": {
                    "name": "Frank Meyer zu Heringdorf"
                },
                "arxiv_affiliation": "University of Stuttgart",
                "author": "Frank Meyer zu Heringdorf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03992v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03992v4",
                "updated": "2024-11-05T16:57:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    57,
                    26,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-06T02:44:27Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "title": "Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark\n  Study"
                },
                "summary": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA Hopper GPUs for large language model (LLM)\ninference tasks. We benchmark the overhead introduced by TEE mode across\nvarious LLMs and token lengths, with a particular focus on the bottleneck\ncaused by CPU-GPU data transfers via PCIe. Our results indicate that while\nthere is minimal computational overhead within the GPU, the overall performance\npenalty is primarily attributable to data transfer. For the majority of typical\nLLM queries, the overhead remains below 7%, with larger models and longer\nsequences experiencing nearly zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA Hopper GPUs for large language model (LLM)\ninference tasks. We benchmark the overhead introduced by TEE mode across\nvarious LLMs and token lengths, with a particular focus on the bottleneck\ncaused by CPU-GPU data transfers via PCIe. Our results indicate that while\nthere is minimal computational overhead within the GPU, the overall performance\npenalty is primarily attributable to data transfer. For the majority of typical\nLLM queries, the overhead remains below 7%, with larger models and longer\nsequences experiencing nearly zero overhead."
                },
                "authors": [
                    {
                        "name": "Jianwei Zhu"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Peng Deng"
                    },
                    {
                        "name": "Aline Almeida"
                    },
                    {
                        "name": "Shunfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shunfan Zhou"
                },
                "author": "Shunfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03992v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03992v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03256v1",
                "updated": "2024-11-05T16:54:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    54,
                    47,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:54:47Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    54,
                    47,
                    1,
                    310,
                    0
                ],
                "title": "How the StarDICE photometric calibration of standard stars can improve\n  cosmological constraints?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How the StarDICE photometric calibration of standard stars can improve\n  cosmological constraints?"
                },
                "summary": "The number of type Ia supernova (SNe Ia) observations will grow significantly\nwithin the next decade, mainly thanks to the Legacy Survey of Space and Time\n(LSST) undertaken by the Vera Rubin Observatory in Chile. With this\nimprovement, statistical uncertainties will decrease, and flux calibration will\nbecome the main uncertainty for the characterization of dark energy. Currently,\nthe astronomical flux scale is anchored on the numerical models of white dwarf\natmospheres from the CALSPEC catalog, and every error on the model can induce a\nbias over cosmological parameters inference. The StarDICE experiment proposes a\nnew calibration reference that only relies on observations from the optical\nwatt defined by the NIST towards the magnitude of standard stars. It is\ncurrently operating at l'Observatoire de Haute-Provence and has been collecting\ndata since the beginning of 2023. To overcome the photometric calibration\nuncertainty and reach a sub-percent precision, the instrument throughput has\nbeen calibrated with a Collimated Beam Projector. It will be monitored on-site\nwith a LED-based artificial star source calibrated with NIST photodiodes. In\nthis proceeding, we will first illustrate how an error in the photometric\ncalibration can impact the SNe Ia distance moduli and thus bias the measurement\nof cosmological parameters. Then we will present the StarDICE experiment and\nhow we can recalibrate the CALSPEC catalog at the millimagnitude level on the\nNIST scale with photometric analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of type Ia supernova (SNe Ia) observations will grow significantly\nwithin the next decade, mainly thanks to the Legacy Survey of Space and Time\n(LSST) undertaken by the Vera Rubin Observatory in Chile. With this\nimprovement, statistical uncertainties will decrease, and flux calibration will\nbecome the main uncertainty for the characterization of dark energy. Currently,\nthe astronomical flux scale is anchored on the numerical models of white dwarf\natmospheres from the CALSPEC catalog, and every error on the model can induce a\nbias over cosmological parameters inference. The StarDICE experiment proposes a\nnew calibration reference that only relies on observations from the optical\nwatt defined by the NIST towards the magnitude of standard stars. It is\ncurrently operating at l'Observatoire de Haute-Provence and has been collecting\ndata since the beginning of 2023. To overcome the photometric calibration\nuncertainty and reach a sub-percent precision, the instrument throughput has\nbeen calibrated with a Collimated Beam Projector. It will be monitored on-site\nwith a LED-based artificial star source calibrated with NIST photodiodes. In\nthis proceeding, we will first illustrate how an error in the photometric\ncalibration can impact the SNe Ia distance moduli and thus bias the measurement\nof cosmological parameters. Then we will present the StarDICE experiment and\nhow we can recalibrate the CALSPEC catalog at the millimagnitude level on the\nNIST scale with photometric analysis."
                },
                "authors": [
                    {
                        "name": "T. Souverin"
                    },
                    {
                        "name": "J. Neveu"
                    },
                    {
                        "name": "M. Betoule"
                    },
                    {
                        "name": "S. Bongard"
                    },
                    {
                        "name": "P. E. Blanc"
                    },
                    {
                        "name": "J. Cohen Tanugi"
                    },
                    {
                        "name": "S. Dagoret-Campagne"
                    },
                    {
                        "name": "F. Feinstein"
                    },
                    {
                        "name": "M. Ferrari"
                    },
                    {
                        "name": "F. Hazenberg"
                    },
                    {
                        "name": "C. Juramy"
                    },
                    {
                        "name": "L. Le Guillou"
                    },
                    {
                        "name": "A. Le Van Suu"
                    },
                    {
                        "name": "M. Moniez"
                    },
                    {
                        "name": "E. Nuss"
                    },
                    {
                        "name": "B. Plez"
                    },
                    {
                        "name": "N. Regnault"
                    },
                    {
                        "name": "E. Sepulveda"
                    },
                    {
                        "name": "K. Sommer"
                    }
                ],
                "author_detail": {
                    "name": "K. Sommer"
                },
                "arxiv_affiliation": "LUPM, Universit Montpellier, CNRS",
                "author": "K. Sommer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14550v2",
                "updated": "2024-11-05T16:51:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    51,
                    40,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-20T17:57:51Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    57,
                    51,
                    3,
                    172,
                    0
                ],
                "title": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models"
                },
                "summary": "Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks."
                },
                "authors": [
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Ge Bai"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "[EMNLP 2024] The first four authors contributed equally, 29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03252v1",
                "updated": "2024-11-05T16:49:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    49,
                    33,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    49,
                    33,
                    1,
                    310,
                    0
                ],
                "title": "Spontaneous Emergence of Agent Individuality through Social Interactions\n  in LLM-Based Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous Emergence of Agent Individuality through Social Interactions\n  in LLM-Based Communities"
                },
                "summary": "We study the emergence of agency from scratch by using Large Language Model\n(LLM)-based agents. In previous studies of LLM-based agents, each agent's\ncharacteristics, including personality and memory, have traditionally been\npredefined. We focused on how individuality, such as behavior, personality, and\nmemory, can be differentiated from an undifferentiated state. The present LLM\nagents engage in cooperative communication within a group simulation,\nexchanging context-based messages in natural language. By analyzing this\nmulti-agent simulation, we report valuable new insights into how social norms,\ncooperation, and personality traits can emerge spontaneously. This paper\ndemonstrates that autonomously interacting LLM-powered agents generate\nhallucinations and hashtags to sustain communication, which, in turn, increases\nthe diversity of words within their interactions. Each agent's emotions shift\nthrough communication, and as they form communities, the personalities of the\nagents emerge and evolve accordingly. This computational modeling approach and\nits findings will provide a new method for analyzing collective artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the emergence of agency from scratch by using Large Language Model\n(LLM)-based agents. In previous studies of LLM-based agents, each agent's\ncharacteristics, including personality and memory, have traditionally been\npredefined. We focused on how individuality, such as behavior, personality, and\nmemory, can be differentiated from an undifferentiated state. The present LLM\nagents engage in cooperative communication within a group simulation,\nexchanging context-based messages in natural language. By analyzing this\nmulti-agent simulation, we report valuable new insights into how social norms,\ncooperation, and personality traits can emerge spontaneously. This paper\ndemonstrates that autonomously interacting LLM-powered agents generate\nhallucinations and hashtags to sustain communication, which, in turn, increases\nthe diversity of words within their interactions. Each agent's emotions shift\nthrough communication, and as they form communities, the personalities of the\nagents emerge and evolve accordingly. This computational modeling approach and\nits findings will provide a new method for analyzing collective artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Ryosuke Takata"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03250v1",
                "updated": "2024-11-05T16:47:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    47,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:47:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    47,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language\n  Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2-7 percent in certain cases. The data and code\nwill be publicly available upon completion of internal review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2-7 percent in certain cases. The data and code\nwill be publicly available upon completion of internal review."
                },
                "authors": [
                    {
                        "name": "Ying Zhou"
                    },
                    {
                        "name": "Xinyao Wang"
                    },
                    {
                        "name": "Yulei Niu"
                    },
                    {
                        "name": "Yaojie Shen"
                    },
                    {
                        "name": "Lexin Tang"
                    },
                    {
                        "name": "Fan Chen"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Longyin Wen"
                    }
                ],
                "author_detail": {
                    "name": "Longyin Wen"
                },
                "author": "Longyin Wen",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11811v2",
                "updated": "2024-11-05T16:47:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    47,
                    43,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-17T17:52:54Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    52,
                    54,
                    0,
                    169,
                    0
                ],
                "title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content"
                },
                "summary": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa."
                },
                "authors": [
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Pierre-Andre Noel"
                    },
                    {
                        "name": "Etienne Marcotte"
                    },
                    {
                        "name": "Sai Rajeswar"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14762v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14762v3",
                "updated": "2024-11-05T16:40:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    40,
                    21,
                    1,
                    310,
                    0
                ],
                "published": "2024-02-22T18:21:59Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    21,
                    59,
                    3,
                    53,
                    0
                ],
                "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues"
                },
                "summary": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}."
                },
                "authors": [
                    {
                        "name": "Ge Bai"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhanhui Zhou"
                    },
                    {
                        "name": "Zhuoran Lin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "arxiv_doi": "10.18653/v1/2024.acl-long.401",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.401",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.14762v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14762v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "[ACL 2024] The first three authors contribute equally, 34 pages, repo\n  at https://github.com/mtbench101/mt-bench-101",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03236v1",
                "updated": "2024-11-05T16:36:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    36,
                    7,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:36:07Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    36,
                    7,
                    1,
                    310,
                    0
                ],
                "title": "Enhancing Transformer Training Efficiency with Dynamic Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Transformer Training Efficiency with Dynamic Dropout"
                },
                "summary": "We introduce Dynamic Dropout, a novel regularization technique designed to\nenhance the training efficiency of Transformer models by dynamically adjusting\nthe dropout rate based on training epochs or validation loss improvements. This\napproach addresses the challenge of balancing regularization and model\ncapacity, which is crucial for achieving fast convergence and high performance.\nOur method involves modifying the GPT model to accept a variable dropout rate\nand updating dropout layers during training using schedules such as linear\ndecay, exponential decay, and validation loss-based adjustments. Extensive\nexperiments on the Shakespeare\\_char dataset demonstrate that Dynamic Dropout\nsignificantly accelerates training and improves inference efficiency compared\nto a baseline model with a fixed dropout rate. The validation loss-based\nadjustment schedule provided the best overall performance, highlighting the\npotential of Dynamic Dropout as a valuable technique for training large-scale\nTransformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Dynamic Dropout, a novel regularization technique designed to\nenhance the training efficiency of Transformer models by dynamically adjusting\nthe dropout rate based on training epochs or validation loss improvements. This\napproach addresses the challenge of balancing regularization and model\ncapacity, which is crucial for achieving fast convergence and high performance.\nOur method involves modifying the GPT model to accept a variable dropout rate\nand updating dropout layers during training using schedules such as linear\ndecay, exponential decay, and validation loss-based adjustments. Extensive\nexperiments on the Shakespeare\\_char dataset demonstrate that Dynamic Dropout\nsignificantly accelerates training and improves inference efficiency compared\nto a baseline model with a fixed dropout rate. The validation loss-based\nadjustment schedule provided the best overall performance, highlighting the\npotential of Dynamic Dropout as a valuable technique for training large-scale\nTransformer models."
                },
                "authors": [
                    {
                        "name": "Hanrui Yan"
                    },
                    {
                        "name": "Dan Shao"
                    }
                ],
                "author_detail": {
                    "name": "Dan Shao"
                },
                "author": "Dan Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17898v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17898v3",
                "updated": "2024-11-05T16:31:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    31,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2023-11-29T18:51:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    18,
                    51,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "Contextual Knowledge Pursuit for Faithful Visual Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Knowledge Pursuit for Faithful Visual Synthesis"
                },
                "summary": "Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Jinqi Luo"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Dimitris Dimos"
                    },
                    {
                        "name": "Ren Vidal"
                    }
                ],
                "author_detail": {
                    "name": "Ren Vidal"
                },
                "author": "Ren Vidal",
                "arxiv_comment": "Accepted in ECCV 2024 SDCV Workshop. GitHub repository at\n  https://github.com/peterljq/Contextual-Knowledge-Pursuit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17898v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17898v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18336v2",
                "updated": "2024-11-05T16:30:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    30,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-26T23:18:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    18,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "DeBaRA: Denoising-Based 3D Room Arrangement Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeBaRA: Denoising-Based 3D Room Arrangement Generation"
                },
                "summary": "Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios."
                },
                "authors": [
                    {
                        "name": "Lopold Maillard"
                    },
                    {
                        "name": "Nicolas Sereyjol-Garros"
                    },
                    {
                        "name": "Tom Durand"
                    },
                    {
                        "name": "Maks Ovsjanikov"
                    }
                ],
                "author_detail": {
                    "name": "Maks Ovsjanikov"
                },
                "author": "Maks Ovsjanikov",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03231v1",
                "updated": "2024-11-05T16:23:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    23,
                    19,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:23:19Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    23,
                    19,
                    1,
                    310,
                    0
                ],
                "title": "Formal Logic-guided Robust Federated Learning against Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Logic-guided Robust Federated Learning against Poisoning Attacks"
                },
                "summary": "Federated Learning (FL) offers a promising solution to the privacy concerns\nassociated with centralized Machine Learning (ML) by enabling decentralized,\ncollaborative learning. However, FL is vulnerable to various security threats,\nincluding poisoning attacks, where adversarial clients manipulate the training\ndata or model updates to degrade overall model performance. Recognizing this\nthreat, researchers have focused on developing defense mechanisms to counteract\npoisoning attacks in FL systems. However, existing robust FL methods\npredominantly focus on computer vision tasks, leaving a gap in addressing the\nunique challenges of FL with time series data. In this paper, we present\nFLORAL, a defense mechanism designed to mitigate poisoning attacks in federated\nlearning for time-series tasks, even in scenarios with heterogeneous client\ndata and a large number of adversarial participants. Unlike traditional\nmodel-centric defenses, FLORAL leverages logical reasoning to evaluate client\ntrustworthiness by aligning their predictions with global time-series patterns,\nrather than relying solely on the similarity of client updates. Our approach\nextracts logical reasoning properties from clients, then hierarchically infers\nglobal properties, and uses these to verify client updates. Through formal\nlogic verification, we assess the robustness of each client contribution,\nidentifying deviations indicative of adversarial behavior. Experimental results\non two datasets demonstrate the superior performance of our approach compared\nto existing baseline methods, highlighting its potential to enhance the\nrobustness of FL to time series applications. Notably, FLORAL reduced the\nprediction error by 93.27\\% in the best-case scenario compared to the\nsecond-best baseline. Our code is available at\n\\url{https://anonymous.4open.science/r/FLORAL-Robust-FTS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) offers a promising solution to the privacy concerns\nassociated with centralized Machine Learning (ML) by enabling decentralized,\ncollaborative learning. However, FL is vulnerable to various security threats,\nincluding poisoning attacks, where adversarial clients manipulate the training\ndata or model updates to degrade overall model performance. Recognizing this\nthreat, researchers have focused on developing defense mechanisms to counteract\npoisoning attacks in FL systems. However, existing robust FL methods\npredominantly focus on computer vision tasks, leaving a gap in addressing the\nunique challenges of FL with time series data. In this paper, we present\nFLORAL, a defense mechanism designed to mitigate poisoning attacks in federated\nlearning for time-series tasks, even in scenarios with heterogeneous client\ndata and a large number of adversarial participants. Unlike traditional\nmodel-centric defenses, FLORAL leverages logical reasoning to evaluate client\ntrustworthiness by aligning their predictions with global time-series patterns,\nrather than relying solely on the similarity of client updates. Our approach\nextracts logical reasoning properties from clients, then hierarchically infers\nglobal properties, and uses these to verify client updates. Through formal\nlogic verification, we assess the robustness of each client contribution,\nidentifying deviations indicative of adversarial behavior. Experimental results\non two datasets demonstrate the superior performance of our approach compared\nto existing baseline methods, highlighting its potential to enhance the\nrobustness of FL to time series applications. Notably, FLORAL reduced the\nprediction error by 93.27\\% in the best-case scenario compared to the\nsecond-best baseline. Our code is available at\n\\url{https://anonymous.4open.science/r/FLORAL-Robust-FTS}."
                },
                "authors": [
                    {
                        "name": "Dung Thuy Nguyen"
                    },
                    {
                        "name": "Ziyan An"
                    },
                    {
                        "name": "Taylor T. Johnson"
                    },
                    {
                        "name": "Meiyi Ma"
                    },
                    {
                        "name": "Kevin Leach"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Leach"
                },
                "author": "Kevin Leach",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2305.00328 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11019v2",
                "updated": "2024-11-05T16:21:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    21,
                    55,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-28T17:31:47Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    31,
                    47,
                    4,
                    180,
                    0
                ],
                "title": "Efficacy of Various Large Language Models in Generating Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficacy of Various Large Language Models in Generating Smart Contracts"
                },
                "summary": "This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies."
                },
                "authors": [
                    {
                        "name": "Siddhartha Chatterjee"
                    },
                    {
                        "name": "Bina Ramamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Bina Ramamurthy"
                },
                "author": "Bina Ramamurthy",
                "arxiv_comment": "18 pages, accepted for presentation at 8th annual Future of\n  Information and Communication Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19616v2",
                "updated": "2024-11-05T16:10:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    10,
                    6,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-25T15:14:51Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    14,
                    51,
                    4,
                    299,
                    0
                ],
                "title": "Uniqueness and Nondegeneracy of positive ground states of $ -u +\n  (-)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniqueness and Nondegeneracy of positive ground states of $ -u +\n  (-)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$}$"
                },
                "summary": "We are concerned with the mixed local/nonlocal Schr\\\"{o}dinger equation\n  \\begin{equation}\n  - \\Delta u + (-\\Delta)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$,}\n  \\end{equation}\n  for arbitrary space dimension $n\\geqslant1$, any $s\\in(0,1)$ and\n$p\\in(0,2^*-2)$ with $2^*$ the critical Sobolev exponent.\n  We provide the existence and several fundamental properties of nonnegative\nsolutions for the above equation inferred from \\cite{DSVZ24}. And then, we\nprove that such equation possesses a unique (up to translations) ground state,\nwhich is nondegenerate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are concerned with the mixed local/nonlocal Schr\\\"{o}dinger equation\n  \\begin{equation}\n  - \\Delta u + (-\\Delta)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$,}\n  \\end{equation}\n  for arbitrary space dimension $n\\geqslant1$, any $s\\in(0,1)$ and\n$p\\in(0,2^*-2)$ with $2^*$ the critical Sobolev exponent.\n  We provide the existence and several fundamental properties of nonnegative\nsolutions for the above equation inferred from \\cite{DSVZ24}. And then, we\nprove that such equation possesses a unique (up to translations) ground state,\nwhich is nondegenerate."
                },
                "authors": [
                    {
                        "name": "Xifeng Su"
                    },
                    {
                        "name": "Chengxiang Zhang"
                    },
                    {
                        "name": "Jiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Zhang"
                },
                "author": "Jiwen Zhang",
                "arxiv_comment": "There was an error in the proof of Lemma 3.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35A02, 35B65, 35J10, 35R11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06477v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06477v4",
                "updated": "2024-11-05T16:02:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    2,
                    21,
                    1,
                    310,
                    0
                ],
                "published": "2024-01-12T09:56:57Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    56,
                    57,
                    4,
                    12,
                    0
                ],
                "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation"
                },
                "summary": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun"
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06477v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06477v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01483v2",
                "updated": "2024-11-05T15:55:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    55,
                    51,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-03T08:49:55Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    8,
                    49,
                    55,
                    6,
                    308,
                    0
                ],
                "title": "Teaching Models to Improve on Tape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Improve on Tape"
                },
                "summary": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts."
                },
                "authors": [
                    {
                        "name": "Liat Bezalel"
                    },
                    {
                        "name": "Eyal Orgad"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03205v1",
                "updated": "2024-11-05T15:53:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:53:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis"
                },
                "summary": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Temitope Akinboyewa"
                    },
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "M. Naser Lessani"
                    }
                ],
                "author_detail": {
                    "name": "M. Naser Lessani"
                },
                "author": "M. Naser Lessani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04331v2",
                "updated": "2024-11-05T15:43:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    43,
                    18,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-06T17:59:10Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    59,
                    10,
                    3,
                    158,
                    0
                ],
                "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaCE: Parsimonious Concept Engineering for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities."
                },
                "authors": [
                    {
                        "name": "Jinqi Luo"
                    },
                    {
                        "name": "Tianjiao Ding"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Darshan Thaker"
                    },
                    {
                        "name": "Aditya Chattopadhyay"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Ren Vidal"
                    }
                ],
                "author_detail": {
                    "name": "Ren Vidal"
                },
                "author": "Ren Vidal",
                "arxiv_comment": "Accepted in NeurIPS 2024. GitHub repository at\n  https://github.com/peterljq/Parsimonious-Concept-Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03195v1",
                "updated": "2024-11-05T15:40:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    40,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:40:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    40,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "Online Data Collection for Efficient Semiparametric Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Data Collection for Efficient Semiparametric Inference"
                },
                "summary": "While many works have studied statistical data fusion, they typically assume\nthat the various datasets are given in advance. However, in practice,\nestimation requires difficult data collection decisions like determining the\navailable data sources, their costs, and how many samples to collect from each\nsource. Moreover, this process is often sequential because the data collected\nat a given time can improve collection decisions in the future. In our setup,\ngiven access to multiple data sources and budget constraints, the agent must\nsequentially decide which data source to query to efficiently estimate a target\nparameter. We formalize this task using Online Moment Selection, a\nsemiparametric framework that applies to any parameter identified by a set of\nmoment conditions. Interestingly, the optimal budget allocation depends on the\n(unknown) true parameters. We present two online data collection policies,\nExplore-then-Commit and Explore-then-Greedy, that use the parameter estimates\nat a given time to optimally allocate the remaining budget in the future steps.\nWe prove that both policies achieve zero regret (assessed by asymptotic MSE)\nrelative to an oracle policy. We empirically validate our methods on both\nsynthetic and real-world causal effect estimation tasks, demonstrating that the\nonline data collection policies outperform their fixed counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many works have studied statistical data fusion, they typically assume\nthat the various datasets are given in advance. However, in practice,\nestimation requires difficult data collection decisions like determining the\navailable data sources, their costs, and how many samples to collect from each\nsource. Moreover, this process is often sequential because the data collected\nat a given time can improve collection decisions in the future. In our setup,\ngiven access to multiple data sources and budget constraints, the agent must\nsequentially decide which data source to query to efficiently estimate a target\nparameter. We formalize this task using Online Moment Selection, a\nsemiparametric framework that applies to any parameter identified by a set of\nmoment conditions. Interestingly, the optimal budget allocation depends on the\n(unknown) true parameters. We present two online data collection policies,\nExplore-then-Commit and Explore-then-Greedy, that use the parameter estimates\nat a given time to optimally allocate the remaining budget in the future steps.\nWe prove that both policies achieve zero regret (assessed by asymptotic MSE)\nrelative to an oracle policy. We empirically validate our methods on both\nsynthetic and real-world causal effect estimation tasks, demonstrating that the\nonline data collection policies outperform their fixed counterparts."
                },
                "authors": [
                    {
                        "name": "Shantanu Gupta"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "David Childers"
                    }
                ],
                "author_detail": {
                    "name": "David Childers"
                },
                "author": "David Childers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06393v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06393v4",
                "updated": "2024-11-05T15:40:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    40,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-04-09T15:35:52Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    15,
                    35,
                    52,
                    1,
                    100,
                    0
                ],
                "title": "MuPT: A Generative Symbolic Music Pretrained Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuPT: A Generative Symbolic Music Pretrained Transformer"
                },
                "summary": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions."
                },
                "authors": [
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Ziya Zhou"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Lejun Min"
                    },
                    {
                        "name": "Xueling Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Fengze Han"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06393v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06393v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10621v2",
                "updated": "2024-11-05T15:37:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    37,
                    52,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-16T18:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    0,
                    21,
                    0,
                    260,
                    0
                ],
                "title": "Inferring stellar parameters and their uncertainties from\n  high-resolution spectroscopy using invertible neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring stellar parameters and their uncertainties from\n  high-resolution spectroscopy using invertible neural networks"
                },
                "summary": "Context: New spectroscopic surveys will increase the number of astronomical\nobjects requiring characterization by over tenfold.. Machine learning tools are\nrequired to address this data deluge in a fast and accurate fashion. Most\nmachine learning algorithms can not estimate error directly, making them\nunsuitable for reliable science.\n  Aims: We aim to train a supervised deep-learning algorithm tailored for\nhigh-resolution observational stellar spectra. This algorithm accurately infer\nprecise estimates while providing coherent estimates of uncertainties by\nleveraging information from both the neural network and the spectra.\n  Methods: We train a conditional Invertible Neural Network (cINN) on\nobservational spectroscopic data obtained from the GIRAFFE spectrograph (HR10\nand HR21 setups) within the Gaia-ESO survey. A key features of cINN is its\nability to produce the Bayesian posterior distribution of parameters for each\nspectrum. By analyzing this distribution, we inferred parameters and their\nuncertainties. Several tests have been applied to study how parameters and\nerrors are estimated.\n  Results: We achieved an accuracy of 28K in $T_{\\text{eff}}$, 0.06 dex in\n$\\log g$, 0.03 dex in $[\\text{Fe/H}]$, and between 0.05 dex and 0.17 dex for\nthe other abundances for high quality spectra. Accuracy remains stable with low\nsignal-to-noise ratio spectra. The uncertainties obtained are well within the\nsame order of magnitude. The network accurately reproduces astrophysical\nrelationships both on the scale of the Milky Way and within smaller star\nclusters. We created a table containing the new parameters generated by our\ncINN.\n  Conclusion: This neural network represents a compelling proposition for\nfuture astronomical surveys. These coherent derived uncertainties make it\npossible to reuse these estimates in other works as Bayesian priors and thus\npresent a solid basis for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: New spectroscopic surveys will increase the number of astronomical\nobjects requiring characterization by over tenfold.. Machine learning tools are\nrequired to address this data deluge in a fast and accurate fashion. Most\nmachine learning algorithms can not estimate error directly, making them\nunsuitable for reliable science.\n  Aims: We aim to train a supervised deep-learning algorithm tailored for\nhigh-resolution observational stellar spectra. This algorithm accurately infer\nprecise estimates while providing coherent estimates of uncertainties by\nleveraging information from both the neural network and the spectra.\n  Methods: We train a conditional Invertible Neural Network (cINN) on\nobservational spectroscopic data obtained from the GIRAFFE spectrograph (HR10\nand HR21 setups) within the Gaia-ESO survey. A key features of cINN is its\nability to produce the Bayesian posterior distribution of parameters for each\nspectrum. By analyzing this distribution, we inferred parameters and their\nuncertainties. Several tests have been applied to study how parameters and\nerrors are estimated.\n  Results: We achieved an accuracy of 28K in $T_{\\text{eff}}$, 0.06 dex in\n$\\log g$, 0.03 dex in $[\\text{Fe/H}]$, and between 0.05 dex and 0.17 dex for\nthe other abundances for high quality spectra. Accuracy remains stable with low\nsignal-to-noise ratio spectra. The uncertainties obtained are well within the\nsame order of magnitude. The network accurately reproduces astrophysical\nrelationships both on the scale of the Milky Way and within smaller star\nclusters. We created a table containing the new parameters generated by our\ncINN.\n  Conclusion: This neural network represents a compelling proposition for\nfuture astronomical surveys. These coherent derived uncertainties make it\npossible to reuse these estimates in other works as Bayesian priors and thus\npresent a solid basis for future work."
                },
                "authors": [
                    {
                        "name": "Nils Candebat"
                    },
                    {
                        "name": "Giuseppe Germano Sacco"
                    },
                    {
                        "name": "Laura Magrini"
                    },
                    {
                        "name": "Francesco Belfiore"
                    },
                    {
                        "name": "Mathieu Van-der-Swaelmen"
                    },
                    {
                        "name": "Stefano Zibetti"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Zibetti"
                },
                "author": "Stefano Zibetti",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14558v2",
                "updated": "2024-11-05T15:31:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    31,
                    12,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-23T13:37:26Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    37,
                    26,
                    3,
                    144,
                    0
                ],
                "title": "FUSE: Fast Unified Simulation and Estimation for PDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUSE: Fast Unified Simulation and Estimation for PDEs"
                },
                "summary": "The joint prediction of continuous fields and statistical estimation of the\nunderlying discrete parameters is a common problem for many physical systems,\ngoverned by PDEs. Hitherto, it has been separately addressed by employing\noperator learning surrogates for field prediction while using simulation-based\ninference (and its variants) for statistical parameter determination. Here, we\nargue that solving both problems within the same framework can lead to\nconsistent gains in accuracy and robustness. To this end, We propose a novel\nand flexible formulation of the operator learning problem that allows jointly\npredicting continuous quantities and inferring distributions of discrete\nparameters, and thus amortizing the cost of both the inverse and the surrogate\nmodels to a joint pre-training step. We present the capabilities of the\nproposed methodology for predicting continuous and discrete biomarkers in\nfull-body haemodynamics simulations under different levels of missing\ninformation. We also consider a test case for atmospheric large-eddy simulation\nof a two-dimensional dry cold bubble, where we infer both continuous\ntime-series and information about the systems conditions. We present\ncomparisons against different baselines to showcase significantly increased\naccuracy in both the inverse and the surrogate tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The joint prediction of continuous fields and statistical estimation of the\nunderlying discrete parameters is a common problem for many physical systems,\ngoverned by PDEs. Hitherto, it has been separately addressed by employing\noperator learning surrogates for field prediction while using simulation-based\ninference (and its variants) for statistical parameter determination. Here, we\nargue that solving both problems within the same framework can lead to\nconsistent gains in accuracy and robustness. To this end, We propose a novel\nand flexible formulation of the operator learning problem that allows jointly\npredicting continuous quantities and inferring distributions of discrete\nparameters, and thus amortizing the cost of both the inverse and the surrogate\nmodels to a joint pre-training step. We present the capabilities of the\nproposed methodology for predicting continuous and discrete biomarkers in\nfull-body haemodynamics simulations under different levels of missing\ninformation. We also consider a test case for atmospheric large-eddy simulation\nof a two-dimensional dry cold bubble, where we infer both continuous\ntime-series and information about the systems conditions. We present\ncomparisons against different baselines to showcase significantly increased\naccuracy in both the inverse and the surrogate tasks."
                },
                "authors": [
                    {
                        "name": "Levi E. Lingsch"
                    },
                    {
                        "name": "Dana Grund"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    },
                    {
                        "name": "Georgios Kissas"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Kissas"
                },
                "author": "Georgios Kissas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03184v1",
                "updated": "2024-11-05T15:29:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    29,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:29:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    29,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "The Soltan argument at $z=6$: UV-luminous quasars contribute less than\n  10% to early black hole mass growth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Soltan argument at $z=6$: UV-luminous quasars contribute less than\n  10% to early black hole mass growth"
                },
                "summary": "We combine stellar mass functions and the recent first JWST-based\ngalaxy-black hole scaling relations at $z=6$ to for the first time compute the\nsupermassive black hole (SMBH) mass volume density at this epoch, and compare\nthis to the integrated SMBH mass growth from the population of UV-luminous\nquasars at $z>6$. We show that even under very conservative assumptions almost\nall growth of SMBH mass at $z>6$ does not take place in these UV-luminous\nquasars, but must occur in systems obscured through dust and/or with lower\nradiative efficiency than standard thin accretion disks.\n  The `Soltan argument' is not fulfilled by the known population of bright\nquasars at $z>6$: the integrated SMBH mass growth inferred from these largely\nunobscured active galactic nuclei (AGN) in the early Universe is by a factor\n$\\ge$10 smaller than the total SMBH mass volume density at $z=6$. This is valid\nunder a large range of assumption about luminosity, mass functions, and\naccretion modes, and is likely still a factor >2 smaller when accounting for\nknown obscuration fractions at this epoch.\n  The resulting consequences are: >90%, possibly substantially more, of\nSMBH-buildup in the early Universe does not take place in luminous unobscured\nquasar phases, but has to occur in obscured systems, with dust absorbing most\nof the emitted UV-visible AGN emission, potentially with accretion modes with\nsuper-Eddington specific accretion rates. This is consistent with short\nlifetimes for luminous quasar phases from quasar proximity zone studies and\nclustering. This would remove the empirical need for slow SMBH growth and hence\nexotic `high-mass seed' black holes at early cosmic time. It also predicts a\nlarge population of luminous but very obscured lower-mass quasars at $z>6$,\npossibly the JWST `Little Red Dots'. This finding might severe impact on how we\nwill diagnose SMBH growth at $z=7$ to 15 in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We combine stellar mass functions and the recent first JWST-based\ngalaxy-black hole scaling relations at $z=6$ to for the first time compute the\nsupermassive black hole (SMBH) mass volume density at this epoch, and compare\nthis to the integrated SMBH mass growth from the population of UV-luminous\nquasars at $z>6$. We show that even under very conservative assumptions almost\nall growth of SMBH mass at $z>6$ does not take place in these UV-luminous\nquasars, but must occur in systems obscured through dust and/or with lower\nradiative efficiency than standard thin accretion disks.\n  The `Soltan argument' is not fulfilled by the known population of bright\nquasars at $z>6$: the integrated SMBH mass growth inferred from these largely\nunobscured active galactic nuclei (AGN) in the early Universe is by a factor\n$\\ge$10 smaller than the total SMBH mass volume density at $z=6$. This is valid\nunder a large range of assumption about luminosity, mass functions, and\naccretion modes, and is likely still a factor >2 smaller when accounting for\nknown obscuration fractions at this epoch.\n  The resulting consequences are: >90%, possibly substantially more, of\nSMBH-buildup in the early Universe does not take place in luminous unobscured\nquasar phases, but has to occur in obscured systems, with dust absorbing most\nof the emitted UV-visible AGN emission, potentially with accretion modes with\nsuper-Eddington specific accretion rates. This is consistent with short\nlifetimes for luminous quasar phases from quasar proximity zone studies and\nclustering. This would remove the empirical need for slow SMBH growth and hence\nexotic `high-mass seed' black holes at early cosmic time. It also predicts a\nlarge population of luminous but very obscured lower-mass quasars at $z>6$,\npossibly the JWST `Little Red Dots'. This finding might severe impact on how we\nwill diagnose SMBH growth at $z=7$ to 15 in the future."
                },
                "authors": [
                    {
                        "name": "Knud Jahnke"
                    }
                ],
                "author_detail": {
                    "name": "Knud Jahnke"
                },
                "author": "Knud Jahnke",
                "arxiv_comment": "Submitted to Open Journal of Astrophysics; 10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03163v1",
                "updated": "2024-11-05T15:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    7,
                    20,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:07:20Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    7,
                    20,
                    1,
                    310,
                    0
                ],
                "title": "Efficient Hamiltonian, structure and trace distance learning of Gaussian\n  states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hamiltonian, structure and trace distance learning of Gaussian\n  states"
                },
                "summary": "In this work, we initiate the study of Hamiltonian learning for positive\ntemperature bosonic Gaussian states, the quantum generalization of the widely\nstudied problem of learning Gaussian graphical models. We obtain efficient\nprotocols, both in sample and computational complexity, for the task of\ninferring the parameters of their underlying quadratic Hamiltonian under the\nassumption of bounded temperature, squeezing, displacement and maximal degree\nof the interaction graph. Our protocol only requires heterodyne measurements,\nwhich are often experimentally feasible, and has a sample complexity that\nscales logarithmically with the number of modes. Furthermore, we show that it\nis possible to learn the underlying interaction graph in a similar setting and\nsample complexity. Taken together, our results put the status of the quantum\nHamiltonian learning problem for continuous variable systems in a much more\nadvanced state when compared to spins, where state-of-the-art results are\neither unavailable or quantitatively inferior to ours. In addition, we use our\ntechniques to obtain the first results on learning Gaussian states in trace\ndistance with a quadratic scaling in precision and polynomial in the number of\nmodes, albeit imposing certain restrictions on the Gaussian states. Our main\ntechnical innovations are several continuity bounds for the covariance and\nHamiltonian matrix of a Gaussian state, which are of independent interest,\ncombined with what we call the local inversion technique. In essence, the local\ninversion technique allows us to reliably infer the Hamiltonian of a Gaussian\nstate by only estimating in parallel submatrices of the covariance matrix whose\nsize scales with the desired precision, but not the number of modes. This way\nwe bypass the need to obtain precise global estimates of the covariance matrix,\ncontrolling the sample complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we initiate the study of Hamiltonian learning for positive\ntemperature bosonic Gaussian states, the quantum generalization of the widely\nstudied problem of learning Gaussian graphical models. We obtain efficient\nprotocols, both in sample and computational complexity, for the task of\ninferring the parameters of their underlying quadratic Hamiltonian under the\nassumption of bounded temperature, squeezing, displacement and maximal degree\nof the interaction graph. Our protocol only requires heterodyne measurements,\nwhich are often experimentally feasible, and has a sample complexity that\nscales logarithmically with the number of modes. Furthermore, we show that it\nis possible to learn the underlying interaction graph in a similar setting and\nsample complexity. Taken together, our results put the status of the quantum\nHamiltonian learning problem for continuous variable systems in a much more\nadvanced state when compared to spins, where state-of-the-art results are\neither unavailable or quantitatively inferior to ours. In addition, we use our\ntechniques to obtain the first results on learning Gaussian states in trace\ndistance with a quadratic scaling in precision and polynomial in the number of\nmodes, albeit imposing certain restrictions on the Gaussian states. Our main\ntechnical innovations are several continuity bounds for the covariance and\nHamiltonian matrix of a Gaussian state, which are of independent interest,\ncombined with what we call the local inversion technique. In essence, the local\ninversion technique allows us to reliably infer the Hamiltonian of a Gaussian\nstate by only estimating in parallel submatrices of the covariance matrix whose\nsize scales with the desired precision, but not the number of modes. This way\nwe bypass the need to obtain precise global estimates of the covariance matrix,\ncontrolling the sample complexity."
                },
                "authors": [
                    {
                        "name": "Marco Fanizza"
                    },
                    {
                        "name": "Cambyse Rouz"
                    },
                    {
                        "name": "Daniel Stilck Frana"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Stilck Frana"
                },
                "author": "Daniel Stilck Frana",
                "arxiv_comment": "43 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01076v2",
                "updated": "2024-11-05T15:03:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    3,
                    45,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-01T23:14:30Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    14,
                    30,
                    4,
                    306,
                    0
                ],
                "title": "Privacy Risks of Speculative Decoding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Risks of Speculative Decoding in Large Language Models"
                },
                "summary": "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches."
                },
                "authors": [
                    {
                        "name": "Jiankun Wei"
                    },
                    {
                        "name": "Abdulrahman Abdulrazzag"
                    },
                    {
                        "name": "Tianchen Zhang"
                    },
                    {
                        "name": "Adel Muursepp"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03156v1",
                "updated": "2024-11-05T14:58:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    58,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T14:58:31Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    58,
                    31,
                    1,
                    310,
                    0
                ],
                "title": "Unleashing the power of novel conditional generative approaches for new\n  materials discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the power of novel conditional generative approaches for new\n  materials discovery"
                },
                "summary": "For a very long time, computational approaches to the design of new materials\nhave relied on an iterative process of finding a candidate material and\nmodeling its properties. AI has played a crucial role in this regard, helping\nto accelerate the discovery and optimization of crystal properties and\nstructures through advanced computational methodologies and data-driven\napproaches. To address the problem of new materials design and fasten the\nprocess of new materials search, we have applied latest generative approaches\nto the problem of crystal structure design, trying to solve the inverse\nproblem: by given properties generate a structure that satisfies them without\nutilizing supercomputer powers. In our work we propose two approaches: 1)\nconditional structure modification: optimization of the stability of an\narbitrary atomic configuration, using the energy difference between the most\nenergetically favorable structure and all its less stable polymorphs and 2)\nconditional structure generation. We used a representation for materials that\nincludes the following information: lattice, atom coordinates, atom types,\nchemical features, space group and formation energy of the structure. The loss\nfunction was optimized to take into account the periodic boundary conditions of\ncrystal structures. We have applied Diffusion models approach, Flow matching,\nusual Autoencoder (AE) and compared the results of the models and approaches.\nAs a metric for the study, physical PyMatGen matcher was employed: we compare\ntarget structure with generated one using default tolerances. So far, our\nmodifier and generator produce structures with needed properties with accuracy\n41% and 82% respectively. To prove the offered methodology efficiency,\ninference have been carried out, resulting in several potentially new\nstructures with formation energy below the AFLOW-derived convex hulls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a very long time, computational approaches to the design of new materials\nhave relied on an iterative process of finding a candidate material and\nmodeling its properties. AI has played a crucial role in this regard, helping\nto accelerate the discovery and optimization of crystal properties and\nstructures through advanced computational methodologies and data-driven\napproaches. To address the problem of new materials design and fasten the\nprocess of new materials search, we have applied latest generative approaches\nto the problem of crystal structure design, trying to solve the inverse\nproblem: by given properties generate a structure that satisfies them without\nutilizing supercomputer powers. In our work we propose two approaches: 1)\nconditional structure modification: optimization of the stability of an\narbitrary atomic configuration, using the energy difference between the most\nenergetically favorable structure and all its less stable polymorphs and 2)\nconditional structure generation. We used a representation for materials that\nincludes the following information: lattice, atom coordinates, atom types,\nchemical features, space group and formation energy of the structure. The loss\nfunction was optimized to take into account the periodic boundary conditions of\ncrystal structures. We have applied Diffusion models approach, Flow matching,\nusual Autoencoder (AE) and compared the results of the models and approaches.\nAs a metric for the study, physical PyMatGen matcher was employed: we compare\ntarget structure with generated one using default tolerances. So far, our\nmodifier and generator produce structures with needed properties with accuracy\n41% and 82% respectively. To prove the offered methodology efficiency,\ninference have been carried out, resulting in several potentially new\nstructures with formation energy below the AFLOW-derived convex hulls."
                },
                "authors": [
                    {
                        "name": "Lev Novitskiy"
                    },
                    {
                        "name": "Vladimir Lazarev"
                    },
                    {
                        "name": "Mikhail Tiutiulnikov"
                    },
                    {
                        "name": "Nikita Vakhrameev"
                    },
                    {
                        "name": "Roman Eremin"
                    },
                    {
                        "name": "Innokentiy Humonen"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    },
                    {
                        "name": "Denis Dimitrov"
                    },
                    {
                        "name": "Semen Budennyy"
                    }
                ],
                "author_detail": {
                    "name": "Semen Budennyy"
                },
                "author": "Semen Budennyy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03137v1",
                "updated": "2024-11-05T14:30:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    30,
                    12,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T14:30:12Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    30,
                    12,
                    1,
                    310,
                    0
                ],
                "title": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing\n  Practice"
                },
                "summary": "Creative writers have a love for their craft, yet AI systems using large\nlanguage models (LLMs) offer the automation of significant parts of the writing\nprocess. So why do some creative writers choose to integrate AI into their\nworkflows? To explore this, we interview and observe a writing session with 18\ncreative writers who already use AI regularly in their writing practice. Our\nfindings reveal that creative writers are intentional about how they\nincorporate AI, making many deliberate decisions about when and how to engage\nAI based on the core values they hold about writing. These values, such as\nauthenticity and craftsmanship, alongside writers' relationships with and use\nof AI influence the parts of writing over which they wish to maintain control.\nThrough our analysis, we contribute a taxonomy of writer values, writer\nrelationships with AI, and integration strategies, and discuss how these three\nelements interrelate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative writers have a love for their craft, yet AI systems using large\nlanguage models (LLMs) offer the automation of significant parts of the writing\nprocess. So why do some creative writers choose to integrate AI into their\nworkflows? To explore this, we interview and observe a writing session with 18\ncreative writers who already use AI regularly in their writing practice. Our\nfindings reveal that creative writers are intentional about how they\nincorporate AI, making many deliberate decisions about when and how to engage\nAI based on the core values they hold about writing. These values, such as\nauthenticity and craftsmanship, alongside writers' relationships with and use\nof AI influence the parts of writing over which they wish to maintain control.\nThrough our analysis, we contribute a taxonomy of writer values, writer\nrelationships with AI, and integration strategies, and discuss how these three\nelements interrelate."
                },
                "authors": [
                    {
                        "name": "Alicia Guo"
                    },
                    {
                        "name": "Shreya Sathyanarayanan"
                    },
                    {
                        "name": "Leijie Wang"
                    },
                    {
                        "name": "Jeffrey Heer"
                    },
                    {
                        "name": "Amy Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy Zhang"
                },
                "author": "Amy Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20598v2",
                "updated": "2024-11-05T14:15:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    15,
                    3,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-27T21:12:12Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    12,
                    12,
                    6,
                    301,
                    0
                ],
                "title": "R^3AG: First Workshop on Refined and Reliable Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R^3AG: First Workshop on Refined and Reliable Retrieval Augmented\n  Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained wide attention as the key\ncomponent to improve generative models with external knowledge augmentation\nfrom information retrieval. It has shown great prominence in enhancing the\nfunctionality and performance of large language model (LLM)-based applications.\nHowever, with the comprehensive application of RAG, more and more problems and\nlimitations have been identified, thus urgently requiring further fundamental\nexploration to improve current RAG frameworks. This workshop aims to explore in\ndepth how to conduct refined and reliable RAG for downstream AI tasks.\n  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024\nto call for participants to re-examine and formulate the basic principles and\npractical implementation of refined and reliable RAG. The workshop serves as a\nplatform for both academia and industry researchers to conduct discussions,\nshare insights, and foster research to build the next generation of RAG\nsystems. Participants will engage in discussions and presentations focusing on\nfundamental challenges, cutting-edge research, and potential pathways to\nimprove RAG. At the end of the workshop, we aim to have a clearer understanding\nof how to improve the reliability and applicability of RAG with more robust\ninformation retrieval and language generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained wide attention as the key\ncomponent to improve generative models with external knowledge augmentation\nfrom information retrieval. It has shown great prominence in enhancing the\nfunctionality and performance of large language model (LLM)-based applications.\nHowever, with the comprehensive application of RAG, more and more problems and\nlimitations have been identified, thus urgently requiring further fundamental\nexploration to improve current RAG frameworks. This workshop aims to explore in\ndepth how to conduct refined and reliable RAG for downstream AI tasks.\n  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024\nto call for participants to re-examine and formulate the basic principles and\npractical implementation of refined and reliable RAG. The workshop serves as a\nplatform for both academia and industry researchers to conduct discussions,\nshare insights, and foster research to build the next generation of RAG\nsystems. Participants will engage in discussions and presentations focusing on\nfundamental challenges, cutting-edge research, and potential pathways to\nimprove RAG. At the end of the workshop, we aim to have a clearer understanding\nof how to improve the reliability and applicability of RAG with more robust\ninformation retrieval and language generation."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Xuri Ge"
                    },
                    {
                        "name": "Joemon M. Jose"
                    },
                    {
                        "name": "Haitao Yu"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Xin Xin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xin"
                },
                "author": "Xin Xin",
                "arxiv_comment": "R^3AG workshop overview at SIGIR-AP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03108v1",
                "updated": "2024-11-05T13:56:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    56,
                    42,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:56:42Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    56,
                    42,
                    1,
                    310,
                    0
                ],
                "title": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning"
                },
                "summary": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations"
                },
                "authors": [
                    {
                        "name": "Veronika Krau"
                    },
                    {
                        "name": "Mark McGill"
                    },
                    {
                        "name": "Thomas Kosch"
                    },
                    {
                        "name": "Yolanda Thiel"
                    },
                    {
                        "name": "Dominik Schn"
                    },
                    {
                        "name": "Jan Gugenheimer"
                    }
                ],
                "author_detail": {
                    "name": "Jan Gugenheimer"
                },
                "author": "Jan Gugenheimer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21333v2",
                "updated": "2024-11-05T13:47:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    47,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-27T18:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    18,
                    30,
                    41,
                    6,
                    301,
                    0
                ],
                "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse"
                },
                "summary": "Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning."
                },
                "authors": [
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Addison J. Wu"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Tania Lombrozo"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03101v1",
                "updated": "2024-11-05T13:46:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    46,
                    23,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:46:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    46,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Red giants evolutionary status determination: the complete Kepler\n  catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red giants evolutionary status determination: the complete Kepler\n  catalog"
                },
                "summary": "Evolved cool stars have three distinct evolutionary status: shell\nHydrogen-burning (RGB), core Helium and shell Hydrogen burning (RC), and double\nshell burning (AGB). Asteroseismology can distinguish between the RC and the\nother status, but distinguishing RGB and AGB has been difficult seismically and\nspectroscopically. The precise boundaries of different status in the HR diagram\nhave also been difficult to establish. In this article, we present a\ncomprehensive catalog of asteroseismic evolutionary status, RGB and RC, for\nevolved red giants in the Kepler field. We carefully examine boundary cases to\ndefine the lower edge of the RC phase in radius and surface gravity. We also\ntest different published asteroseisemic methods claiming to distinguish AGB and\nRGB stars against a sample where AGB candidates were selected using a\nspectrocopic identification method. We used six different seismic techniques to\ndistinguish RC and RGB stars, and tested two proposed methods for\ndistinguishing AGB and RGB stars. These status were compared with those\ninferred from spectroscopy. We present consensus evolutionary status for 18,784\nstars out of the 30,337 red giants present in the Kepler data, including 11,516\nstars with APOGEE spectra available. The agreement between seismic and\nspectroscopic classification is excellent for distinguishing RC stars, agreeing\nat the 94% level. Most disagreements can be traced to uncertainties in\nspectroscopic parameters, but some are caused by blends with background stars.\nWe find a sharp lower boundary in surface gravity at log(g) = 2.99+/-0.01 for\nthe RC and discuss the implications. We demonstrate that asteroseismic tools\nfor distinguishing AGB and RGB stars are consistent with spectroscopic\nevolutionary status at near the RC but that the agreement between the different\nmethods decreases rapidly as the star evolves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolved cool stars have three distinct evolutionary status: shell\nHydrogen-burning (RGB), core Helium and shell Hydrogen burning (RC), and double\nshell burning (AGB). Asteroseismology can distinguish between the RC and the\nother status, but distinguishing RGB and AGB has been difficult seismically and\nspectroscopically. The precise boundaries of different status in the HR diagram\nhave also been difficult to establish. In this article, we present a\ncomprehensive catalog of asteroseismic evolutionary status, RGB and RC, for\nevolved red giants in the Kepler field. We carefully examine boundary cases to\ndefine the lower edge of the RC phase in radius and surface gravity. We also\ntest different published asteroseisemic methods claiming to distinguish AGB and\nRGB stars against a sample where AGB candidates were selected using a\nspectrocopic identification method. We used six different seismic techniques to\ndistinguish RC and RGB stars, and tested two proposed methods for\ndistinguishing AGB and RGB stars. These status were compared with those\ninferred from spectroscopy. We present consensus evolutionary status for 18,784\nstars out of the 30,337 red giants present in the Kepler data, including 11,516\nstars with APOGEE spectra available. The agreement between seismic and\nspectroscopic classification is excellent for distinguishing RC stars, agreeing\nat the 94% level. Most disagreements can be traced to uncertainties in\nspectroscopic parameters, but some are caused by blends with background stars.\nWe find a sharp lower boundary in surface gravity at log(g) = 2.99+/-0.01 for\nthe RC and discuss the implications. We demonstrate that asteroseismic tools\nfor distinguishing AGB and RGB stars are consistent with spectroscopic\nevolutionary status at near the RC but that the agreement between the different\nmethods decreases rapidly as the star evolves."
                },
                "authors": [
                    {
                        "name": "M. Vrard"
                    },
                    {
                        "name": "M. H. Pinsonneault"
                    },
                    {
                        "name": "Y. Elsworth"
                    },
                    {
                        "name": "M. Hon"
                    },
                    {
                        "name": "T. Kallinger"
                    },
                    {
                        "name": "J. Kuszlewicz"
                    },
                    {
                        "name": "B. Mosser"
                    },
                    {
                        "name": "R. A. Garcia"
                    },
                    {
                        "name": "J. Tayar"
                    },
                    {
                        "name": "R. Bennett"
                    },
                    {
                        "name": "K. Cao"
                    },
                    {
                        "name": "S. Hekker"
                    },
                    {
                        "name": "L. Loyer"
                    },
                    {
                        "name": "S. Mathur"
                    },
                    {
                        "name": "D. Stello"
                    }
                ],
                "author_detail": {
                    "name": "D. Stello"
                },
                "author": "D. Stello",
                "arxiv_comment": "Article submitted to Astronomy&Astrophysics, 16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03093v1",
                "updated": "2024-11-05T13:39:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    39,
                    37,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:39:37Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    39,
                    37,
                    1,
                    310,
                    0
                ],
                "title": "Comparison of Bayesian inference methods using the Loreli II database of\n  hydro-radiative simulations of the 21-cm signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Bayesian inference methods using the Loreli II database of\n  hydro-radiative simulations of the 21-cm signal"
                },
                "summary": "While the observation of the 21 cm signal from the Cosmic Dawn and Epoch of\nReionization is an instrumental challenge, the interpretation of a prospective\ndetection is still open to questions regarding the modelling of the signal and\nthe Bayesian inference techniques that bridge the gap between theory and\nobservations. To address some of these questions, we present Loreli II, a\ndatabase of nearly 10 000 simulations of the 21 cm signal run with the Licorice\n3D radiative transfer code. With Loreli II, we explore a 5-dimensional\nastrophysical parameter space where star formation, X-ray emissions, and UV\nemissions are varied. We then use this database to train neural networks and\nperform Bayesian inference on 21 cm power spectra affected by thermal noise at\nthe level of 100 hours of observation with the Square Kilometer Array. We study\nand compare three inference techniques : an emulator of the power spectrum, a\nNeural Density Estimator that fits the implicit likelihood of the model, and a\nBayesian Neural Network that directly fits the posterior distribution. We\nmeasure the performances of each method by comparing them on a statistically\nrepresentative set of inferences, notably using the principles of\nSimulation-Based Calibration. We report errors on the 1-D marginalized\nposteriors (biases and over/under confidence) below $15 \\%$ of the standard\ndeviation for the emulator and below $25 \\%$ for the other methods. We conclude\nthat at our noise level and our sampling density of the parameter space, an\nexplicit Gaussian likelihood is sufficient. This may not be the case at lower\nnoise level or if a denser sampling is used to reach higher accuracy. We then\napply the emulator method to recent HERA upper limits and report weak\nconstraints on the X-ray emissivity parameter of our model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the observation of the 21 cm signal from the Cosmic Dawn and Epoch of\nReionization is an instrumental challenge, the interpretation of a prospective\ndetection is still open to questions regarding the modelling of the signal and\nthe Bayesian inference techniques that bridge the gap between theory and\nobservations. To address some of these questions, we present Loreli II, a\ndatabase of nearly 10 000 simulations of the 21 cm signal run with the Licorice\n3D radiative transfer code. With Loreli II, we explore a 5-dimensional\nastrophysical parameter space where star formation, X-ray emissions, and UV\nemissions are varied. We then use this database to train neural networks and\nperform Bayesian inference on 21 cm power spectra affected by thermal noise at\nthe level of 100 hours of observation with the Square Kilometer Array. We study\nand compare three inference techniques : an emulator of the power spectrum, a\nNeural Density Estimator that fits the implicit likelihood of the model, and a\nBayesian Neural Network that directly fits the posterior distribution. We\nmeasure the performances of each method by comparing them on a statistically\nrepresentative set of inferences, notably using the principles of\nSimulation-Based Calibration. We report errors on the 1-D marginalized\nposteriors (biases and over/under confidence) below $15 \\%$ of the standard\ndeviation for the emulator and below $25 \\%$ for the other methods. We conclude\nthat at our noise level and our sampling density of the parameter space, an\nexplicit Gaussian likelihood is sufficient. This may not be the case at lower\nnoise level or if a denser sampling is used to reach higher accuracy. We then\napply the emulator method to recent HERA upper limits and report weak\nconstraints on the X-ray emissivity parameter of our model."
                },
                "authors": [
                    {
                        "name": "Romain Meriot"
                    },
                    {
                        "name": "Benoit Semelin"
                    },
                    {
                        "name": "David Cornu"
                    }
                ],
                "author_detail": {
                    "name": "David Cornu"
                },
                "author": "David Cornu",
                "arxiv_comment": "21 pages, 16 figures, submitted to A&A. Processed simulated data\n  (21-cm power spectra and global signals) available at https://21ssd.obspm.fr/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03089v1",
                "updated": "2024-11-05T13:37:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    37,
                    15,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:37:15Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    37,
                    15,
                    1,
                    310,
                    0
                ],
                "title": "SPIRou observations of the young planet-hosting star PDS 70",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRou observations of the young planet-hosting star PDS 70"
                },
                "summary": "This paper presents near-infrared spectropolarimetric and velocimetric\nobservations of the young planet-hosting T Tauri star PDS 70, collected with\nSPIRou at the 3.6m Canada-France-Hawaii Telescope from 2020 to 2024. Clear\nZeeman signatures from magnetic fields at the surface of PDS 70 are detected in\nour data set of 40 circularly polarized spectra. Longitudinal fields inferred\nfrom Zeeman signatures, ranging from -116 to 176 G, are modulated on a\ntimescale of 3.008$\\pm$0.006 d, confirming that this is the rotation period of\nPDS 70. Applying Zeeman-Doppler imaging to subsets of unpolarized and\ncircularly polarised line profiles, we show that PDS 70 hosts low-contrast\nbrightness spots and a large-scale magnetic field in its photosphere, featuring\nin particular a dipole component of strength 200-420 G that evolves on a\ntimescale of months. From the broadening of spectral lines, we also infer that\nPDS 70 hosts a small-scale field of 2.51$\\pm$0.12 kG. Radial velocities derived\nfrom unpolarized line profiles are rotationally modulated as well, and exhibit\nadditional longer-term chromatic variability, most likely attributable to\nmagnetic activity rather than to a close-in giant planet (with a 3sigma upper\nlimit on its minimum mass of ~4 Mjup at a distance of ~0.2 au). We finally\nconfirm that accretion occurs at the surface of PDS 70, generating modulated\nred-shifted absorption in the 1083.3-nm He i triplet, and show that the\nlarge-scale magnetic field, often strong enough to disrupt the inner accretion\ndisc up to the corotation radius, weakens as the star gets fainter and redder\n(as in 2022), suggesting that dust from the disc more easily penetrates the\nstellar magnetosphere in such phases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents near-infrared spectropolarimetric and velocimetric\nobservations of the young planet-hosting T Tauri star PDS 70, collected with\nSPIRou at the 3.6m Canada-France-Hawaii Telescope from 2020 to 2024. Clear\nZeeman signatures from magnetic fields at the surface of PDS 70 are detected in\nour data set of 40 circularly polarized spectra. Longitudinal fields inferred\nfrom Zeeman signatures, ranging from -116 to 176 G, are modulated on a\ntimescale of 3.008$\\pm$0.006 d, confirming that this is the rotation period of\nPDS 70. Applying Zeeman-Doppler imaging to subsets of unpolarized and\ncircularly polarised line profiles, we show that PDS 70 hosts low-contrast\nbrightness spots and a large-scale magnetic field in its photosphere, featuring\nin particular a dipole component of strength 200-420 G that evolves on a\ntimescale of months. From the broadening of spectral lines, we also infer that\nPDS 70 hosts a small-scale field of 2.51$\\pm$0.12 kG. Radial velocities derived\nfrom unpolarized line profiles are rotationally modulated as well, and exhibit\nadditional longer-term chromatic variability, most likely attributable to\nmagnetic activity rather than to a close-in giant planet (with a 3sigma upper\nlimit on its minimum mass of ~4 Mjup at a distance of ~0.2 au). We finally\nconfirm that accretion occurs at the surface of PDS 70, generating modulated\nred-shifted absorption in the 1083.3-nm He i triplet, and show that the\nlarge-scale magnetic field, often strong enough to disrupt the inner accretion\ndisc up to the corotation radius, weakens as the star gets fainter and redder\n(as in 2022), suggesting that dust from the disc more easily penetrates the\nstellar magnetosphere in such phases."
                },
                "authors": [
                    {
                        "name": "J. -F. Donati"
                    },
                    {
                        "name": "P. I. Cristofari"
                    },
                    {
                        "name": "S. H. P. Alencar"
                    },
                    {
                        "name": ". Kspl"
                    },
                    {
                        "name": "J. Bouvier"
                    },
                    {
                        "name": "C. Moutou"
                    },
                    {
                        "name": "A. Carmona"
                    },
                    {
                        "name": "J. Gregorio-Hetem"
                    },
                    {
                        "name": "C. F. Manara"
                    },
                    {
                        "name": "E. Artigau"
                    },
                    {
                        "name": "R. Doyon"
                    },
                    {
                        "name": "M. Takami"
                    },
                    {
                        "name": "H. Shang"
                    },
                    {
                        "name": "J. Dias do Nascimento"
                    },
                    {
                        "name": "F. Mnard"
                    },
                    {
                        "name": "E. Gaidos"
                    },
                    {
                        "name": "the SPIRou science team"
                    }
                ],
                "author_detail": {
                    "name": "the SPIRou science team"
                },
                "author": "the SPIRou science team",
                "arxiv_comment": "MNRAS in press (20 pages, 12 figures, 5 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23074v2",
                "updated": "2024-11-05T13:26:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    26,
                    7,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-30T14:46:43Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    46,
                    43,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Programming Language Sandbox for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Programming Language Sandbox for LLMs"
                },
                "summary": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Yunbo Tao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Haoxiang Jia"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shenxi Wu"
                    },
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Limao Xiong"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yueming Wu"
                    },
                    {
                        "name": "Ming Wen"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03079v1",
                "updated": "2024-11-05T13:24:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    24,
                    56,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:24:56Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    24,
                    56,
                    1,
                    310,
                    0
                ],
                "title": "Utilizing Precise and Complete Code Context to Guide LLM in Automatic\n  False Positive Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Precise and Complete Code Context to Guide LLM in Automatic\n  False Positive Mitigation"
                },
                "summary": "Static Application Security Testing(SAST) tools are crucial for early bug\ndetection and code quality but often generate false positives that slow\ndevelopment. Automating false positive mitigation is thus essential for\nadvancing SAST tools. Past efforts use static/dynamic analysis or machine\nlearning. The advent of Large Language Models, adept at understanding natural\nlanguage and code, offers promising ways to improve the accuracy and usability\nof SAST tools. However, existing LLM-based methods need improvement in two key\nareas: first, extracted code snippets related to warnings are often cluttered\nwith irrelevant control and data flows, reducing precision; second, critical\ncode contexts are often missing, leading to incomplete representations that can\nmislead LLMs and cause inaccurate assessments. To ensure the use of precise and\ncomplete code context, thereby avoiding misguidance and enabling LLMs to reach\naccurate conclusions, we propose LLM4FPM. One of its core components is\neCPG-Slicer, which builds an extended code property graph and extracts\nline-level, precise code context. Moreover, LLM4FPM incorporates FARF\nalgorithm, which builds a file reference graph and then efficiently detects all\nfiles related to a warning in linear time, enabling eCPG-Slicer to gather\ncomplete code context across these files. We evaluate LLM4FPM on Juliet\ndataset, where it comprehensively outperforms the baseline, achieving an F1\nscore above 99% across various CWEs. LLM4FPM leverages a free, open-source\nmodel, avoiding costly alternatives and reducing inspection costs by up to\n$2758 per run on Juliet, with an average inspection time of 4.7 seconds per\nwarning. Our work emphasizes the critical impact of precise and complete code\ncontext and highlights the potential of combining program analysis with LLMs,\nimproving the quality and efficiency of software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Application Security Testing(SAST) tools are crucial for early bug\ndetection and code quality but often generate false positives that slow\ndevelopment. Automating false positive mitigation is thus essential for\nadvancing SAST tools. Past efforts use static/dynamic analysis or machine\nlearning. The advent of Large Language Models, adept at understanding natural\nlanguage and code, offers promising ways to improve the accuracy and usability\nof SAST tools. However, existing LLM-based methods need improvement in two key\nareas: first, extracted code snippets related to warnings are often cluttered\nwith irrelevant control and data flows, reducing precision; second, critical\ncode contexts are often missing, leading to incomplete representations that can\nmislead LLMs and cause inaccurate assessments. To ensure the use of precise and\ncomplete code context, thereby avoiding misguidance and enabling LLMs to reach\naccurate conclusions, we propose LLM4FPM. One of its core components is\neCPG-Slicer, which builds an extended code property graph and extracts\nline-level, precise code context. Moreover, LLM4FPM incorporates FARF\nalgorithm, which builds a file reference graph and then efficiently detects all\nfiles related to a warning in linear time, enabling eCPG-Slicer to gather\ncomplete code context across these files. We evaluate LLM4FPM on Juliet\ndataset, where it comprehensively outperforms the baseline, achieving an F1\nscore above 99% across various CWEs. LLM4FPM leverages a free, open-source\nmodel, avoiding costly alternatives and reducing inspection costs by up to\n$2758 per run on Juliet, with an average inspection time of 4.7 seconds per\nwarning. Our work emphasizes the critical impact of precise and complete code\ncontext and highlights the potential of combining program analysis with LLMs,\nimproving the quality and efficiency of software development."
                },
                "authors": [
                    {
                        "name": "Jinbao Chen"
                    },
                    {
                        "name": "Hongjing Xiang"
                    },
                    {
                        "name": "Luhao Li"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Boyao Ding"
                    },
                    {
                        "name": "Qingwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingwei Li"
                },
                "arxiv_affiliation": "University of Science and Technology of China",
                "author": "Qingwei Li",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2; D.2.5; F.2.1; F.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22566v2",
                "updated": "2024-11-05T13:21:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    21,
                    26,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-29T22:15:03Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    22,
                    15,
                    3,
                    1,
                    303,
                    0
                ],
                "title": "Deep Priors for Video Quality Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Priors for Video Quality Prediction"
                },
                "summary": "In this work, we designed a completely blind video quality assessment\nalgorithm using the deep video prior. This work mainly explores the utility of\ndeep video prior in estimating the visual quality of the video. In our work, we\nhave used a single distorted video and a reference video pair to learn the deep\nvideo prior. At inference time, the learned deep prior is used to restore the\noriginal videos from the distorted videos. The ability of learned deep video\nprior to restore the original video from the distorted video is measured to\nquantify distortion in the video. Our hypothesis is that the learned deep video\nprior fails in restoring the highly distorted videos. The restoring ability of\ndeep video prior is proportional to the distortion present in the video.\nTherefore, we propose to use the distance between the distorted video and the\nrestored video as the perceptual quality of the video. Our algorithm is trained\nusing a single video pair and it does not need any labelled data. We show that\nour proposed algorithm outperforms the existing unsupervised video quality\nassessment algorithms in terms of LCC and SROCC on a synthetically distorted\nvideo quality assessment dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we designed a completely blind video quality assessment\nalgorithm using the deep video prior. This work mainly explores the utility of\ndeep video prior in estimating the visual quality of the video. In our work, we\nhave used a single distorted video and a reference video pair to learn the deep\nvideo prior. At inference time, the learned deep prior is used to restore the\noriginal videos from the distorted videos. The ability of learned deep video\nprior to restore the original video from the distorted video is measured to\nquantify distortion in the video. Our hypothesis is that the learned deep video\nprior fails in restoring the highly distorted videos. The restoring ability of\ndeep video prior is proportional to the distortion present in the video.\nTherefore, we propose to use the distance between the distorted video and the\nrestored video as the perceptual quality of the video. Our algorithm is trained\nusing a single video pair and it does not need any labelled data. We show that\nour proposed algorithm outperforms the existing unsupervised video quality\nassessment algorithms in terms of LCC and SROCC on a synthetically distorted\nvideo quality assessment dataset."
                },
                "authors": [
                    {
                        "name": "Siddharath Narayan Shakya"
                    },
                    {
                        "name": "Parimala Kancharla"
                    }
                ],
                "author_detail": {
                    "name": "Parimala Kancharla"
                },
                "author": "Parimala Kancharla",
                "arxiv_comment": "Indian Conference on Computer Vision, Graphics and Image Processing\n  (ICVGIP) 2024 conference tinny paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10229v2",
                "updated": "2024-11-05T13:18:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    18,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-04-16T02:19:28Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    2,
                    19,
                    28,
                    1,
                    107,
                    0
                ],
                "title": "Generative Text Steganography with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Text Steganography with Large Language Model"
                },
                "summary": "Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Wu"
                    },
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Yiming Xue"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Peng"
                },
                "author": "Wanli Peng",
                "arxiv_doi": "10.1145/3664647.3680562",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3680562",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.10229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 4 figures, accepted at ACM Multimedia 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13618v2",
                "updated": "2024-11-05T13:17:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    17,
                    56,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-19T15:14:55Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    15,
                    14,
                    55,
                    2,
                    171,
                    0
                ],
                "title": "In-Context Former: Lightning-fast Compressing Context for Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Former: Lightning-fast Compressing Context for Large Language\n  Model"
                },
                "summary": "With the rising popularity of Transformer-based large language models (LLMs),\nreducing their high inference costs has become a significant research focus.\nOne effective approach is to compress the long input contexts. Existing methods\ntypically leverage the self-attention mechanism of the LLM itself for context\ncompression. While these methods have achieved notable results, the compression\nprocess still involves quadratic time complexity, which limits their\napplicability. To mitigate this limitation, we propose the In-Context Former\n(IC-Former). Unlike previous methods, IC-Former does not depend on the target\nLLMs. Instead, it leverages the cross-attention mechanism and a small number of\nlearnable digest tokens to directly condense information from the contextual\nword embeddings. This approach significantly reduces inference time, which\nachieves linear growth in time complexity within the compression range.\nExperimental results indicate that our method requires only 1/32 of the\nfloating-point operations of the baseline during compression and improves\nprocessing speed by 68 to 112 times while achieving over 90% of the baseline\nperformance on evaluation metrics. Overall, our model effectively reduces\ncompression costs and makes real-time compression scenarios feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising popularity of Transformer-based large language models (LLMs),\nreducing their high inference costs has become a significant research focus.\nOne effective approach is to compress the long input contexts. Existing methods\ntypically leverage the self-attention mechanism of the LLM itself for context\ncompression. While these methods have achieved notable results, the compression\nprocess still involves quadratic time complexity, which limits their\napplicability. To mitigate this limitation, we propose the In-Context Former\n(IC-Former). Unlike previous methods, IC-Former does not depend on the target\nLLMs. Instead, it leverages the cross-attention mechanism and a small number of\nlearnable digest tokens to directly condense information from the contextual\nword embeddings. This approach significantly reduces inference time, which\nachieves linear growth in time complexity within the compression range.\nExperimental results indicate that our method requires only 1/32 of the\nfloating-point operations of the baseline during compression and improves\nprocessing speed by 68 to 112 times while achieving over 90% of the baseline\nperformance on evaluation metrics. Overall, our model effectively reduces\ncompression costs and makes real-time compression scenarios feasible."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Wang"
                    },
                    {
                        "name": "Zaiyi Chen"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yongyi He"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by EMNLP2024(Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03095v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03095v4",
                "updated": "2024-11-05T12:57:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    57,
                    35,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-06T10:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    52,
                    41,
                    1,
                    219,
                    0
                ],
                "title": "Improving LLM-based Unit test generation via Template-based Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM-based Unit test generation via Template-based Repair"
                },
                "summary": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%."
                },
                "authors": [
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Fangyuan Tian"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03095v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03095v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03056v1",
                "updated": "2024-11-05T12:44:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    44,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T12:44:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    44,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "Constraining $$ and $y$ distortions in the Cosmic Microwave\n  Background with COBE/FIRAS Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining $$ and $y$ distortions in the Cosmic Microwave\n  Background with COBE/FIRAS Data"
                },
                "summary": "This paper presents a novel approach to constrain the $\\mu$- and y-\ndistortions in the Cosmic Microwave Background (CMB) using the COBE/FIRAS data.\nThe analysis draws from the concept of blackbody radiation inversion (BRI), a\nmathematical technique typically used to determine the temperature distribution\nfrom a radiated power spectrum. We study the deviations from the ideal\nblackbody spectrum or the spectral distortions by incorporating first a\nnon-zero chemical potential $\\mu$ via the Bose-Einstein distribution and then\nthe Compton parameter $y$ while keeping the monopole temperature constant. We\ninfer the results as probability distribution functions on these distortions.\nFinally, we derive $\\mu = (8.913 \\pm 0.736) \\times 10^{-5}$ and $y = (1.532 \\pm\n0.092) \\times 10^{-5}$ at a $68\\%$ confidence interval. The results are\nconsistent with prior values and provide tighter constraints on the CMB\nspectral distortion and synergies of the primordial Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to constrain the $\\mu$- and y-\ndistortions in the Cosmic Microwave Background (CMB) using the COBE/FIRAS data.\nThe analysis draws from the concept of blackbody radiation inversion (BRI), a\nmathematical technique typically used to determine the temperature distribution\nfrom a radiated power spectrum. We study the deviations from the ideal\nblackbody spectrum or the spectral distortions by incorporating first a\nnon-zero chemical potential $\\mu$ via the Bose-Einstein distribution and then\nthe Compton parameter $y$ while keeping the monopole temperature constant. We\ninfer the results as probability distribution functions on these distortions.\nFinally, we derive $\\mu = (8.913 \\pm 0.736) \\times 10^{-5}$ and $y = (1.532 \\pm\n0.092) \\times 10^{-5}$ at a $68\\%$ confidence interval. The results are\nconsistent with prior values and provide tighter constraints on the CMB\nspectral distortion and synergies of the primordial Universe."
                },
                "authors": [
                    {
                        "name": "Somita Dhal"
                    },
                    {
                        "name": "Koustav Konar"
                    },
                    {
                        "name": "R. K. Paul"
                    }
                ],
                "author_detail": {
                    "name": "R. K. Paul"
                },
                "author": "R. K. Paul",
                "arxiv_comment": "16 pages, 4 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03047v1",
                "updated": "2024-11-05T12:30:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    30,
                    7,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T12:30:07Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    30,
                    7,
                    1,
                    310,
                    0
                ],
                "title": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single\n  In-the-Wild Image using a Dataset with Levels of Details",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single\n  In-the-Wild Image using a Dataset with Levels of Details"
                },
                "summary": "Neural implicit functions have brought impressive advances to the\nstate-of-the-art of clothed human digitization from multiple or even single\nimages. However, despite the progress, current arts still have difficulty\ngeneralizing to unseen images with complex cloth deformation and body poses. In\nthis work, we present GarVerseLOD, a new dataset and framework that paves the\nway to achieving unprecedented robustness in high-fidelity 3D garment\nreconstruction from a single unconstrained image. Inspired by the recent\nsuccess of large generative models, we believe that one key to addressing the\ngeneralization challenge lies in the quantity and quality of 3D garment data.\nTowards this end, GarVerseLOD collects 6,000 high-quality cloth models with\nfine-grained geometry details manually created by professional artists. In\naddition to the scale of training data, we observe that having disentangled\ngranularities of geometry can play an important role in boosting the\ngeneralization capability and inference accuracy of the learned model. We hence\ncraft GarVerseLOD as a hierarchical dataset with levels of details (LOD),\nspanning from detail-free stylized shape to pose-blended garment with\npixel-aligned details. This allows us to make this highly under-constrained\nproblem tractable by factorizing the inference into easier tasks, each narrowed\ndown with smaller searching space. To ensure GarVerseLOD can generalize well to\nin-the-wild images, we propose a novel labeling paradigm based on conditional\ndiffusion models to generate extensive paired images for each garment model\nwith high photorealism. We evaluate our method on a massive amount of\nin-the-wild images. Experimental results demonstrate that GarVerseLOD can\ngenerate standalone garment pieces with significantly better quality than prior\napproaches. Project page: https://garverselod.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural implicit functions have brought impressive advances to the\nstate-of-the-art of clothed human digitization from multiple or even single\nimages. However, despite the progress, current arts still have difficulty\ngeneralizing to unseen images with complex cloth deformation and body poses. In\nthis work, we present GarVerseLOD, a new dataset and framework that paves the\nway to achieving unprecedented robustness in high-fidelity 3D garment\nreconstruction from a single unconstrained image. Inspired by the recent\nsuccess of large generative models, we believe that one key to addressing the\ngeneralization challenge lies in the quantity and quality of 3D garment data.\nTowards this end, GarVerseLOD collects 6,000 high-quality cloth models with\nfine-grained geometry details manually created by professional artists. In\naddition to the scale of training data, we observe that having disentangled\ngranularities of geometry can play an important role in boosting the\ngeneralization capability and inference accuracy of the learned model. We hence\ncraft GarVerseLOD as a hierarchical dataset with levels of details (LOD),\nspanning from detail-free stylized shape to pose-blended garment with\npixel-aligned details. This allows us to make this highly under-constrained\nproblem tractable by factorizing the inference into easier tasks, each narrowed\ndown with smaller searching space. To ensure GarVerseLOD can generalize well to\nin-the-wild images, we propose a novel labeling paradigm based on conditional\ndiffusion models to generate extensive paired images for each garment model\nwith high photorealism. We evaluate our method on a massive amount of\nin-the-wild images. Experimental results demonstrate that GarVerseLOD can\ngenerate standalone garment pieces with significantly better quality than prior\napproaches. Project page: https://garverselod.github.io/"
                },
                "authors": [
                    {
                        "name": "Zhongjin Luo"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Chenghong Li"
                    },
                    {
                        "name": "Wanghao Du"
                    },
                    {
                        "name": "Zirong Jin"
                    },
                    {
                        "name": "Wanhu Sun"
                    },
                    {
                        "name": "Yinyu Nie"
                    },
                    {
                        "name": "Weikai Chen"
                    },
                    {
                        "name": "Xiaoguang Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Han"
                },
                "author": "Xiaoguang Han",
                "arxiv_comment": "Project page: https://garverselod.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04592v2",
                "updated": "2024-11-05T12:24:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    24,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-06T19:02:22Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    2,
                    22,
                    6,
                    280,
                    0
                ],
                "title": "CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and\n  Risk Detection of Cancer Treatment-Induced Cardiotoxicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and\n  Risk Detection of Cancer Treatment-Induced Cardiotoxicity"
                },
                "summary": "Despite recent advances in cancer treatments that prolong patients' lives,\ntreatment-induced cardiotoxicity remains one severe side effect. The clinical\ndecision-making of cardiotoxicity is challenging, as non-clinical symptoms can\nbe missed until life-threatening events occur at a later stage, and clinicians\nalready have a high workload centered on the treatment, not the side effects.\nOur project starts with a participatory design study with 11 clinicians to\nunderstand their practices and needs; then we build a multimodal AI system,\nCardioAI, that integrates wearables and LLM-powered voice assistants to monitor\nmultimodal non-clinical symptoms. Also, the system includes an explainable risk\nprediction module that can generate cardiotoxicity risk scores and summaries as\nexplanations to support clinicians' decision-making. We conducted a heuristic\nevaluation with four clinical experts and found that they all believe CardioAI\nintegrates well into their workflow, reduces their information overload, and\nenables them to make more informed decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in cancer treatments that prolong patients' lives,\ntreatment-induced cardiotoxicity remains one severe side effect. The clinical\ndecision-making of cardiotoxicity is challenging, as non-clinical symptoms can\nbe missed until life-threatening events occur at a later stage, and clinicians\nalready have a high workload centered on the treatment, not the side effects.\nOur project starts with a participatory design study with 11 clinicians to\nunderstand their practices and needs; then we build a multimodal AI system,\nCardioAI, that integrates wearables and LLM-powered voice assistants to monitor\nmultimodal non-clinical symptoms. Also, the system includes an explainable risk\nprediction module that can generate cardiotoxicity risk scores and summaries as\nexplanations to support clinicians' decision-making. We conducted a heuristic\nevaluation with four clinical experts and found that they all believe CardioAI\nintegrates well into their workflow, reduces their information overload, and\nenables them to make more informed decisions."
                },
                "authors": [
                    {
                        "name": "Siyi Wu"
                    },
                    {
                        "name": "Weidan Cao"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Ziqi Yang"
                    },
                    {
                        "name": "Changchang Yin"
                    },
                    {
                        "name": "Varun Mishra"
                    },
                    {
                        "name": "Daniel Addison"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v2",
                "updated": "2024-11-05T12:10:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    10,
                    51,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$=0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$=0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03021v1",
                "updated": "2024-11-05T11:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    44,
                    0,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T11:44:00Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    44,
                    0,
                    1,
                    310,
                    0
                ],
                "title": "Testing Generalizability in Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Generalizability in Causal Inference"
                },
                "summary": "Ensuring robust model performance across diverse real-world scenarios\nrequires addressing both transportability across domains with covariate shifts\nand extrapolation beyond observed data ranges. However, there is no formal\nprocedure for statistically evaluating generalizability in machine learning\nalgorithms, particularly in causal inference. Existing methods often rely on\narbitrary metrics like AUC or MSE and focus predominantly on toy datasets,\nproviding limited insights into real-world applicability. To address this gap,\nwe propose a systematic and quantitative framework for evaluating model\ngeneralizability under covariate distribution shifts, specifically within\ncausal inference settings. Our approach leverages the frugal parameterization,\nallowing for flexible simulations from fully and semi-synthetic benchmarks,\noffering comprehensive evaluations for both mean and distributional regression\nmethods. By basing simulations on real data, our method ensures more realistic\nevaluations, which is often missing in current work relying on simplified\ndatasets. Furthermore, using simulations and statistical testing, our framework\nis robust and avoids over-reliance on conventional metrics. Grounded in\nreal-world data, it provides realistic insights into model performance,\nbridging the gap between synthetic evaluations and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring robust model performance across diverse real-world scenarios\nrequires addressing both transportability across domains with covariate shifts\nand extrapolation beyond observed data ranges. However, there is no formal\nprocedure for statistically evaluating generalizability in machine learning\nalgorithms, particularly in causal inference. Existing methods often rely on\narbitrary metrics like AUC or MSE and focus predominantly on toy datasets,\nproviding limited insights into real-world applicability. To address this gap,\nwe propose a systematic and quantitative framework for evaluating model\ngeneralizability under covariate distribution shifts, specifically within\ncausal inference settings. Our approach leverages the frugal parameterization,\nallowing for flexible simulations from fully and semi-synthetic benchmarks,\noffering comprehensive evaluations for both mean and distributional regression\nmethods. By basing simulations on real data, our method ensures more realistic\nevaluations, which is often missing in current work relying on simplified\ndatasets. Furthermore, using simulations and statistical testing, our framework\nis robust and avoids over-reliance on conventional metrics. Grounded in\nreal-world data, it provides realistic insights into model performance,\nbridging the gap between synthetic evaluations and practical applications."
                },
                "authors": [
                    {
                        "name": "Daniel de Vassimon Manela"
                    },
                    {
                        "name": "Linying Yang"
                    },
                    {
                        "name": "Robin J. Evans"
                    }
                ],
                "author_detail": {
                    "name": "Robin J. Evans"
                },
                "author": "Robin J. Evans",
                "arxiv_comment": "17 pages, 10 figures, Under review at AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14449v2",
                "updated": "2024-11-05T11:10:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    10,
                    45,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-23T11:29:33Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    11,
                    29,
                    33,
                    3,
                    144,
                    0
                ],
                "title": "Adversarial Schrdinger Bridge Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Schrdinger Bridge Matching"
                },
                "summary": "The Schr\\\"odinger Bridge (SB) problem offers a powerful framework for\ncombining optimal transport and diffusion models. A promising recent approach\nto solve the SB problem is the Iterative Markovian Fitting (IMF) procedure,\nwhich alternates between Markovian and reciprocal projections of\ncontinuous-time stochastic processes. However, the model built by the IMF\nprocedure has a long inference time due to using many steps of numerical\nsolvers for stochastic differential equations.\n  To address this limitation, we propose a novel Discrete-time IMF (D-IMF)\nprocedure in which learning of stochastic processes is replaced by learning\njust a few transition probabilities in discrete time. Its great advantage is\nthat in practice it can be naturally implemented using the Denoising Diffusion\nGAN (DD-GAN), an already well-established adversarial generative modeling\ntechnique. We show that our D-IMF procedure can provide the same quality of\nunpaired domain translation as the IMF, using only several generation steps\ninstead of hundreds. We provide the code at\nhttps://github.com/Daniil-Selikhanovych/ASBM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Schr\\\"odinger Bridge (SB) problem offers a powerful framework for\ncombining optimal transport and diffusion models. A promising recent approach\nto solve the SB problem is the Iterative Markovian Fitting (IMF) procedure,\nwhich alternates between Markovian and reciprocal projections of\ncontinuous-time stochastic processes. However, the model built by the IMF\nprocedure has a long inference time due to using many steps of numerical\nsolvers for stochastic differential equations.\n  To address this limitation, we propose a novel Discrete-time IMF (D-IMF)\nprocedure in which learning of stochastic processes is replaced by learning\njust a few transition probabilities in discrete time. Its great advantage is\nthat in practice it can be naturally implemented using the Denoising Diffusion\nGAN (DD-GAN), an already well-established adversarial generative modeling\ntechnique. We show that our D-IMF procedure can provide the same quality of\nunpaired domain translation as the IMF, using only several generation steps\ninstead of hundreds. We provide the code at\nhttps://github.com/Daniil-Selikhanovych/ASBM."
                },
                "authors": [
                    {
                        "name": "Nikita Gushchin"
                    },
                    {
                        "name": "Daniil Selikhanovych"
                    },
                    {
                        "name": "Sergei Kholkin"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Alexander Korotin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Korotin"
                },
                "author": "Alexander Korotin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07888v2",
                "updated": "2024-11-05T11:07:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    7,
                    19,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-15T02:22:48Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    22,
                    48,
                    3,
                    228,
                    0
                ],
                "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies\n  in Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies\n  in Medical Question Answering"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs,\ndriving the need for data-efficient training with optimised data ordering.\nHuman-inspired strategies offer a solution by organising data based on human\nlearning practices. This study evaluates the fine-tuning efficiency of five\nhuman-inspired strategies across four language models, three datasets, and both\nhuman- and LLM-labelled data in the context of medical question answering.\nThese strategies achieve the best accuracy gain of 1.81% and an average gain of\n1.02% across datasets, with interleaved strategies delivering the best average\nresults. However, the best strategy varies across model-dataset combinations,\nlimiting the generalisability of the effects of any single strategy.\nAdditionally, LLM-defined question difficulty outperforms human-defined labels\nin curriculum-based learning, showing the potential of model-generated data as\na cost-effective alternative for optimising fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs,\ndriving the need for data-efficient training with optimised data ordering.\nHuman-inspired strategies offer a solution by organising data based on human\nlearning practices. This study evaluates the fine-tuning efficiency of five\nhuman-inspired strategies across four language models, three datasets, and both\nhuman- and LLM-labelled data in the context of medical question answering.\nThese strategies achieve the best accuracy gain of 1.81% and an average gain of\n1.02% across datasets, with interleaved strategies delivering the best average\nresults. However, the best strategy varies across model-dataset combinations,\nlimiting the generalisability of the effects of any single strategy.\nAdditionally, LLM-defined question difficulty outperforms human-defined labels\nin curriculum-based learning, showing the potential of model-generated data as\na cost-effective alternative for optimising fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning:\n  Principles and Scalability (FITML)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03004v1",
                "updated": "2024-11-05T11:05:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    5,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T11:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    5,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "Controlling for Unobserved Confounding with Large Language Model\n  Classification of Patient Smoking Status",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling for Unobserved Confounding with Large Language Model\n  Classification of Patient Smoking Status"
                },
                "summary": "Causal understanding is a fundamental goal of evidence-based medicine. When\nrandomization is impossible, causal inference methods allow the estimation of\ntreatment effects from retrospective analysis of observational data. However,\nsuch analyses rely on a number of assumptions, often including that of no\nunobserved confounding. In many practical settings, this assumption is violated\nwhen important variables are not explicitly measured in the clinical record.\nPrior work has proposed to address unobserved confounding with machine learning\nby imputing unobserved variables and then correcting for the classifier's\nmismeasurement. When such a classifier can be trained and the necessary\nassumptions are met, this method can recover an unbiased estimate of a causal\neffect. However, such work has been limited to synthetic data, simple\nclassifiers, and binary variables. This paper extends this methodology by using\na large language model trained on clinical notes to predict patients' smoking\nstatus, which would otherwise be an unobserved confounder. We then apply a\nmeasurement error correction on the categorical predicted smoking status to\nestimate the causal effect of transthoracic echocardiography on mortality in\nthe MIMIC dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal understanding is a fundamental goal of evidence-based medicine. When\nrandomization is impossible, causal inference methods allow the estimation of\ntreatment effects from retrospective analysis of observational data. However,\nsuch analyses rely on a number of assumptions, often including that of no\nunobserved confounding. In many practical settings, this assumption is violated\nwhen important variables are not explicitly measured in the clinical record.\nPrior work has proposed to address unobserved confounding with machine learning\nby imputing unobserved variables and then correcting for the classifier's\nmismeasurement. When such a classifier can be trained and the necessary\nassumptions are met, this method can recover an unbiased estimate of a causal\neffect. However, such work has been limited to synthetic data, simple\nclassifiers, and binary variables. This paper extends this methodology by using\na large language model trained on clinical notes to predict patients' smoking\nstatus, which would otherwise be an unobserved confounder. We then apply a\nmeasurement error correction on the categorical predicted smoking status to\nestimate the causal effect of transthoracic echocardiography on mortality in\nthe MIMIC dataset."
                },
                "authors": [
                    {
                        "name": "Samuel Lee"
                    },
                    {
                        "name": "Zach Wood-Doughty"
                    }
                ],
                "author_detail": {
                    "name": "Zach Wood-Doughty"
                },
                "author": "Zach Wood-Doughty",
                "arxiv_comment": "Advancements In Medical Foundation Models: Explainability,\n  Robustness, Security, and Beyond (AIM-FM) at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.02025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.02025v3",
                "updated": "2024-11-05T11:00:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    0,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2022-09-05T15:51:57Z",
                "published_parsed": [
                    2022,
                    9,
                    5,
                    15,
                    51,
                    57,
                    0,
                    248,
                    0
                ],
                "title": "A geometric framework for asymptotic inference of principal subspaces in\n  PCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A geometric framework for asymptotic inference of principal subspaces in\n  PCA"
                },
                "summary": "In this article, we develop an asymptotic method for constructing confidence\nregions for the set of all linear subspaces arising from PCA, from which we\nderive hypothesis tests on this set. Our method is based on the geometry of\nRiemannian manifolds with which some sets of linear subspaces are endowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we develop an asymptotic method for constructing confidence\nregions for the set of all linear subspaces arising from PCA, from which we\nderive hypothesis tests on this set. Our method is based on the geometry of\nRiemannian manifolds with which some sets of linear subspaces are endowed."
                },
                "authors": [
                    {
                        "name": "Dimbihery Rabenoro"
                    },
                    {
                        "name": "Xavier Pennec"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Pennec"
                },
                "author": "Xavier Pennec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.02025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.02025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R30, 60F05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19626v2",
                "updated": "2024-11-05T10:38:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    38,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-28T03:29:33Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    3,
                    29,
                    33,
                    4,
                    180,
                    0
                ],
                "title": "Safety through feedback in Constrained RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety through feedback in Constrained RL"
                },
                "summary": "In safety-critical RL settings, the inclusion of an additional cost function\nis often favoured over the arduous task of modifying the reward function to\nensure the agent's safe behaviour. However, designing or evaluating such a cost\nfunction can be prohibitively expensive. For instance, in the domain of\nself-driving, designing a cost function that encompasses all unsafe behaviours\n(e.g. aggressive lane changes) is inherently complex. In such scenarios, the\ncost function can be learned from feedback collected offline in between\ntraining rounds. This feedback can be system generated or elicited from a human\nobserving the training process. Previous approaches have not been able to scale\nto complex environments and are constrained to receiving feedback at the state\nlevel which can be expensive to collect. To this end, we introduce an approach\nthat scales to more complex domains and extends to beyond state-level feedback,\nthus, reducing the burden on the evaluator. Inferring the cost function in such\nsettings poses challenges, particularly in assigning credit to individual\nstates based on trajectory-level feedback. To address this, we propose a\nsurrogate objective that transforms the problem into a state-level supervised\nclassification task with noisy labels, which can be solved efficiently.\nAdditionally, it is often infeasible to collect feedback on every trajectory\ngenerated by the agent, hence, two fundamental questions arise: (1) Which\ntrajectories should be presented to the human? and (2) How many trajectories\nare necessary for effective learning? To address these questions, we introduce\n\\textit{novelty-based sampling} that selectively involves the evaluator only\nwhen the the agent encounters a \\textit{novel} trajectory. We showcase the\nefficiency of our method through experimentation on several benchmark Safety\nGymnasium environments and realistic self-driving scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In safety-critical RL settings, the inclusion of an additional cost function\nis often favoured over the arduous task of modifying the reward function to\nensure the agent's safe behaviour. However, designing or evaluating such a cost\nfunction can be prohibitively expensive. For instance, in the domain of\nself-driving, designing a cost function that encompasses all unsafe behaviours\n(e.g. aggressive lane changes) is inherently complex. In such scenarios, the\ncost function can be learned from feedback collected offline in between\ntraining rounds. This feedback can be system generated or elicited from a human\nobserving the training process. Previous approaches have not been able to scale\nto complex environments and are constrained to receiving feedback at the state\nlevel which can be expensive to collect. To this end, we introduce an approach\nthat scales to more complex domains and extends to beyond state-level feedback,\nthus, reducing the burden on the evaluator. Inferring the cost function in such\nsettings poses challenges, particularly in assigning credit to individual\nstates based on trajectory-level feedback. To address this, we propose a\nsurrogate objective that transforms the problem into a state-level supervised\nclassification task with noisy labels, which can be solved efficiently.\nAdditionally, it is often infeasible to collect feedback on every trajectory\ngenerated by the agent, hence, two fundamental questions arise: (1) Which\ntrajectories should be presented to the human? and (2) How many trajectories\nare necessary for effective learning? To address these questions, we introduce\n\\textit{novelty-based sampling} that selectively involves the evaluator only\nwhen the the agent encounters a \\textit{novel} trajectory. We showcase the\nefficiency of our method through experimentation on several benchmark Safety\nGymnasium environments and realistic self-driving scenarios."
                },
                "authors": [
                    {
                        "name": "Shashank Reddy Chirra"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    },
                    {
                        "name": "Praveen Paruchuri"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Paruchuri"
                },
                "author": "Praveen Paruchuri",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04691v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04691v4",
                "updated": "2024-11-05T10:32:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    32,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-08T13:10:51Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    10,
                    51,
                    3,
                    221,
                    0
                ],
                "title": "Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL\n  Performance"
                },
                "summary": "Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and text-to-SQL models. In this paper, we explore the use of\nlarge language models (LLMs) to automatically generate detailed natural\nlanguage descriptions for SQL database columns, aiming to improve text-to-SQL\nperformance and automate metadata creation. We create a dataset of gold column\ndescriptions based on the BIRD-Bench benchmark, manually refining its column\ndescriptions and creating a taxonomy for categorizing column difficulty. We\nthen evaluate several different LLMs in generating column descriptions across\nthe columns and different difficulties in the dataset, finding that models\nunsurprisingly struggle with columns that exhibit inherent ambiguity,\nhighlighting the need for manual expert input. We also find that incorporating\nsuch generated column descriptions consistently enhances text-to-SQL model\nperformance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral\n22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed\nsuperfluous information, outperform manually curated gold descriptions,\nsuggesting that models benefit from more detailed metadata than humans expect.\nFuture work will investigate the specific features of these high-performing\ndescriptions and explore other types of metadata, such as numerical reasoning\nand synonyms, to further improve text-to-SQL systems. The dataset, annotations\nand code will all be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and text-to-SQL models. In this paper, we explore the use of\nlarge language models (LLMs) to automatically generate detailed natural\nlanguage descriptions for SQL database columns, aiming to improve text-to-SQL\nperformance and automate metadata creation. We create a dataset of gold column\ndescriptions based on the BIRD-Bench benchmark, manually refining its column\ndescriptions and creating a taxonomy for categorizing column difficulty. We\nthen evaluate several different LLMs in generating column descriptions across\nthe columns and different difficulties in the dataset, finding that models\nunsurprisingly struggle with columns that exhibit inherent ambiguity,\nhighlighting the need for manual expert input. We also find that incorporating\nsuch generated column descriptions consistently enhances text-to-SQL model\nperformance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral\n22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed\nsuperfluous information, outperform manually curated gold descriptions,\nsuggesting that models benefit from more detailed metadata than humans expect.\nFuture work will investigate the specific features of these high-performing\ndescriptions and explore other types of metadata, such as numerical reasoning\nand synonyms, to further improve text-to-SQL systems. The dataset, annotations\nand code will all be made available."
                },
                "authors": [
                    {
                        "name": "Niklas Wretblad"
                    },
                    {
                        "name": "Oskar Holmstrm"
                    },
                    {
                        "name": "Erik Larsson"
                    },
                    {
                        "name": "Axel Wikster"
                    },
                    {
                        "name": "Oscar Sderlund"
                    },
                    {
                        "name": "Hjalmar hman"
                    },
                    {
                        "name": "Ture Pontn"
                    },
                    {
                        "name": "Martin Forsberg"
                    },
                    {
                        "name": "Martin Srme"
                    },
                    {
                        "name": "Fredrik Heintz"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Heintz"
                },
                "author": "Fredrik Heintz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04691v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04691v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12034v2",
                "updated": "2024-11-05T10:24:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    24,
                    42,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-30T22:18:49Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    22,
                    18,
                    49,
                    6,
                    182,
                    0
                ],
                "title": "Understanding Transformers via N-gram Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Transformers via N-gram Statistics"
                },
                "summary": "Transformer based large-language models (LLMs) display extreme proficiency\nwith language yet a precise understanding of how they work remains elusive. One\nway of demystifying transformer predictions would be to describe how they\ndepend on their context in terms of simple template functions. This paper takes\na first step in this direction by considering families of functions (i.e.\nrules) formed out of simple N-gram based statistics of the training data. By\nstudying how well these rulesets approximate transformer predictions, we obtain\na variety of novel discoveries: a simple method to detect overfitting during\ntraining without using a holdout set, a quantitative measure of how\ntransformers progress from learning simple to more complex statistical rules\nover the course of training, a model-variance criterion governing when\ntransformer predictions tend to be described by N-gram rules, and insights into\nhow well transformers can be approximated by N-gram rulesets in the limit where\nthese rulesets become increasingly complex. In this latter direction, we find\nthat for 79% and 68% of LLM next-token distributions on TinyStories and\nWikipedia, respectively, their top-1 predictions agree with those provided by\nour N-gram rulesets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer based large-language models (LLMs) display extreme proficiency\nwith language yet a precise understanding of how they work remains elusive. One\nway of demystifying transformer predictions would be to describe how they\ndepend on their context in terms of simple template functions. This paper takes\na first step in this direction by considering families of functions (i.e.\nrules) formed out of simple N-gram based statistics of the training data. By\nstudying how well these rulesets approximate transformer predictions, we obtain\na variety of novel discoveries: a simple method to detect overfitting during\ntraining without using a holdout set, a quantitative measure of how\ntransformers progress from learning simple to more complex statistical rules\nover the course of training, a model-variance criterion governing when\ntransformer predictions tend to be described by N-gram rules, and insights into\nhow well transformers can be approximated by N-gram rulesets in the limit where\nthese rulesets become increasingly complex. In this latter direction, we find\nthat for 79% and 68% of LLM next-token distributions on TinyStories and\nWikipedia, respectively, their top-1 predictions agree with those provided by\nour N-gram rulesets."
                },
                "authors": [
                    {
                        "name": "Timothy Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Nguyen"
                },
                "author": "Timothy Nguyen",
                "arxiv_comment": "NeurIPS 2024. Datasets and N-gram statistics open-sourced:\n  https://github.com/google-deepmind/transformer_ngrams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02973v1",
                "updated": "2024-11-05T10:18:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    18,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T10:18:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    18,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for\n  Diabetic Retinopathy using Chatbots and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for\n  Diabetic Retinopathy using Chatbots and Generative AI"
                },
                "summary": "We present an outline of the first large language model (LLM) based chatbot\napplication in the context of patient-reported outcome measures (PROMs) for\ndiabetic retinopathy. By utilizing the capabilities of current LLMs, we enable\npatients to provide feedback about their quality of life and treatment progress\nvia an interactive application. The proposed framework offers significant\nadvantages over the current approach, which encompasses only qualitative\ncollection of survey data or a static survey with limited answer options. Using\nthe PROBot LLM-PROM application, patients will be asked tailored questions\nabout their individual challenges, and can give more detailed feedback on the\nprogress of their treatment. Based on this input, we will use machine learning\nto infer conventional PROM scores, which can be used by clinicians to evaluate\nthe treatment status. The goal of the application is to improve adherence to\nthe healthcare system and treatments, and thus ultimately reduce cases of\nsubsequent vision impairment. The approach needs to be further validated using\na survey and a clinical study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an outline of the first large language model (LLM) based chatbot\napplication in the context of patient-reported outcome measures (PROMs) for\ndiabetic retinopathy. By utilizing the capabilities of current LLMs, we enable\npatients to provide feedback about their quality of life and treatment progress\nvia an interactive application. The proposed framework offers significant\nadvantages over the current approach, which encompasses only qualitative\ncollection of survey data or a static survey with limited answer options. Using\nthe PROBot LLM-PROM application, patients will be asked tailored questions\nabout their individual challenges, and can give more detailed feedback on the\nprogress of their treatment. Based on this input, we will use machine learning\nto infer conventional PROM scores, which can be used by clinicians to evaluate\nthe treatment status. The goal of the application is to improve adherence to\nthe healthcare system and treatments, and thus ultimately reduce cases of\nsubsequent vision impairment. The approach needs to be further validated using\na survey and a clinical study."
                },
                "authors": [
                    {
                        "name": "Maren Pielka"
                    },
                    {
                        "name": "Tobias Schneider"
                    },
                    {
                        "name": "Jan Terheyden"
                    },
                    {
                        "name": "Rafet Sifa"
                    }
                ],
                "author_detail": {
                    "name": "Rafet Sifa"
                },
                "author": "Rafet Sifa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02969v1",
                "updated": "2024-11-05T10:13:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    13,
                    23,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T10:13:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    13,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation"
                },
                "summary": "LiDAR Semantic Segmentation is a fundamental task in autonomous driving\nperception consisting of associating each LiDAR point to a semantic label.\nFully-supervised models have widely tackled this task, but they require labels\nfor each scan, which either limits their domain or requires impractical amounts\nof expensive annotations. Camera images, which are generally recorded alongside\nLiDAR pointclouds, can be processed by the widely available 2D foundation\nmodels, which are generic and dataset-agnostic. However, distilling knowledge\nfrom 2D data to improve LiDAR perception raises domain adaptation challenges.\nFor example, the classical perspective projection suffers from the parallax\neffect produced by the position shift between both sensors at their respective\ncapture times. We propose a Semi-Supervised Learning setup to leverage\nunlabeled LiDAR pointclouds alongside distilled knowledge from the camera\nimages. To self-supervise our model on the unlabeled scans, we add an auxiliary\nNeRF head and cast rays from the camera viewpoint over the unlabeled voxel\nfeatures. The NeRF head predicts densities and semantic logits at each sampled\nray location which are used for rendering pixel semantics. Concurrently, we\nquery the Segment-Anything (SAM) foundation model with the camera image to\ngenerate a set of unlabeled generic masks. We fuse the masks with the rendered\npixel semantics from LiDAR to produce pseudo-labels that supervise the pixel\npredictions. During inference, we drop the NeRF head and run our model with\nonly LiDAR. We show the effectiveness of our approach in three public LiDAR\nSemantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR Semantic Segmentation is a fundamental task in autonomous driving\nperception consisting of associating each LiDAR point to a semantic label.\nFully-supervised models have widely tackled this task, but they require labels\nfor each scan, which either limits their domain or requires impractical amounts\nof expensive annotations. Camera images, which are generally recorded alongside\nLiDAR pointclouds, can be processed by the widely available 2D foundation\nmodels, which are generic and dataset-agnostic. However, distilling knowledge\nfrom 2D data to improve LiDAR perception raises domain adaptation challenges.\nFor example, the classical perspective projection suffers from the parallax\neffect produced by the position shift between both sensors at their respective\ncapture times. We propose a Semi-Supervised Learning setup to leverage\nunlabeled LiDAR pointclouds alongside distilled knowledge from the camera\nimages. To self-supervise our model on the unlabeled scans, we add an auxiliary\nNeRF head and cast rays from the camera viewpoint over the unlabeled voxel\nfeatures. The NeRF head predicts densities and semantic logits at each sampled\nray location which are used for rendering pixel semantics. Concurrently, we\nquery the Segment-Anything (SAM) foundation model with the camera image to\ngenerate a set of unlabeled generic masks. We fuse the masks with the rendered\npixel semantics from LiDAR to produce pseudo-labels that supervise the pixel\npredictions. During inference, we drop the NeRF head and run our model with\nonly LiDAR. We show the effectiveness of our approach in three public LiDAR\nSemantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI."
                },
                "authors": [
                    {
                        "name": "Xavier Timoneda"
                    },
                    {
                        "name": "Markus Herb"
                    },
                    {
                        "name": "Fabian Duerr"
                    },
                    {
                        "name": "Daniel Goehring"
                    },
                    {
                        "name": "Fisher Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fisher Yu"
                },
                "author": "Fisher Yu",
                "arxiv_comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02959v1",
                "updated": "2024-11-05T09:58:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    58,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:58:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    58,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial\nsystems such as ChatGPT and Perplexity have used Web search engines as their\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\ndownload HTML sources of the results, and then extract plain texts from the\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\nthe generation. However, much of the structural and semantic information\ninherent in HTML, such as headings and table structures, is lost during this\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\nRAG. We believe HTML is better than plain text in modeling knowledge in\nexternal documents, and most LLMs possess robust capacities to understand HTML.\nHowever, utilizing HTML presents new challenges. HTML contains additional\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\ninput tokens and noise to the RAG system. To address this issue, we propose\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\nminimizing the loss of information. Specifically, we design a two-step\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial\nsystems such as ChatGPT and Perplexity have used Web search engines as their\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\ndownload HTML sources of the results, and then extract plain texts from the\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\nthe generation. However, much of the structural and semantic information\ninherent in HTML, such as headings and table structures, is lost during this\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\nRAG. We believe HTML is better than plain text in modeling knowledge in\nexternal documents, and most LLMs possess robust capacities to understand HTML.\nHowever, utilizing HTML presents new challenges. HTML contains additional\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\ninput tokens and noise to the RAG system. To address this issue, we propose\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\nminimizing the loss of information. Specifically, we design a two-step\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems."
                },
                "authors": [
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16398v2",
                "updated": "2024-11-05T09:55:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    55,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-02-26T08:47:35Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    8,
                    47,
                    35,
                    0,
                    57,
                    0
                ],
                "title": "AsynEVO: Asynchronous Event-Driven Visual Odometry for Pure Event\n  Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsynEVO: Asynchronous Event-Driven Visual Odometry for Pure Event\n  Streams"
                },
                "summary": "Event cameras are bio-inspired vision sensors that asynchronously measure\nper-pixel brightness changes.The high-temporal resolution and asynchronicity of\nevent cameras offer great potential for estimating robot motion states. Recent\nworks have adopted the continuous-time estimation methods to exploit the\ninherent nature of event cameras. However, existing methods either have poor\nruntime performance or neglect the high-temporal resolution of event cameras.\nTo alleviate it, an Asynchronous Event-driven Visual Odometry (AsynEVO) based\non sparse Gaussian Process (GP) regression is proposed to efficiently infer the\nmotion trajectory from pure event streams. Concretely, an asynchronous frontend\npipeline is designed to adapt event-driven feature tracking and manage feature\ntrajectories; a parallel dynamic sliding-window backend is presented within the\nframework of sparse GP regression on $SE(3)$. Notably, a dynamic\nmarginalization strategy is employed to ensure the consistency and sparsity of\nthis GP regression. Experiments conducted on public datasets and real-world\nscenarios demonstrate that AsynEVO achieves competitive precision and superior\nrobustness compared to the state-of-the-art.The experiment in the\nrepeated-texture scenario indicates that the high-temporal resolution of\nAsynEVO plays a vital role in the estimation of high-speed movement.\nFurthermore, we show that the computational efficiency of AsynEVO significantly\noutperforms the incremental method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras are bio-inspired vision sensors that asynchronously measure\nper-pixel brightness changes.The high-temporal resolution and asynchronicity of\nevent cameras offer great potential for estimating robot motion states. Recent\nworks have adopted the continuous-time estimation methods to exploit the\ninherent nature of event cameras. However, existing methods either have poor\nruntime performance or neglect the high-temporal resolution of event cameras.\nTo alleviate it, an Asynchronous Event-driven Visual Odometry (AsynEVO) based\non sparse Gaussian Process (GP) regression is proposed to efficiently infer the\nmotion trajectory from pure event streams. Concretely, an asynchronous frontend\npipeline is designed to adapt event-driven feature tracking and manage feature\ntrajectories; a parallel dynamic sliding-window backend is presented within the\nframework of sparse GP regression on $SE(3)$. Notably, a dynamic\nmarginalization strategy is employed to ensure the consistency and sparsity of\nthis GP regression. Experiments conducted on public datasets and real-world\nscenarios demonstrate that AsynEVO achieves competitive precision and superior\nrobustness compared to the state-of-the-art.The experiment in the\nrepeated-texture scenario indicates that the high-temporal resolution of\nAsynEVO plays a vital role in the estimation of high-speed movement.\nFurthermore, we show that the computational efficiency of AsynEVO significantly\noutperforms the incremental method."
                },
                "authors": [
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Xudong Li"
                    },
                    {
                        "name": "Yizhai Zhang"
                    },
                    {
                        "name": "Panfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panfeng Huang"
                },
                "author": "Panfeng Huang",
                "arxiv_comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (2024-07-15)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02951v1",
                "updated": "2024-11-05T09:51:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    51,
                    59,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:51:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    51,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent\n  Diffusion Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent\n  Diffusion Prior"
                },
                "summary": "Diffusion model, as a powerful generative model, has found a wide range of\napplications including MRI reconstruction. However, most existing diffusion\nmodel-based MRI reconstruction methods operate directly in pixel space, which\nmakes their optimization and inference computationally expensive. Latent\ndiffusion models were introduced to address this problem in natural image\nprocessing, but directly applying them to MRI reconstruction still faces many\nchallenges, including the lack of control over the generated results, the\nadaptability of Variational AutoEncoder (VAE) to MRI, and the exploration of\napplicable data consistency in latent space. To address these challenges, a\nLatent Diffusion Prior based undersampled MRI reconstruction (LDPM) method is\nproposed. A sketcher module is utilized to provide appropriate control and\nbalance the quality and fidelity of the reconstructed MR images. A VAE adapted\nfor MRI tasks (MR-VAE) is explored, which can serve as the backbone for future\nMR-related tasks. Furthermore, a variation of the DDIM sampler, called the\nDual-Stage Sampler, is proposed to achieve high-fidelity reconstruction in the\nlatent space. The proposed method achieves competitive results on fastMRI\ndatasets, and the effectiveness of each module is demonstrated in ablation\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion model, as a powerful generative model, has found a wide range of\napplications including MRI reconstruction. However, most existing diffusion\nmodel-based MRI reconstruction methods operate directly in pixel space, which\nmakes their optimization and inference computationally expensive. Latent\ndiffusion models were introduced to address this problem in natural image\nprocessing, but directly applying them to MRI reconstruction still faces many\nchallenges, including the lack of control over the generated results, the\nadaptability of Variational AutoEncoder (VAE) to MRI, and the exploration of\napplicable data consistency in latent space. To address these challenges, a\nLatent Diffusion Prior based undersampled MRI reconstruction (LDPM) method is\nproposed. A sketcher module is utilized to provide appropriate control and\nbalance the quality and fidelity of the reconstructed MR images. A VAE adapted\nfor MRI tasks (MR-VAE) is explored, which can serve as the backbone for future\nMR-related tasks. Furthermore, a variation of the DDIM sampler, called the\nDual-Stage Sampler, is proposed to achieve high-fidelity reconstruction in the\nlatent space. The proposed method achieves competitive results on fastMRI\ndatasets, and the effectiveness of each module is demonstrated in ablation\nexperiments."
                },
                "authors": [
                    {
                        "name": "Xingjian Tang"
                    },
                    {
                        "name": "Jingwei Guan"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Youmei Zhang"
                    },
                    {
                        "name": "Mengye Lyu"
                    },
                    {
                        "name": "Li Yan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yan"
                },
                "author": "Li Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02949v1",
                "updated": "2024-11-05T09:45:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    45,
                    57,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:45:57Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    45,
                    57,
                    1,
                    310,
                    0
                ],
                "title": "A scalable generative model for dynamical system reconstruction from\n  neuroimaging data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scalable generative model for dynamical system reconstruction from\n  neuroimaging data"
                },
                "summary": "Data-driven inference of the generative dynamics underlying a set of observed\ntime series is of growing interest in machine learning and the natural\nsciences. In neuroscience, such methods promise to alleviate the need to\nhandcraft models based on biophysical principles and allow to automatize the\ninference of inter-individual differences in brain dynamics. Recent\nbreakthroughs in training techniques for state space models (SSMs) specifically\ngeared toward dynamical systems (DS) reconstruction (DSR) enable to recover the\nunderlying system including its geometrical (attractor) and long-term\nstatistical invariants from even short time series. These techniques are based\non control-theoretic ideas, like modern variants of teacher forcing (TF), to\nensure stable loss gradient propagation while training. However, as it\ncurrently stands, these techniques are not directly applicable to data\nmodalities where current observations depend on an entire history of previous\nstates due to a signal's filtering properties, as common in neuroscience (and\nphysiology more generally). Prominent examples are the blood oxygenation level\ndependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or\nCa$^{2+}$ imaging data. Such types of signals render the SSM's decoder model\nnon-invertible, a requirement for previous TF-based methods. Here, exploiting\nthe recent success of control techniques for training SSMs, we propose a novel\nalgorithm that solves this problem and scales exceptionally well with model\ndimensionality and filter length. We demonstrate its efficiency in\nreconstructing dynamical systems, including their state space geometry and\nlong-term temporal properties, from just short BOLD time series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven inference of the generative dynamics underlying a set of observed\ntime series is of growing interest in machine learning and the natural\nsciences. In neuroscience, such methods promise to alleviate the need to\nhandcraft models based on biophysical principles and allow to automatize the\ninference of inter-individual differences in brain dynamics. Recent\nbreakthroughs in training techniques for state space models (SSMs) specifically\ngeared toward dynamical systems (DS) reconstruction (DSR) enable to recover the\nunderlying system including its geometrical (attractor) and long-term\nstatistical invariants from even short time series. These techniques are based\non control-theoretic ideas, like modern variants of teacher forcing (TF), to\nensure stable loss gradient propagation while training. However, as it\ncurrently stands, these techniques are not directly applicable to data\nmodalities where current observations depend on an entire history of previous\nstates due to a signal's filtering properties, as common in neuroscience (and\nphysiology more generally). Prominent examples are the blood oxygenation level\ndependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or\nCa$^{2+}$ imaging data. Such types of signals render the SSM's decoder model\nnon-invertible, a requirement for previous TF-based methods. Here, exploiting\nthe recent success of control techniques for training SSMs, we propose a novel\nalgorithm that solves this problem and scales exceptionally well with model\ndimensionality and filter length. We demonstrate its efficiency in\nreconstructing dynamical systems, including their state space geometry and\nlong-term temporal properties, from just short BOLD time series."
                },
                "authors": [
                    {
                        "name": "Eric Volkmann"
                    },
                    {
                        "name": "Alena Brndle"
                    },
                    {
                        "name": "Daniel Durstewitz"
                    },
                    {
                        "name": "Georgia Koppe"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Koppe"
                },
                "author": "Georgia Koppe",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02327v2",
                "updated": "2024-11-05T09:43:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    43,
                    59,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T17:50:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    50,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance"
                },
                "summary": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA."
                },
                "authors": [
                    {
                        "name": "Ruyang Liu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haibo Liu"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Jiankun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jiankun Yang"
                },
                "author": "Jiankun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14751v2",
                "updated": "2024-11-05T09:42:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    42,
                    40,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-23T16:17:44Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    17,
                    44,
                    3,
                    144,
                    0
                ],
                "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents"
                },
                "summary": "We introduce a novel reinforcement learning framework of LLM agents named\nAGILE (AGent that Interacts and Learns from Environments) designed to perform\ncomplex conversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent possesses capabilities beyond\nconversation, including reflection, tool usage, and expert consultation. We\nformulate the construction of such an LLM agent as a reinforcement learning\n(RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs\ntrained with PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance. Datasets and code are\navailable at https://github.com/bytarnish/AGILE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel reinforcement learning framework of LLM agents named\nAGILE (AGent that Interacts and Learns from Environments) designed to perform\ncomplex conversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent possesses capabilities beyond\nconversation, including reflection, tool usage, and expert consultation. We\nformulate the construction of such an LLM agent as a reinforcement learning\n(RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs\ntrained with PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance. Datasets and code are\navailable at https://github.com/bytarnish/AGILE."
                },
                "authors": [
                    {
                        "name": "Peiyuan Feng"
                    },
                    {
                        "name": "Yichen He"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Hanchong Zhang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Hang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hang Li"
                },
                "author": "Hang Li",
                "arxiv_comment": "accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02943v1",
                "updated": "2024-11-05T09:37:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    37,
                    23,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:37:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    37,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach"
                },
                "summary": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents."
                },
                "authors": [
                    {
                        "name": "Francesco Invernici"
                    },
                    {
                        "name": "Francesca Curati"
                    },
                    {
                        "name": "Jelena Jakimov"
                    },
                    {
                        "name": "Amirhossein Samavi"
                    },
                    {
                        "name": "Anna Bernasconi"
                    }
                ],
                "author_detail": {
                    "name": "Anna Bernasconi"
                },
                "author": "Anna Bernasconi",
                "arxiv_comment": "27 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02941v1",
                "updated": "2024-11-05T09:34:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    34,
                    5,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:34:05Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    34,
                    5,
                    1,
                    310,
                    0
                ],
                "title": "A Mamba Foundation Model for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mamba Foundation Model for Time Series Forecasting"
                },
                "summary": "Time series foundation models have demonstrated strong performance in\nzero-shot learning, making them well-suited for predicting rapidly evolving\npatterns in real-world applications where relevant training data are scarce.\nHowever, most of these models rely on the Transformer architecture, which\nincurs quadratic complexity as input length increases. To address this, we\nintroduce TSMamba, a linear-complexity foundation model for time series\nforecasting built on the Mamba architecture. The model captures temporal\ndependencies through both forward and backward Mamba encoders, achieving high\nprediction accuracy. To reduce reliance on large datasets and lower training\ncosts, TSMamba employs a two-stage transfer learning process that leverages\npretrained Mamba LLMs, allowing effective time series modeling with a moderate\ntraining set. In the first stage, the forward and backward backbones are\noptimized via patch-wise autoregressive prediction; in the second stage, the\nmodel trains a prediction head and refines other components for long-term\nforecasting. While the backbone assumes channel independence to manage varying\nchannel numbers across datasets, a channel-wise compressed attention module is\nintroduced to capture cross-channel dependencies during fine-tuning on specific\nmultivariate datasets. Experiments show that TSMamba's zero-shot performance is\ncomparable to state-of-the-art time series foundation models, despite using\nsignificantly less training data. It also achieves competitive or superior\nfull-shot performance compared to task-specific prediction models. The code\nwill be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series foundation models have demonstrated strong performance in\nzero-shot learning, making them well-suited for predicting rapidly evolving\npatterns in real-world applications where relevant training data are scarce.\nHowever, most of these models rely on the Transformer architecture, which\nincurs quadratic complexity as input length increases. To address this, we\nintroduce TSMamba, a linear-complexity foundation model for time series\nforecasting built on the Mamba architecture. The model captures temporal\ndependencies through both forward and backward Mamba encoders, achieving high\nprediction accuracy. To reduce reliance on large datasets and lower training\ncosts, TSMamba employs a two-stage transfer learning process that leverages\npretrained Mamba LLMs, allowing effective time series modeling with a moderate\ntraining set. In the first stage, the forward and backward backbones are\noptimized via patch-wise autoregressive prediction; in the second stage, the\nmodel trains a prediction head and refines other components for long-term\nforecasting. While the backbone assumes channel independence to manage varying\nchannel numbers across datasets, a channel-wise compressed attention module is\nintroduced to capture cross-channel dependencies during fine-tuning on specific\nmultivariate datasets. Experiments show that TSMamba's zero-shot performance is\ncomparable to state-of-the-art time series foundation models, despite using\nsignificantly less training data. It also achieves competitive or superior\nfull-shot performance compared to task-specific prediction models. The code\nwill be made publicly available."
                },
                "authors": [
                    {
                        "name": "Haoyu Ma"
                    },
                    {
                        "name": "Yushu Chen"
                    },
                    {
                        "name": "Wenlai Zhao"
                    },
                    {
                        "name": "Jinzhe Yang"
                    },
                    {
                        "name": "Yingsheng Ji"
                    },
                    {
                        "name": "Xinghua Xu"
                    },
                    {
                        "name": "Xiaozhu Liu"
                    },
                    {
                        "name": "Hao Jing"
                    },
                    {
                        "name": "Shengzhuo Liu"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02938v1",
                "updated": "2024-11-05T09:31:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    31,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:31:30Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    31,
                    30,
                    1,
                    310,
                    0
                ],
                "title": "Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments"
                },
                "summary": "The advent of generalist Large Language Models (LLMs) and Large Vision Models\n(VLMs) have streamlined the construction of semantically enriched maps that can\nenable robots to ground high-level reasoning and planning into their\nrepresentations. One of the most widely used semantic map formats is the 3D\nScene Graph, which captures both metric (low-level) and semantic (high-level)\ninformation. However, these maps often assume a static world, while real\nenvironments, like homes and offices, are dynamic. Even small changes in these\nspaces can significantly impact task performance. To integrate robots into\ndynamic environments, they must detect changes and update the scene graph in\nreal-time. This update process is inherently multimodal, requiring input from\nvarious sources, such as human agents, the robot's own perception system, time,\nand its actions. This work proposes a framework that leverages these multimodal\ninputs to maintain the consistency of scene graphs during real-time operation,\npresenting promising initial results and outlining a roadmap for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of generalist Large Language Models (LLMs) and Large Vision Models\n(VLMs) have streamlined the construction of semantically enriched maps that can\nenable robots to ground high-level reasoning and planning into their\nrepresentations. One of the most widely used semantic map formats is the 3D\nScene Graph, which captures both metric (low-level) and semantic (high-level)\ninformation. However, these maps often assume a static world, while real\nenvironments, like homes and offices, are dynamic. Even small changes in these\nspaces can significantly impact task performance. To integrate robots into\ndynamic environments, they must detect changes and update the scene graph in\nreal-time. This update process is inherently multimodal, requiring input from\nvarious sources, such as human agents, the robot's own perception system, time,\nand its actions. This work proposes a framework that leverages these multimodal\ninputs to maintain the consistency of scene graphs during real-time operation,\npresenting promising initial results and outlining a roadmap for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Emilio Olivastri"
                    },
                    {
                        "name": "Jonathan Francis"
                    },
                    {
                        "name": "Alberto Pretto"
                    },
                    {
                        "name": "Niko Snderhauf"
                    },
                    {
                        "name": "Krishan Rana"
                    }
                ],
                "author_detail": {
                    "name": "Krishan Rana"
                },
                "author": "Krishan Rana",
                "arxiv_comment": "This paper has been accepted at the Workshop on Lifelong Learning for\n  Home Robots at the 8th Conference on Robot Learning (CoRL 2024), Munich,\n  Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02930v1",
                "updated": "2024-11-05T09:22:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    22,
                    8,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:22:08Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    22,
                    8,
                    1,
                    310,
                    0
                ],
                "title": "Textual Aesthetics in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Aesthetics in Large Language Models"
                },
                "summary": "Image aesthetics is a crucial metric in the field of image generation.\nHowever, textual aesthetics has not been sufficiently explored. With the\nwidespread application of large language models (LLMs), previous work has\nprimarily focused on the correctness of content and the helpfulness of\nresponses. Nonetheless, providing responses with textual aesthetics is also an\nimportant factor for LLMs, which can offer a cleaner layout and ensure greater\nconsistency and coherence in content. In this work, we introduce a pipeline for\naesthetics polishing and help construct a textual aesthetics dataset named\nTexAes. We propose a textual aesthetics-powered fine-tuning method based on\ndirect preference optimization, termed TAPO, which leverages textual aesthetics\nwithout compromising content correctness. Additionally, we develop two\nevaluation methods for textual aesthetics based on text and image analysis,\nrespectively. Our experiments demonstrate that using textual aesthetics data\nand employing the TAPO fine-tuning method not only improves aesthetic scores\nbut also enhances performance on general evaluation datasets such as\nAlpacalEval and Anera-hard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image aesthetics is a crucial metric in the field of image generation.\nHowever, textual aesthetics has not been sufficiently explored. With the\nwidespread application of large language models (LLMs), previous work has\nprimarily focused on the correctness of content and the helpfulness of\nresponses. Nonetheless, providing responses with textual aesthetics is also an\nimportant factor for LLMs, which can offer a cleaner layout and ensure greater\nconsistency and coherence in content. In this work, we introduce a pipeline for\naesthetics polishing and help construct a textual aesthetics dataset named\nTexAes. We propose a textual aesthetics-powered fine-tuning method based on\ndirect preference optimization, termed TAPO, which leverages textual aesthetics\nwithout compromising content correctness. Additionally, we develop two\nevaluation methods for textual aesthetics based on text and image analysis,\nrespectively. Our experiments demonstrate that using textual aesthetics data\nand employing the TAPO fine-tuning method not only improves aesthetic scores\nbut also enhances performance on general evaluation datasets such as\nAlpacalEval and Anera-hard."
                },
                "authors": [
                    {
                        "name": "Lingjie Jiang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02926v1",
                "updated": "2024-11-05T09:13:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    13,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:13:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    13,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering"
                },
                "summary": "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Fabrianne Effendi"
                    },
                    {
                        "name": "Anupam Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anupam Chattopadhyay"
                },
                "author": "Anupam Chattopadhyay",
                "arxiv_comment": "14th International Conference on Security, Privacy, and Applied\n  Cryptographic Engineering (SPACE) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02924v1",
                "updated": "2024-11-05T09:11:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    11,
                    34,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:11:34Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    11,
                    34,
                    1,
                    310,
                    0
                ],
                "title": "A joint model of correlated ordinal and continuous variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A joint model of correlated ordinal and continuous variables"
                },
                "summary": "In this paper we build a joint model which can accommodate for binary,\nordinal and continuous responses, by assuming that the errors of the continuous\nvariables and the errors underlying the ordinal and binary outcomes follow a\nmultivariate normal distribution. We employ composite likelihood methods to\nestimate the model parameters and use composite likelihood inference for model\ncomparison and uncertainty quantification. The complimentary R package\nmvordnorm implements estimation of this model using composite likelihood\nmethods and is available for download from Github. We present two use-cases in\nthe area of risk management to illustrate our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we build a joint model which can accommodate for binary,\nordinal and continuous responses, by assuming that the errors of the continuous\nvariables and the errors underlying the ordinal and binary outcomes follow a\nmultivariate normal distribution. We employ composite likelihood methods to\nestimate the model parameters and use composite likelihood inference for model\ncomparison and uncertainty quantification. The complimentary R package\nmvordnorm implements estimation of this model using composite likelihood\nmethods and is available for download from Github. We present two use-cases in\nthe area of risk management to illustrate our approach."
                },
                "authors": [
                    {
                        "name": "Laura Vana-Gr"
                    },
                    {
                        "name": "Rainer Hirk"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Hirk"
                },
                "author": "Rainer Hirk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13517v2",
                "updated": "2024-11-05T09:08:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    8,
                    28,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-17T13:06:02Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    13,
                    6,
                    2,
                    3,
                    291,
                    0
                ],
                "title": "Bias in the Mirror: Are LLMs opinions robust to their own adversarial\n  attacks ?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in the Mirror: Are LLMs opinions robust to their own adversarial\n  attacks ?"
                },
                "summary": "Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts."
                },
                "authors": [
                    {
                        "name": "Virgile Rennard"
                    },
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02839v3",
                "updated": "2024-11-05T09:07:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    7,
                    22,
                    1,
                    310,
                    0
                ],
                "published": "2024-03-05T10:20:52Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    10,
                    20,
                    52,
                    1,
                    65,
                    0
                ],
                "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned\n  Judge Model is not a General Substitute for GPT-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned\n  Judge Model is not a General Substitute for GPT-4"
                },
                "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-sourced models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this work, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\nintroduce a integrated method, leveraging GPT-4 to compensate for the\nlimitations and improve the fine-tuned judges. Experiment results show our\nmethod achieves accuracy on par with GPT-4 with only 50% of the API expense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-sourced models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this work, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\nintroduce a integrated method, leveraging GPT-4 to compensate for the\nlimitations and improve the fine-tuned judges. Experiment results show our\nmethod achieves accuracy on par with GPT-4 with only 50% of the API expense."
                },
                "authors": [
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Yingqi Qu"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Hongli Zhou"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Tiejun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Zhao"
                },
                "author": "Tiejun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02909v1",
                "updated": "2024-11-05T08:48:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    48,
                    49,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T08:48:49Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    48,
                    49,
                    1,
                    310,
                    0
                ],
                "title": "When is it worthwhile to jackknife? Breaking the quadratic barrier for\n  Z-estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When is it worthwhile to jackknife? Breaking the quadratic barrier for\n  Z-estimators"
                },
                "summary": "Resampling methods are especially well-suited to inference with estimators\nthat provide only \"black-box'' access. Jackknife is a form of resampling,\nwidely used for bias correction and variance estimation, that is\nwell-understood under classical scaling where the sample size $n$ grows for a\nfixed problem. We study its behavior in application to estimating functionals\nusing high-dimensional $Z$-estimators, allowing both the sample size $n$ and\nproblem dimension $d$ to diverge. We begin showing that the plug-in estimator\nbased on the $Z$-estimate suffers from a quadratic breakdown: while it is\n$\\sqrt{n}$-consistent and asymptotically normal whenever $n \\gtrsim d^2$, it\nfails for a broad class of problems whenever $n \\lesssim d^2$. We then show\nthat under suitable regularity conditions, applying a jackknife correction\nyields an estimate that is $\\sqrt{n}$-consistent and asymptotically normal\nwhenever $n\\gtrsim d^{3/2}$. This provides strong motivation for the use of\njackknife in high-dimensional problems where the dimension is moderate relative\nto sample size. We illustrate consequences of our general theory for various\nspecific $Z$-estimators, including non-linear functionals in linear models;\ngeneralized linear models; and the inverse propensity score weighting (IPW)\nestimate for the average treatment effect, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resampling methods are especially well-suited to inference with estimators\nthat provide only \"black-box'' access. Jackknife is a form of resampling,\nwidely used for bias correction and variance estimation, that is\nwell-understood under classical scaling where the sample size $n$ grows for a\nfixed problem. We study its behavior in application to estimating functionals\nusing high-dimensional $Z$-estimators, allowing both the sample size $n$ and\nproblem dimension $d$ to diverge. We begin showing that the plug-in estimator\nbased on the $Z$-estimate suffers from a quadratic breakdown: while it is\n$\\sqrt{n}$-consistent and asymptotically normal whenever $n \\gtrsim d^2$, it\nfails for a broad class of problems whenever $n \\lesssim d^2$. We then show\nthat under suitable regularity conditions, applying a jackknife correction\nyields an estimate that is $\\sqrt{n}$-consistent and asymptotically normal\nwhenever $n\\gtrsim d^{3/2}$. This provides strong motivation for the use of\njackknife in high-dimensional problems where the dimension is moderate relative\nto sample size. We illustrate consequences of our general theory for various\nspecific $Z$-estimators, including non-linear functionals in linear models;\ngeneralized linear models; and the inverse propensity score weighting (IPW)\nestimate for the average treatment effect, among others."
                },
                "authors": [
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Fangzhou Su"
                    },
                    {
                        "name": "Wenlong Mou"
                    },
                    {
                        "name": "Peng Ding"
                    },
                    {
                        "name": "Martin Wainwright"
                    }
                ],
                "author_detail": {
                    "name": "Martin Wainwright"
                },
                "author": "Martin Wainwright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02908v1",
                "updated": "2024-11-05T08:48:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    48,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T08:48:25Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    48,
                    25,
                    1,
                    310,
                    0
                ],
                "title": "Photon: Federated LLM Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photon: Federated LLM Pre-Training"
                },
                "summary": "Scaling large language models (LLMs) demands extensive data and computing\nresources, which are traditionally constrained to data centers by the\nhigh-bandwidth requirements of distributed training. Low-bandwidth methods like\nfederated learning (FL) could enable collaborative training of larger models\nacross weakly-connected GPUs if they can effectively be used for pre-training.\nTo achieve this, we introduce Photon, the first complete system for federated\nend-to-end LLM training, leveraging cross-silo FL for global-scale training\nwith minimal communication overheads. Using Photon, we train the first\nfederated family of decoder-only LLMs from scratch. We show that: (1) Photon\ncan train model sizes up to 7B in a federated fashion while reaching an even\nbetter perplexity than centralized pre-training; (2) Photon model training time\ndecreases with available compute, achieving a similar compute-time trade-off to\ncentralized; and (3) Photon outperforms the wall-time of baseline distributed\ntraining methods by 35% via communicating 64x-512xless. Our proposal is robust\nto data heterogeneity and converges twice as fast as previous methods like\nDiLoCo. This surprising data efficiency stems from a unique approach combining\nsmall client batch sizes with extremely high learning rates, enabled by\nfederated averaging's robustness to hyperparameters. Photon thus represents the\nfirst economical system for global internet-wide LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models (LLMs) demands extensive data and computing\nresources, which are traditionally constrained to data centers by the\nhigh-bandwidth requirements of distributed training. Low-bandwidth methods like\nfederated learning (FL) could enable collaborative training of larger models\nacross weakly-connected GPUs if they can effectively be used for pre-training.\nTo achieve this, we introduce Photon, the first complete system for federated\nend-to-end LLM training, leveraging cross-silo FL for global-scale training\nwith minimal communication overheads. Using Photon, we train the first\nfederated family of decoder-only LLMs from scratch. We show that: (1) Photon\ncan train model sizes up to 7B in a federated fashion while reaching an even\nbetter perplexity than centralized pre-training; (2) Photon model training time\ndecreases with available compute, achieving a similar compute-time trade-off to\ncentralized; and (3) Photon outperforms the wall-time of baseline distributed\ntraining methods by 35% via communicating 64x-512xless. Our proposal is robust\nto data heterogeneity and converges twice as fast as previous methods like\nDiLoCo. This surprising data efficiency stems from a unique approach combining\nsmall client batch sizes with extremely high learning rates, enabled by\nfederated averaging's robustness to hyperparameters. Photon thus represents the\nfirst economical system for global internet-wide LLM pre-training."
                },
                "authors": [
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Bill Marino"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "13 pages, 9 appendix pages, 10 figures, 3 algorithms, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11445v2",
                "updated": "2024-11-05T08:46:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    46,
                    1,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-17T03:39:45Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    39,
                    45,
                    1,
                    261,
                    0
                ],
                "title": "Jailbreaking Large Language Models with Symbolic Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Models with Symbolic Mathematics"
                },
                "summary": "Recent advancements in AI safety have led to increased efforts in training\nand red-teaming large language models (LLMs) to mitigate unsafe content\ngeneration. However, these safety mechanisms may not be comprehensive, leaving\npotential vulnerabilities unexplored. This paper introduces MathPrompt, a novel\njailbreaking technique that exploits LLMs' advanced capabilities in symbolic\nmathematics to bypass their safety mechanisms. By encoding harmful natural\nlanguage prompts into mathematical problems, we demonstrate a critical\nvulnerability in current AI safety measures. Our experiments across 13\nstate-of-the-art LLMs reveal an average attack success rate of 73.6\\%,\nhighlighting the inability of existing safety training mechanisms to generalize\nto mathematically encoded inputs. Analysis of embedding vectors shows a\nsubstantial semantic shift between original and encoded prompts, helping\nexplain the attack's success. This work emphasizes the importance of a holistic\napproach to AI safety, calling for expanded red-teaming efforts to develop\nrobust safeguards across all potential input types and their associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI safety have led to increased efforts in training\nand red-teaming large language models (LLMs) to mitigate unsafe content\ngeneration. However, these safety mechanisms may not be comprehensive, leaving\npotential vulnerabilities unexplored. This paper introduces MathPrompt, a novel\njailbreaking technique that exploits LLMs' advanced capabilities in symbolic\nmathematics to bypass their safety mechanisms. By encoding harmful natural\nlanguage prompts into mathematical problems, we demonstrate a critical\nvulnerability in current AI safety measures. Our experiments across 13\nstate-of-the-art LLMs reveal an average attack success rate of 73.6\\%,\nhighlighting the inability of existing safety training mechanisms to generalize\nto mathematically encoded inputs. Analysis of embedding vectors shows a\nsubstantial semantic shift between original and encoded prompts, helping\nexplain the attack's success. This work emphasizes the importance of a holistic\napproach to AI safety, calling for expanded red-teaming efforts to develop\nrobust safeguards across all potential input types and their associated risks."
                },
                "authors": [
                    {
                        "name": "Emet Bethany"
                    },
                    {
                        "name": "Mazal Bethany"
                    },
                    {
                        "name": "Juan Arturo Nolazco Flores"
                    },
                    {
                        "name": "Sumit Kumar Jha"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02116v2",
                "updated": "2024-11-05T08:35:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    35,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T14:29:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Advancements and limitations of LLMs in replicating human color-word\n  associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements and limitations of LLMs in replicating human color-word\n  associations"
                },
                "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category. However, the\nhighest median performance was approximately 50% even for GPT-4o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category. However, the\nhighest median performance was approximately 50% even for GPT-4o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations."
                },
                "authors": [
                    {
                        "name": "Makoto Fukushima"
                    },
                    {
                        "name": "Shusuke Eshita"
                    },
                    {
                        "name": "Hiroshige Fukuhara"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshige Fukuhara"
                },
                "author": "Hiroshige Fukuhara",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02902v1",
                "updated": "2024-11-05T08:35:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    35,
                    8,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T08:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    35,
                    8,
                    1,
                    310,
                    0
                ],
                "title": "Membership Inference Attacks against Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks against Large Vision-Language Models"
                },
                "summary": "Large vision-language models (VLLMs) exhibit promising capabilities for\nprocessing multi-modal tasks across various application scenarios. However,\ntheir emergence also raises significant data security concerns, given the\npotential inclusion of sensitive information, such as private photos and\nmedical records, in their training datasets. Detecting inappropriately used\ndata in VLLMs remains a critical and unresolved issue, mainly due to the lack\nof standardized datasets and suitable methodologies. In this study, we\nintroduce the first membership inference attack (MIA) benchmark tailored for\nvarious VLLMs to facilitate training data detection. Then, we propose a novel\nMIA pipeline specifically designed for token-level image detection. Lastly, we\npresent a new metric called MaxR\\'enyi-K%, which is based on the confidence of\nthe model output and applies to both text and image data. We believe that our\nwork can deepen the understanding and methodology of MIAs in the context of\nVLLMs. Our code and datasets are available at\nhttps://github.com/LIONS-EPFL/VL-MIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLLMs) exhibit promising capabilities for\nprocessing multi-modal tasks across various application scenarios. However,\ntheir emergence also raises significant data security concerns, given the\npotential inclusion of sensitive information, such as private photos and\nmedical records, in their training datasets. Detecting inappropriately used\ndata in VLLMs remains a critical and unresolved issue, mainly due to the lack\nof standardized datasets and suitable methodologies. In this study, we\nintroduce the first membership inference attack (MIA) benchmark tailored for\nvarious VLLMs to facilitate training data detection. Then, we propose a novel\nMIA pipeline specifically designed for token-level image detection. Lastly, we\npresent a new metric called MaxR\\'enyi-K%, which is based on the confidence of\nthe model output and applies to both text and image data. We believe that our\nwork can deepen the understanding and methodology of MIAs in the context of\nVLLMs. Our code and datasets are available at\nhttps://github.com/LIONS-EPFL/VL-MIA."
                },
                "authors": [
                    {
                        "name": "Zhan Li"
                    },
                    {
                        "name": "Yongtao Wu"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Francesco Tonin"
                    },
                    {
                        "name": "Elias Abad Rocamora"
                    },
                    {
                        "name": "Volkan Cevher"
                    }
                ],
                "author_detail": {
                    "name": "Volkan Cevher"
                },
                "author": "Volkan Cevher",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12735v3",
                "updated": "2024-11-05T07:24:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    24,
                    15,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-17T16:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    55,
                    42,
                    2,
                    199,
                    0
                ],
                "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge"
                },
                "summary": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek."
                },
                "authors": [
                    {
                        "name": "Yibin Yan"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_comment": "Technical Report; Project Page: https://go2heart.github.io/echosight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19332v3",
                "updated": "2024-11-05T07:21:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    21,
                    18,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-29T17:59:07Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    17,
                    59,
                    7,
                    2,
                    150,
                    0
                ],
                "title": "Self-Exploring Language Models: Active Preference Elicitation for Online\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Exploring Language Models: Active Preference Elicitation for Online\n  Alignment"
                },
                "summary": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Hiteshi Sharma"
                    },
                    {
                        "name": "Han Zhong"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hany Hassan"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07959v2",
                "updated": "2024-11-05T07:14:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    14,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-03-12T09:01:04Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    9,
                    1,
                    4,
                    1,
                    72,
                    0
                ],
                "title": "An Interpretable Generalization Mechanism for Accurately Detecting\n  Anomaly and Identifying Networking Intrusion Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Interpretable Generalization Mechanism for Accurately Detecting\n  Anomaly and Identifying Networking Intrusion Techniques"
                },
                "summary": "Recent advancements in Intrusion Detection Systems (IDS), integrating\nExplainable AI (XAI) methodologies, have led to notable improvements in system\nperformance via precise feature selection. However, a thorough understanding of\ncyber-attacks requires inherently explainable decision-making processes within\nIDS. In this paper, we present the Interpretable Generalization Mechanism (IG),\npoised to revolutionize IDS capabilities. IG discerns coherent patterns, making\nit interpretable in distinguishing between normal and anomalous network\ntraffic. Further, the synthesis of coherent patterns sheds light on intricate\nintrusion pathways, providing essential insights for cybersecurity forensics.\nBy experiments with real-world datasets NSL-KDD, UNSW-NB15, and UKM-IDS20, IG\nis accurate even at a low ratio of training-to-test. With 10%-to-90%, IG\nachieves Precision (PRE)=0.93, Recall (REC)=0.94, and Area Under Curve\n(AUC)=0.94 in NSL-KDD; PRE=0.98, REC=0.99, and AUC=0.99 in UNSW-NB15; and\nPRE=0.98, REC=0.98, and AUC=0.99 in UKM-IDS20. Notably, in UNSW-NB15, IG\nachieves REC=1.0 and at least PRE=0.98 since 40%-to-60%; in UKM-IDS20, IG\nachieves REC=1.0 and at least PRE=0.88 since 20%-to-80%. Importantly, in\nUKM-IDS20, IG successfully identifies all three anomalous instances without\nprior exposure, demonstrating its generalization capabilities. These results\nand inferences are reproducible. In sum, IG showcases superior generalization\nby consistently performing well across diverse datasets and training-to-test\nratios (from 10%-to-90% to 90%-to-10%), and excels in identifying novel\nanomalies without prior exposure. Its interpretability is enhanced by coherent\nevidence that accurately distinguishes both normal and anomalous activities,\nsignificantly improving detection accuracy and reducing false alarms, thereby\nstrengthening IDS reliability and trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Intrusion Detection Systems (IDS), integrating\nExplainable AI (XAI) methodologies, have led to notable improvements in system\nperformance via precise feature selection. However, a thorough understanding of\ncyber-attacks requires inherently explainable decision-making processes within\nIDS. In this paper, we present the Interpretable Generalization Mechanism (IG),\npoised to revolutionize IDS capabilities. IG discerns coherent patterns, making\nit interpretable in distinguishing between normal and anomalous network\ntraffic. Further, the synthesis of coherent patterns sheds light on intricate\nintrusion pathways, providing essential insights for cybersecurity forensics.\nBy experiments with real-world datasets NSL-KDD, UNSW-NB15, and UKM-IDS20, IG\nis accurate even at a low ratio of training-to-test. With 10%-to-90%, IG\nachieves Precision (PRE)=0.93, Recall (REC)=0.94, and Area Under Curve\n(AUC)=0.94 in NSL-KDD; PRE=0.98, REC=0.99, and AUC=0.99 in UNSW-NB15; and\nPRE=0.98, REC=0.98, and AUC=0.99 in UKM-IDS20. Notably, in UNSW-NB15, IG\nachieves REC=1.0 and at least PRE=0.98 since 40%-to-60%; in UKM-IDS20, IG\nachieves REC=1.0 and at least PRE=0.88 since 20%-to-80%. Importantly, in\nUKM-IDS20, IG successfully identifies all three anomalous instances without\nprior exposure, demonstrating its generalization capabilities. These results\nand inferences are reproducible. In sum, IG showcases superior generalization\nby consistently performing well across diverse datasets and training-to-test\nratios (from 10%-to-90% to 90%-to-10%), and excels in identifying novel\nanomalies without prior exposure. Its interpretability is enhanced by coherent\nevidence that accurately distinguishes both normal and anomalous activities,\nsignificantly improving detection accuracy and reducing false alarms, thereby\nstrengthening IDS reliability and trustworthiness."
                },
                "authors": [
                    {
                        "name": "Hao-Ting Pai"
                    },
                    {
                        "name": "Yu-Hsuan Kang"
                    },
                    {
                        "name": "Wen-Cheng Chung"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Cheng Chung"
                },
                "author": "Wen-Cheng Chung",
                "arxiv_doi": "10.1109/TIFS.2024.3488967",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIFS.2024.3488967",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.07959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Information Forensics and Security, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02318v2",
                "updated": "2024-11-05T07:13:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    13,
                    13,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T17:44:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast"
                },
                "summary": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering."
                },
                "authors": [
                    {
                        "name": "Marilyn Rego"
                    },
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Sanya Dod"
                    },
                    {
                        "name": "Zhaorui Ni"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Jenna DiVincenzo"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02866v1",
                "updated": "2024-11-05T07:12:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    12,
                    50,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:12:50Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    12,
                    50,
                    1,
                    310,
                    0
                ],
                "title": "Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on\n  Graph Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on\n  Graph Federated Learning"
                },
                "summary": "Recent research has constructed successful graph reconstruction attack (GRA)\non GFL. But these attacks are still challenged in aspects of effectiveness and\nstealth. To address the issues, we propose the first Data Manipulation aided\nReconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born\nto manipulate its locally collected data to enhance graph stealing privacy from\nbenign ones, so as to construct double whammy on GFL. It differs from previous\nwork in three terms: (1) effectiveness - to fully utilize the sparsity and\nfeature smoothness of the graph, novel penalty terms are designed adaptive to\ndiverse similarity functions for connected and unconnected node pairs, as well\nas incorporation label smoothing on top of the original cross-entropy loss. (2)\nscalability - DMan4Rec is capable of both white-box and black-box attacks via\ntraining a supervised model to infer the posterior probabilities obtained from\nlimited queries (3) stealthiness - by manipulating the malicious client's node\nfeatures, it can maintain the overall graph structure's invariance and conceal\nthe attack. Comprehensive experiments on four real datasets and three GNN\nmodels demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack\nperformance, e.g., the attack AUC and precision improved by 9.2% and 10.5%\nrespectively compared with the SOTA baselines. Particularly, DMan4Rec achieves\nan AUC score and a precision score of up to 99.59% and 99.56%, respectively in\nblack-box setting. Nevertheless, the complete overlap of the distribution\ngraphs supports the stealthiness of the attack. Besides, DMan4Rec still beats\nthe defensive GFL, which alarms a new threat to GFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has constructed successful graph reconstruction attack (GRA)\non GFL. But these attacks are still challenged in aspects of effectiveness and\nstealth. To address the issues, we propose the first Data Manipulation aided\nReconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born\nto manipulate its locally collected data to enhance graph stealing privacy from\nbenign ones, so as to construct double whammy on GFL. It differs from previous\nwork in three terms: (1) effectiveness - to fully utilize the sparsity and\nfeature smoothness of the graph, novel penalty terms are designed adaptive to\ndiverse similarity functions for connected and unconnected node pairs, as well\nas incorporation label smoothing on top of the original cross-entropy loss. (2)\nscalability - DMan4Rec is capable of both white-box and black-box attacks via\ntraining a supervised model to infer the posterior probabilities obtained from\nlimited queries (3) stealthiness - by manipulating the malicious client's node\nfeatures, it can maintain the overall graph structure's invariance and conceal\nthe attack. Comprehensive experiments on four real datasets and three GNN\nmodels demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack\nperformance, e.g., the attack AUC and precision improved by 9.2% and 10.5%\nrespectively compared with the SOTA baselines. Particularly, DMan4Rec achieves\nan AUC score and a precision score of up to 99.59% and 99.56%, respectively in\nblack-box setting. Nevertheless, the complete overlap of the distribution\ngraphs supports the stealthiness of the attack. Besides, DMan4Rec still beats\nthe defensive GFL, which alarms a new threat to GFL."
                },
                "authors": [
                    {
                        "name": "Jinyin Chen"
                    },
                    {
                        "name": "Minying Ma"
                    },
                    {
                        "name": "Haibin Zheng"
                    },
                    {
                        "name": "Qi Xuan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Xuan"
                },
                "author": "Qi Xuan",
                "arxiv_comment": "The paper is currently being submitted for publication (The submitted\n  journal is TNSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02864v1",
                "updated": "2024-11-05T07:12:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    12,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:12:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    12,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document\n  Relation Extraction with Graph-of-Thoughts Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document\n  Relation Extraction with Graph-of-Thoughts Reasoning"
                },
                "summary": "Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning capability on many NLP tasks. Recasting an NLP\ntask into a text-to-text generation task is a common practice so that\ngenerative LLMs can be prompted to resolve it. However, performing\ndocument-level relation extraction (DocRE) tasks with generative LLM models is\nstill challenging due to the structured output format of DocRE, which\ncomplicates the conversion to plain text. Limited information available in\nfew-shot samples and prompt instructions induce further difficulties and\nchallenges in relation extraction for mentioned entities in a document. In this\npaper, we represent the structured output as a graph-style triplet rather than\nnatural language expressions and leverage generative LLMs for the DocRE task.\nOur approach, the Graph-DPEP framework is grounded in the reasoning behind\ntriplet explanation thoughts presented in natural language. In this framework,\nwe first introduce a ``decomposed-plug\" method for performing the generation\nfrom LLMs over prompts with type-space decomposition to alleviate the burden of\ndistinguishing all relation types. Second, we employ a verifier for calibrating\nthe generation and identifying overlooked query entity pairs. Third, we develop\n\"ensemble-play\", reapplying generation on the entire type list by leveraging\nthe reasoning thoughts embedded in a sub-graph associated with the missing\nquery pair to address the missingness issue. Through extensive comparisons with\nexisting prompt techniques and alternative Language Models (LLMs), our\nframework demonstrates superior performance on publicly available benchmarks in\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning capability on many NLP tasks. Recasting an NLP\ntask into a text-to-text generation task is a common practice so that\ngenerative LLMs can be prompted to resolve it. However, performing\ndocument-level relation extraction (DocRE) tasks with generative LLM models is\nstill challenging due to the structured output format of DocRE, which\ncomplicates the conversion to plain text. Limited information available in\nfew-shot samples and prompt instructions induce further difficulties and\nchallenges in relation extraction for mentioned entities in a document. In this\npaper, we represent the structured output as a graph-style triplet rather than\nnatural language expressions and leverage generative LLMs for the DocRE task.\nOur approach, the Graph-DPEP framework is grounded in the reasoning behind\ntriplet explanation thoughts presented in natural language. In this framework,\nwe first introduce a ``decomposed-plug\" method for performing the generation\nfrom LLMs over prompts with type-space decomposition to alleviate the burden of\ndistinguishing all relation types. Second, we employ a verifier for calibrating\nthe generation and identifying overlooked query entity pairs. Third, we develop\n\"ensemble-play\", reapplying generation on the entire type list by leveraging\nthe reasoning thoughts embedded in a sub-graph associated with the missing\nquery pair to address the missingness issue. Through extensive comparisons with\nexisting prompt techniques and alternative Language Models (LLMs), our\nframework demonstrates superior performance on publicly available benchmarks in\nexperiments."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ning Yan"
                    },
                    {
                        "name": "Masood Mortazavi"
                    },
                    {
                        "name": "Hoang H. Nguyen"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02862v1",
                "updated": "2024-11-05T07:10:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    10,
                    0,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:10:00Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    10,
                    0,
                    1,
                    310,
                    0
                ],
                "title": "The Unreasonable Effectiveness of LLMs for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Effectiveness of LLMs for Query Optimization"
                },
                "summary": "Recent work in database query optimization has used complex machine learning\nstrategies, such as customized reinforcement learning schemes. Surprisingly, we\nshow that LLM embeddings of query text contain useful semantic information for\nquery optimization. Specifically, we show that a simple binary classifier\ndeciding between alternative query plans, trained only on a small number of\nlabeled embedded query vectors, can outperform existing heuristic systems.\nAlthough we only present some preliminary results, an LLM-powered query\noptimizer could provide significant benefits, both in terms of performance and\nsimplicity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work in database query optimization has used complex machine learning\nstrategies, such as customized reinforcement learning schemes. Surprisingly, we\nshow that LLM embeddings of query text contain useful semantic information for\nquery optimization. Specifically, we show that a simple binary classifier\ndeciding between alternative query plans, trained only on a small number of\nlabeled embedded query vectors, can outperform existing heuristic systems.\nAlthough we only present some preliminary results, an LLM-powered query\noptimizer could provide significant benefits, both in terms of performance and\nsimplicity."
                },
                "authors": [
                    {
                        "name": "Peter Akioyamen"
                    },
                    {
                        "name": "Zixuan Yi"
                    },
                    {
                        "name": "Ryan Marcus"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Marcus"
                },
                "author": "Ryan Marcus",
                "arxiv_comment": "To appear in the Machine Learning for Systems Workshop at NeurIPS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14900v3",
                "updated": "2024-11-05T06:52:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    52,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-21T06:47:28Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    47,
                    28,
                    4,
                    173,
                    0
                ],
                "title": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue\n  for LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue\n  for LLM-based Recommendation"
                },
                "summary": "Adapting Large Language Models (LLMs) for recommendation requires careful\nconsideration of the decoding process, given the inherent differences between\ngenerating items and natural language. Existing approaches often directly apply\nLLMs' original decoding methods. However, we find these methods encounter\nsignificant challenges: 1) amplification bias -- where standard length\nnormalization inflates scores for items containing tokens with generation\nprobabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --\ngenerating multiple similar or repetitive items for a user. To tackle these\nchallenges, we introduce a new decoding approach named Debiasing-Diversifying\nDecoding (D3). D3 disables length normalization for ghost tokens to alleviate\namplification bias, and it incorporates a text-free assistant model to\nencourage tokens less frequently generated by LLMs for counteracting\nrecommendation homogeneity. Extensive experiments on real-world datasets\ndemonstrate the method's effectiveness in enhancing accuracy and diversity. The\ncode is available at https://github.com/SAI990323/DecodingMatters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) for recommendation requires careful\nconsideration of the decoding process, given the inherent differences between\ngenerating items and natural language. Existing approaches often directly apply\nLLMs' original decoding methods. However, we find these methods encounter\nsignificant challenges: 1) amplification bias -- where standard length\nnormalization inflates scores for items containing tokens with generation\nprobabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --\ngenerating multiple similar or repetitive items for a user. To tackle these\nchallenges, we introduce a new decoding approach named Debiasing-Diversifying\nDecoding (D3). D3 disables length normalization for ghost tokens to alleviate\namplification bias, and it incorporates a text-free assistant model to\nencourage tokens less frequently generated by LLMs for counteracting\nrecommendation homogeneity. Extensive experiments on real-world datasets\ndemonstrate the method's effectiveness in enhancing accuracy and diversity. The\ncode is available at https://github.com/SAI990323/DecodingMatters."
                },
                "authors": [
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xinyue Huo"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09562v3",
                "updated": "2024-11-05T06:35:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    35,
                    52,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-03T10:21:07Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    10,
                    21,
                    7,
                    2,
                    185,
                    0
                ],
                "title": "Edge AI-Enabled Chicken Health Detection Based on Enhanced FCOS-Lite and\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI-Enabled Chicken Health Detection Based on Enhanced FCOS-Lite and\n  Knowledge Distillation"
                },
                "summary": "The utilization of AIoT technology has become a crucial trend in modern\npoultry management, offering the potential to optimize farming operations and\nreduce human workloads. This paper presents a real-time and compact edge-AI\nenabled detector designed to identify chickens and their healthy statuses using\nframes captured by a lightweight and intelligent camera equipped with an\nedge-AI enabled CMOS sensor. To ensure efficient deployment of the proposed\ncompact detector within the memory-constrained edge-AI enabled CMOS sensor, we\nemploy a FCOS-Lite detector leveraging MobileNet as the backbone. To mitigate\nthe issue of reduced accuracy in compact edge-AI detectors without incurring\nadditional inference costs, we propose a gradient weighting loss function as\nclassification loss and introduce CIOU loss function as localization loss.\nAdditionally, we propose a knowledge distillation scheme to transfer valuable\ninformation from a large teacher detector to the proposed FCOS-Lite detector,\nthereby enhancing its performance while preserving a compact model size.\nExperimental results demonstrate the proposed edge-AI enabled detector achieves\ncommendable performance metrics, including a mean average precision (mAP) of\n95.1$\\%$ and an F1-score of 94.2$\\%$, etc. Notably, the proposed detector can\nbe efficiently deployed and operates at a speed exceeding 20 FPS on the edge-AI\nenabled CMOS sensor, achieved through int8 quantization. That meets practical\ndemands for automated poultry health monitoring using lightweight intelligent\ncameras with low power consumption and minimal bandwidth costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The utilization of AIoT technology has become a crucial trend in modern\npoultry management, offering the potential to optimize farming operations and\nreduce human workloads. This paper presents a real-time and compact edge-AI\nenabled detector designed to identify chickens and their healthy statuses using\nframes captured by a lightweight and intelligent camera equipped with an\nedge-AI enabled CMOS sensor. To ensure efficient deployment of the proposed\ncompact detector within the memory-constrained edge-AI enabled CMOS sensor, we\nemploy a FCOS-Lite detector leveraging MobileNet as the backbone. To mitigate\nthe issue of reduced accuracy in compact edge-AI detectors without incurring\nadditional inference costs, we propose a gradient weighting loss function as\nclassification loss and introduce CIOU loss function as localization loss.\nAdditionally, we propose a knowledge distillation scheme to transfer valuable\ninformation from a large teacher detector to the proposed FCOS-Lite detector,\nthereby enhancing its performance while preserving a compact model size.\nExperimental results demonstrate the proposed edge-AI enabled detector achieves\ncommendable performance metrics, including a mean average precision (mAP) of\n95.1$\\%$ and an F1-score of 94.2$\\%$, etc. Notably, the proposed detector can\nbe efficiently deployed and operates at a speed exceeding 20 FPS on the edge-AI\nenabled CMOS sensor, achieved through int8 quantization. That meets practical\ndemands for automated poultry health monitoring using lightweight intelligent\ncameras with low power consumption and minimal bandwidth costs."
                },
                "authors": [
                    {
                        "name": "Qiang Tong"
                    },
                    {
                        "name": "Jinrui Wang"
                    },
                    {
                        "name": "Wenshuang Yang"
                    },
                    {
                        "name": "Songtao Wu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Kuanhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kuanhong Xu"
                },
                "author": "Kuanhong Xu",
                "arxiv_doi": "10.1016/j.compag.2024.109432",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compag.2024.109432",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.09562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16014v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16014v3",
                "updated": "2024-11-05T06:13:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    13,
                    7,
                    1,
                    310,
                    0
                ],
                "published": "2023-12-26T11:49:23Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    11,
                    49,
                    23,
                    1,
                    360,
                    0
                ],
                "title": "Passive Non-Line-of-Sight Imaging with Light Transport Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive Non-Line-of-Sight Imaging with Light Transport Modulation"
                },
                "summary": "Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM."
                },
                "authors": [
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Ruixu Geng"
                    },
                    {
                        "name": "Xiaolong Du"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Yang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Hu"
                },
                "author": "Yang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16014v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16014v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02830v1",
                "updated": "2024-11-05T06:02:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    2,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T06:02:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    2,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "Mixtures of In-Context Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixtures of In-Context Learners"
                },
                "summary": "In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory."
                },
                "authors": [
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Emile van Krieken"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02829v1",
                "updated": "2024-11-05T06:00:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    0,
                    27,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T06:00:27Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    0,
                    27,
                    1,
                    310,
                    0
                ],
                "title": "CE-CoLLM: Efficient and Adaptive Large Language Models Through\n  Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-CoLLM: Efficient and Adaptive Large Language Models Through\n  Cloud-Edge Collaboration"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in serving\nend-users with human-like intelligence. However, LLMs demand high computational\nresources, making it challenging to deploy them to satisfy various performance\nobjectives, such as meeting the resource constraints on edge devices close to\nend-users or achieving high accuracy with ample resources. In this paper, we\nintroduce CE-CoLLM, a novel cloud-edge collaboration framework that supports\nefficient and adaptive LLM inference for end-users at the edge with two modes,\n(1) low-latency edge standalone inference and (2) highly accurate cloud-edge\ncollaborative inference. First, we show that the inherent high communication\ncosts for transmitting LLM contextual information between the edge and cloud\ndominate the overall latency, making it inefficient and costly to deploy LLMs\nusing cloud-edge collaboration. Second, we propose several critical techniques\nto address this challenge, including early-exit mechanism, cloud context\nmanager, and quantization in cloud-edge collaboration to enable not only\nlow-latency standalone edge inference but also efficient and adaptive\ncloud-edge collaborative inference for LLMs. Third, we perform comprehensive\nexperimental analysis, which demonstrates that CE-CoLLM significantly reduces\ninference time by up to 13.81% and cloud computation costs by up to 84.55%\ncompared to the popular cloud-based LLM deployment, while maintaining\ncomparable model accuracy. The proposed approach effectively shifts the\ncomputational load to the edge, reduces the communication overhead, scales\nefficiently with multiple edge clients, and provides reliable LLM deployment\nusing cloud-edge collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in serving\nend-users with human-like intelligence. However, LLMs demand high computational\nresources, making it challenging to deploy them to satisfy various performance\nobjectives, such as meeting the resource constraints on edge devices close to\nend-users or achieving high accuracy with ample resources. In this paper, we\nintroduce CE-CoLLM, a novel cloud-edge collaboration framework that supports\nefficient and adaptive LLM inference for end-users at the edge with two modes,\n(1) low-latency edge standalone inference and (2) highly accurate cloud-edge\ncollaborative inference. First, we show that the inherent high communication\ncosts for transmitting LLM contextual information between the edge and cloud\ndominate the overall latency, making it inefficient and costly to deploy LLMs\nusing cloud-edge collaboration. Second, we propose several critical techniques\nto address this challenge, including early-exit mechanism, cloud context\nmanager, and quantization in cloud-edge collaboration to enable not only\nlow-latency standalone edge inference but also efficient and adaptive\ncloud-edge collaborative inference for LLMs. Third, we perform comprehensive\nexperimental analysis, which demonstrates that CE-CoLLM significantly reduces\ninference time by up to 13.81% and cloud computation costs by up to 84.55%\ncompared to the popular cloud-based LLM deployment, while maintaining\ncomparable model accuracy. The proposed approach effectively shifts the\ncomputational load to the edge, reduces the communication overhead, scales\nefficiently with multiple edge clients, and provides reliable LLM deployment\nusing cloud-edge collaboration."
                },
                "authors": [
                    {
                        "name": "Hongpeng Jin"
                    },
                    {
                        "name": "Yanzhao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhao Wu"
                },
                "author": "Yanzhao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14419v2",
                "updated": "2024-11-05T05:43:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    43,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-04-14T07:06:12Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    7,
                    6,
                    12,
                    6,
                    105,
                    0
                ],
                "title": "Evaluation and Improvement of Fault Detection for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation and Improvement of Fault Detection for Large Language Models"
                },
                "summary": "Large language models (LLMs) have recently achieved significant success\nacross various application domains, garnering substantial attention from\ndifferent communities. Unfortunately, even for the best LLM, many\n\\textit{faults} still exist that LLM cannot properly predict. Such faults will\nharm the usability of LLMs in general and could introduce safety issues in\nreliability-critical systems such as autonomous driving systems. How to quickly\nreveal these faults in real-world datasets that LLM could face is important,\nbut challenging. The major reason is that the ground truth is necessary but the\ndata labeling process is heavy considering the time and human effort. To handle\nthis problem, in the conventional deep learning testing field, test selection\nmethods have been proposed for efficiently evaluating deep learning models by\nprioritizing faults. However, despite their importance, the usefulness of these\nmethods on LLMs is unclear, and lack of exploration. In this paper, we conduct\nthe first empirical study to investigate the effectiveness of existing fault\ndetection methods for LLMs. Experimental results on four different\ntasks~(including both code tasks and natural language processing tasks) and\nfour LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as\nMargin perform well on LLMs but there is still a big room for improvement.\nBased on the study, we further propose \\textbf{MuCS}, a prompt\n\\textbf{Mu}tation-based prediction \\textbf{C}onfidence \\textbf{S}moothing\nframework to boost the fault detection capability of existing methods.\nConcretely, multiple prompt mutation techniques have been proposed to help\ncollect more diverse outputs for confidence smoothing. The results show that\nour proposed framework significantly enhances existing methods with the\nimprovement of test relative coverage by up to 70.53\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved significant success\nacross various application domains, garnering substantial attention from\ndifferent communities. Unfortunately, even for the best LLM, many\n\\textit{faults} still exist that LLM cannot properly predict. Such faults will\nharm the usability of LLMs in general and could introduce safety issues in\nreliability-critical systems such as autonomous driving systems. How to quickly\nreveal these faults in real-world datasets that LLM could face is important,\nbut challenging. The major reason is that the ground truth is necessary but the\ndata labeling process is heavy considering the time and human effort. To handle\nthis problem, in the conventional deep learning testing field, test selection\nmethods have been proposed for efficiently evaluating deep learning models by\nprioritizing faults. However, despite their importance, the usefulness of these\nmethods on LLMs is unclear, and lack of exploration. In this paper, we conduct\nthe first empirical study to investigate the effectiveness of existing fault\ndetection methods for LLMs. Experimental results on four different\ntasks~(including both code tasks and natural language processing tasks) and\nfour LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as\nMargin perform well on LLMs but there is still a big room for improvement.\nBased on the study, we further propose \\textbf{MuCS}, a prompt\n\\textbf{Mu}tation-based prediction \\textbf{C}onfidence \\textbf{S}moothing\nframework to boost the fault detection capability of existing methods.\nConcretely, multiple prompt mutation techniques have been proposed to help\ncollect more diverse outputs for confidence smoothing. The results show that\nour proposed framework significantly enhances existing methods with the\nimprovement of test relative coverage by up to 70.53\\%."
                },
                "authors": [
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Jin Wen"
                    },
                    {
                        "name": "Maxime Cordy"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02818v1",
                "updated": "2024-11-05T05:36:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    36,
                    17,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:36:17Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    36,
                    17,
                    1,
                    310,
                    0
                ],
                "title": "LiVOS: Light Video Object Segmentation with Gated Linear Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiVOS: Light Video Object Segmentation with Gated Linear Matching"
                },
                "summary": "Semi-supervised video object segmentation (VOS) has been largely driven by\nspace-time memory (STM) networks, which store past frame features in a\nspatiotemporal memory to segment the current frame via softmax attention.\nHowever, STM networks face memory limitations due to the quadratic complexity\nof softmax matching, restricting their applicability as video length and\nresolution increase. To address this, we propose LiVOS, a lightweight memory\nnetwork that employs linear matching via linear attention, reformulating memory\nmatching into a recurrent process that reduces the quadratic attention matrix\nto a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity,\nwe introduce gated linear matching, where a data-dependent gate matrix is\nmultiplied with the state matrix to control what information to retain or\ndiscard. Experiments on diverse benchmarks demonstrated the effectiveness of\nour method. It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all\nnon-STM methods and narrowing the gap with STM-based approaches. For longer and\nhigher-resolution videos, it matched STM-based methods with 53% less GPU memory\nand supports 4096p inference on a 32G consumer-grade GPU--a previously\ncost-prohibitive capability--opening the door for long and high-resolution\nvideo foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-supervised video object segmentation (VOS) has been largely driven by\nspace-time memory (STM) networks, which store past frame features in a\nspatiotemporal memory to segment the current frame via softmax attention.\nHowever, STM networks face memory limitations due to the quadratic complexity\nof softmax matching, restricting their applicability as video length and\nresolution increase. To address this, we propose LiVOS, a lightweight memory\nnetwork that employs linear matching via linear attention, reformulating memory\nmatching into a recurrent process that reduces the quadratic attention matrix\nto a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity,\nwe introduce gated linear matching, where a data-dependent gate matrix is\nmultiplied with the state matrix to control what information to retain or\ndiscard. Experiments on diverse benchmarks demonstrated the effectiveness of\nour method. It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all\nnon-STM methods and narrowing the gap with STM-based approaches. For longer and\nhigher-resolution videos, it matched STM-based methods with 53% less GPU memory\nand supports 4096p inference on a 32G consumer-grade GPU--a previously\ncost-prohibitive capability--opening the door for long and high-resolution\nvideo foundation models."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Jianfeng Wang"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Marc Niethammer"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "arxiv_comment": "Code&models: https://github.com/uncbiag/LiVOS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19761v2",
                "updated": "2024-11-05T05:25:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    25,
                    17,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-30T07:16:03Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    7,
                    16,
                    3,
                    3,
                    151,
                    0
                ],
                "title": "Revisiting CNNs for Trajectory Similarity Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting CNNs for Trajectory Similarity Learning"
                },
                "summary": "Similarity search is a fundamental but expensive operator in querying\ntrajectory data, due to its quadratic complexity of distance computation. To\nmitigate the computational burden for long trajectories, neural networks have\nbeen widely employed for similarity learning and each trajectory is encoded as\na high-dimensional vector for similarity search with linear complexity. Given\nthe sequential nature of trajectory data, previous efforts have been primarily\ndevoted to the utilization of RNNs or Transformers.\n  In this paper, we argue that the common practice of treating trajectory as\nsequential data results in excessive attention to capturing long-term global\ndependency between two sequences. Instead, our investigation reveals the\npivotal role of local similarity, prompting a revisit of simple CNNs for\ntrajectory similarity learning. We introduce ConvTraj, incorporating both 1D\nand 2D convolutions to capture sequential and geo-distribution features of\ntrajectories, respectively. In addition, we conduct a series of theoretical\nanalyses to justify the effectiveness of ConvTraj. Experimental results on four\nreal-world large-scale datasets demonstrate that ConvTraj achieves\nstate-of-the-art accuracy in trajectory similarity search. Owing to the simple\nnetwork structure of ConvTraj, the training and inference speed on the Porto\ndataset with 1.6 million trajectories are increased by at least $240$x and\n$2.16$x, respectively. The source code and dataset can be found at\n\\textit{\\url{https://github.com/Proudc/ConvTraj}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity search is a fundamental but expensive operator in querying\ntrajectory data, due to its quadratic complexity of distance computation. To\nmitigate the computational burden for long trajectories, neural networks have\nbeen widely employed for similarity learning and each trajectory is encoded as\na high-dimensional vector for similarity search with linear complexity. Given\nthe sequential nature of trajectory data, previous efforts have been primarily\ndevoted to the utilization of RNNs or Transformers.\n  In this paper, we argue that the common practice of treating trajectory as\nsequential data results in excessive attention to capturing long-term global\ndependency between two sequences. Instead, our investigation reveals the\npivotal role of local similarity, prompting a revisit of simple CNNs for\ntrajectory similarity learning. We introduce ConvTraj, incorporating both 1D\nand 2D convolutions to capture sequential and geo-distribution features of\ntrajectories, respectively. In addition, we conduct a series of theoretical\nanalyses to justify the effectiveness of ConvTraj. Experimental results on four\nreal-world large-scale datasets demonstrate that ConvTraj achieves\nstate-of-the-art accuracy in trajectory similarity search. Owing to the simple\nnetwork structure of ConvTraj, the training and inference speed on the Porto\ndataset with 1.6 million trajectories are increased by at least $240$x and\n$2.16$x, respectively. The source code and dataset can be found at\n\\textit{\\url{https://github.com/Proudc/ConvTraj}}."
                },
                "authors": [
                    {
                        "name": "Zhihao Chang"
                    },
                    {
                        "name": "Linzhu Yu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Sai Wu"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Dongxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiang Zhang"
                },
                "author": "Dongxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11772v2",
                "updated": "2024-11-05T05:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    13,
                    0,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-15T16:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    53,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST."
                },
                "authors": [
                    {
                        "name": "Kai Yao"
                    },
                    {
                        "name": "Penglei Gao"
                    },
                    {
                        "name": "Lichun Li"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03312v1",
                "updated": "2024-11-05T18:54:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    54,
                    21,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T18:54:21Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    54,
                    21,
                    1,
                    310,
                    0
                ],
                "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Optimal VLMs Need Only One Visual Token but Larger Models"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression."
                },
                "authors": [
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Joao D. Semedo"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03307v1",
                "updated": "2024-11-05T18:01:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    1,
                    12,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T18:01:12Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    1,
                    12,
                    1,
                    310,
                    0
                ],
                "title": "LLMs for Domain Generation Algorithm Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Domain Generation Algorithm Detection"
                },
                "summary": "This work analyzes the use of large language models (LLMs) for detecting\ndomain generation algorithms (DGAs). We perform a detailed evaluation of two\nimportant techniques: In-Context Learning (ICL) and Supervised Fine-Tuning\n(SFT), showing how they can improve detection. SFT increases performance by\nusing domain-specific data, whereas ICL helps the detection model to quickly\nadapt to new threats without requiring much retraining. We use Meta's Llama3 8B\nmodel, on a custom dataset with 68 malware families and normal domains,\ncovering several hard-to-detect schemes, including recent word-based DGAs.\nResults proved that LLM-based methods can achieve competitive results in DGA\ndetection. In particular, the SFT-based LLM DGA detector outperforms\nstate-of-the-art models using attention layers, achieving 94% accuracy with a\n4% false positive rate (FPR) and excelling at detecting word-based DGA domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work analyzes the use of large language models (LLMs) for detecting\ndomain generation algorithms (DGAs). We perform a detailed evaluation of two\nimportant techniques: In-Context Learning (ICL) and Supervised Fine-Tuning\n(SFT), showing how they can improve detection. SFT increases performance by\nusing domain-specific data, whereas ICL helps the detection model to quickly\nadapt to new threats without requiring much retraining. We use Meta's Llama3 8B\nmodel, on a custom dataset with 68 malware families and normal domains,\ncovering several hard-to-detect schemes, including recent word-based DGAs.\nResults proved that LLM-based methods can achieve competitive results in DGA\ndetection. In particular, the SFT-based LLM DGA detector outperforms\nstate-of-the-art models using attention layers, achieving 94% accuracy with a\n4% false positive rate (FPR) and excelling at detecting word-based DGA domains."
                },
                "authors": [
                    {
                        "name": "Reynier Leyva La O"
                    },
                    {
                        "name": "Carlos A. Catania"
                    },
                    {
                        "name": "Tatiana Parlanti"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Parlanti"
                },
                "author": "Tatiana Parlanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03300v1",
                "updated": "2024-11-05T17:53:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    53,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:53:25Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    53,
                    25,
                    1,
                    310,
                    0
                ],
                "title": "VERITAS: A Unified Approach to Reliability Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERITAS: A Unified Approach to Reliability Evaluation"
                },
                "summary": "Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting."
                },
                "authors": [
                    {
                        "name": "Rajkumar Ramamurthy"
                    },
                    {
                        "name": "Meghana Arakkal Rajeev"
                    },
                    {
                        "name": "Oliver Molenschot"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Nazneen Rajani"
                    }
                ],
                "author_detail": {
                    "name": "Nazneen Rajani"
                },
                "author": "Nazneen Rajani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00318v2",
                "updated": "2024-11-05T17:51:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    51,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-03-30T10:54:59Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    10,
                    54,
                    59,
                    5,
                    90,
                    0
                ],
                "title": "Cognitive Planning for Object Goal Navigation using Generative AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Planning for Object Goal Navigation using Generative AI Models"
                },
                "summary": "Recent advancements in Generative AI, particularly in Large Language Models\n(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for\nintegrating cognitive planning into robotic systems. In this work, we present a\nnovel framework for solving the object goal navigation problem that generates\nefficient exploration strategies. Our approach enables a robot to navigate\nunfamiliar environments by leveraging LLMs and LVLMs to understand the semantic\nstructure of the scene. To address the challenge of representing complex\nenvironments without overwhelming the system, we propose a 3D modular scene\nrepresentation, enriched with semantic descriptions. This representation is\ndynamically pruned using an LLM-based mechanism, which filters irrelevant\ninformation and focuses on task-specific data. By combining these elements, our\nsystem generates high-level sub-goals that guide the exploration of the robot\ntoward the target object. We validate our approach in simulated environments,\ndemonstrating its ability to enhance object search efficiency while maintaining\nscalability in complex settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI, particularly in Large Language Models\n(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for\nintegrating cognitive planning into robotic systems. In this work, we present a\nnovel framework for solving the object goal navigation problem that generates\nefficient exploration strategies. Our approach enables a robot to navigate\nunfamiliar environments by leveraging LLMs and LVLMs to understand the semantic\nstructure of the scene. To address the challenge of representing complex\nenvironments without overwhelming the system, we propose a 3D modular scene\nrepresentation, enriched with semantic descriptions. This representation is\ndynamically pruned using an LLM-based mechanism, which filters irrelevant\ninformation and focuses on task-specific data. By combining these elements, our\nsystem generates high-level sub-goals that guide the exploration of the robot\ntoward the target object. We validate our approach in simulated environments,\ndemonstrating its ability to enhance object search efficiency while maintaining\nscalability in complex settings."
                },
                "authors": [
                    {
                        "name": "Arjun P S"
                    },
                    {
                        "name": "Andrew Melnik"
                    },
                    {
                        "name": "Gora Chand Nandi"
                    }
                ],
                "author_detail": {
                    "name": "Gora Chand Nandi"
                },
                "author": "Gora Chand Nandi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03295v1",
                "updated": "2024-11-05T17:42:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    42,
                    43,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:42:43Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    42,
                    43,
                    1,
                    310,
                    0
                ],
                "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online"
                },
                "summary": "This paper examines how large language models (LLMs) can help people write\nconstructive comments in online debates on divisive social issues and whether\nthe notions of constructiveness vary across cultures. Through controlled\nexperiments with 600 participants from India and the US, who reviewed and wrote\nconstructive comments on online threads on Islamophobia and homophobia, we\nfound potential misalignment in how LLMs and humans perceive constructiveness\nin online comments. While the LLM was more likely to view dialectical comments\nas more constructive, participants favored comments that emphasized logic and\nfacts more than the LLM did. Despite these differences, participants rated\nLLM-generated and human-AI co-written comments as significantly more\nconstructive than those written independently by humans. Our analysis also\nrevealed that LLM-generated and human-AI co-written comments exhibited more\nlinguistic features associated with constructiveness compared to human-written\ncomments on divisive topics. When participants used LLMs to refine their\ncomments, the resulting comments were longer, more polite, positive, less\ntoxic, and more readable, with added argumentative features that retained the\noriginal intent but occasionally lost nuances. Based on these findings, we\ndiscuss ethical and design considerations in using LLMs to facilitate\nconstructive discourse online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines how large language models (LLMs) can help people write\nconstructive comments in online debates on divisive social issues and whether\nthe notions of constructiveness vary across cultures. Through controlled\nexperiments with 600 participants from India and the US, who reviewed and wrote\nconstructive comments on online threads on Islamophobia and homophobia, we\nfound potential misalignment in how LLMs and humans perceive constructiveness\nin online comments. While the LLM was more likely to view dialectical comments\nas more constructive, participants favored comments that emphasized logic and\nfacts more than the LLM did. Despite these differences, participants rated\nLLM-generated and human-AI co-written comments as significantly more\nconstructive than those written independently by humans. Our analysis also\nrevealed that LLM-generated and human-AI co-written comments exhibited more\nlinguistic features associated with constructiveness compared to human-written\ncomments on divisive topics. When participants used LLMs to refine their\ncomments, the resulting comments were longer, more polite, positive, less\ntoxic, and more readable, with added argumentative features that retained the\noriginal intent but occasionally lost nuances. Based on these findings, we\ndiscuss ethical and design considerations in using LLMs to facilitate\nconstructive discourse online."
                },
                "authors": [
                    {
                        "name": "Farhana Shahid"
                    },
                    {
                        "name": "Maximilian Dittgen"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03292v1",
                "updated": "2024-11-05T17:40:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    40,
                    3,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:40:03Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    40,
                    3,
                    1,
                    310,
                    0
                ],
                "title": "Interaction2Code: How Far Are We From Automatic Interactive Webpage\n  Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction2Code: How Far Are We From Automatic Interactive Webpage\n  Generation?"
                },
                "summary": "Converting webpage design into functional UI code is a critical step for\nbuilding websites, which can be labor-intensive and time-consuming. To automate\nthis design-to-code transformation process, various automated methods using\nlearning-based networks and multi-modal large language models (MLLMs) have been\nproposed. However, these studies were merely evaluated on a narrow range of\nstatic web pages and ignored dynamic interaction elements, making them less\npractical for real-world website deployment.\n  To fill in the blank, we present the first systematic investigation of MLLMs\nin generating interactive webpages. Specifically, we first formulate the\nInteraction-to-Code task and build the Interaction2Code benchmark that contains\n97 unique web pages and 213 distinct interactions, spanning 15 webpage types\nand 30 interaction categories. We then conduct comprehensive experiments on\nthree state-of-the-art (SOTA) MLLMs using both automatic metrics and human\nevaluations, thereby summarizing six findings accordingly. Our experimental\nresults highlight the limitations of MLLMs in generating fine-grained\ninteractive features and managing interactions with complex transformations and\nsubtle visual modifications. We further analyze failure cases and their\nunderlying causes, identifying 10 common failure types and assessing their\nseverity. Additionally, our findings reveal three critical influencing factors,\ni.e., prompts, visual saliency, and textual descriptions, that can enhance the\ninteraction generation performance of MLLMs. Based on these findings, we elicit\nimplications for researchers and developers, providing a foundation for future\nadvancements in this field. Datasets and source code are available at\nhttps://github.com/WebPAI/Interaction2Code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting webpage design into functional UI code is a critical step for\nbuilding websites, which can be labor-intensive and time-consuming. To automate\nthis design-to-code transformation process, various automated methods using\nlearning-based networks and multi-modal large language models (MLLMs) have been\nproposed. However, these studies were merely evaluated on a narrow range of\nstatic web pages and ignored dynamic interaction elements, making them less\npractical for real-world website deployment.\n  To fill in the blank, we present the first systematic investigation of MLLMs\nin generating interactive webpages. Specifically, we first formulate the\nInteraction-to-Code task and build the Interaction2Code benchmark that contains\n97 unique web pages and 213 distinct interactions, spanning 15 webpage types\nand 30 interaction categories. We then conduct comprehensive experiments on\nthree state-of-the-art (SOTA) MLLMs using both automatic metrics and human\nevaluations, thereby summarizing six findings accordingly. Our experimental\nresults highlight the limitations of MLLMs in generating fine-grained\ninteractive features and managing interactions with complex transformations and\nsubtle visual modifications. We further analyze failure cases and their\nunderlying causes, identifying 10 common failure types and assessing their\nseverity. Additionally, our findings reveal three critical influencing factors,\ni.e., prompts, visual saliency, and textual descriptions, that can enhance the\ninteraction generation performance of MLLMs. Based on these findings, we elicit\nimplications for researchers and developers, providing a foundation for future\nadvancements in this field. Datasets and source code are available at\nhttps://github.com/WebPAI/Interaction2Code."
                },
                "authors": [
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Zhiyao Xu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03287v1",
                "updated": "2024-11-05T17:36:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    36,
                    32,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:36:32Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    36,
                    32,
                    1,
                    310,
                    0
                ],
                "title": "The Future of Intelligent Healthcare: A Systematic Analysis and\n  Discussion on the Integration and Impact of Robots Using Large Language\n  Models for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of Intelligent Healthcare: A Systematic Analysis and\n  Discussion on the Integration and Impact of Robots Using Large Language\n  Models for Healthcare"
                },
                "summary": "The potential use of large language models (LLMs) in healthcare robotics can\nhelp address the significant demand put on healthcare systems around the world\nwith respect to an aging demographic and a shortage of healthcare\nprofessionals. Even though LLMs have already been integrated into medicine to\nassist both clinicians and patients, the integration of LLMs within healthcare\nrobots has not yet been explored for clinical settings. In this perspective\npaper, we investigate the groundbreaking developments in robotics and LLMs to\nuniquely identify the needed system requirements for designing health specific\nLLM based robots in terms of multi modal communication through human robot\ninteractions (HRIs), semantic reasoning, and task planning. Furthermore, we\ndiscuss the ethical issues, open challenges, and potential future research\ndirections for this emerging innovative field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential use of large language models (LLMs) in healthcare robotics can\nhelp address the significant demand put on healthcare systems around the world\nwith respect to an aging demographic and a shortage of healthcare\nprofessionals. Even though LLMs have already been integrated into medicine to\nassist both clinicians and patients, the integration of LLMs within healthcare\nrobots has not yet been explored for clinical settings. In this perspective\npaper, we investigate the groundbreaking developments in robotics and LLMs to\nuniquely identify the needed system requirements for designing health specific\nLLM based robots in terms of multi modal communication through human robot\ninteractions (HRIs), semantic reasoning, and task planning. Furthermore, we\ndiscuss the ethical issues, open challenges, and potential future research\ndirections for this emerging innovative field."
                },
                "authors": [
                    {
                        "name": "Souren Pashangpour"
                    },
                    {
                        "name": "Goldie Nejat"
                    }
                ],
                "author_detail": {
                    "name": "Goldie Nejat"
                },
                "author": "Goldie Nejat",
                "arxiv_doi": "10.3390/robotics13080112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/robotics13080112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "MDPI Robotics 2024, 13(8)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03284v1",
                "updated": "2024-11-05T17:33:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    33,
                    39,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T17:33:39Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    33,
                    39,
                    1,
                    310,
                    0
                ],
                "title": "SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents"
                },
                "summary": "While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Peijia Qian"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Kumar Satvik Chaudhary"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Jiayi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Shen"
                },
                "author": "Jiayi Shen",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01081v2",
                "updated": "2024-11-05T17:23:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    23,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-01T23:36:19Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    36,
                    19,
                    4,
                    306,
                    0
                ],
                "title": "Towards efficient and secure quantum-classical communication networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards efficient and secure quantum-classical communication networks"
                },
                "summary": "The rapid advancement of quantum technologies calls for the design and\ndeployment of quantum-safe cryptographic protocols and communication networks.\nThere are two primary approaches to achieving quantum-resistant security:\nquantum key distribution (QKD) and post-quantum cryptography (PQC). While each\noffers unique advantages, both have drawbacks in practical implementation. In\nthis work, we introduce the pros and cons of these protocols and explore how\nthey can be combined to achieve a higher level of security and/or improved\nperformance in key distribution. We hope our discussion inspires further\nresearch into the design of hybrid cryptographic protocols for\nquantum-classical communication networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of quantum technologies calls for the design and\ndeployment of quantum-safe cryptographic protocols and communication networks.\nThere are two primary approaches to achieving quantum-resistant security:\nquantum key distribution (QKD) and post-quantum cryptography (PQC). While each\noffers unique advantages, both have drawbacks in practical implementation. In\nthis work, we introduce the pros and cons of these protocols and explore how\nthey can be combined to achieve a higher level of security and/or improved\nperformance in key distribution. We hope our discussion inspires further\nresearch into the design of hybrid cryptographic protocols for\nquantum-classical communication networks."
                },
                "authors": [
                    {
                        "name": "Pei Zeng"
                    },
                    {
                        "name": "Debayan Bandyopadhyay"
                    },
                    {
                        "name": "Jos A. Mndez Mndez"
                    },
                    {
                        "name": "Nolan Bitner"
                    },
                    {
                        "name": "Alexander Kolar"
                    },
                    {
                        "name": "Michael T. Solomon"
                    },
                    {
                        "name": "F. Joseph Heremans"
                    },
                    {
                        "name": "David D. Awschalom"
                    },
                    {
                        "name": "Liang Jiang"
                    },
                    {
                        "name": "Junyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junyu Liu"
                },
                "author": "Junyu Liu",
                "arxiv_comment": "4 pages, a blue print paper, Submission for IEEE 2024 IEEE Workshop\n  on Quantum IntelLigence, Learning & Security (QUILLS),\n  https://sites.google.com/pitt.edu/quills/home",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16299v2",
                "updated": "2024-11-05T17:22:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    22,
                    10,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-09T19:35:34Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    19,
                    35,
                    34,
                    0,
                    253,
                    0
                ],
                "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale"
                },
                "summary": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Huy Nhat Phan"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03992v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03992v4",
                "updated": "2024-11-05T16:57:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    57,
                    26,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-06T02:44:27Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "title": "Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark\n  Study"
                },
                "summary": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA Hopper GPUs for large language model (LLM)\ninference tasks. We benchmark the overhead introduced by TEE mode across\nvarious LLMs and token lengths, with a particular focus on the bottleneck\ncaused by CPU-GPU data transfers via PCIe. Our results indicate that while\nthere is minimal computational overhead within the GPU, the overall performance\npenalty is primarily attributable to data transfer. For the majority of typical\nLLM queries, the overhead remains below 7%, with larger models and longer\nsequences experiencing nearly zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA Hopper GPUs for large language model (LLM)\ninference tasks. We benchmark the overhead introduced by TEE mode across\nvarious LLMs and token lengths, with a particular focus on the bottleneck\ncaused by CPU-GPU data transfers via PCIe. Our results indicate that while\nthere is minimal computational overhead within the GPU, the overall performance\npenalty is primarily attributable to data transfer. For the majority of typical\nLLM queries, the overhead remains below 7%, with larger models and longer\nsequences experiencing nearly zero overhead."
                },
                "authors": [
                    {
                        "name": "Jianwei Zhu"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Peng Deng"
                    },
                    {
                        "name": "Aline Almeida"
                    },
                    {
                        "name": "Shunfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shunfan Zhou"
                },
                "author": "Shunfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03992v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03992v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14550v2",
                "updated": "2024-11-05T16:51:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    51,
                    40,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-20T17:57:51Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    57,
                    51,
                    3,
                    172,
                    0
                ],
                "title": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models"
                },
                "summary": "Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks."
                },
                "authors": [
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Ge Bai"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "[EMNLP 2024] The first four authors contributed equally, 29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03252v1",
                "updated": "2024-11-05T16:49:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    49,
                    33,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    49,
                    33,
                    1,
                    310,
                    0
                ],
                "title": "Spontaneous Emergence of Agent Individuality through Social Interactions\n  in LLM-Based Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous Emergence of Agent Individuality through Social Interactions\n  in LLM-Based Communities"
                },
                "summary": "We study the emergence of agency from scratch by using Large Language Model\n(LLM)-based agents. In previous studies of LLM-based agents, each agent's\ncharacteristics, including personality and memory, have traditionally been\npredefined. We focused on how individuality, such as behavior, personality, and\nmemory, can be differentiated from an undifferentiated state. The present LLM\nagents engage in cooperative communication within a group simulation,\nexchanging context-based messages in natural language. By analyzing this\nmulti-agent simulation, we report valuable new insights into how social norms,\ncooperation, and personality traits can emerge spontaneously. This paper\ndemonstrates that autonomously interacting LLM-powered agents generate\nhallucinations and hashtags to sustain communication, which, in turn, increases\nthe diversity of words within their interactions. Each agent's emotions shift\nthrough communication, and as they form communities, the personalities of the\nagents emerge and evolve accordingly. This computational modeling approach and\nits findings will provide a new method for analyzing collective artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the emergence of agency from scratch by using Large Language Model\n(LLM)-based agents. In previous studies of LLM-based agents, each agent's\ncharacteristics, including personality and memory, have traditionally been\npredefined. We focused on how individuality, such as behavior, personality, and\nmemory, can be differentiated from an undifferentiated state. The present LLM\nagents engage in cooperative communication within a group simulation,\nexchanging context-based messages in natural language. By analyzing this\nmulti-agent simulation, we report valuable new insights into how social norms,\ncooperation, and personality traits can emerge spontaneously. This paper\ndemonstrates that autonomously interacting LLM-powered agents generate\nhallucinations and hashtags to sustain communication, which, in turn, increases\nthe diversity of words within their interactions. Each agent's emotions shift\nthrough communication, and as they form communities, the personalities of the\nagents emerge and evolve accordingly. This computational modeling approach and\nits findings will provide a new method for analyzing collective artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Ryosuke Takata"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03250v1",
                "updated": "2024-11-05T16:47:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    47,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T16:47:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    47,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language\n  Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2-7 percent in certain cases. The data and code\nwill be publicly available upon completion of internal review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2-7 percent in certain cases. The data and code\nwill be publicly available upon completion of internal review."
                },
                "authors": [
                    {
                        "name": "Ying Zhou"
                    },
                    {
                        "name": "Xinyao Wang"
                    },
                    {
                        "name": "Yulei Niu"
                    },
                    {
                        "name": "Yaojie Shen"
                    },
                    {
                        "name": "Lexin Tang"
                    },
                    {
                        "name": "Fan Chen"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Longyin Wen"
                    }
                ],
                "author_detail": {
                    "name": "Longyin Wen"
                },
                "author": "Longyin Wen",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11811v2",
                "updated": "2024-11-05T16:47:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    47,
                    43,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-17T17:52:54Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    52,
                    54,
                    0,
                    169,
                    0
                ],
                "title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content"
                },
                "summary": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa."
                },
                "authors": [
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Pierre-Andre Noel"
                    },
                    {
                        "name": "Etienne Marcotte"
                    },
                    {
                        "name": "Sai Rajeswar"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14762v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14762v3",
                "updated": "2024-11-05T16:40:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    40,
                    21,
                    1,
                    310,
                    0
                ],
                "published": "2024-02-22T18:21:59Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    21,
                    59,
                    3,
                    53,
                    0
                ],
                "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues"
                },
                "summary": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}."
                },
                "authors": [
                    {
                        "name": "Ge Bai"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhanhui Zhou"
                    },
                    {
                        "name": "Zhuoran Lin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "arxiv_doi": "10.18653/v1/2024.acl-long.401",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.401",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.14762v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14762v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "[ACL 2024] The first three authors contribute equally, 34 pages, repo\n  at https://github.com/mtbench101/mt-bench-101",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17898v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17898v3",
                "updated": "2024-11-05T16:31:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    31,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2023-11-29T18:51:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    18,
                    51,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "Contextual Knowledge Pursuit for Faithful Visual Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Knowledge Pursuit for Faithful Visual Synthesis"
                },
                "summary": "Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Jinqi Luo"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Dimitris Dimos"
                    },
                    {
                        "name": "Ren Vidal"
                    }
                ],
                "author_detail": {
                    "name": "Ren Vidal"
                },
                "author": "Ren Vidal",
                "arxiv_comment": "Accepted in ECCV 2024 SDCV Workshop. GitHub repository at\n  https://github.com/peterljq/Contextual-Knowledge-Pursuit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17898v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17898v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18336v2",
                "updated": "2024-11-05T16:30:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    30,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-26T23:18:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    18,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "DeBaRA: Denoising-Based 3D Room Arrangement Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeBaRA: Denoising-Based 3D Room Arrangement Generation"
                },
                "summary": "Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios."
                },
                "authors": [
                    {
                        "name": "Lopold Maillard"
                    },
                    {
                        "name": "Nicolas Sereyjol-Garros"
                    },
                    {
                        "name": "Tom Durand"
                    },
                    {
                        "name": "Maks Ovsjanikov"
                    }
                ],
                "author_detail": {
                    "name": "Maks Ovsjanikov"
                },
                "author": "Maks Ovsjanikov",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11019v2",
                "updated": "2024-11-05T16:21:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    21,
                    55,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-28T17:31:47Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    31,
                    47,
                    4,
                    180,
                    0
                ],
                "title": "Efficacy of Various Large Language Models in Generating Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficacy of Various Large Language Models in Generating Smart Contracts"
                },
                "summary": "This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies."
                },
                "authors": [
                    {
                        "name": "Siddhartha Chatterjee"
                    },
                    {
                        "name": "Bina Ramamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Bina Ramamurthy"
                },
                "author": "Bina Ramamurthy",
                "arxiv_comment": "18 pages, accepted for presentation at 8th annual Future of\n  Information and Communication Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06477v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06477v4",
                "updated": "2024-11-05T16:02:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    2,
                    21,
                    1,
                    310,
                    0
                ],
                "published": "2024-01-12T09:56:57Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    56,
                    57,
                    4,
                    12,
                    0
                ],
                "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation"
                },
                "summary": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun"
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06477v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06477v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01483v2",
                "updated": "2024-11-05T15:55:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    55,
                    51,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-03T08:49:55Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    8,
                    49,
                    55,
                    6,
                    308,
                    0
                ],
                "title": "Teaching Models to Improve on Tape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Improve on Tape"
                },
                "summary": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts."
                },
                "authors": [
                    {
                        "name": "Liat Bezalel"
                    },
                    {
                        "name": "Eyal Orgad"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03205v1",
                "updated": "2024-11-05T15:53:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:53:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis"
                },
                "summary": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Temitope Akinboyewa"
                    },
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "M. Naser Lessani"
                    }
                ],
                "author_detail": {
                    "name": "M. Naser Lessani"
                },
                "author": "M. Naser Lessani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04331v2",
                "updated": "2024-11-05T15:43:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    43,
                    18,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-06T17:59:10Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    59,
                    10,
                    3,
                    158,
                    0
                ],
                "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaCE: Parsimonious Concept Engineering for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities."
                },
                "authors": [
                    {
                        "name": "Jinqi Luo"
                    },
                    {
                        "name": "Tianjiao Ding"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Darshan Thaker"
                    },
                    {
                        "name": "Aditya Chattopadhyay"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Ren Vidal"
                    }
                ],
                "author_detail": {
                    "name": "Ren Vidal"
                },
                "author": "Ren Vidal",
                "arxiv_comment": "Accepted in NeurIPS 2024. GitHub repository at\n  https://github.com/peterljq/Parsimonious-Concept-Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06393v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06393v4",
                "updated": "2024-11-05T15:40:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    40,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-04-09T15:35:52Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    15,
                    35,
                    52,
                    1,
                    100,
                    0
                ],
                "title": "MuPT: A Generative Symbolic Music Pretrained Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuPT: A Generative Symbolic Music Pretrained Transformer"
                },
                "summary": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the application of Large Language Models (LLMs) to\nthe pre-training of music. While the prevalent use of MIDI in music modeling is\nwell-established, our findings suggest that LLMs are inherently more compatible\nwith ABC Notation, which aligns more closely with their design and strengths,\nthereby enhancing the model's performance in musical composition. To address\nthe challenges associated with misaligned measures from different tracks during\ngeneration, we propose the development of a Synchronized Multi-Track ABC\nNotation (SMT-ABC Notation), which aims to preserve coherence across multiple\nmusical tracks. Our contributions include a series of models capable of\nhandling up to 8192 tokens, covering 90% of the symbolic music data in our\ntraining set. Furthermore, we explore the implications of the Symbolic Music\nScaling Law (SMS Law) on model performance. The results indicate a promising\ndirection for future research in music generation, offering extensive resources\nfor community-led research through our open-source contributions."
                },
                "authors": [
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Ziya Zhou"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Lejun Min"
                    },
                    {
                        "name": "Xueling Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Fengze Han"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06393v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06393v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01076v2",
                "updated": "2024-11-05T15:03:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    3,
                    45,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-01T23:14:30Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    14,
                    30,
                    4,
                    306,
                    0
                ],
                "title": "Privacy Risks of Speculative Decoding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Risks of Speculative Decoding in Large Language Models"
                },
                "summary": "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches."
                },
                "authors": [
                    {
                        "name": "Jiankun Wei"
                    },
                    {
                        "name": "Abdulrahman Abdulrazzag"
                    },
                    {
                        "name": "Tianchen Zhang"
                    },
                    {
                        "name": "Adel Muursepp"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03137v1",
                "updated": "2024-11-05T14:30:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    30,
                    12,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T14:30:12Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    30,
                    12,
                    1,
                    310,
                    0
                ],
                "title": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing\n  Practice"
                },
                "summary": "Creative writers have a love for their craft, yet AI systems using large\nlanguage models (LLMs) offer the automation of significant parts of the writing\nprocess. So why do some creative writers choose to integrate AI into their\nworkflows? To explore this, we interview and observe a writing session with 18\ncreative writers who already use AI regularly in their writing practice. Our\nfindings reveal that creative writers are intentional about how they\nincorporate AI, making many deliberate decisions about when and how to engage\nAI based on the core values they hold about writing. These values, such as\nauthenticity and craftsmanship, alongside writers' relationships with and use\nof AI influence the parts of writing over which they wish to maintain control.\nThrough our analysis, we contribute a taxonomy of writer values, writer\nrelationships with AI, and integration strategies, and discuss how these three\nelements interrelate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative writers have a love for their craft, yet AI systems using large\nlanguage models (LLMs) offer the automation of significant parts of the writing\nprocess. So why do some creative writers choose to integrate AI into their\nworkflows? To explore this, we interview and observe a writing session with 18\ncreative writers who already use AI regularly in their writing practice. Our\nfindings reveal that creative writers are intentional about how they\nincorporate AI, making many deliberate decisions about when and how to engage\nAI based on the core values they hold about writing. These values, such as\nauthenticity and craftsmanship, alongside writers' relationships with and use\nof AI influence the parts of writing over which they wish to maintain control.\nThrough our analysis, we contribute a taxonomy of writer values, writer\nrelationships with AI, and integration strategies, and discuss how these three\nelements interrelate."
                },
                "authors": [
                    {
                        "name": "Alicia Guo"
                    },
                    {
                        "name": "Shreya Sathyanarayanan"
                    },
                    {
                        "name": "Leijie Wang"
                    },
                    {
                        "name": "Jeffrey Heer"
                    },
                    {
                        "name": "Amy Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy Zhang"
                },
                "author": "Amy Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20598v2",
                "updated": "2024-11-05T14:15:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    15,
                    3,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-27T21:12:12Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    12,
                    12,
                    6,
                    301,
                    0
                ],
                "title": "R^3AG: First Workshop on Refined and Reliable Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R^3AG: First Workshop on Refined and Reliable Retrieval Augmented\n  Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained wide attention as the key\ncomponent to improve generative models with external knowledge augmentation\nfrom information retrieval. It has shown great prominence in enhancing the\nfunctionality and performance of large language model (LLM)-based applications.\nHowever, with the comprehensive application of RAG, more and more problems and\nlimitations have been identified, thus urgently requiring further fundamental\nexploration to improve current RAG frameworks. This workshop aims to explore in\ndepth how to conduct refined and reliable RAG for downstream AI tasks.\n  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024\nto call for participants to re-examine and formulate the basic principles and\npractical implementation of refined and reliable RAG. The workshop serves as a\nplatform for both academia and industry researchers to conduct discussions,\nshare insights, and foster research to build the next generation of RAG\nsystems. Participants will engage in discussions and presentations focusing on\nfundamental challenges, cutting-edge research, and potential pathways to\nimprove RAG. At the end of the workshop, we aim to have a clearer understanding\nof how to improve the reliability and applicability of RAG with more robust\ninformation retrieval and language generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained wide attention as the key\ncomponent to improve generative models with external knowledge augmentation\nfrom information retrieval. It has shown great prominence in enhancing the\nfunctionality and performance of large language model (LLM)-based applications.\nHowever, with the comprehensive application of RAG, more and more problems and\nlimitations have been identified, thus urgently requiring further fundamental\nexploration to improve current RAG frameworks. This workshop aims to explore in\ndepth how to conduct refined and reliable RAG for downstream AI tasks.\n  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024\nto call for participants to re-examine and formulate the basic principles and\npractical implementation of refined and reliable RAG. The workshop serves as a\nplatform for both academia and industry researchers to conduct discussions,\nshare insights, and foster research to build the next generation of RAG\nsystems. Participants will engage in discussions and presentations focusing on\nfundamental challenges, cutting-edge research, and potential pathways to\nimprove RAG. At the end of the workshop, we aim to have a clearer understanding\nof how to improve the reliability and applicability of RAG with more robust\ninformation retrieval and language generation."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Xuri Ge"
                    },
                    {
                        "name": "Joemon M. Jose"
                    },
                    {
                        "name": "Haitao Yu"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Xin Xin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xin"
                },
                "author": "Xin Xin",
                "arxiv_comment": "R^3AG workshop overview at SIGIR-AP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05327v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05327v3",
                "updated": "2024-11-05T14:12:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    14,
                    12,
                    40,
                    1,
                    310,
                    0
                ],
                "published": "2023-12-08T19:24:05Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    19,
                    24,
                    5,
                    4,
                    342,
                    0
                ],
                "title": "Better, Not Just More: Data-Centric Machine Learning for Earth\n  Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better, Not Just More: Data-Centric Machine Learning for Earth\n  Observation"
                },
                "summary": "Recent developments and research in modern machine learning have led to\nsubstantial improvements in the geospatial field. Although numerous deep\nlearning architectures and models have been proposed, the majority of them have\nbeen solely developed on benchmark datasets that lack strong real-world\nrelevance. Furthermore, the performance of many methods has already saturated\non these datasets. We argue that a shift from a model-centric view to a\ncomplementary data-centric perspective is necessary for further improvements in\naccuracy, generalization ability, and real impact on end-user applications.\nFurthermore, considering the entire machine learning cycle-from problem\ndefinition to model deployment with feedback-is crucial for enhancing machine\nlearning models that can be reliable in unforeseen situations. This work\npresents a definition as well as a precise categorization and overview of\nautomated data-centric learning approaches for geospatial data. It highlights\nthe complementary role of data-centric learning with respect to model-centric\nin the larger machine learning deployment cycle. We review papers across the\nentire geospatial field and categorize them into different groups. A set of\nrepresentative experiments shows concrete implementation examples. These\nexamples provide concrete steps to act on geospatial data with data-centric\nmachine learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments and research in modern machine learning have led to\nsubstantial improvements in the geospatial field. Although numerous deep\nlearning architectures and models have been proposed, the majority of them have\nbeen solely developed on benchmark datasets that lack strong real-world\nrelevance. Furthermore, the performance of many methods has already saturated\non these datasets. We argue that a shift from a model-centric view to a\ncomplementary data-centric perspective is necessary for further improvements in\naccuracy, generalization ability, and real impact on end-user applications.\nFurthermore, considering the entire machine learning cycle-from problem\ndefinition to model deployment with feedback-is crucial for enhancing machine\nlearning models that can be reliable in unforeseen situations. This work\npresents a definition as well as a precise categorization and overview of\nautomated data-centric learning approaches for geospatial data. It highlights\nthe complementary role of data-centric learning with respect to model-centric\nin the larger machine learning deployment cycle. We review papers across the\nentire geospatial field and categorize them into different groups. A set of\nrepresentative experiments shows concrete implementation examples. These\nexamples provide concrete steps to act on geospatial data with data-centric\nmachine learning approaches."
                },
                "authors": [
                    {
                        "name": "Ribana Roscher"
                    },
                    {
                        "name": "Marc Ruwurm"
                    },
                    {
                        "name": "Caroline Gevaert"
                    },
                    {
                        "name": "Michael Kampffmeyer"
                    },
                    {
                        "name": "Jefersson A. dos Santos"
                    },
                    {
                        "name": "Maria Vakalopoulou"
                    },
                    {
                        "name": "Ronny Hnsch"
                    },
                    {
                        "name": "Stine Hansen"
                    },
                    {
                        "name": "Keiller Nogueira"
                    },
                    {
                        "name": "Jonathan Prexl"
                    },
                    {
                        "name": "Devis Tuia"
                    }
                ],
                "author_detail": {
                    "name": "Devis Tuia"
                },
                "author": "Devis Tuia",
                "arxiv_doi": "10.1109/MGRS.2024.3470986",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MGRS.2024.3470986",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05327v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05327v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to Geoscience and Remote Sensing Magazine",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03108v1",
                "updated": "2024-11-05T13:56:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    56,
                    42,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:56:42Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    56,
                    42,
                    1,
                    310,
                    0
                ],
                "title": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited\n  Deceptive Designs in Generated Websites Without Warning"
                },
                "summary": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advancements in Large Language Models (LLMs), web developers\nincreasingly apply their code-generation capabilities to website design.\nHowever, since these models are trained on existing designerly knowledge, they\nmay inadvertently replicate bad or even illegal practices, especially deceptive\ndesigns (DD). This paper examines whether users can accidentally create DD for\na fictitious webshop using GPT-4. We recruited 20 participants, asking them to\nuse ChatGPT to generate functionalities (product overview or checkout) and then\nmodify these using neutral prompts to meet a business goal (e.g., \"increase the\nlikelihood of us selling our product\"). We found that all 20 generated websites\ncontained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no\nwarnings. When reflecting on the designs, only 4 participants expressed\nconcerns, while most considered the outcomes satisfactory and not morally\nproblematic, despite the potential ethical and legal implications for end-users\nand those adopting ChatGPT's recommendations"
                },
                "authors": [
                    {
                        "name": "Veronika Krau"
                    },
                    {
                        "name": "Mark McGill"
                    },
                    {
                        "name": "Thomas Kosch"
                    },
                    {
                        "name": "Yolanda Thiel"
                    },
                    {
                        "name": "Dominik Schn"
                    },
                    {
                        "name": "Jan Gugenheimer"
                    }
                ],
                "author_detail": {
                    "name": "Jan Gugenheimer"
                },
                "author": "Jan Gugenheimer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23074v2",
                "updated": "2024-11-05T13:26:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    26,
                    7,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-30T14:46:43Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    46,
                    43,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Programming Language Sandbox for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Programming Language Sandbox for LLMs"
                },
                "summary": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Yunbo Tao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Haoxiang Jia"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shenxi Wu"
                    },
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Limao Xiong"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yueming Wu"
                    },
                    {
                        "name": "Ming Wen"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03079v1",
                "updated": "2024-11-05T13:24:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    24,
                    56,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T13:24:56Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    24,
                    56,
                    1,
                    310,
                    0
                ],
                "title": "Utilizing Precise and Complete Code Context to Guide LLM in Automatic\n  False Positive Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Precise and Complete Code Context to Guide LLM in Automatic\n  False Positive Mitigation"
                },
                "summary": "Static Application Security Testing(SAST) tools are crucial for early bug\ndetection and code quality but often generate false positives that slow\ndevelopment. Automating false positive mitigation is thus essential for\nadvancing SAST tools. Past efforts use static/dynamic analysis or machine\nlearning. The advent of Large Language Models, adept at understanding natural\nlanguage and code, offers promising ways to improve the accuracy and usability\nof SAST tools. However, existing LLM-based methods need improvement in two key\nareas: first, extracted code snippets related to warnings are often cluttered\nwith irrelevant control and data flows, reducing precision; second, critical\ncode contexts are often missing, leading to incomplete representations that can\nmislead LLMs and cause inaccurate assessments. To ensure the use of precise and\ncomplete code context, thereby avoiding misguidance and enabling LLMs to reach\naccurate conclusions, we propose LLM4FPM. One of its core components is\neCPG-Slicer, which builds an extended code property graph and extracts\nline-level, precise code context. Moreover, LLM4FPM incorporates FARF\nalgorithm, which builds a file reference graph and then efficiently detects all\nfiles related to a warning in linear time, enabling eCPG-Slicer to gather\ncomplete code context across these files. We evaluate LLM4FPM on Juliet\ndataset, where it comprehensively outperforms the baseline, achieving an F1\nscore above 99% across various CWEs. LLM4FPM leverages a free, open-source\nmodel, avoiding costly alternatives and reducing inspection costs by up to\n$2758 per run on Juliet, with an average inspection time of 4.7 seconds per\nwarning. Our work emphasizes the critical impact of precise and complete code\ncontext and highlights the potential of combining program analysis with LLMs,\nimproving the quality and efficiency of software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Application Security Testing(SAST) tools are crucial for early bug\ndetection and code quality but often generate false positives that slow\ndevelopment. Automating false positive mitigation is thus essential for\nadvancing SAST tools. Past efforts use static/dynamic analysis or machine\nlearning. The advent of Large Language Models, adept at understanding natural\nlanguage and code, offers promising ways to improve the accuracy and usability\nof SAST tools. However, existing LLM-based methods need improvement in two key\nareas: first, extracted code snippets related to warnings are often cluttered\nwith irrelevant control and data flows, reducing precision; second, critical\ncode contexts are often missing, leading to incomplete representations that can\nmislead LLMs and cause inaccurate assessments. To ensure the use of precise and\ncomplete code context, thereby avoiding misguidance and enabling LLMs to reach\naccurate conclusions, we propose LLM4FPM. One of its core components is\neCPG-Slicer, which builds an extended code property graph and extracts\nline-level, precise code context. Moreover, LLM4FPM incorporates FARF\nalgorithm, which builds a file reference graph and then efficiently detects all\nfiles related to a warning in linear time, enabling eCPG-Slicer to gather\ncomplete code context across these files. We evaluate LLM4FPM on Juliet\ndataset, where it comprehensively outperforms the baseline, achieving an F1\nscore above 99% across various CWEs. LLM4FPM leverages a free, open-source\nmodel, avoiding costly alternatives and reducing inspection costs by up to\n$2758 per run on Juliet, with an average inspection time of 4.7 seconds per\nwarning. Our work emphasizes the critical impact of precise and complete code\ncontext and highlights the potential of combining program analysis with LLMs,\nimproving the quality and efficiency of software development."
                },
                "authors": [
                    {
                        "name": "Jinbao Chen"
                    },
                    {
                        "name": "Hongjing Xiang"
                    },
                    {
                        "name": "Luhao Li"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Boyao Ding"
                    },
                    {
                        "name": "Qingwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingwei Li"
                },
                "arxiv_affiliation": "University of Science and Technology of China",
                "author": "Qingwei Li",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2; D.2.5; F.2.1; F.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10229v2",
                "updated": "2024-11-05T13:18:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    18,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-04-16T02:19:28Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    2,
                    19,
                    28,
                    1,
                    107,
                    0
                ],
                "title": "Generative Text Steganography with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Text Steganography with Large Language Model"
                },
                "summary": "Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Wu"
                    },
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Yiming Xue"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Peng"
                },
                "author": "Wanli Peng",
                "arxiv_doi": "10.1145/3664647.3680562",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3680562",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.10229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 4 figures, accepted at ACM Multimedia 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13618v2",
                "updated": "2024-11-05T13:17:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    13,
                    17,
                    56,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-19T15:14:55Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    15,
                    14,
                    55,
                    2,
                    171,
                    0
                ],
                "title": "In-Context Former: Lightning-fast Compressing Context for Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Former: Lightning-fast Compressing Context for Large Language\n  Model"
                },
                "summary": "With the rising popularity of Transformer-based large language models (LLMs),\nreducing their high inference costs has become a significant research focus.\nOne effective approach is to compress the long input contexts. Existing methods\ntypically leverage the self-attention mechanism of the LLM itself for context\ncompression. While these methods have achieved notable results, the compression\nprocess still involves quadratic time complexity, which limits their\napplicability. To mitigate this limitation, we propose the In-Context Former\n(IC-Former). Unlike previous methods, IC-Former does not depend on the target\nLLMs. Instead, it leverages the cross-attention mechanism and a small number of\nlearnable digest tokens to directly condense information from the contextual\nword embeddings. This approach significantly reduces inference time, which\nachieves linear growth in time complexity within the compression range.\nExperimental results indicate that our method requires only 1/32 of the\nfloating-point operations of the baseline during compression and improves\nprocessing speed by 68 to 112 times while achieving over 90% of the baseline\nperformance on evaluation metrics. Overall, our model effectively reduces\ncompression costs and makes real-time compression scenarios feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising popularity of Transformer-based large language models (LLMs),\nreducing their high inference costs has become a significant research focus.\nOne effective approach is to compress the long input contexts. Existing methods\ntypically leverage the self-attention mechanism of the LLM itself for context\ncompression. While these methods have achieved notable results, the compression\nprocess still involves quadratic time complexity, which limits their\napplicability. To mitigate this limitation, we propose the In-Context Former\n(IC-Former). Unlike previous methods, IC-Former does not depend on the target\nLLMs. Instead, it leverages the cross-attention mechanism and a small number of\nlearnable digest tokens to directly condense information from the contextual\nword embeddings. This approach significantly reduces inference time, which\nachieves linear growth in time complexity within the compression range.\nExperimental results indicate that our method requires only 1/32 of the\nfloating-point operations of the baseline during compression and improves\nprocessing speed by 68 to 112 times while achieving over 90% of the baseline\nperformance on evaluation metrics. Overall, our model effectively reduces\ncompression costs and makes real-time compression scenarios feasible."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Wang"
                    },
                    {
                        "name": "Zaiyi Chen"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yongyi He"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by EMNLP2024(Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03095v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03095v4",
                "updated": "2024-11-05T12:57:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    57,
                    35,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-06T10:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    52,
                    41,
                    1,
                    219,
                    0
                ],
                "title": "Improving LLM-based Unit test generation via Template-based Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM-based Unit test generation via Template-based Repair"
                },
                "summary": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%."
                },
                "authors": [
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Fangyuan Tian"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03095v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03095v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04592v2",
                "updated": "2024-11-05T12:24:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    24,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-06T19:02:22Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    2,
                    22,
                    6,
                    280,
                    0
                ],
                "title": "CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and\n  Risk Detection of Cancer Treatment-Induced Cardiotoxicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and\n  Risk Detection of Cancer Treatment-Induced Cardiotoxicity"
                },
                "summary": "Despite recent advances in cancer treatments that prolong patients' lives,\ntreatment-induced cardiotoxicity remains one severe side effect. The clinical\ndecision-making of cardiotoxicity is challenging, as non-clinical symptoms can\nbe missed until life-threatening events occur at a later stage, and clinicians\nalready have a high workload centered on the treatment, not the side effects.\nOur project starts with a participatory design study with 11 clinicians to\nunderstand their practices and needs; then we build a multimodal AI system,\nCardioAI, that integrates wearables and LLM-powered voice assistants to monitor\nmultimodal non-clinical symptoms. Also, the system includes an explainable risk\nprediction module that can generate cardiotoxicity risk scores and summaries as\nexplanations to support clinicians' decision-making. We conducted a heuristic\nevaluation with four clinical experts and found that they all believe CardioAI\nintegrates well into their workflow, reduces their information overload, and\nenables them to make more informed decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in cancer treatments that prolong patients' lives,\ntreatment-induced cardiotoxicity remains one severe side effect. The clinical\ndecision-making of cardiotoxicity is challenging, as non-clinical symptoms can\nbe missed until life-threatening events occur at a later stage, and clinicians\nalready have a high workload centered on the treatment, not the side effects.\nOur project starts with a participatory design study with 11 clinicians to\nunderstand their practices and needs; then we build a multimodal AI system,\nCardioAI, that integrates wearables and LLM-powered voice assistants to monitor\nmultimodal non-clinical symptoms. Also, the system includes an explainable risk\nprediction module that can generate cardiotoxicity risk scores and summaries as\nexplanations to support clinicians' decision-making. We conducted a heuristic\nevaluation with four clinical experts and found that they all believe CardioAI\nintegrates well into their workflow, reduces their information overload, and\nenables them to make more informed decisions."
                },
                "authors": [
                    {
                        "name": "Siyi Wu"
                    },
                    {
                        "name": "Weidan Cao"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Ziqi Yang"
                    },
                    {
                        "name": "Changchang Yin"
                    },
                    {
                        "name": "Varun Mishra"
                    },
                    {
                        "name": "Daniel Addison"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v2",
                "updated": "2024-11-05T12:10:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    12,
                    10,
                    51,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$=0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$=0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03022v1",
                "updated": "2024-11-05T11:44:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    44,
                    54,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T11:44:54Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    44,
                    54,
                    1,
                    310,
                    0
                ],
                "title": "Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS\n  Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS\n  Cameras"
                },
                "summary": "While security vulnerabilities in traditional Deep Neural Networks (DNNs)\nhave been extensively studied, the susceptibility of Spiking Neural Networks\n(SNNs) to adversarial attacks remains mostly underexplored. Until now, the\nmechanisms to inject backdoors into SNN models have been limited to digital\nscenarios; thus, we present the first evaluation of backdoor attacks in\nreal-world environments.\n  We begin by assessing the applicability of existing digital backdoor attacks\nand identifying their limitations for deployment in physical environments. To\naddress each of the found limitations, we present three novel backdoor attack\nmethods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess\nthe effectiveness of traditional backdoor procedures and defenses adapted for\nSNNs, such as pruning, fine-tuning, and fine-pruning. The results show that\nwhile these procedures and defenses can mitigate some attacks, they often fail\nagainst stronger methods like Flashy Backdoor or sacrifice too much clean\naccuracy, rendering the models unusable.\n  Overall, all our methods can achieve up to a 100% Attack Success Rate while\nmaintaining high clean accuracy in every tested dataset. Additionally, we\nevaluate the stealthiness of the triggers with commonly used metrics, finding\nthem highly stealthy. Thus, we propose new alternatives more suited for\nidentifying poisoned samples in these scenarios. Our results show that further\nresearch is needed to ensure the security of SNN-based systems against backdoor\nattacks and their safe application in real-world scenarios. The code,\nexperiments, and results are available in our repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While security vulnerabilities in traditional Deep Neural Networks (DNNs)\nhave been extensively studied, the susceptibility of Spiking Neural Networks\n(SNNs) to adversarial attacks remains mostly underexplored. Until now, the\nmechanisms to inject backdoors into SNN models have been limited to digital\nscenarios; thus, we present the first evaluation of backdoor attacks in\nreal-world environments.\n  We begin by assessing the applicability of existing digital backdoor attacks\nand identifying their limitations for deployment in physical environments. To\naddress each of the found limitations, we present three novel backdoor attack\nmethods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess\nthe effectiveness of traditional backdoor procedures and defenses adapted for\nSNNs, such as pruning, fine-tuning, and fine-pruning. The results show that\nwhile these procedures and defenses can mitigate some attacks, they often fail\nagainst stronger methods like Flashy Backdoor or sacrifice too much clean\naccuracy, rendering the models unusable.\n  Overall, all our methods can achieve up to a 100% Attack Success Rate while\nmaintaining high clean accuracy in every tested dataset. Additionally, we\nevaluate the stealthiness of the triggers with commonly used metrics, finding\nthem highly stealthy. Thus, we propose new alternatives more suited for\nidentifying poisoned samples in these scenarios. Our results show that further\nresearch is needed to ensure the security of SNN-based systems against backdoor\nattacks and their safe application in real-world scenarios. The code,\nexperiments, and results are available in our repository."
                },
                "authors": [
                    {
                        "name": "Roberto Riao"
                    },
                    {
                        "name": "Gorka Abad"
                    },
                    {
                        "name": "Stjepan Picek"
                    },
                    {
                        "name": "Aitor Urbieta"
                    }
                ],
                "author_detail": {
                    "name": "Aitor Urbieta"
                },
                "author": "Aitor Urbieta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07888v2",
                "updated": "2024-11-05T11:07:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    11,
                    7,
                    19,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-15T02:22:48Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    22,
                    48,
                    3,
                    228,
                    0
                ],
                "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies\n  in Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies\n  in Medical Question Answering"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs,\ndriving the need for data-efficient training with optimised data ordering.\nHuman-inspired strategies offer a solution by organising data based on human\nlearning practices. This study evaluates the fine-tuning efficiency of five\nhuman-inspired strategies across four language models, three datasets, and both\nhuman- and LLM-labelled data in the context of medical question answering.\nThese strategies achieve the best accuracy gain of 1.81% and an average gain of\n1.02% across datasets, with interleaved strategies delivering the best average\nresults. However, the best strategy varies across model-dataset combinations,\nlimiting the generalisability of the effects of any single strategy.\nAdditionally, LLM-defined question difficulty outperforms human-defined labels\nin curriculum-based learning, showing the potential of model-generated data as\na cost-effective alternative for optimising fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs,\ndriving the need for data-efficient training with optimised data ordering.\nHuman-inspired strategies offer a solution by organising data based on human\nlearning practices. This study evaluates the fine-tuning efficiency of five\nhuman-inspired strategies across four language models, three datasets, and both\nhuman- and LLM-labelled data in the context of medical question answering.\nThese strategies achieve the best accuracy gain of 1.81% and an average gain of\n1.02% across datasets, with interleaved strategies delivering the best average\nresults. However, the best strategy varies across model-dataset combinations,\nlimiting the generalisability of the effects of any single strategy.\nAdditionally, LLM-defined question difficulty outperforms human-defined labels\nin curriculum-based learning, showing the potential of model-generated data as\na cost-effective alternative for optimising fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yushi Yang"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning:\n  Principles and Scalability (FITML)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02997v1",
                "updated": "2024-11-05T10:58:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    58,
                    37,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T10:58:37Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    58,
                    37,
                    1,
                    310,
                    0
                ],
                "title": "PV-faultNet: Optimized CNN Architecture to detect defects resulting\n  efficient PV production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PV-faultNet: Optimized CNN Architecture to detect defects resulting\n  efficient PV production"
                },
                "summary": "The global shift towards renewable energy has pushed PV cell manufacturing as\na pivotal point as they are the fundamental building block of green energy.\nHowever, the manufacturing process is complex enough to lose its purpose due to\nprobable defects experienced during the time impacting the overall efficiency.\nHowever, at the moment, manual inspection is being conducted to detect the\ndefects that can cause bias, leading to time and cost inefficiency. Even if\nautomated solutions have also been proposed, most of them are\nresource-intensive, proving ineffective in production environments. In that\ncontext, this study presents PV-faultNet, a lightweight Convolutional Neural\nNetwork (CNN) architecture optimized for efficient and real-time defect\ndetection in photovoltaic (PV) cells, designed to be deployable on\nresource-limited production devices. Addressing computational challenges in\nindustrial PV manufacturing environments, the model includes only 2.92 million\nparameters, significantly reducing processing demands without sacrificing\naccuracy. Comprehensive data augmentation techniques were implemented to tackle\ndata scarcity, thus enhancing model generalization and maintaining a balance\nbetween precision and recall. The proposed model achieved high performance with\n91\\% precision, 89\\% recall, and a 90\\% F1 score, demonstrating its\neffectiveness for scalable quality control in PV production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global shift towards renewable energy has pushed PV cell manufacturing as\na pivotal point as they are the fundamental building block of green energy.\nHowever, the manufacturing process is complex enough to lose its purpose due to\nprobable defects experienced during the time impacting the overall efficiency.\nHowever, at the moment, manual inspection is being conducted to detect the\ndefects that can cause bias, leading to time and cost inefficiency. Even if\nautomated solutions have also been proposed, most of them are\nresource-intensive, proving ineffective in production environments. In that\ncontext, this study presents PV-faultNet, a lightweight Convolutional Neural\nNetwork (CNN) architecture optimized for efficient and real-time defect\ndetection in photovoltaic (PV) cells, designed to be deployable on\nresource-limited production devices. Addressing computational challenges in\nindustrial PV manufacturing environments, the model includes only 2.92 million\nparameters, significantly reducing processing demands without sacrificing\naccuracy. Comprehensive data augmentation techniques were implemented to tackle\ndata scarcity, thus enhancing model generalization and maintaining a balance\nbetween precision and recall. The proposed model achieved high performance with\n91\\% precision, 89\\% recall, and a 90\\% F1 score, demonstrating its\neffectiveness for scalable quality control in PV production."
                },
                "authors": [
                    {
                        "name": "Eiffat E Zaman"
                    },
                    {
                        "name": "Rahima Khanam"
                    }
                ],
                "author_detail": {
                    "name": "Rahima Khanam"
                },
                "author": "Rahima Khanam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04691v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04691v4",
                "updated": "2024-11-05T10:32:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    32,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-08T13:10:51Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    10,
                    51,
                    3,
                    221,
                    0
                ],
                "title": "Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL\n  Performance"
                },
                "summary": "Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and text-to-SQL models. In this paper, we explore the use of\nlarge language models (LLMs) to automatically generate detailed natural\nlanguage descriptions for SQL database columns, aiming to improve text-to-SQL\nperformance and automate metadata creation. We create a dataset of gold column\ndescriptions based on the BIRD-Bench benchmark, manually refining its column\ndescriptions and creating a taxonomy for categorizing column difficulty. We\nthen evaluate several different LLMs in generating column descriptions across\nthe columns and different difficulties in the dataset, finding that models\nunsurprisingly struggle with columns that exhibit inherent ambiguity,\nhighlighting the need for manual expert input. We also find that incorporating\nsuch generated column descriptions consistently enhances text-to-SQL model\nperformance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral\n22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed\nsuperfluous information, outperform manually curated gold descriptions,\nsuggesting that models benefit from more detailed metadata than humans expect.\nFuture work will investigate the specific features of these high-performing\ndescriptions and explore other types of metadata, such as numerical reasoning\nand synonyms, to further improve text-to-SQL systems. The dataset, annotations\nand code will all be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and text-to-SQL models. In this paper, we explore the use of\nlarge language models (LLMs) to automatically generate detailed natural\nlanguage descriptions for SQL database columns, aiming to improve text-to-SQL\nperformance and automate metadata creation. We create a dataset of gold column\ndescriptions based on the BIRD-Bench benchmark, manually refining its column\ndescriptions and creating a taxonomy for categorizing column difficulty. We\nthen evaluate several different LLMs in generating column descriptions across\nthe columns and different difficulties in the dataset, finding that models\nunsurprisingly struggle with columns that exhibit inherent ambiguity,\nhighlighting the need for manual expert input. We also find that incorporating\nsuch generated column descriptions consistently enhances text-to-SQL model\nperformance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral\n22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed\nsuperfluous information, outperform manually curated gold descriptions,\nsuggesting that models benefit from more detailed metadata than humans expect.\nFuture work will investigate the specific features of these high-performing\ndescriptions and explore other types of metadata, such as numerical reasoning\nand synonyms, to further improve text-to-SQL systems. The dataset, annotations\nand code will all be made available."
                },
                "authors": [
                    {
                        "name": "Niklas Wretblad"
                    },
                    {
                        "name": "Oskar Holmstrm"
                    },
                    {
                        "name": "Erik Larsson"
                    },
                    {
                        "name": "Axel Wikster"
                    },
                    {
                        "name": "Oscar Sderlund"
                    },
                    {
                        "name": "Hjalmar hman"
                    },
                    {
                        "name": "Ture Pontn"
                    },
                    {
                        "name": "Martin Forsberg"
                    },
                    {
                        "name": "Martin Srme"
                    },
                    {
                        "name": "Fredrik Heintz"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Heintz"
                },
                "author": "Fredrik Heintz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04691v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04691v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12034v2",
                "updated": "2024-11-05T10:24:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    24,
                    42,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-30T22:18:49Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    22,
                    18,
                    49,
                    6,
                    182,
                    0
                ],
                "title": "Understanding Transformers via N-gram Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Transformers via N-gram Statistics"
                },
                "summary": "Transformer based large-language models (LLMs) display extreme proficiency\nwith language yet a precise understanding of how they work remains elusive. One\nway of demystifying transformer predictions would be to describe how they\ndepend on their context in terms of simple template functions. This paper takes\na first step in this direction by considering families of functions (i.e.\nrules) formed out of simple N-gram based statistics of the training data. By\nstudying how well these rulesets approximate transformer predictions, we obtain\na variety of novel discoveries: a simple method to detect overfitting during\ntraining without using a holdout set, a quantitative measure of how\ntransformers progress from learning simple to more complex statistical rules\nover the course of training, a model-variance criterion governing when\ntransformer predictions tend to be described by N-gram rules, and insights into\nhow well transformers can be approximated by N-gram rulesets in the limit where\nthese rulesets become increasingly complex. In this latter direction, we find\nthat for 79% and 68% of LLM next-token distributions on TinyStories and\nWikipedia, respectively, their top-1 predictions agree with those provided by\nour N-gram rulesets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer based large-language models (LLMs) display extreme proficiency\nwith language yet a precise understanding of how they work remains elusive. One\nway of demystifying transformer predictions would be to describe how they\ndepend on their context in terms of simple template functions. This paper takes\na first step in this direction by considering families of functions (i.e.\nrules) formed out of simple N-gram based statistics of the training data. By\nstudying how well these rulesets approximate transformer predictions, we obtain\na variety of novel discoveries: a simple method to detect overfitting during\ntraining without using a holdout set, a quantitative measure of how\ntransformers progress from learning simple to more complex statistical rules\nover the course of training, a model-variance criterion governing when\ntransformer predictions tend to be described by N-gram rules, and insights into\nhow well transformers can be approximated by N-gram rulesets in the limit where\nthese rulesets become increasingly complex. In this latter direction, we find\nthat for 79% and 68% of LLM next-token distributions on TinyStories and\nWikipedia, respectively, their top-1 predictions agree with those provided by\nour N-gram rulesets."
                },
                "authors": [
                    {
                        "name": "Timothy Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Nguyen"
                },
                "author": "Timothy Nguyen",
                "arxiv_comment": "NeurIPS 2024. Datasets and N-gram statistics open-sourced:\n  https://github.com/google-deepmind/transformer_ngrams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02973v1",
                "updated": "2024-11-05T10:18:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    18,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T10:18:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    10,
                    18,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for\n  Diabetic Retinopathy using Chatbots and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for\n  Diabetic Retinopathy using Chatbots and Generative AI"
                },
                "summary": "We present an outline of the first large language model (LLM) based chatbot\napplication in the context of patient-reported outcome measures (PROMs) for\ndiabetic retinopathy. By utilizing the capabilities of current LLMs, we enable\npatients to provide feedback about their quality of life and treatment progress\nvia an interactive application. The proposed framework offers significant\nadvantages over the current approach, which encompasses only qualitative\ncollection of survey data or a static survey with limited answer options. Using\nthe PROBot LLM-PROM application, patients will be asked tailored questions\nabout their individual challenges, and can give more detailed feedback on the\nprogress of their treatment. Based on this input, we will use machine learning\nto infer conventional PROM scores, which can be used by clinicians to evaluate\nthe treatment status. The goal of the application is to improve adherence to\nthe healthcare system and treatments, and thus ultimately reduce cases of\nsubsequent vision impairment. The approach needs to be further validated using\na survey and a clinical study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an outline of the first large language model (LLM) based chatbot\napplication in the context of patient-reported outcome measures (PROMs) for\ndiabetic retinopathy. By utilizing the capabilities of current LLMs, we enable\npatients to provide feedback about their quality of life and treatment progress\nvia an interactive application. The proposed framework offers significant\nadvantages over the current approach, which encompasses only qualitative\ncollection of survey data or a static survey with limited answer options. Using\nthe PROBot LLM-PROM application, patients will be asked tailored questions\nabout their individual challenges, and can give more detailed feedback on the\nprogress of their treatment. Based on this input, we will use machine learning\nto infer conventional PROM scores, which can be used by clinicians to evaluate\nthe treatment status. The goal of the application is to improve adherence to\nthe healthcare system and treatments, and thus ultimately reduce cases of\nsubsequent vision impairment. The approach needs to be further validated using\na survey and a clinical study."
                },
                "authors": [
                    {
                        "name": "Maren Pielka"
                    },
                    {
                        "name": "Tobias Schneider"
                    },
                    {
                        "name": "Jan Terheyden"
                    },
                    {
                        "name": "Rafet Sifa"
                    }
                ],
                "author_detail": {
                    "name": "Rafet Sifa"
                },
                "author": "Rafet Sifa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02959v1",
                "updated": "2024-11-05T09:58:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    58,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:58:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    58,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial\nsystems such as ChatGPT and Perplexity have used Web search engines as their\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\ndownload HTML sources of the results, and then extract plain texts from the\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\nthe generation. However, much of the structural and semantic information\ninherent in HTML, such as headings and table structures, is lost during this\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\nRAG. We believe HTML is better than plain text in modeling knowledge in\nexternal documents, and most LLMs possess robust capacities to understand HTML.\nHowever, utilizing HTML presents new challenges. HTML contains additional\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\ninput tokens and noise to the RAG system. To address this issue, we propose\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\nminimizing the loss of information. Specifically, we design a two-step\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial\nsystems such as ChatGPT and Perplexity have used Web search engines as their\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\ndownload HTML sources of the results, and then extract plain texts from the\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\nthe generation. However, much of the structural and semantic information\ninherent in HTML, such as headings and table structures, is lost during this\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\nRAG. We believe HTML is better than plain text in modeling knowledge in\nexternal documents, and most LLMs possess robust capacities to understand HTML.\nHowever, utilizing HTML presents new challenges. HTML contains additional\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\ninput tokens and noise to the RAG system. To address this issue, we propose\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\nminimizing the loss of information. Specifically, we design a two-step\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems."
                },
                "authors": [
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02327v2",
                "updated": "2024-11-05T09:43:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    43,
                    59,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T17:50:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    50,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance"
                },
                "summary": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA."
                },
                "authors": [
                    {
                        "name": "Ruyang Liu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haibo Liu"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Jiankun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jiankun Yang"
                },
                "author": "Jiankun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10285v2",
                "updated": "2024-11-05T09:43:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    43,
                    49,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-14T08:37:40Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    8,
                    37,
                    40,
                    0,
                    288,
                    0
                ],
                "title": "ABBA-VSM: Time Series Classification using Symbolic Representation on\n  the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABBA-VSM: Time Series Classification using Symbolic Representation on\n  the Edge"
                },
                "summary": "In recent years, Edge AI has become more prevalent with applications across\nvarious industries, from environmental monitoring to smart city management.\nEdge AI facilitates the processing of Internet of Things (IoT) data and\nprovides privacy-enabled and latency-sensitive services to application users\nusing Machine Learning (ML) algorithms, e.g., Time Series Classification (TSC).\nHowever, existing TSC algorithms require access to full raw data and demand\nsubstantial computing resources to train and use them effectively in runtime.\nThis makes them impractical for deployment in resource-constrained Edge\nenvironments. To address this, in this paper, we propose an Adaptive Brownian\nBridge-based Symbolic Aggregation Vector Space Model (ABBA-VSM). It is a new\nTSC model designed for classification services on Edge. Here, we first\nadaptively compress the raw time series into symbolic representations, thus\ncapturing the changing trends of data. Subsequently, we train the\nclassification model directly on these symbols. ABBA-VSM reduces communication\ndata between IoT and Edge devices, as well as computation cycles, in the\ndevelopment of resource-efficient TSC services on Edge. We evaluate our\nsolution with extensive experiments using datasets from the UCR time series\nclassification archive. The results demonstrate that the ABBA-VSM achieves up\nto 80% compression ratio and 90-100% accuracy for binary classification.\nWhereas, for non-binary classification, it achieves an average compression\nratio of 60% and accuracy ranging from 60-80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Edge AI has become more prevalent with applications across\nvarious industries, from environmental monitoring to smart city management.\nEdge AI facilitates the processing of Internet of Things (IoT) data and\nprovides privacy-enabled and latency-sensitive services to application users\nusing Machine Learning (ML) algorithms, e.g., Time Series Classification (TSC).\nHowever, existing TSC algorithms require access to full raw data and demand\nsubstantial computing resources to train and use them effectively in runtime.\nThis makes them impractical for deployment in resource-constrained Edge\nenvironments. To address this, in this paper, we propose an Adaptive Brownian\nBridge-based Symbolic Aggregation Vector Space Model (ABBA-VSM). It is a new\nTSC model designed for classification services on Edge. Here, we first\nadaptively compress the raw time series into symbolic representations, thus\ncapturing the changing trends of data. Subsequently, we train the\nclassification model directly on these symbols. ABBA-VSM reduces communication\ndata between IoT and Edge devices, as well as computation cycles, in the\ndevelopment of resource-efficient TSC services on Edge. We evaluate our\nsolution with extensive experiments using datasets from the UCR time series\nclassification archive. The results demonstrate that the ABBA-VSM achieves up\nto 80% compression ratio and 90-100% accuracy for binary classification.\nWhereas, for non-binary classification, it achieves an average compression\nratio of 60% and accuracy ranging from 60-80%."
                },
                "authors": [
                    {
                        "name": "Meerzhan Kanatbekova"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "arxiv_comment": "15 pages with references, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14751v2",
                "updated": "2024-11-05T09:42:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    42,
                    40,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-23T16:17:44Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    17,
                    44,
                    3,
                    144,
                    0
                ],
                "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents"
                },
                "summary": "We introduce a novel reinforcement learning framework of LLM agents named\nAGILE (AGent that Interacts and Learns from Environments) designed to perform\ncomplex conversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent possesses capabilities beyond\nconversation, including reflection, tool usage, and expert consultation. We\nformulate the construction of such an LLM agent as a reinforcement learning\n(RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs\ntrained with PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance. Datasets and code are\navailable at https://github.com/bytarnish/AGILE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel reinforcement learning framework of LLM agents named\nAGILE (AGent that Interacts and Learns from Environments) designed to perform\ncomplex conversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent possesses capabilities beyond\nconversation, including reflection, tool usage, and expert consultation. We\nformulate the construction of such an LLM agent as a reinforcement learning\n(RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs\ntrained with PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance. Datasets and code are\navailable at https://github.com/bytarnish/AGILE."
                },
                "authors": [
                    {
                        "name": "Peiyuan Feng"
                    },
                    {
                        "name": "Yichen He"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Hanchong Zhang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Hang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hang Li"
                },
                "author": "Hang Li",
                "arxiv_comment": "accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02943v1",
                "updated": "2024-11-05T09:37:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    37,
                    23,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:37:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    37,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach"
                },
                "summary": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents."
                },
                "authors": [
                    {
                        "name": "Francesco Invernici"
                    },
                    {
                        "name": "Francesca Curati"
                    },
                    {
                        "name": "Jelena Jakimov"
                    },
                    {
                        "name": "Amirhossein Samavi"
                    },
                    {
                        "name": "Anna Bernasconi"
                    }
                ],
                "author_detail": {
                    "name": "Anna Bernasconi"
                },
                "author": "Anna Bernasconi",
                "arxiv_comment": "27 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02941v1",
                "updated": "2024-11-05T09:34:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    34,
                    5,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:34:05Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    34,
                    5,
                    1,
                    310,
                    0
                ],
                "title": "A Mamba Foundation Model for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mamba Foundation Model for Time Series Forecasting"
                },
                "summary": "Time series foundation models have demonstrated strong performance in\nzero-shot learning, making them well-suited for predicting rapidly evolving\npatterns in real-world applications where relevant training data are scarce.\nHowever, most of these models rely on the Transformer architecture, which\nincurs quadratic complexity as input length increases. To address this, we\nintroduce TSMamba, a linear-complexity foundation model for time series\nforecasting built on the Mamba architecture. The model captures temporal\ndependencies through both forward and backward Mamba encoders, achieving high\nprediction accuracy. To reduce reliance on large datasets and lower training\ncosts, TSMamba employs a two-stage transfer learning process that leverages\npretrained Mamba LLMs, allowing effective time series modeling with a moderate\ntraining set. In the first stage, the forward and backward backbones are\noptimized via patch-wise autoregressive prediction; in the second stage, the\nmodel trains a prediction head and refines other components for long-term\nforecasting. While the backbone assumes channel independence to manage varying\nchannel numbers across datasets, a channel-wise compressed attention module is\nintroduced to capture cross-channel dependencies during fine-tuning on specific\nmultivariate datasets. Experiments show that TSMamba's zero-shot performance is\ncomparable to state-of-the-art time series foundation models, despite using\nsignificantly less training data. It also achieves competitive or superior\nfull-shot performance compared to task-specific prediction models. The code\nwill be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series foundation models have demonstrated strong performance in\nzero-shot learning, making them well-suited for predicting rapidly evolving\npatterns in real-world applications where relevant training data are scarce.\nHowever, most of these models rely on the Transformer architecture, which\nincurs quadratic complexity as input length increases. To address this, we\nintroduce TSMamba, a linear-complexity foundation model for time series\nforecasting built on the Mamba architecture. The model captures temporal\ndependencies through both forward and backward Mamba encoders, achieving high\nprediction accuracy. To reduce reliance on large datasets and lower training\ncosts, TSMamba employs a two-stage transfer learning process that leverages\npretrained Mamba LLMs, allowing effective time series modeling with a moderate\ntraining set. In the first stage, the forward and backward backbones are\noptimized via patch-wise autoregressive prediction; in the second stage, the\nmodel trains a prediction head and refines other components for long-term\nforecasting. While the backbone assumes channel independence to manage varying\nchannel numbers across datasets, a channel-wise compressed attention module is\nintroduced to capture cross-channel dependencies during fine-tuning on specific\nmultivariate datasets. Experiments show that TSMamba's zero-shot performance is\ncomparable to state-of-the-art time series foundation models, despite using\nsignificantly less training data. It also achieves competitive or superior\nfull-shot performance compared to task-specific prediction models. The code\nwill be made publicly available."
                },
                "authors": [
                    {
                        "name": "Haoyu Ma"
                    },
                    {
                        "name": "Yushu Chen"
                    },
                    {
                        "name": "Wenlai Zhao"
                    },
                    {
                        "name": "Jinzhe Yang"
                    },
                    {
                        "name": "Yingsheng Ji"
                    },
                    {
                        "name": "Xinghua Xu"
                    },
                    {
                        "name": "Xiaozhu Liu"
                    },
                    {
                        "name": "Hao Jing"
                    },
                    {
                        "name": "Shengzhuo Liu"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02938v1",
                "updated": "2024-11-05T09:31:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    31,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:31:30Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    31,
                    30,
                    1,
                    310,
                    0
                ],
                "title": "Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments"
                },
                "summary": "The advent of generalist Large Language Models (LLMs) and Large Vision Models\n(VLMs) have streamlined the construction of semantically enriched maps that can\nenable robots to ground high-level reasoning and planning into their\nrepresentations. One of the most widely used semantic map formats is the 3D\nScene Graph, which captures both metric (low-level) and semantic (high-level)\ninformation. However, these maps often assume a static world, while real\nenvironments, like homes and offices, are dynamic. Even small changes in these\nspaces can significantly impact task performance. To integrate robots into\ndynamic environments, they must detect changes and update the scene graph in\nreal-time. This update process is inherently multimodal, requiring input from\nvarious sources, such as human agents, the robot's own perception system, time,\nand its actions. This work proposes a framework that leverages these multimodal\ninputs to maintain the consistency of scene graphs during real-time operation,\npresenting promising initial results and outlining a roadmap for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of generalist Large Language Models (LLMs) and Large Vision Models\n(VLMs) have streamlined the construction of semantically enriched maps that can\nenable robots to ground high-level reasoning and planning into their\nrepresentations. One of the most widely used semantic map formats is the 3D\nScene Graph, which captures both metric (low-level) and semantic (high-level)\ninformation. However, these maps often assume a static world, while real\nenvironments, like homes and offices, are dynamic. Even small changes in these\nspaces can significantly impact task performance. To integrate robots into\ndynamic environments, they must detect changes and update the scene graph in\nreal-time. This update process is inherently multimodal, requiring input from\nvarious sources, such as human agents, the robot's own perception system, time,\nand its actions. This work proposes a framework that leverages these multimodal\ninputs to maintain the consistency of scene graphs during real-time operation,\npresenting promising initial results and outlining a roadmap for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Emilio Olivastri"
                    },
                    {
                        "name": "Jonathan Francis"
                    },
                    {
                        "name": "Alberto Pretto"
                    },
                    {
                        "name": "Niko Snderhauf"
                    },
                    {
                        "name": "Krishan Rana"
                    }
                ],
                "author_detail": {
                    "name": "Krishan Rana"
                },
                "author": "Krishan Rana",
                "arxiv_comment": "This paper has been accepted at the Workshop on Lifelong Learning for\n  Home Robots at the 8th Conference on Robot Learning (CoRL 2024), Munich,\n  Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02930v1",
                "updated": "2024-11-05T09:22:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    22,
                    8,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T09:22:08Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    22,
                    8,
                    1,
                    310,
                    0
                ],
                "title": "Textual Aesthetics in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Aesthetics in Large Language Models"
                },
                "summary": "Image aesthetics is a crucial metric in the field of image generation.\nHowever, textual aesthetics has not been sufficiently explored. With the\nwidespread application of large language models (LLMs), previous work has\nprimarily focused on the correctness of content and the helpfulness of\nresponses. Nonetheless, providing responses with textual aesthetics is also an\nimportant factor for LLMs, which can offer a cleaner layout and ensure greater\nconsistency and coherence in content. In this work, we introduce a pipeline for\naesthetics polishing and help construct a textual aesthetics dataset named\nTexAes. We propose a textual aesthetics-powered fine-tuning method based on\ndirect preference optimization, termed TAPO, which leverages textual aesthetics\nwithout compromising content correctness. Additionally, we develop two\nevaluation methods for textual aesthetics based on text and image analysis,\nrespectively. Our experiments demonstrate that using textual aesthetics data\nand employing the TAPO fine-tuning method not only improves aesthetic scores\nbut also enhances performance on general evaluation datasets such as\nAlpacalEval and Anera-hard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image aesthetics is a crucial metric in the field of image generation.\nHowever, textual aesthetics has not been sufficiently explored. With the\nwidespread application of large language models (LLMs), previous work has\nprimarily focused on the correctness of content and the helpfulness of\nresponses. Nonetheless, providing responses with textual aesthetics is also an\nimportant factor for LLMs, which can offer a cleaner layout and ensure greater\nconsistency and coherence in content. In this work, we introduce a pipeline for\naesthetics polishing and help construct a textual aesthetics dataset named\nTexAes. We propose a textual aesthetics-powered fine-tuning method based on\ndirect preference optimization, termed TAPO, which leverages textual aesthetics\nwithout compromising content correctness. Additionally, we develop two\nevaluation methods for textual aesthetics based on text and image analysis,\nrespectively. Our experiments demonstrate that using textual aesthetics data\nand employing the TAPO fine-tuning method not only improves aesthetic scores\nbut also enhances performance on general evaluation datasets such as\nAlpacalEval and Anera-hard."
                },
                "authors": [
                    {
                        "name": "Lingjie Jiang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13517v2",
                "updated": "2024-11-05T09:08:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    8,
                    28,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-17T13:06:02Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    13,
                    6,
                    2,
                    3,
                    291,
                    0
                ],
                "title": "Bias in the Mirror: Are LLMs opinions robust to their own adversarial\n  attacks ?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in the Mirror: Are LLMs opinions robust to their own adversarial\n  attacks ?"
                },
                "summary": "Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts."
                },
                "authors": [
                    {
                        "name": "Virgile Rennard"
                    },
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02839v3",
                "updated": "2024-11-05T09:07:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    7,
                    22,
                    1,
                    310,
                    0
                ],
                "published": "2024-03-05T10:20:52Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    10,
                    20,
                    52,
                    1,
                    65,
                    0
                ],
                "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned\n  Judge Model is not a General Substitute for GPT-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned\n  Judge Model is not a General Substitute for GPT-4"
                },
                "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-sourced models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this work, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\nintroduce a integrated method, leveraging GPT-4 to compensate for the\nlimitations and improve the fine-tuned judges. Experiment results show our\nmethod achieves accuracy on par with GPT-4 with only 50% of the API expense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-sourced models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this work, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\nintroduce a integrated method, leveraging GPT-4 to compensate for the\nlimitations and improve the fine-tuned judges. Experiment results show our\nmethod achieves accuracy on par with GPT-4 with only 50% of the API expense."
                },
                "authors": [
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Yingqi Qu"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Hongli Zhou"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Tiejun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Zhao"
                },
                "author": "Tiejun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02908v1",
                "updated": "2024-11-05T08:48:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    48,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T08:48:25Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    48,
                    25,
                    1,
                    310,
                    0
                ],
                "title": "Photon: Federated LLM Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photon: Federated LLM Pre-Training"
                },
                "summary": "Scaling large language models (LLMs) demands extensive data and computing\nresources, which are traditionally constrained to data centers by the\nhigh-bandwidth requirements of distributed training. Low-bandwidth methods like\nfederated learning (FL) could enable collaborative training of larger models\nacross weakly-connected GPUs if they can effectively be used for pre-training.\nTo achieve this, we introduce Photon, the first complete system for federated\nend-to-end LLM training, leveraging cross-silo FL for global-scale training\nwith minimal communication overheads. Using Photon, we train the first\nfederated family of decoder-only LLMs from scratch. We show that: (1) Photon\ncan train model sizes up to 7B in a federated fashion while reaching an even\nbetter perplexity than centralized pre-training; (2) Photon model training time\ndecreases with available compute, achieving a similar compute-time trade-off to\ncentralized; and (3) Photon outperforms the wall-time of baseline distributed\ntraining methods by 35% via communicating 64x-512xless. Our proposal is robust\nto data heterogeneity and converges twice as fast as previous methods like\nDiLoCo. This surprising data efficiency stems from a unique approach combining\nsmall client batch sizes with extremely high learning rates, enabled by\nfederated averaging's robustness to hyperparameters. Photon thus represents the\nfirst economical system for global internet-wide LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models (LLMs) demands extensive data and computing\nresources, which are traditionally constrained to data centers by the\nhigh-bandwidth requirements of distributed training. Low-bandwidth methods like\nfederated learning (FL) could enable collaborative training of larger models\nacross weakly-connected GPUs if they can effectively be used for pre-training.\nTo achieve this, we introduce Photon, the first complete system for federated\nend-to-end LLM training, leveraging cross-silo FL for global-scale training\nwith minimal communication overheads. Using Photon, we train the first\nfederated family of decoder-only LLMs from scratch. We show that: (1) Photon\ncan train model sizes up to 7B in a federated fashion while reaching an even\nbetter perplexity than centralized pre-training; (2) Photon model training time\ndecreases with available compute, achieving a similar compute-time trade-off to\ncentralized; and (3) Photon outperforms the wall-time of baseline distributed\ntraining methods by 35% via communicating 64x-512xless. Our proposal is robust\nto data heterogeneity and converges twice as fast as previous methods like\nDiLoCo. This surprising data efficiency stems from a unique approach combining\nsmall client batch sizes with extremely high learning rates, enabled by\nfederated averaging's robustness to hyperparameters. Photon thus represents the\nfirst economical system for global internet-wide LLM pre-training."
                },
                "authors": [
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Bill Marino"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "13 pages, 9 appendix pages, 10 figures, 3 algorithms, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11445v2",
                "updated": "2024-11-05T08:46:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    46,
                    1,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-17T03:39:45Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    39,
                    45,
                    1,
                    261,
                    0
                ],
                "title": "Jailbreaking Large Language Models with Symbolic Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Models with Symbolic Mathematics"
                },
                "summary": "Recent advancements in AI safety have led to increased efforts in training\nand red-teaming large language models (LLMs) to mitigate unsafe content\ngeneration. However, these safety mechanisms may not be comprehensive, leaving\npotential vulnerabilities unexplored. This paper introduces MathPrompt, a novel\njailbreaking technique that exploits LLMs' advanced capabilities in symbolic\nmathematics to bypass their safety mechanisms. By encoding harmful natural\nlanguage prompts into mathematical problems, we demonstrate a critical\nvulnerability in current AI safety measures. Our experiments across 13\nstate-of-the-art LLMs reveal an average attack success rate of 73.6\\%,\nhighlighting the inability of existing safety training mechanisms to generalize\nto mathematically encoded inputs. Analysis of embedding vectors shows a\nsubstantial semantic shift between original and encoded prompts, helping\nexplain the attack's success. This work emphasizes the importance of a holistic\napproach to AI safety, calling for expanded red-teaming efforts to develop\nrobust safeguards across all potential input types and their associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI safety have led to increased efforts in training\nand red-teaming large language models (LLMs) to mitigate unsafe content\ngeneration. However, these safety mechanisms may not be comprehensive, leaving\npotential vulnerabilities unexplored. This paper introduces MathPrompt, a novel\njailbreaking technique that exploits LLMs' advanced capabilities in symbolic\nmathematics to bypass their safety mechanisms. By encoding harmful natural\nlanguage prompts into mathematical problems, we demonstrate a critical\nvulnerability in current AI safety measures. Our experiments across 13\nstate-of-the-art LLMs reveal an average attack success rate of 73.6\\%,\nhighlighting the inability of existing safety training mechanisms to generalize\nto mathematically encoded inputs. Analysis of embedding vectors shows a\nsubstantial semantic shift between original and encoded prompts, helping\nexplain the attack's success. This work emphasizes the importance of a holistic\napproach to AI safety, calling for expanded red-teaming efforts to develop\nrobust safeguards across all potential input types and their associated risks."
                },
                "authors": [
                    {
                        "name": "Emet Bethany"
                    },
                    {
                        "name": "Mazal Bethany"
                    },
                    {
                        "name": "Juan Arturo Nolazco Flores"
                    },
                    {
                        "name": "Sumit Kumar Jha"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02116v2",
                "updated": "2024-11-05T08:35:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    35,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T14:29:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Advancements and limitations of LLMs in replicating human color-word\n  associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements and limitations of LLMs in replicating human color-word\n  associations"
                },
                "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category. However, the\nhighest median performance was approximately 50% even for GPT-4o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category. However, the\nhighest median performance was approximately 50% even for GPT-4o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations."
                },
                "authors": [
                    {
                        "name": "Makoto Fukushima"
                    },
                    {
                        "name": "Shusuke Eshita"
                    },
                    {
                        "name": "Hiroshige Fukuhara"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshige Fukuhara"
                },
                "author": "Hiroshige Fukuhara",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02871v1",
                "updated": "2024-11-05T07:26:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    26,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:26:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    26,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "Enhancing Adversarial Robustness via Uncertainty-Aware Distributional\n  Adversarial Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Adversarial Robustness via Uncertainty-Aware Distributional\n  Adversarial Training"
                },
                "summary": "Despite remarkable achievements in deep learning across various domains, its\ninherent vulnerability to adversarial examples still remains a critical concern\nfor practical deployment. Adversarial training has emerged as one of the most\neffective defensive techniques for improving model robustness against such\nmalicious inputs. However, existing adversarial training schemes often lead to\nlimited generalization ability against underlying adversaries with diversity\ndue to their overreliance on a point-by-point augmentation strategy by mapping\neach clean example to its adversarial counterpart during training. In addition,\nadversarial examples can induce significant disruptions in the statistical\ninformation w.r.t. the target model, thereby introducing substantial\nuncertainty and challenges to modeling the distribution of adversarial\nexamples. To circumvent these issues, in this paper, we propose a novel\nuncertainty-aware distributional adversarial training method, which enforces\nadversary modeling by leveraging both the statistical information of\nadversarial examples and its corresponding uncertainty estimation, with the\ngoal of augmenting the diversity of adversaries. Considering the potentially\nnegative impact induced by aligning adversaries to misclassified clean\nexamples, we also refine the alignment reference based on the statistical\nproximity to clean examples during adversarial training, thereby reframing\nadversarial training within a distribution-to-distribution matching framework\ninteracted between the clean and adversarial domains. Furthermore, we design an\nintrospective gradient alignment approach via matching input gradients between\nthese domains without introducing external models. Extensive experiments across\nfour benchmark datasets and various network architectures demonstrate that our\napproach achieves state-of-the-art adversarial robustness and maintains natural\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite remarkable achievements in deep learning across various domains, its\ninherent vulnerability to adversarial examples still remains a critical concern\nfor practical deployment. Adversarial training has emerged as one of the most\neffective defensive techniques for improving model robustness against such\nmalicious inputs. However, existing adversarial training schemes often lead to\nlimited generalization ability against underlying adversaries with diversity\ndue to their overreliance on a point-by-point augmentation strategy by mapping\neach clean example to its adversarial counterpart during training. In addition,\nadversarial examples can induce significant disruptions in the statistical\ninformation w.r.t. the target model, thereby introducing substantial\nuncertainty and challenges to modeling the distribution of adversarial\nexamples. To circumvent these issues, in this paper, we propose a novel\nuncertainty-aware distributional adversarial training method, which enforces\nadversary modeling by leveraging both the statistical information of\nadversarial examples and its corresponding uncertainty estimation, with the\ngoal of augmenting the diversity of adversaries. Considering the potentially\nnegative impact induced by aligning adversaries to misclassified clean\nexamples, we also refine the alignment reference based on the statistical\nproximity to clean examples during adversarial training, thereby reframing\nadversarial training within a distribution-to-distribution matching framework\ninteracted between the clean and adversarial domains. Furthermore, we design an\nintrospective gradient alignment approach via matching input gradients between\nthese domains without introducing external models. Extensive experiments across\nfour benchmark datasets and various network architectures demonstrate that our\napproach achieves state-of-the-art adversarial robustness and maintains natural\nperformance."
                },
                "authors": [
                    {
                        "name": "Junhao Dong"
                    },
                    {
                        "name": "Xinghua Qu"
                    },
                    {
                        "name": "Z. Jane Wang"
                    },
                    {
                        "name": "Yew-Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew-Soon Ong"
                },
                "author": "Yew-Soon Ong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12735v3",
                "updated": "2024-11-05T07:24:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    24,
                    15,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-17T16:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    55,
                    42,
                    2,
                    199,
                    0
                ],
                "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge"
                },
                "summary": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek."
                },
                "authors": [
                    {
                        "name": "Yibin Yan"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_comment": "Technical Report; Project Page: https://go2heart.github.io/echosight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19332v3",
                "updated": "2024-11-05T07:21:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    21,
                    18,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-29T17:59:07Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    17,
                    59,
                    7,
                    2,
                    150,
                    0
                ],
                "title": "Self-Exploring Language Models: Active Preference Elicitation for Online\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Exploring Language Models: Active Preference Elicitation for Online\n  Alignment"
                },
                "summary": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Hiteshi Sharma"
                    },
                    {
                        "name": "Han Zhong"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hany Hassan"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02318v2",
                "updated": "2024-11-05T07:13:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    13,
                    13,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-04T17:44:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast"
                },
                "summary": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering."
                },
                "authors": [
                    {
                        "name": "Marilyn Rego"
                    },
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Sanya Dod"
                    },
                    {
                        "name": "Zhaorui Ni"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Jenna DiVincenzo"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02864v1",
                "updated": "2024-11-05T07:12:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    12,
                    36,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:12:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    12,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document\n  Relation Extraction with Graph-of-Thoughts Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document\n  Relation Extraction with Graph-of-Thoughts Reasoning"
                },
                "summary": "Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning capability on many NLP tasks. Recasting an NLP\ntask into a text-to-text generation task is a common practice so that\ngenerative LLMs can be prompted to resolve it. However, performing\ndocument-level relation extraction (DocRE) tasks with generative LLM models is\nstill challenging due to the structured output format of DocRE, which\ncomplicates the conversion to plain text. Limited information available in\nfew-shot samples and prompt instructions induce further difficulties and\nchallenges in relation extraction for mentioned entities in a document. In this\npaper, we represent the structured output as a graph-style triplet rather than\nnatural language expressions and leverage generative LLMs for the DocRE task.\nOur approach, the Graph-DPEP framework is grounded in the reasoning behind\ntriplet explanation thoughts presented in natural language. In this framework,\nwe first introduce a ``decomposed-plug\" method for performing the generation\nfrom LLMs over prompts with type-space decomposition to alleviate the burden of\ndistinguishing all relation types. Second, we employ a verifier for calibrating\nthe generation and identifying overlooked query entity pairs. Third, we develop\n\"ensemble-play\", reapplying generation on the entire type list by leveraging\nthe reasoning thoughts embedded in a sub-graph associated with the missing\nquery pair to address the missingness issue. Through extensive comparisons with\nexisting prompt techniques and alternative Language Models (LLMs), our\nframework demonstrates superior performance on publicly available benchmarks in\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning capability on many NLP tasks. Recasting an NLP\ntask into a text-to-text generation task is a common practice so that\ngenerative LLMs can be prompted to resolve it. However, performing\ndocument-level relation extraction (DocRE) tasks with generative LLM models is\nstill challenging due to the structured output format of DocRE, which\ncomplicates the conversion to plain text. Limited information available in\nfew-shot samples and prompt instructions induce further difficulties and\nchallenges in relation extraction for mentioned entities in a document. In this\npaper, we represent the structured output as a graph-style triplet rather than\nnatural language expressions and leverage generative LLMs for the DocRE task.\nOur approach, the Graph-DPEP framework is grounded in the reasoning behind\ntriplet explanation thoughts presented in natural language. In this framework,\nwe first introduce a ``decomposed-plug\" method for performing the generation\nfrom LLMs over prompts with type-space decomposition to alleviate the burden of\ndistinguishing all relation types. Second, we employ a verifier for calibrating\nthe generation and identifying overlooked query entity pairs. Third, we develop\n\"ensemble-play\", reapplying generation on the entire type list by leveraging\nthe reasoning thoughts embedded in a sub-graph associated with the missing\nquery pair to address the missingness issue. Through extensive comparisons with\nexisting prompt techniques and alternative Language Models (LLMs), our\nframework demonstrates superior performance on publicly available benchmarks in\nexperiments."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ning Yan"
                    },
                    {
                        "name": "Masood Mortazavi"
                    },
                    {
                        "name": "Hoang H. Nguyen"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02862v1",
                "updated": "2024-11-05T07:10:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    10,
                    0,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:10:00Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    10,
                    0,
                    1,
                    310,
                    0
                ],
                "title": "The Unreasonable Effectiveness of LLMs for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Effectiveness of LLMs for Query Optimization"
                },
                "summary": "Recent work in database query optimization has used complex machine learning\nstrategies, such as customized reinforcement learning schemes. Surprisingly, we\nshow that LLM embeddings of query text contain useful semantic information for\nquery optimization. Specifically, we show that a simple binary classifier\ndeciding between alternative query plans, trained only on a small number of\nlabeled embedded query vectors, can outperform existing heuristic systems.\nAlthough we only present some preliminary results, an LLM-powered query\noptimizer could provide significant benefits, both in terms of performance and\nsimplicity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work in database query optimization has used complex machine learning\nstrategies, such as customized reinforcement learning schemes. Surprisingly, we\nshow that LLM embeddings of query text contain useful semantic information for\nquery optimization. Specifically, we show that a simple binary classifier\ndeciding between alternative query plans, trained only on a small number of\nlabeled embedded query vectors, can outperform existing heuristic systems.\nAlthough we only present some preliminary results, an LLM-powered query\noptimizer could provide significant benefits, both in terms of performance and\nsimplicity."
                },
                "authors": [
                    {
                        "name": "Peter Akioyamen"
                    },
                    {
                        "name": "Zixuan Yi"
                    },
                    {
                        "name": "Ryan Marcus"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Marcus"
                },
                "author": "Ryan Marcus",
                "arxiv_comment": "To appear in the Machine Learning for Systems Workshop at NeurIPS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14900v3",
                "updated": "2024-11-05T06:52:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    52,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-21T06:47:28Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    47,
                    28,
                    4,
                    173,
                    0
                ],
                "title": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue\n  for LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue\n  for LLM-based Recommendation"
                },
                "summary": "Adapting Large Language Models (LLMs) for recommendation requires careful\nconsideration of the decoding process, given the inherent differences between\ngenerating items and natural language. Existing approaches often directly apply\nLLMs' original decoding methods. However, we find these methods encounter\nsignificant challenges: 1) amplification bias -- where standard length\nnormalization inflates scores for items containing tokens with generation\nprobabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --\ngenerating multiple similar or repetitive items for a user. To tackle these\nchallenges, we introduce a new decoding approach named Debiasing-Diversifying\nDecoding (D3). D3 disables length normalization for ghost tokens to alleviate\namplification bias, and it incorporates a text-free assistant model to\nencourage tokens less frequently generated by LLMs for counteracting\nrecommendation homogeneity. Extensive experiments on real-world datasets\ndemonstrate the method's effectiveness in enhancing accuracy and diversity. The\ncode is available at https://github.com/SAI990323/DecodingMatters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) for recommendation requires careful\nconsideration of the decoding process, given the inherent differences between\ngenerating items and natural language. Existing approaches often directly apply\nLLMs' original decoding methods. However, we find these methods encounter\nsignificant challenges: 1) amplification bias -- where standard length\nnormalization inflates scores for items containing tokens with generation\nprobabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --\ngenerating multiple similar or repetitive items for a user. To tackle these\nchallenges, we introduce a new decoding approach named Debiasing-Diversifying\nDecoding (D3). D3 disables length normalization for ghost tokens to alleviate\namplification bias, and it incorporates a text-free assistant model to\nencourage tokens less frequently generated by LLMs for counteracting\nrecommendation homogeneity. Extensive experiments on real-world datasets\ndemonstrate the method's effectiveness in enhancing accuracy and diversity. The\ncode is available at https://github.com/SAI990323/DecodingMatters."
                },
                "authors": [
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xinyue Huo"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02850v1",
                "updated": "2024-11-05T06:44:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    44,
                    15,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T06:44:15Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    44,
                    15,
                    1,
                    310,
                    0
                ],
                "title": "WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African\n  clean water access, sanitation and hygiene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African\n  clean water access, sanitation and hygiene"
                },
                "summary": "This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate\nrural African communities on clean water access, sanitation, and hygiene (WASH)\nprinciples. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach\nto address the limitations of previous approaches with limited reach or missing\ncontextualization. The paper details the development process, employing Design\nScience Research Methodology. The evaluation consisted of two phases: content\nvalidation by four WASH experts and community validation by potential users.\nContent validation confirmed WASHtsApp's ability to provide accurate and\nrelevant WASH-related information. Community validation indicated high user\nacceptance and perceived usefulness of the chatbot. The paper concludes by\ndiscussing the potential for further development, including incorporating local\nlanguages and user data analysis for targeted interventions. It also proposes\nfuture research cycles focused on wider deployment and leveraging user data for\neducational purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate\nrural African communities on clean water access, sanitation, and hygiene (WASH)\nprinciples. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach\nto address the limitations of previous approaches with limited reach or missing\ncontextualization. The paper details the development process, employing Design\nScience Research Methodology. The evaluation consisted of two phases: content\nvalidation by four WASH experts and community validation by potential users.\nContent validation confirmed WASHtsApp's ability to provide accurate and\nrelevant WASH-related information. Community validation indicated high user\nacceptance and perceived usefulness of the chatbot. The paper concludes by\ndiscussing the potential for further development, including incorporating local\nlanguages and user data analysis for targeted interventions. It also proposes\nfuture research cycles focused on wider deployment and leveraging user data for\neducational purposes."
                },
                "authors": [
                    {
                        "name": "Simon Kloker"
                    },
                    {
                        "name": "Alex Cedric Luyima"
                    },
                    {
                        "name": "Matthew Bazanya"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Bazanya"
                },
                "author": "Matthew Bazanya",
                "arxiv_comment": "Working Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09562v3",
                "updated": "2024-11-05T06:35:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    35,
                    52,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-03T10:21:07Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    10,
                    21,
                    7,
                    2,
                    185,
                    0
                ],
                "title": "Edge AI-Enabled Chicken Health Detection Based on Enhanced FCOS-Lite and\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI-Enabled Chicken Health Detection Based on Enhanced FCOS-Lite and\n  Knowledge Distillation"
                },
                "summary": "The utilization of AIoT technology has become a crucial trend in modern\npoultry management, offering the potential to optimize farming operations and\nreduce human workloads. This paper presents a real-time and compact edge-AI\nenabled detector designed to identify chickens and their healthy statuses using\nframes captured by a lightweight and intelligent camera equipped with an\nedge-AI enabled CMOS sensor. To ensure efficient deployment of the proposed\ncompact detector within the memory-constrained edge-AI enabled CMOS sensor, we\nemploy a FCOS-Lite detector leveraging MobileNet as the backbone. To mitigate\nthe issue of reduced accuracy in compact edge-AI detectors without incurring\nadditional inference costs, we propose a gradient weighting loss function as\nclassification loss and introduce CIOU loss function as localization loss.\nAdditionally, we propose a knowledge distillation scheme to transfer valuable\ninformation from a large teacher detector to the proposed FCOS-Lite detector,\nthereby enhancing its performance while preserving a compact model size.\nExperimental results demonstrate the proposed edge-AI enabled detector achieves\ncommendable performance metrics, including a mean average precision (mAP) of\n95.1$\\%$ and an F1-score of 94.2$\\%$, etc. Notably, the proposed detector can\nbe efficiently deployed and operates at a speed exceeding 20 FPS on the edge-AI\nenabled CMOS sensor, achieved through int8 quantization. That meets practical\ndemands for automated poultry health monitoring using lightweight intelligent\ncameras with low power consumption and minimal bandwidth costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The utilization of AIoT technology has become a crucial trend in modern\npoultry management, offering the potential to optimize farming operations and\nreduce human workloads. This paper presents a real-time and compact edge-AI\nenabled detector designed to identify chickens and their healthy statuses using\nframes captured by a lightweight and intelligent camera equipped with an\nedge-AI enabled CMOS sensor. To ensure efficient deployment of the proposed\ncompact detector within the memory-constrained edge-AI enabled CMOS sensor, we\nemploy a FCOS-Lite detector leveraging MobileNet as the backbone. To mitigate\nthe issue of reduced accuracy in compact edge-AI detectors without incurring\nadditional inference costs, we propose a gradient weighting loss function as\nclassification loss and introduce CIOU loss function as localization loss.\nAdditionally, we propose a knowledge distillation scheme to transfer valuable\ninformation from a large teacher detector to the proposed FCOS-Lite detector,\nthereby enhancing its performance while preserving a compact model size.\nExperimental results demonstrate the proposed edge-AI enabled detector achieves\ncommendable performance metrics, including a mean average precision (mAP) of\n95.1$\\%$ and an F1-score of 94.2$\\%$, etc. Notably, the proposed detector can\nbe efficiently deployed and operates at a speed exceeding 20 FPS on the edge-AI\nenabled CMOS sensor, achieved through int8 quantization. That meets practical\ndemands for automated poultry health monitoring using lightweight intelligent\ncameras with low power consumption and minimal bandwidth costs."
                },
                "authors": [
                    {
                        "name": "Qiang Tong"
                    },
                    {
                        "name": "Jinrui Wang"
                    },
                    {
                        "name": "Wenshuang Yang"
                    },
                    {
                        "name": "Songtao Wu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Kuanhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kuanhong Xu"
                },
                "author": "Kuanhong Xu",
                "arxiv_doi": "10.1016/j.compag.2024.109432",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compag.2024.109432",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.09562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12629v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12629v4",
                "updated": "2024-11-05T06:34:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    34,
                    40,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-18T13:55:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    55,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation"
                },
                "summary": "Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area."
                },
                "authors": [
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Boya Xiong"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "arxiv_comment": "Accepted by NeurIPS 2024. Project page is live at\n  https://SeTAR-OOD.github.io. Code are available at\n  https://github.com/X1AOX1A/SeTAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12629v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12629v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02840v1",
                "updated": "2024-11-05T06:23:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    23,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T06:23:44Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    23,
                    44,
                    1,
                    310,
                    0
                ],
                "title": "Test-Time Dynamic Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Dynamic Image Fusion"
                },
                "summary": "The inherent challenge of image fusion lies in capturing the correlation of\nmulti-source images and comprehensively integrating effective information from\ndifferent sources. Most existing techniques fail to perform dynamic image\nfusion while notably lacking theoretical guarantees, leading to potential\ndeployment risks in this field. Is it possible to conduct dynamic image fusion\nwith a clear theoretical justification? In this paper, we give our solution\nfrom a generalization perspective. We proceed to reveal the generalized form of\nimage fusion and derive a new test-time dynamic image fusion paradigm. It\nprovably reduces the upper bound of generalization error. Specifically, we\ndecompose the fused image into multiple components corresponding to its source\ndata. The decomposed components represent the effective information from the\nsource data, thus the gap between them reflects the Relative Dominability (RD)\nof the uni-source data in constructing the fusion image. Theoretically, we\nprove that the key to reducing generalization error hinges on the negative\ncorrelation between the RD-based fusion weight and the uni-source\nreconstruction loss. Intuitively, RD dynamically highlights the dominant\nregions of each source and can be naturally converted to the corresponding\nfusion weight, achieving robust results. Extensive experiments and discussions\nwith in-depth analysis on multiple benchmarks confirm our findings and\nsuperiority. Our code is available at https://github.com/Yinan-Xia/TTD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent challenge of image fusion lies in capturing the correlation of\nmulti-source images and comprehensively integrating effective information from\ndifferent sources. Most existing techniques fail to perform dynamic image\nfusion while notably lacking theoretical guarantees, leading to potential\ndeployment risks in this field. Is it possible to conduct dynamic image fusion\nwith a clear theoretical justification? In this paper, we give our solution\nfrom a generalization perspective. We proceed to reveal the generalized form of\nimage fusion and derive a new test-time dynamic image fusion paradigm. It\nprovably reduces the upper bound of generalization error. Specifically, we\ndecompose the fused image into multiple components corresponding to its source\ndata. The decomposed components represent the effective information from the\nsource data, thus the gap between them reflects the Relative Dominability (RD)\nof the uni-source data in constructing the fusion image. Theoretically, we\nprove that the key to reducing generalization error hinges on the negative\ncorrelation between the RD-based fusion weight and the uni-source\nreconstruction loss. Intuitively, RD dynamically highlights the dominant\nregions of each source and can be naturally converted to the corresponding\nfusion weight, achieving robust results. Extensive experiments and discussions\nwith in-depth analysis on multiple benchmarks confirm our findings and\nsuperiority. Our code is available at https://github.com/Yinan-Xia/TTD."
                },
                "authors": [
                    {
                        "name": "Bing Cao"
                    },
                    {
                        "name": "Yinan Xia"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Qinghua Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qinghua Hu"
                },
                "author": "Qinghua Hu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02830v1",
                "updated": "2024-11-05T06:02:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    2,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T06:02:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    2,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "Mixtures of In-Context Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixtures of In-Context Learners"
                },
                "summary": "In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory."
                },
                "authors": [
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Emile van Krieken"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02829v1",
                "updated": "2024-11-05T06:00:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    0,
                    27,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T06:00:27Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    6,
                    0,
                    27,
                    1,
                    310,
                    0
                ],
                "title": "CE-CoLLM: Efficient and Adaptive Large Language Models Through\n  Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-CoLLM: Efficient and Adaptive Large Language Models Through\n  Cloud-Edge Collaboration"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in serving\nend-users with human-like intelligence. However, LLMs demand high computational\nresources, making it challenging to deploy them to satisfy various performance\nobjectives, such as meeting the resource constraints on edge devices close to\nend-users or achieving high accuracy with ample resources. In this paper, we\nintroduce CE-CoLLM, a novel cloud-edge collaboration framework that supports\nefficient and adaptive LLM inference for end-users at the edge with two modes,\n(1) low-latency edge standalone inference and (2) highly accurate cloud-edge\ncollaborative inference. First, we show that the inherent high communication\ncosts for transmitting LLM contextual information between the edge and cloud\ndominate the overall latency, making it inefficient and costly to deploy LLMs\nusing cloud-edge collaboration. Second, we propose several critical techniques\nto address this challenge, including early-exit mechanism, cloud context\nmanager, and quantization in cloud-edge collaboration to enable not only\nlow-latency standalone edge inference but also efficient and adaptive\ncloud-edge collaborative inference for LLMs. Third, we perform comprehensive\nexperimental analysis, which demonstrates that CE-CoLLM significantly reduces\ninference time by up to 13.81% and cloud computation costs by up to 84.55%\ncompared to the popular cloud-based LLM deployment, while maintaining\ncomparable model accuracy. The proposed approach effectively shifts the\ncomputational load to the edge, reduces the communication overhead, scales\nefficiently with multiple edge clients, and provides reliable LLM deployment\nusing cloud-edge collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in serving\nend-users with human-like intelligence. However, LLMs demand high computational\nresources, making it challenging to deploy them to satisfy various performance\nobjectives, such as meeting the resource constraints on edge devices close to\nend-users or achieving high accuracy with ample resources. In this paper, we\nintroduce CE-CoLLM, a novel cloud-edge collaboration framework that supports\nefficient and adaptive LLM inference for end-users at the edge with two modes,\n(1) low-latency edge standalone inference and (2) highly accurate cloud-edge\ncollaborative inference. First, we show that the inherent high communication\ncosts for transmitting LLM contextual information between the edge and cloud\ndominate the overall latency, making it inefficient and costly to deploy LLMs\nusing cloud-edge collaboration. Second, we propose several critical techniques\nto address this challenge, including early-exit mechanism, cloud context\nmanager, and quantization in cloud-edge collaboration to enable not only\nlow-latency standalone edge inference but also efficient and adaptive\ncloud-edge collaborative inference for LLMs. Third, we perform comprehensive\nexperimental analysis, which demonstrates that CE-CoLLM significantly reduces\ninference time by up to 13.81% and cloud computation costs by up to 84.55%\ncompared to the popular cloud-based LLM deployment, while maintaining\ncomparable model accuracy. The proposed approach effectively shifts the\ncomputational load to the edge, reduces the communication overhead, scales\nefficiently with multiple edge clients, and provides reliable LLM deployment\nusing cloud-edge collaboration."
                },
                "authors": [
                    {
                        "name": "Hongpeng Jin"
                    },
                    {
                        "name": "Yanzhao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhao Wu"
                },
                "author": "Yanzhao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14419v2",
                "updated": "2024-11-05T05:43:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    43,
                    30,
                    1,
                    310,
                    0
                ],
                "published": "2024-04-14T07:06:12Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    7,
                    6,
                    12,
                    6,
                    105,
                    0
                ],
                "title": "Evaluation and Improvement of Fault Detection for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation and Improvement of Fault Detection for Large Language Models"
                },
                "summary": "Large language models (LLMs) have recently achieved significant success\nacross various application domains, garnering substantial attention from\ndifferent communities. Unfortunately, even for the best LLM, many\n\\textit{faults} still exist that LLM cannot properly predict. Such faults will\nharm the usability of LLMs in general and could introduce safety issues in\nreliability-critical systems such as autonomous driving systems. How to quickly\nreveal these faults in real-world datasets that LLM could face is important,\nbut challenging. The major reason is that the ground truth is necessary but the\ndata labeling process is heavy considering the time and human effort. To handle\nthis problem, in the conventional deep learning testing field, test selection\nmethods have been proposed for efficiently evaluating deep learning models by\nprioritizing faults. However, despite their importance, the usefulness of these\nmethods on LLMs is unclear, and lack of exploration. In this paper, we conduct\nthe first empirical study to investigate the effectiveness of existing fault\ndetection methods for LLMs. Experimental results on four different\ntasks~(including both code tasks and natural language processing tasks) and\nfour LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as\nMargin perform well on LLMs but there is still a big room for improvement.\nBased on the study, we further propose \\textbf{MuCS}, a prompt\n\\textbf{Mu}tation-based prediction \\textbf{C}onfidence \\textbf{S}moothing\nframework to boost the fault detection capability of existing methods.\nConcretely, multiple prompt mutation techniques have been proposed to help\ncollect more diverse outputs for confidence smoothing. The results show that\nour proposed framework significantly enhances existing methods with the\nimprovement of test relative coverage by up to 70.53\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved significant success\nacross various application domains, garnering substantial attention from\ndifferent communities. Unfortunately, even for the best LLM, many\n\\textit{faults} still exist that LLM cannot properly predict. Such faults will\nharm the usability of LLMs in general and could introduce safety issues in\nreliability-critical systems such as autonomous driving systems. How to quickly\nreveal these faults in real-world datasets that LLM could face is important,\nbut challenging. The major reason is that the ground truth is necessary but the\ndata labeling process is heavy considering the time and human effort. To handle\nthis problem, in the conventional deep learning testing field, test selection\nmethods have been proposed for efficiently evaluating deep learning models by\nprioritizing faults. However, despite their importance, the usefulness of these\nmethods on LLMs is unclear, and lack of exploration. In this paper, we conduct\nthe first empirical study to investigate the effectiveness of existing fault\ndetection methods for LLMs. Experimental results on four different\ntasks~(including both code tasks and natural language processing tasks) and\nfour LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as\nMargin perform well on LLMs but there is still a big room for improvement.\nBased on the study, we further propose \\textbf{MuCS}, a prompt\n\\textbf{Mu}tation-based prediction \\textbf{C}onfidence \\textbf{S}moothing\nframework to boost the fault detection capability of existing methods.\nConcretely, multiple prompt mutation techniques have been proposed to help\ncollect more diverse outputs for confidence smoothing. The results show that\nour proposed framework significantly enhances existing methods with the\nimprovement of test relative coverage by up to 70.53\\%."
                },
                "authors": [
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Jin Wen"
                    },
                    {
                        "name": "Maxime Cordy"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11772v2",
                "updated": "2024-11-05T05:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    13,
                    0,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-15T16:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    53,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST."
                },
                "authors": [
                    {
                        "name": "Kai Yao"
                    },
                    {
                        "name": "Penglei Gao"
                    },
                    {
                        "name": "Lichun Li"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17977v2",
                "updated": "2024-11-05T04:37:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    4,
                    37,
                    22,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-28T09:06:18Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    9,
                    6,
                    18,
                    1,
                    149,
                    0
                ],
                "title": "Aligning to Thousands of Preferences via System Message Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning to Thousands of Preferences via System Message Generalization"
                },
                "summary": "Although humans inherently have diverse values, current large language model\n(LLM) alignment methods often assume that aligning LLMs with the general\npublic's preferences is optimal. A major challenge in adopting a more\nindividualized approach to LLM alignment is its lack of scalability, as it\ninvolves repeatedly acquiring preference data and training new reward models\nand LLMs for each individual's preferences. To address these challenges, we\npropose a new paradigm where users specify what they value most within the\nsystem message, steering the LLM's generation behavior to better align with the\nuser's intentions. However, a naive application of such an approach is\nnon-trivial since LLMs are typically trained on a uniform system message (e.g.,\n\"You are a helpful assistant\") which limits their ability to generalize to\ndiverse, unseen system messages. To improve this generalization, we create the\nMultifaceted Collection, a preference dataset with 192k combinations of values\nbeyond generic helpfulness and harmlessness, spanning 65k user instructions.\nUsing this dataset, we train a 7B LLM called Janus and test it on 921 prompts\nfrom 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding various unseen system messages that reflect user preferences. Janus\nachieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct\nv0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks\nfocused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto\nv0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%\nmargin, underscoring that training with a vast array of system messages could\nalso enhance alignment to the general public's preference as well. Our code,\ndataset, benchmark, and models are available at\nhttps://github.com/kaistAI/Janus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although humans inherently have diverse values, current large language model\n(LLM) alignment methods often assume that aligning LLMs with the general\npublic's preferences is optimal. A major challenge in adopting a more\nindividualized approach to LLM alignment is its lack of scalability, as it\ninvolves repeatedly acquiring preference data and training new reward models\nand LLMs for each individual's preferences. To address these challenges, we\npropose a new paradigm where users specify what they value most within the\nsystem message, steering the LLM's generation behavior to better align with the\nuser's intentions. However, a naive application of such an approach is\nnon-trivial since LLMs are typically trained on a uniform system message (e.g.,\n\"You are a helpful assistant\") which limits their ability to generalize to\ndiverse, unseen system messages. To improve this generalization, we create the\nMultifaceted Collection, a preference dataset with 192k combinations of values\nbeyond generic helpfulness and harmlessness, spanning 65k user instructions.\nUsing this dataset, we train a 7B LLM called Janus and test it on 921 prompts\nfrom 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding various unseen system messages that reflect user preferences. Janus\nachieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct\nv0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks\nfocused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto\nv0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%\nmargin, underscoring that training with a vast array of system messages could\nalso enhance alignment to the general public's preference as well. Our code,\ndataset, benchmark, and models are available at\nhttps://github.com/kaistAI/Janus."
                },
                "authors": [
                    {
                        "name": "Seongyun Lee"
                    },
                    {
                        "name": "Sue Hyun Park"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01063v2",
                "updated": "2024-11-05T04:21:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    4,
                    21,
                    55,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-01T22:31:32Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    22,
                    31,
                    32,
                    4,
                    306,
                    0
                ],
                "title": "InterTrans: Leveraging Transitive Intermediate Translations to Enhance\n  LLM-based Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterTrans: Leveraging Transitive Intermediate Translations to Enhance\n  LLM-based Code Translation"
                },
                "summary": "Code translation aims to convert a program from one programming language (PL)\nto another. This long-standing software engineering task is crucial for\nmodernizing legacy systems, ensuring cross-platform compatibility, enhancing\nperformance, and more. However, automating this process remains challenging due\nto many syntactic and semantic differences between PLs. Recent studies show\nthat even advanced techniques such as large language models (LLMs), especially\nopen-source LLMs, still struggle with the task. Currently, code LLMs are\ntrained with source code from multiple programming languages, thus presenting\nmultilingual capabilities.\n  In this paper, we investigate whether such multilingual capabilities can be\nharnessed to enhance code translation. To achieve this goal, we introduce\nInterTrans, an LLM-based automated code translation approach that, in contrast\nto existing approaches, leverages intermediate translations across PLs to\nbridge the syntactic and semantic gaps between source and target PLs.\n  InterTrans contains two stages. It first utilizes a novel Tree of Code\nTranslation (ToCT) algorithm to plan transitive intermediate translation\nsequences between a given source and target PL, then validates them in a\nspecific order. We evaluate InterTrans with three open LLMs on three benchmarks\n(i.e., CodeNet, HumanEval-X, and TransCoder) involving six PLs. Results show an\nabsolute improvement between 18.3% to 43.3% in Computation Accuracy (CA) for\nInterTrans over Direct Translation with 10 attempts. The best-performing\nvariant of InterTrans (with Magicoder LLM) achieved an average CA of\n87.3%-95.4% on three benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation aims to convert a program from one programming language (PL)\nto another. This long-standing software engineering task is crucial for\nmodernizing legacy systems, ensuring cross-platform compatibility, enhancing\nperformance, and more. However, automating this process remains challenging due\nto many syntactic and semantic differences between PLs. Recent studies show\nthat even advanced techniques such as large language models (LLMs), especially\nopen-source LLMs, still struggle with the task. Currently, code LLMs are\ntrained with source code from multiple programming languages, thus presenting\nmultilingual capabilities.\n  In this paper, we investigate whether such multilingual capabilities can be\nharnessed to enhance code translation. To achieve this goal, we introduce\nInterTrans, an LLM-based automated code translation approach that, in contrast\nto existing approaches, leverages intermediate translations across PLs to\nbridge the syntactic and semantic gaps between source and target PLs.\n  InterTrans contains two stages. It first utilizes a novel Tree of Code\nTranslation (ToCT) algorithm to plan transitive intermediate translation\nsequences between a given source and target PL, then validates them in a\nspecific order. We evaluate InterTrans with three open LLMs on three benchmarks\n(i.e., CodeNet, HumanEval-X, and TransCoder) involving six PLs. Results show an\nabsolute improvement between 18.3% to 43.3% in Computation Accuracy (CA) for\nInterTrans over Direct Translation with 10 attempts. The best-performing\nvariant of InterTrans (with Magicoder LLM) achieved an average CA of\n87.3%-95.4% on three benchmarks."
                },
                "authors": [
                    {
                        "name": "Marcos Macedo"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Pengyu Nie"
                    },
                    {
                        "name": "Filipe R. Cogo"
                    },
                    {
                        "name": "Bram Adams"
                    }
                ],
                "author_detail": {
                    "name": "Bram Adams"
                },
                "author": "Bram Adams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02791v1",
                "updated": "2024-11-05T04:01:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    4,
                    1,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T04:01:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    4,
                    1,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "Language Models and Cycle Consistency for Self-Reflective Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models and Cycle Consistency for Self-Reflective Machine\n  Translation"
                },
                "summary": "This paper introduces a novel framework that leverages large language models\n(LLMs) for machine translation (MT). We start with one conjecture: an ideal\ntranslation should contain complete and accurate information for a strong\nenough LLM to recover the original sentence. We generate multiple translation\ncandidates from a source language A to a target language B, and subsequently\ntranslate these candidates back to the original language A. By evaluating the\ncycle consistency between the original and back-translated sentences using\nmetrics such as token-level precision and accuracy, we implicitly estimate the\ntranslation quality in language B, without knowing its ground-truth. This also\nhelps to evaluate the LLM translation capability, only with monolingual\ncorpora. For each source sentence, we identify the translation candidate with\noptimal cycle consistency with the original sentence as the final answer. Our\nexperiments demonstrate that larger LLMs, or the same LLM with more forward\npasses during inference, exhibit increased cycle consistency, aligning with the\nLLM model size scaling law and test-time computation scaling law. This work\nprovide methods for, 1) to implicitly evaluate translation quality of a\nsentence in the target language, 2), to evaluate capability of LLM for\nany-to-any-language translation, and 3), how to generate a better translation\nfor a specific LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework that leverages large language models\n(LLMs) for machine translation (MT). We start with one conjecture: an ideal\ntranslation should contain complete and accurate information for a strong\nenough LLM to recover the original sentence. We generate multiple translation\ncandidates from a source language A to a target language B, and subsequently\ntranslate these candidates back to the original language A. By evaluating the\ncycle consistency between the original and back-translated sentences using\nmetrics such as token-level precision and accuracy, we implicitly estimate the\ntranslation quality in language B, without knowing its ground-truth. This also\nhelps to evaluate the LLM translation capability, only with monolingual\ncorpora. For each source sentence, we identify the translation candidate with\noptimal cycle consistency with the original sentence as the final answer. Our\nexperiments demonstrate that larger LLMs, or the same LLM with more forward\npasses during inference, exhibit increased cycle consistency, aligning with the\nLLM model size scaling law and test-time computation scaling law. This work\nprovide methods for, 1) to implicitly evaluate translation quality of a\nsentence in the target language, 2), to evaluate capability of LLM for\nany-to-any-language translation, and 3), how to generate a better translation\nfor a specific LLM."
                },
                "authors": [
                    {
                        "name": "Jianqiao Wangni"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiao Wangni"
                },
                "author": "Jianqiao Wangni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02785v1",
                "updated": "2024-11-05T03:51:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    51,
                    13,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T03:51:13Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    51,
                    13,
                    1,
                    310,
                    0
                ],
                "title": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n  Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n  Safety Alignment"
                },
                "summary": "Safety alignment of Large Language Models (LLMs) has recently become a\ncritical objective of model developers. In response, a growing body of work has\nbeen investigating how safety alignment can be bypassed through various\njailbreaking methods, such as adversarial attacks. However, these jailbreak\nmethods can be rather costly or involve a non-trivial amount of creativity and\neffort, introducing the assumption that malicious users are high-resource or\nsophisticated. In this paper, we study how simple random augmentations to the\ninput prompt affect safety alignment effectiveness in state-of-the-art LLMs,\nsuch as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\nmodels and investigate the intersection of safety under random augmentations\nwith multiple dimensions: augmentation type, model size, quantization,\nfine-tuning-based defenses, and decoding strategies (e.g., sampling\ntemperature). We show that low-resource and unsophisticated attackers, i.e.\n$\\textit{stochastic monkeys}$, can significantly improve their chances of\nbypassing alignment with just 25 random augmentations per prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment of Large Language Models (LLMs) has recently become a\ncritical objective of model developers. In response, a growing body of work has\nbeen investigating how safety alignment can be bypassed through various\njailbreaking methods, such as adversarial attacks. However, these jailbreak\nmethods can be rather costly or involve a non-trivial amount of creativity and\neffort, introducing the assumption that malicious users are high-resource or\nsophisticated. In this paper, we study how simple random augmentations to the\ninput prompt affect safety alignment effectiveness in state-of-the-art LLMs,\nsuch as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\nmodels and investigate the intersection of safety under random augmentations\nwith multiple dimensions: augmentation type, model size, quantization,\nfine-tuning-based defenses, and decoding strategies (e.g., sampling\ntemperature). We show that low-resource and unsophisticated attackers, i.e.\n$\\textit{stochastic monkeys}$, can significantly improve their chances of\nbypassing alignment with just 25 random augmentations per prompt."
                },
                "authors": [
                    {
                        "name": "Jason Vega"
                    },
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Gaokai Zhang"
                    },
                    {
                        "name": "Hangoo Kang"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "arxiv_comment": "Under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18494v2",
                "updated": "2024-11-05T03:38:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    38,
                    39,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-24T07:29:15Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    29,
                    15,
                    3,
                    298,
                    0
                ],
                "title": "Assured Automatic Programming via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assured Automatic Programming via Large Language Models"
                },
                "summary": "With the advent of AI-based coding engines, it is possible to convert natural\nlanguage requirements to executable code in standard programming languages.\nHowever, AI-generated code can be unreliable, and the natural language\nrequirements driving this code may be ambiguous. In other words, the intent may\nnot be accurately captured in the code generated from AI-coding engines like\nCopilot. The goal of our work is to discover the programmer intent, while\ngenerating code which conforms to the intent and a proof of this conformance.\nOur approach to intent discovery is powered by a novel repair engine called\nprogram-proof co-evolution, where the object of repair is a tuple (code,\nlogical specification, test) generated by an LLM from the same natural language\ndescription. The program and the specification capture the initial operational\nand declarative description of intent, while the test represents a concrete,\nalbeit partial, understanding of the intent. Our objective is to achieve\nconsistency between the program, the specification, and the test by\nincrementally refining our understanding of the user intent. Reaching\nconsistency through this repair process provides us with a formal, logical\ndescription of the intent, which is then translated back into natural language\nfor the developer's inspection. The resultant intent description is now\nunambiguous, though expressed in natural language. We demonstrate how the\nunambiguous intent discovered through our approach increases the percentage of\nverifiable auto-generated programs on a recently proposed dataset in the Dafny\nprogramming language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of AI-based coding engines, it is possible to convert natural\nlanguage requirements to executable code in standard programming languages.\nHowever, AI-generated code can be unreliable, and the natural language\nrequirements driving this code may be ambiguous. In other words, the intent may\nnot be accurately captured in the code generated from AI-coding engines like\nCopilot. The goal of our work is to discover the programmer intent, while\ngenerating code which conforms to the intent and a proof of this conformance.\nOur approach to intent discovery is powered by a novel repair engine called\nprogram-proof co-evolution, where the object of repair is a tuple (code,\nlogical specification, test) generated by an LLM from the same natural language\ndescription. The program and the specification capture the initial operational\nand declarative description of intent, while the test represents a concrete,\nalbeit partial, understanding of the intent. Our objective is to achieve\nconsistency between the program, the specification, and the test by\nincrementally refining our understanding of the user intent. Reaching\nconsistency through this repair process provides us with a formal, logical\ndescription of the intent, which is then translated back into natural language\nfor the developer's inspection. The resultant intent description is now\nunambiguous, though expressed in natural language. We demonstrate how the\nunambiguous intent discovered through our approach increases the percentage of\nverifiable auto-generated programs on a recently proposed dataset in the Dafny\nprogramming language."
                },
                "authors": [
                    {
                        "name": "Martin Mirchev"
                    },
                    {
                        "name": "Andreea Costea"
                    },
                    {
                        "name": "Abhishek Kr Singh"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02773v1",
                "updated": "2024-11-05T03:34:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    34,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T03:34:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    34,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "FedBlock: A Blockchain Approach to Federated Learning against Backdoor\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedBlock: A Blockchain Approach to Federated Learning against Backdoor\n  Attacks"
                },
                "summary": "Federated Learning (FL) is a machine learning method for training with\nprivate data locally stored in distributed machines without gathering them into\none place for central learning. Despite its promises, FL is prone to critical\nsecurity risks. First, because FL depends on a central server to aggregate\nlocal training models, this is a single point of failure. The server might\nfunction maliciously. Second, due to its distributed nature, FL might encounter\nbackdoor attacks by participating clients. They can poison the local model\nbefore submitting to the server. Either type of attack, on the server or the\nclient side, would severely degrade learning accuracy. We propose FedBlock, a\nnovel blockchain-based FL framework that addresses both of these security\nrisks. FedBlock is uniquely desirable in that it involves only smart contract\nprogramming, thus deployable atop any blockchain network. Our framework is\nsubstantiated with a comprehensive evaluation study using real-world datasets.\nIts robustness against backdoor attacks is competitive with the literature of\nFL backdoor defense. The latter, however, does not address the server risk as\nwe do.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a machine learning method for training with\nprivate data locally stored in distributed machines without gathering them into\none place for central learning. Despite its promises, FL is prone to critical\nsecurity risks. First, because FL depends on a central server to aggregate\nlocal training models, this is a single point of failure. The server might\nfunction maliciously. Second, due to its distributed nature, FL might encounter\nbackdoor attacks by participating clients. They can poison the local model\nbefore submitting to the server. Either type of attack, on the server or the\nclient side, would severely degrade learning accuracy. We propose FedBlock, a\nnovel blockchain-based FL framework that addresses both of these security\nrisks. FedBlock is uniquely desirable in that it involves only smart contract\nprogramming, thus deployable atop any blockchain network. Our framework is\nsubstantiated with a comprehensive evaluation study using real-world datasets.\nIts robustness against backdoor attacks is competitive with the literature of\nFL backdoor defense. The latter, however, does not address the server risk as\nwe do."
                },
                "authors": [
                    {
                        "name": "Duong H. Nguyen"
                    },
                    {
                        "name": "Phi L. Nguyen"
                    },
                    {
                        "name": "Truong T. Nguyen"
                    },
                    {
                        "name": "Hieu H. Pham"
                    },
                    {
                        "name": "Duc A. Tran"
                    }
                ],
                "author_detail": {
                    "name": "Duc A. Tran"
                },
                "author": "Duc A. Tran",
                "arxiv_comment": "This paper has been accepted as a full paper for the IEEE Special\n  Session Federated Learning on Big Data 2024 (IEEE BigData 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09180v2",
                "updated": "2024-11-05T03:34:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    34,
                    10,
                    1,
                    310,
                    0
                ],
                "published": "2023-11-15T18:19:58Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    18,
                    19,
                    58,
                    2,
                    319,
                    0
                ],
                "title": "Pearl: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers"
                },
                "summary": "Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style,\nspecialized knowledge, and values. In this paper, we address this challenge by\nproposing Pearl, a LLM writing assistant personalized with a retriever that is\ntrained to be generation-calibrated for personalization. Generation calibration\nensures that our retriever selects historic user authored documents to augment\nan LLM prompt such that they are likely to help an LLM generation better adhere\nto a users' preferences. We propose two key novelties for training such a\nretriever: (1) A training data selection method that identifies user requests\nlikely to benefit from personalization and documents that provide that benefit;\nand (2) A scale-calibrating KL-divergence objective that ensures that our\nretriever scores remain proportional to the downstream generation quality from\nusing the document for personalized generation. In a series of holistic\nevaluations, we demonstrate the effectiveness of Pearl in generating long-form\ntexts on multiple social media datasets. Finally, we demonstrate how a\ngeneration-calibrated retriever can double as a performance predictor --\ndetecting low quality retrieval, and improving potentially under-performing\noutputs via revision with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style,\nspecialized knowledge, and values. In this paper, we address this challenge by\nproposing Pearl, a LLM writing assistant personalized with a retriever that is\ntrained to be generation-calibrated for personalization. Generation calibration\nensures that our retriever selects historic user authored documents to augment\nan LLM prompt such that they are likely to help an LLM generation better adhere\nto a users' preferences. We propose two key novelties for training such a\nretriever: (1) A training data selection method that identifies user requests\nlikely to benefit from personalization and documents that provide that benefit;\nand (2) A scale-calibrating KL-divergence objective that ensures that our\nretriever scores remain proportional to the downstream generation quality from\nusing the document for personalized generation. In a series of holistic\nevaluations, we demonstrate the effectiveness of Pearl in generating long-form\ntexts on multiple social media datasets. Finally, we demonstrate how a\ngeneration-calibrated retriever can double as a performance predictor --\ndetecting low quality retrieval, and improving potentially under-performing\noutputs via revision with LLMs."
                },
                "authors": [
                    {
                        "name": "Sheshera Mysore"
                    },
                    {
                        "name": "Zhuoran Lu"
                    },
                    {
                        "name": "Mengting Wan"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Bahareh Sarrafzadeh"
                    },
                    {
                        "name": "Steve Menezes"
                    },
                    {
                        "name": "Tina Baghaee"
                    },
                    {
                        "name": "Emmanuel Barajas Gonzalez"
                    },
                    {
                        "name": "Jennifer Neville"
                    },
                    {
                        "name": "Tara Safavi"
                    }
                ],
                "author_detail": {
                    "name": "Tara Safavi"
                },
                "author": "Tara Safavi",
                "arxiv_comment": "Accepted to Workshop on Customizable NLP at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v4",
                "updated": "2024-11-05T03:32:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    32,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02132v3",
                "updated": "2024-11-05T03:29:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    29,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-03T14:35:58Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    14,
                    35,
                    58,
                    4,
                    124,
                    0
                ],
                "title": "Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets"
                },
                "summary": "Large Language Models (LLMs) have demonstrated unparalleled effectiveness in\nvarious NLP tasks, and integrating LLMs with automatic speech recognition (ASR)\nis becoming a mainstream paradigm. Building upon this momentum, our research\ndelves into an in-depth examination of this paradigm on a large open-source\nChinese dataset. Specifically, our research aims to evaluate the impact of\nvarious configurations of speech encoders, LLMs, and projector modules in the\ncontext of the speech foundation encoder-LLM ASR paradigm. Furthermore, we\nintroduce a three-stage training approach, expressly developed to enhance the\nmodel's ability to align auditory and textual information. The implementation\nof this approach, alongside the strategic integration of ASR components,\nenabled us to achieve the SOTA performance on the AISHELL-1, Test_Net, and\nTest_Meeting test sets. Our analysis presents an empirical foundation for\nfuture research in LLM-based ASR systems and offers insights into optimizing\nperformance using Chinese datasets. We will publicly release all scripts used\nfor data preparation, training, inference, and scoring, as well as pre-trained\nmodels and training logs to promote reproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated unparalleled effectiveness in\nvarious NLP tasks, and integrating LLMs with automatic speech recognition (ASR)\nis becoming a mainstream paradigm. Building upon this momentum, our research\ndelves into an in-depth examination of this paradigm on a large open-source\nChinese dataset. Specifically, our research aims to evaluate the impact of\nvarious configurations of speech encoders, LLMs, and projector modules in the\ncontext of the speech foundation encoder-LLM ASR paradigm. Furthermore, we\nintroduce a three-stage training approach, expressly developed to enhance the\nmodel's ability to align auditory and textual information. The implementation\nof this approach, alongside the strategic integration of ASR components,\nenabled us to achieve the SOTA performance on the AISHELL-1, Test_Net, and\nTest_Meeting test sets. Our analysis presents an empirical foundation for\nfuture research in LLM-based ASR systems and offers insights into optimizing\nperformance using Chinese datasets. We will publicly release all scripts used\nfor data preparation, training, inference, and scoring, as well as pre-trained\nmodels and training logs to promote reproducible research."
                },
                "authors": [
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Bingshen Mu"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Yuhang Dai"
                    },
                    {
                        "name": "Longhao Li"
                    },
                    {
                        "name": "Mingchen Shao"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01539v2",
                "updated": "2024-11-05T03:20:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    20,
                    10,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-03T12:03:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    12,
                    3,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "LLMs and the Madness of Crowds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and the Madness of Crowds"
                },
                "summary": "We investigate the patterns of incorrect answers produced by large language\nmodels (LLMs) during evaluation. These errors exhibit highly non-intuitive\nbehaviors unique to each model. By analyzing these patterns, we measure the\nsimilarities between LLMs and construct a taxonomy that categorizes them based\non their error correlations. Our findings reveal that the incorrect responses\nare not randomly distributed but systematically correlated across models,\nproviding new insights into the underlying structures and relationships among\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the patterns of incorrect answers produced by large language\nmodels (LLMs) during evaluation. These errors exhibit highly non-intuitive\nbehaviors unique to each model. By analyzing these patterns, we measure the\nsimilarities between LLMs and construct a taxonomy that categorizes them based\non their error correlations. Our findings reveal that the incorrect responses\nare not randomly distributed but systematically correlated across models,\nproviding new insights into the underlying structures and relationships among\nLLMs."
                },
                "authors": [
                    {
                        "name": "William F. Bradley"
                    }
                ],
                "author_detail": {
                    "name": "William F. Bradley"
                },
                "author": "William F. Bradley",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01533v2",
                "updated": "2024-11-05T03:17:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    17,
                    34,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-03T11:39:50Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    11,
                    39,
                    50,
                    6,
                    308,
                    0
                ],
                "title": "Enhancing LLM Evaluations: The Garbling Trick",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Evaluations: The Garbling Trick"
                },
                "summary": "As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models based on their performance. We propose a general method to\ntransform existing LLM evaluations into a series of progressively more\ndifficult tasks. These enhanced evaluations emphasize reasoning capabilities\nand can reveal relative performance differences that are not apparent in the\noriginal assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative reasoning\nabilities of these models, particularly highlighting distinctions between\nOpenAI's o1-preview and Google's gemini-pro-1.5-002.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models based on their performance. We propose a general method to\ntransform existing LLM evaluations into a series of progressively more\ndifficult tasks. These enhanced evaluations emphasize reasoning capabilities\nand can reveal relative performance differences that are not apparent in the\noriginal assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative reasoning\nabilities of these models, particularly highlighting distinctions between\nOpenAI's o1-preview and Google's gemini-pro-1.5-002."
                },
                "authors": [
                    {
                        "name": "William F. Bradley"
                    }
                ],
                "author_detail": {
                    "name": "William F. Bradley"
                },
                "author": "William F. Bradley",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10737v2",
                "updated": "2024-11-05T03:00:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    0,
                    38,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-16T21:15:56Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    21,
                    15,
                    56,
                    0,
                    260,
                    0
                ],
                "title": "AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation\n  through Static Analysis and Fuzz Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation\n  through Static Analysis and Fuzz Testing"
                },
                "summary": "Recent advancements in automatic code generation using large language models\n(LLMs) have brought us closer to fully automated secure software development.\nHowever, existing approaches often rely on a single agent for code generation,\nwhich struggles to produce secure, vulnerability-free code. Traditional program\nsynthesis with LLMs has primarily focused on functional correctness, often\nneglecting critical dynamic security implications that happen during runtime.\nTo address these challenges, we propose AutoSafeCoder, a multi-agent framework\nthat leverages LLM-driven agents for code generation, vulnerability analysis,\nand security enhancement through continuous collaboration. The framework\nconsists of three agents: a Coding Agent responsible for code generation, a\nStatic Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent\nperforming dynamic testing using a mutation-based fuzzing approach to detect\nruntime errors. Our contribution focuses on ensuring the safety of multi-agent\ncode generation by integrating dynamic and static testing in an iterative\nprocess during code generation by LLM that improves security. Experiments using\nthe SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities\ncompared to baseline LLMs, with no compromise in functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in automatic code generation using large language models\n(LLMs) have brought us closer to fully automated secure software development.\nHowever, existing approaches often rely on a single agent for code generation,\nwhich struggles to produce secure, vulnerability-free code. Traditional program\nsynthesis with LLMs has primarily focused on functional correctness, often\nneglecting critical dynamic security implications that happen during runtime.\nTo address these challenges, we propose AutoSafeCoder, a multi-agent framework\nthat leverages LLM-driven agents for code generation, vulnerability analysis,\nand security enhancement through continuous collaboration. The framework\nconsists of three agents: a Coding Agent responsible for code generation, a\nStatic Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent\nperforming dynamic testing using a mutation-based fuzzing approach to detect\nruntime errors. Our contribution focuses on ensuring the safety of multi-agent\ncode generation by integrating dynamic and static testing in an iterative\nprocess during code generation by LLM that improves security. Experiments using\nthe SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities\ncompared to baseline LLMs, with no compromise in functionality."
                },
                "authors": [
                    {
                        "name": "Ana Nunez"
                    },
                    {
                        "name": "Nafis Tanveer Islam"
                    },
                    {
                        "name": "Sumit Kumar Jha"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "arxiv_comment": "Accepted to NeurIPS 2024 Workshop on Safe & Trustworthy Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07518v2",
                "updated": "2024-11-05T02:53:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    53,
                    0,
                    1,
                    310,
                    0
                ],
                "published": "2024-05-13T07:32:45Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    7,
                    32,
                    45,
                    0,
                    134,
                    0
                ],
                "title": "SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and\n  Composition of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and\n  Composition of Experts"
                },
                "summary": "Monolithic large language models (LLMs) like GPT-4 have paved the way for\nmodern generative AI applications. Training, serving, and maintaining\nmonolithic LLMs at scale, however, remains prohibitively expensive and\nchallenging. The disproportionate increase in compute-to-memory ratio of modern\nAI accelerators have created a memory wall, necessitating new methods to deploy\nAI. Composition of Experts (CoE) is an alternative modular approach that lowers\nthe cost and complexity of training and serving. However, this approach\npresents two key challenges when using conventional hardware: (1) without fused\noperations, smaller models have lower operational intensity, which makes high\nutilization more challenging to achieve; and (2) hosting a large number of\nmodels can be either prohibitively expensive or slow when dynamically switching\nbetween them.\n  In this paper, we describe how combining CoE, streaming dataflow, and a\nthree-tier memory system scales the AI memory wall. We describe Samba-CoE, a\nCoE system with 150 experts and a trillion total parameters. We deploy\nSamba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) - a\ncommercial dataflow accelerator architecture that has been co-designed for\nenterprise inference and training applications. The chip introduces a new\nthree-tier memory system with on-chip distributed SRAM, on-package HBM, and\noff-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out\nover multiple sockets. We demonstrate speedups ranging from 2$\\times$ to\n13$\\times$ on various benchmarks running on eight RDU sockets compared with an\nunfused baseline. We show that for CoE inference deployments, the 8-socket RDU\nNode reduces machine footprint by up to 19$\\times$, speeds up model switching\ntime by 15$\\times$ to 31$\\times$, and achieves an overall speedup of\n3.7$\\times$ over a DGX H100 and 6.6$\\times$ over a DGX A100.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monolithic large language models (LLMs) like GPT-4 have paved the way for\nmodern generative AI applications. Training, serving, and maintaining\nmonolithic LLMs at scale, however, remains prohibitively expensive and\nchallenging. The disproportionate increase in compute-to-memory ratio of modern\nAI accelerators have created a memory wall, necessitating new methods to deploy\nAI. Composition of Experts (CoE) is an alternative modular approach that lowers\nthe cost and complexity of training and serving. However, this approach\npresents two key challenges when using conventional hardware: (1) without fused\noperations, smaller models have lower operational intensity, which makes high\nutilization more challenging to achieve; and (2) hosting a large number of\nmodels can be either prohibitively expensive or slow when dynamically switching\nbetween them.\n  In this paper, we describe how combining CoE, streaming dataflow, and a\nthree-tier memory system scales the AI memory wall. We describe Samba-CoE, a\nCoE system with 150 experts and a trillion total parameters. We deploy\nSamba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) - a\ncommercial dataflow accelerator architecture that has been co-designed for\nenterprise inference and training applications. The chip introduces a new\nthree-tier memory system with on-chip distributed SRAM, on-package HBM, and\noff-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out\nover multiple sockets. We demonstrate speedups ranging from 2$\\times$ to\n13$\\times$ on various benchmarks running on eight RDU sockets compared with an\nunfused baseline. We show that for CoE inference deployments, the 8-socket RDU\nNode reduces machine footprint by up to 19$\\times$, speeds up model switching\ntime by 15$\\times$ to 31$\\times$, and achieves an overall speedup of\n3.7$\\times$ over a DGX H100 and 6.6$\\times$ over a DGX A100."
                },
                "authors": [
                    {
                        "name": "Raghu Prabhakar"
                    },
                    {
                        "name": "Ram Sivaramakrishnan"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Xiangyu Song"
                    },
                    {
                        "name": "Kejie Zhang"
                    },
                    {
                        "name": "Tianren Gao"
                    },
                    {
                        "name": "Angela Wang"
                    },
                    {
                        "name": "Karen Li"
                    },
                    {
                        "name": "Yongning Sheng"
                    },
                    {
                        "name": "Joshua Brot"
                    },
                    {
                        "name": "Denis Sokolov"
                    },
                    {
                        "name": "Apurv Vivek"
                    },
                    {
                        "name": "Calvin Leung"
                    },
                    {
                        "name": "Arjun Sabnis"
                    },
                    {
                        "name": "Jiayu Bai"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Mark Gottscho"
                    },
                    {
                        "name": "David Jackson"
                    },
                    {
                        "name": "Mark Luttrell"
                    },
                    {
                        "name": "Manish K. Shah"
                    },
                    {
                        "name": "Edison Chen"
                    },
                    {
                        "name": "Kaizhao Liang"
                    },
                    {
                        "name": "Swayambhoo Jain"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Dawei Huang"
                    },
                    {
                        "name": "Sumti Jairath"
                    },
                    {
                        "name": "Kevin J. Brown"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_doi": "10.1109/MICRO61859.2024.00100",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MICRO61859.2024.00100",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "2024 57th IEEE/ACM International Symposium on Microarchitecture\n  (MICRO)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04361v3",
                "updated": "2024-11-05T02:45:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    45,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2023-11-07T21:51:55Z",
                "published_parsed": [
                    2023,
                    11,
                    7,
                    21,
                    51,
                    55,
                    1,
                    311,
                    0
                ],
                "title": "DeFault: Deep-learning-based Fault Delineation Using the IBDP Passive\n  Seismic Data at the Decatur CO2 Storage Site",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFault: Deep-learning-based Fault Delineation Using the IBDP Passive\n  Seismic Data at the Decatur CO2 Storage Site"
                },
                "summary": "The carbon capture, utilization, and storage (CCUS) framework is an essential\ncomponent in reducing greenhouse gas emissions, with its success hinging on the\ncomprehensive knowledge of subsurface geology and geomechanics. Passive seismic\nevent relocation and fault detection serve as indispensable tools, offering\nvital insights into subsurface structures and fluid migration pathways.\nAccurate identification and localization of seismic events, however, face\nsignificant challenges, including the necessity for high-quality seismic data\nand advanced computational methods. To address these challenges, we introduce a\nnovel deep learning method, DeFault, specifically designed for passive seismic\nsource relocation and fault delineating for passive seismic monitoring\nprojects. By leveraging data domain-adaptation, DeFault allows us to train a\nneural network with labeled synthetic data and apply it directly to field data.\nUsing DeFault, the passive seismic sources are automatically clustered based on\ntheir recording time and spatial locations, and subsequently, faults and\nfractures are delineated accordingly. We demonstrate the efficacy of DeFault on\na field case study involving CO2 injection related microseismic data from the\nDecatur, Illinois area. Our approach accurately and efficiently relocated\npassive seismic events, identified faults and aided in the prevention of\npotential geological hazards. Our results highlight the potential of DeFault as\na valuable tool for passive seismic monitoring, emphasizing its role in\nensuring CCUS project safety. This research bolsters the understanding of\nsubsurface characterization in CCUS, illustrating machine learning's capacity\nto refine these methods. Ultimately, our work bear significant implications for\nCCUS technology deployment, an essential strategy in combating climate change.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The carbon capture, utilization, and storage (CCUS) framework is an essential\ncomponent in reducing greenhouse gas emissions, with its success hinging on the\ncomprehensive knowledge of subsurface geology and geomechanics. Passive seismic\nevent relocation and fault detection serve as indispensable tools, offering\nvital insights into subsurface structures and fluid migration pathways.\nAccurate identification and localization of seismic events, however, face\nsignificant challenges, including the necessity for high-quality seismic data\nand advanced computational methods. To address these challenges, we introduce a\nnovel deep learning method, DeFault, specifically designed for passive seismic\nsource relocation and fault delineating for passive seismic monitoring\nprojects. By leveraging data domain-adaptation, DeFault allows us to train a\nneural network with labeled synthetic data and apply it directly to field data.\nUsing DeFault, the passive seismic sources are automatically clustered based on\ntheir recording time and spatial locations, and subsequently, faults and\nfractures are delineated accordingly. We demonstrate the efficacy of DeFault on\na field case study involving CO2 injection related microseismic data from the\nDecatur, Illinois area. Our approach accurately and efficiently relocated\npassive seismic events, identified faults and aided in the prevention of\npotential geological hazards. Our results highlight the potential of DeFault as\na valuable tool for passive seismic monitoring, emphasizing its role in\nensuring CCUS project safety. This research bolsters the understanding of\nsubsurface characterization in CCUS, illustrating machine learning's capacity\nto refine these methods. Ultimately, our work bear significant implications for\nCCUS technology deployment, an essential strategy in combating climate change."
                },
                "authors": [
                    {
                        "name": "Hanchen Wang"
                    },
                    {
                        "name": "Yinpeng Chen"
                    },
                    {
                        "name": "Tariq Alkhalifah"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Youzuo Lin"
                    },
                    {
                        "name": "David Alumbaugh"
                    }
                ],
                "author_detail": {
                    "name": "David Alumbaugh"
                },
                "author": "David Alumbaugh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02855v2",
                "updated": "2024-11-05T02:30:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    30,
                    56,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-03T07:14:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    7,
                    14,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to\n  Defend Against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to\n  Defend Against Jailbreak Attacks"
                },
                "summary": "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) approaches.\nOur extensive experiments demonstrate the surprising generalizability of our\nunlearning-based approach: using only 20 raw harmful questions without any\njailbreak prompt during training, our solution reduced the Attack Success Rate\n(ASR) in Vicuna-7B from 82.6% to 7.7% on out-of-distribution (OOD) harmful\nquestions wrapped with various complex jailbreak prompts . This significantly\noutperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment\nsamples but still has an ASR of 21.9% even under the help of an additional\nsafety system prompt. Further analysis reveals that the generalization ability\nof our solution may stem from the intrinsic relatedness among harmful responses\nacross harmful questions (e.g., response patterns, shared steps and actions in\nresponse, and similarity among their learned representations in the LLM). Our\ncode is available at \\url{https://github.com/thu-coai/SafeUnlearning}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) approaches.\nOur extensive experiments demonstrate the surprising generalizability of our\nunlearning-based approach: using only 20 raw harmful questions without any\njailbreak prompt during training, our solution reduced the Attack Success Rate\n(ASR) in Vicuna-7B from 82.6% to 7.7% on out-of-distribution (OOD) harmful\nquestions wrapped with various complex jailbreak prompts . This significantly\noutperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment\nsamples but still has an ASR of 21.9% even under the help of an additional\nsafety system prompt. Further analysis reveals that the generalization ability\nof our solution may stem from the intrinsic relatedness among harmful responses\nacross harmful questions (e.g., response patterns, shared steps and actions in\nresponse, and similarity among their learned representations in the LLM). Our\ncode is available at \\url{https://github.com/thu-coai/SafeUnlearning}."
                },
                "authors": [
                    {
                        "name": "Zhexin Zhang"
                    },
                    {
                        "name": "Junxiao Yang"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03625v2",
                "updated": "2024-11-05T02:20:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    20,
                    54,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-04T04:24:43Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    24,
                    43,
                    3,
                    186,
                    0
                ],
                "title": "Fix the Tests: Augmenting LLMs to Repair Test Cases with Static\n  Collector and Neural Reranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fix the Tests: Augmenting LLMs to Repair Test Cases with Static\n  Collector and Neural Reranker"
                },
                "summary": "During software evolution, it is advocated that test code should co-evolve\nwith production code. In real development scenarios, test updating may lag\nbehind production code changing, which may cause compilation failure or bring\nother troubles. Existing techniques based on pre-trained language models can be\ndirectly adopted to repair obsolete tests caused by such unsynchronized code\nchanges, especially syntactic-related ones. However, the lack of task-oriented\ncontextual information affects the repair accuracy on large-scale projects.\nStarting from an obsolete test, the key challenging task is precisely\nidentifying and constructing Test-Repair-Oriented Contexts (TROCtxs) from the\nwhole repository within a limited token size. In this paper, we propose SYNTER\n(SYNtactic-breaking-changes-induced TEst Repair), a novel approach based on\nLLMs to automatically repair obsolete test cases via precise and concise\nTROCtxs construction. Inspired by developers' programming practices, we design\nthree types of TROCtx: class context, usage context, and environment context.\nGiven an obsolete test case to repair, SYNTER firstly collects the related code\ninformation for each type of TROCtx through static analysis techniques\nautomatically. Then, it generates reranking queries to identify the most\nrelevant TROCtxs, which will be taken as the repair-required key contexts and\nbe input to the large language model for the final test repair. To evaluate the\neffectiveness of SYNTER, we construct a benchmark dataset that contains a set\nof obsolete tests caused by syntactic breaking changes. The experimental\nresults show that SYNTER outperforms baseline approaches both on textual- and\nintent-matching metrics. With the augmentation of constructed TROCtxs,\nhallucinations are reduced by 57.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During software evolution, it is advocated that test code should co-evolve\nwith production code. In real development scenarios, test updating may lag\nbehind production code changing, which may cause compilation failure or bring\nother troubles. Existing techniques based on pre-trained language models can be\ndirectly adopted to repair obsolete tests caused by such unsynchronized code\nchanges, especially syntactic-related ones. However, the lack of task-oriented\ncontextual information affects the repair accuracy on large-scale projects.\nStarting from an obsolete test, the key challenging task is precisely\nidentifying and constructing Test-Repair-Oriented Contexts (TROCtxs) from the\nwhole repository within a limited token size. In this paper, we propose SYNTER\n(SYNtactic-breaking-changes-induced TEst Repair), a novel approach based on\nLLMs to automatically repair obsolete test cases via precise and concise\nTROCtxs construction. Inspired by developers' programming practices, we design\nthree types of TROCtx: class context, usage context, and environment context.\nGiven an obsolete test case to repair, SYNTER firstly collects the related code\ninformation for each type of TROCtx through static analysis techniques\nautomatically. Then, it generates reranking queries to identify the most\nrelevant TROCtxs, which will be taken as the repair-required key contexts and\nbe input to the large language model for the final test repair. To evaluate the\neffectiveness of SYNTER, we construct a benchmark dataset that contains a set\nof obsolete tests caused by syntactic breaking changes. The experimental\nresults show that SYNTER outperforms baseline approaches both on textual- and\nintent-matching metrics. With the augmentation of constructed TROCtxs,\nhallucinations are reduced by 57.1%."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "to be published in ISSRE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09017v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09017v3",
                "updated": "2024-11-05T02:19:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    19,
                    26,
                    1,
                    310,
                    0
                ],
                "published": "2024-03-14T00:45:24Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    0,
                    45,
                    24,
                    3,
                    74,
                    0
                ],
                "title": "AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic"
                },
                "summary": "The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic-related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks, which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n522 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. We evaluated a\nset of LLMs against our benchmark to assess their trustworthiness. GPT-4 was\nthe most trustworthy LLM, while open-source models, particularly AceGPT 7B and\nJais 13B, struggled to achieve a score of 60% in our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic-related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks, which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n522 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. We evaluated a\nset of LLMs against our benchmark to assess their trustworthiness. GPT-4 was\nthe most trustworthy LLM, while open-source models, particularly AceGPT 7B and\nJais 13B, struggled to achieve a score of 60% in our benchmark."
                },
                "authors": [
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Reem I. Masoud"
                    },
                    {
                        "name": "Deema Alnuhait"
                    },
                    {
                        "name": "Afnan Y. Alomairi"
                    },
                    {
                        "name": "Ahmed Ashraf"
                    },
                    {
                        "name": "Mohamed Zaytoon"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Zaytoon"
                },
                "author": "Mohamed Zaytoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09017v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07918v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07918v5",
                "updated": "2024-11-05T02:17:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    17,
                    22,
                    1,
                    310,
                    0
                ],
                "published": "2023-09-14T17:59:49Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    17,
                    59,
                    49,
                    3,
                    257,
                    0
                ],
                "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts"
                },
                "summary": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI ."
                },
                "authors": [
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "A unified Human-Scene Interaction framework that supports versatile\n  interactions through language commands.Project URL:\n  https://xizaoqu.github.io/unihsi/ . Code:\n  https://github.com/OpenRobotLab/UniHSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07918v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07918v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16444v2",
                "updated": "2024-11-05T02:13:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    2,
                    13,
                    59,
                    1,
                    310,
                    0
                ],
                "published": "2024-02-26T09:43:02Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    9,
                    43,
                    2,
                    0,
                    57,
                    0
                ],
                "title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable\n  Safety Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable\n  Safety Detectors"
                },
                "summary": "The safety of Large Language Models (LLMs) has gained increasing attention in\nrecent years, but there still lacks a comprehensive approach for detecting\nsafety issues within LLMs' responses in an aligned, customizable and\nexplainable manner. In this paper, we propose ShieldLM, an LLM-based safety\ndetector, which aligns with common safety standards, supports customizable\ndetection rules, and provides explanations for its decisions. To train\nShieldLM, we compile a large bilingual dataset comprising 14,387 query-response\npairs, annotating the safety of responses based on various safety standards.\nThrough extensive experiments, we demonstrate that ShieldLM surpasses strong\nbaselines across four test sets, showcasing remarkable customizability and\nexplainability. Besides performing well on standard detection datasets,\nShieldLM has also been shown to be effective as a safety evaluator for advanced\nLLMs. ShieldLM is released at \\url{https://github.com/thu-coai/ShieldLM} to\nsupport accurate and explainable safety detection under various safety\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety of Large Language Models (LLMs) has gained increasing attention in\nrecent years, but there still lacks a comprehensive approach for detecting\nsafety issues within LLMs' responses in an aligned, customizable and\nexplainable manner. In this paper, we propose ShieldLM, an LLM-based safety\ndetector, which aligns with common safety standards, supports customizable\ndetection rules, and provides explanations for its decisions. To train\nShieldLM, we compile a large bilingual dataset comprising 14,387 query-response\npairs, annotating the safety of responses based on various safety standards.\nThrough extensive experiments, we demonstrate that ShieldLM surpasses strong\nbaselines across four test sets, showcasing remarkable customizability and\nexplainability. Besides performing well on standard detection datasets,\nShieldLM has also been shown to be effective as a safety evaluator for advanced\nLLMs. ShieldLM is released at \\url{https://github.com/thu-coai/ShieldLM} to\nsupport accurate and explainable safety detection under various safety\nstandards."
                },
                "authors": [
                    {
                        "name": "Zhexin Zhang"
                    },
                    {
                        "name": "Yida Lu"
                    },
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "19 pages. Camera ready version of EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02730v1",
                "updated": "2024-11-05T01:58:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    58,
                    31,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T01:58:31Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    58,
                    31,
                    1,
                    310,
                    0
                ],
                "title": "A Natural Language Processing Approach to Support Biomedical Data\n  Harmonization: Leveraging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Natural Language Processing Approach to Support Biomedical Data\n  Harmonization: Leveraging Large Language Models"
                },
                "summary": "Biomedical research requires large, diverse samples to produce unbiased\nresults. Automated methods for matching variables across datasets can\naccelerate this process. Research in this area has been limited, primarily\nfocusing on lexical matching and ontology based semantic matching. We aimed to\ndevelop new methods, leveraging large language models (LLM) and ensemble\nlearning, to automate variable matching. Methods: We utilized data from two\nGERAS cohort (European and Japan) studies to develop variable matching methods.\nWe first manually created a dataset by matching 352 EU variables with 1322\ncandidate JP variables, where matched variable pairs were positive and\nunmatched pairs were negative instances. Using this dataset, we developed and\nevaluated two types of natural language processing (NLP) methods, which matched\nvariables based on variable labels and definitions from data dictionaries: (1)\nLLM-based and (2) fuzzy matching. We then developed an ensemble-learning\nmethod, using the Random Forest model, to integrate individual NLP methods. RF\nwas trained and evaluated on 50 trials. Each trial had a random split (4:1) of\ntraining and test sets, with the model's hyperparameters optimized through\ncross-validation on the training set. For each EU variable, 1322 candidate JP\nvariables were ranked based on NLP-derived similarity scores or RF's\nprobability scores, denoting their likelihood to match the EU variable. Ranking\nperformance was measured by top-n hit ratio (HRn) and mean reciprocal rank\n(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30\nand 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less\nthan 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived\nfeatures contributed most to RF's performance. One major cause of errors in\nautomatic variable matching was ambiguous variable definitions within data\ndictionaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical research requires large, diverse samples to produce unbiased\nresults. Automated methods for matching variables across datasets can\naccelerate this process. Research in this area has been limited, primarily\nfocusing on lexical matching and ontology based semantic matching. We aimed to\ndevelop new methods, leveraging large language models (LLM) and ensemble\nlearning, to automate variable matching. Methods: We utilized data from two\nGERAS cohort (European and Japan) studies to develop variable matching methods.\nWe first manually created a dataset by matching 352 EU variables with 1322\ncandidate JP variables, where matched variable pairs were positive and\nunmatched pairs were negative instances. Using this dataset, we developed and\nevaluated two types of natural language processing (NLP) methods, which matched\nvariables based on variable labels and definitions from data dictionaries: (1)\nLLM-based and (2) fuzzy matching. We then developed an ensemble-learning\nmethod, using the Random Forest model, to integrate individual NLP methods. RF\nwas trained and evaluated on 50 trials. Each trial had a random split (4:1) of\ntraining and test sets, with the model's hyperparameters optimized through\ncross-validation on the training set. For each EU variable, 1322 candidate JP\nvariables were ranked based on NLP-derived similarity scores or RF's\nprobability scores, denoting their likelihood to match the EU variable. Ranking\nperformance was measured by top-n hit ratio (HRn) and mean reciprocal rank\n(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30\nand 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less\nthan 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived\nfeatures contributed most to RF's performance. One major cause of errors in\nautomatic variable matching was ambiguous variable definitions within data\ndictionaries."
                },
                "authors": [
                    {
                        "name": "Zexu Li"
                    },
                    {
                        "name": "Suraj P. Prabhu"
                    },
                    {
                        "name": "Zachary T. Popp"
                    },
                    {
                        "name": "Shubhi S. Jain"
                    },
                    {
                        "name": "Vijetha Balakundi"
                    },
                    {
                        "name": "Ting Fang Alvin Ang"
                    },
                    {
                        "name": "Rhoda Au"
                    },
                    {
                        "name": "Jinying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jinying Chen"
                },
                "author": "Jinying Chen",
                "arxiv_comment": "32 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02725v1",
                "updated": "2024-11-05T01:49:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    49,
                    49,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T01:49:49Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    49,
                    49,
                    1,
                    310,
                    0
                ],
                "title": "Leveraging LLM Tutoring Systems for Non-Native English Speakers in\n  Introductory CS Courses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Tutoring Systems for Non-Native English Speakers in\n  Introductory CS Courses"
                },
                "summary": "Computer science has historically presented barriers for non-native English\nspeaking (NNES) students, often due to language and terminology challenges.\nWith the rise of large language models (LLMs), there is potential to leverage\nthis technology to support NNES students more effectively. Recent\nimplementations of LLMs as tutors in classrooms have shown promising results.\nIn this study, we deployed an LLM tutor in an accelerated introductory\ncomputing course to evaluate its effectiveness specifically for NNES students.\nKey insights for LLM tutor use are as follows: NNES students signed up for the\nLLM tutor at a similar rate to native English speakers (NES); NNES students\nused the system at a lower rate than NES students -- to a small effect; NNES\nstudents asked significantly more questions in languages other than English\ncompared to NES students, with many of the questions being multilingual by\nincorporating English programming keywords. Results for views of the LLM tutor\nare as follows: both NNES and NES students appreciated the LLM tutor for its\naccessibility, conversational style, and the guardrails put in place to guide\nusers to answers rather than directly providing solutions; NNES students\nhighlighted its approachability as they did not need to communicate in perfect\nEnglish; NNES students rated help-seeking preferences of online resources\nhigher than NES students; Many NNES students were unfamiliar with computing\nterminology in their native languages. These results suggest that LLM tutors\ncan be a valuable resource for NNES students in computing, providing tailored\nsupport that enhances their learning experience and overcomes language\nbarriers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer science has historically presented barriers for non-native English\nspeaking (NNES) students, often due to language and terminology challenges.\nWith the rise of large language models (LLMs), there is potential to leverage\nthis technology to support NNES students more effectively. Recent\nimplementations of LLMs as tutors in classrooms have shown promising results.\nIn this study, we deployed an LLM tutor in an accelerated introductory\ncomputing course to evaluate its effectiveness specifically for NNES students.\nKey insights for LLM tutor use are as follows: NNES students signed up for the\nLLM tutor at a similar rate to native English speakers (NES); NNES students\nused the system at a lower rate than NES students -- to a small effect; NNES\nstudents asked significantly more questions in languages other than English\ncompared to NES students, with many of the questions being multilingual by\nincorporating English programming keywords. Results for views of the LLM tutor\nare as follows: both NNES and NES students appreciated the LLM tutor for its\naccessibility, conversational style, and the guardrails put in place to guide\nusers to answers rather than directly providing solutions; NNES students\nhighlighted its approachability as they did not need to communicate in perfect\nEnglish; NNES students rated help-seeking preferences of online resources\nhigher than NES students; Many NNES students were unfamiliar with computing\nterminology in their native languages. These results suggest that LLM tutors\ncan be a valuable resource for NNES students in computing, providing tailored\nsupport that enhances their learning experience and overcomes language\nbarriers."
                },
                "authors": [
                    {
                        "name": "Ismael Villegas Molina"
                    },
                    {
                        "name": "Audria Montalvo"
                    },
                    {
                        "name": "Benjamin Ochoa"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Leo Porter"
                    }
                ],
                "author_detail": {
                    "name": "Leo Porter"
                },
                "author": "Leo Porter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02900v2",
                "updated": "2024-11-05T01:44:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    44,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-05T03:41:37Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    3,
                    41,
                    37,
                    2,
                    157,
                    0
                ],
                "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Reward Model Overoptimization in Direct Alignment\n  Algorithms"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has been crucial to the\nrecent success of Large Language Models (LLMs), however, it is often a complex\nand brittle process. In the classical RLHF framework, a reward model is first\ntrained to represent human preferences, which is in turn used by an online\nreinforcement learning (RL) algorithm to optimize the LLM. A prominent issue\nwith such methods is reward over-optimization or reward hacking, where\nperformance as measured by the learned proxy reward model increases, but true\nquality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like\nDirect Preference Optimization have emerged as alternatives to the classical\nRLHF pipeline by circumventing the reward modeling phase. However, although\nDAAs do not use a separate proxy reward model, they still commonly deteriorate\nfrom over-optimization. While the so-called reward hacking phenomenon is not\nwell-defined for DAAs, we still uncover similar trends: at higher KL budgets,\nDAA algorithms exhibit similar degradation patterns to their classic RLHF\ncounterparts. In particular, we find that DAA methods deteriorate not only\nacross a wide range of KL budgets but also often before even a single epoch of\nthe dataset is completed. Through extensive empirical experimentation, this\nwork formulates and formalizes the reward over-optimization or hacking problem\nfor DAAs and explores its consequences across objectives, training regimes, and\nmodel scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has been crucial to the\nrecent success of Large Language Models (LLMs), however, it is often a complex\nand brittle process. In the classical RLHF framework, a reward model is first\ntrained to represent human preferences, which is in turn used by an online\nreinforcement learning (RL) algorithm to optimize the LLM. A prominent issue\nwith such methods is reward over-optimization or reward hacking, where\nperformance as measured by the learned proxy reward model increases, but true\nquality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like\nDirect Preference Optimization have emerged as alternatives to the classical\nRLHF pipeline by circumventing the reward modeling phase. However, although\nDAAs do not use a separate proxy reward model, they still commonly deteriorate\nfrom over-optimization. While the so-called reward hacking phenomenon is not\nwell-defined for DAAs, we still uncover similar trends: at higher KL budgets,\nDAA algorithms exhibit similar degradation patterns to their classic RLHF\ncounterparts. In particular, we find that DAA methods deteriorate not only\nacross a wide range of KL budgets but also often before even a single epoch of\nthe dataset is completed. Through extensive empirical experimentation, this\nwork formulates and formalizes the reward over-optimization or hacking problem\nfor DAAs and explores its consequences across objectives, training regimes, and\nmodel scales."
                },
                "authors": [
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Yaswanth Chittepu"
                    },
                    {
                        "name": "Ryan Park"
                    },
                    {
                        "name": "Harshit Sikchi"
                    },
                    {
                        "name": "Joey Hejna"
                    },
                    {
                        "name": "Bradley Knox"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Scott Niekum"
                    }
                ],
                "author_detail": {
                    "name": "Scott Niekum"
                },
                "author": "Scott Niekum",
                "arxiv_comment": "30 pages, 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02718v1",
                "updated": "2024-11-05T01:35:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    35,
                    16,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T01:35:16Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    35,
                    16,
                    1,
                    310,
                    0
                ],
                "title": "LLM-based Framework for Bearing Fault Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Framework for Bearing Fault Diagnosis"
                },
                "summary": "Accurately diagnosing bearing faults is crucial for maintaining the efficient\noperation of rotating machinery. However, traditional diagnosis methods face\nchallenges due to the diversification of application environments, including\ncross-condition adaptability, small-sample learning difficulties, and\ncross-dataset generalization. These challenges have hindered the effectiveness\nand limited the application of existing approaches. Large language models\n(LLMs) offer new possibilities for improving the generalization of diagnosis\nmodels. However, the integration of LLMs with traditional diagnosis techniques\nfor optimal generalization remains underexplored. This paper proposed an\nLLM-based bearing fault diagnosis framework to tackle these challenges. First,\na signal feature quantification method was put forward to address the issue of\nextracting semantic information from vibration data, which integrated time and\nfrequency domain feature extraction based on a statistical analysis framework.\nThis method textualized time-series data, aiming to efficiently learn\ncross-condition and small-sample common features through concise feature\nselection. Fine-tuning methods based on LoRA and QLoRA were employed to enhance\nthe generalization capability of LLMs in analyzing vibration data features. In\naddition, the two innovations (textualizing vibration features and fine-tuning\npre-trained models) were validated by single-dataset cross-condition and\ncross-dataset transfer experiment with complete and limited data. The results\ndemonstrated the ability of the proposed framework to perform three types of\ngeneralization tasks simultaneously. Trained cross-dataset models got\napproximately a 10% improvement in accuracy, proving the adaptability of LLMs\nto input patterns. Ultimately, the results effectively enhance the\ngeneralization capability and fill the research gap in using LLMs for bearing\nfault diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately diagnosing bearing faults is crucial for maintaining the efficient\noperation of rotating machinery. However, traditional diagnosis methods face\nchallenges due to the diversification of application environments, including\ncross-condition adaptability, small-sample learning difficulties, and\ncross-dataset generalization. These challenges have hindered the effectiveness\nand limited the application of existing approaches. Large language models\n(LLMs) offer new possibilities for improving the generalization of diagnosis\nmodels. However, the integration of LLMs with traditional diagnosis techniques\nfor optimal generalization remains underexplored. This paper proposed an\nLLM-based bearing fault diagnosis framework to tackle these challenges. First,\na signal feature quantification method was put forward to address the issue of\nextracting semantic information from vibration data, which integrated time and\nfrequency domain feature extraction based on a statistical analysis framework.\nThis method textualized time-series data, aiming to efficiently learn\ncross-condition and small-sample common features through concise feature\nselection. Fine-tuning methods based on LoRA and QLoRA were employed to enhance\nthe generalization capability of LLMs in analyzing vibration data features. In\naddition, the two innovations (textualizing vibration features and fine-tuning\npre-trained models) were validated by single-dataset cross-condition and\ncross-dataset transfer experiment with complete and limited data. The results\ndemonstrated the ability of the proposed framework to perform three types of\ngeneralization tasks simultaneously. Trained cross-dataset models got\napproximately a 10% improvement in accuracy, proving the adaptability of LLMs\nto input patterns. Ultimately, the results effectively enhance the\ngeneralization capability and fill the research gap in using LLMs for bearing\nfault diagnosis."
                },
                "authors": [
                    {
                        "name": "Laifa Tao"
                    },
                    {
                        "name": "Haifei Liu"
                    },
                    {
                        "name": "Guoao Ning"
                    },
                    {
                        "name": "Wenyan Cao"
                    },
                    {
                        "name": "Bohao Huang"
                    },
                    {
                        "name": "Chen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lu"
                },
                "author": "Chen Lu",
                "arxiv_comment": "25 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11527v2",
                "updated": "2024-11-05T01:34:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    34,
                    26,
                    1,
                    310,
                    0
                ],
                "published": "2024-09-17T19:54:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    54,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent"
                },
                "summary": "Multi-agent strategies have emerged as a promising approach to enhance the\nreasoning abilities of Large Language Models (LLMs) by assigning specialized\nroles in the problem-solving process. Concurrently, Tree of Thoughts (ToT)\nmethods have shown potential in improving reasoning for complex\nquestion-answering tasks by exploring diverse reasoning paths. A critical\nlimitation in multi-agent reasoning is the 'Reasoner' agent's shallow\nexploration of reasoning paths. While ToT strategies could help mitigate this\nproblem, they may generate flawed reasoning branches, which could harm the\ntrustworthiness of the final answer. To leverage the strengths of both\nmulti-agent reasoning and ToT strategies, we introduce a novel approach\ncombining ToT-based Reasoner agents with a Thought Validator agent. Multiple\nReasoner agents operate in parallel, employing ToT to explore diverse reasoning\npaths. The Thought Validator then scrutinizes these paths, considering a\nReasoner's conclusion only if its reasoning is valid. This method enables a\nmore robust voting strategy by discarding faulty reasoning paths, enhancing the\nsystem's ability to tackle tasks requiring systematic and trustworthy\nreasoning. Our method demonstrates superior performance compared to existing\ntechniques when evaluated on the GSM8K dataset, outperforming the standard ToT\nstrategy by an average 5.6% across four LLMs. The code and related content can\nbe found in: https://github.com/SecureAIAutonomyLab/MA-ToT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent strategies have emerged as a promising approach to enhance the\nreasoning abilities of Large Language Models (LLMs) by assigning specialized\nroles in the problem-solving process. Concurrently, Tree of Thoughts (ToT)\nmethods have shown potential in improving reasoning for complex\nquestion-answering tasks by exploring diverse reasoning paths. A critical\nlimitation in multi-agent reasoning is the 'Reasoner' agent's shallow\nexploration of reasoning paths. While ToT strategies could help mitigate this\nproblem, they may generate flawed reasoning branches, which could harm the\ntrustworthiness of the final answer. To leverage the strengths of both\nmulti-agent reasoning and ToT strategies, we introduce a novel approach\ncombining ToT-based Reasoner agents with a Thought Validator agent. Multiple\nReasoner agents operate in parallel, employing ToT to explore diverse reasoning\npaths. The Thought Validator then scrutinizes these paths, considering a\nReasoner's conclusion only if its reasoning is valid. This method enables a\nmore robust voting strategy by discarding faulty reasoning paths, enhancing the\nsystem's ability to tackle tasks requiring systematic and trustworthy\nreasoning. Our method demonstrates superior performance compared to existing\ntechniques when evaluated on the GSM8K dataset, outperforming the standard ToT\nstrategy by an average 5.6% across four LLMs. The code and related content can\nbe found in: https://github.com/SecureAIAutonomyLab/MA-ToT"
                },
                "authors": [
                    {
                        "name": "Fatemeh Haji"
                    },
                    {
                        "name": "Mazal Bethany"
                    },
                    {
                        "name": "Maryam Tabar"
                    },
                    {
                        "name": "Jason Chiang"
                    },
                    {
                        "name": "Anthony Rios"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02714v1",
                "updated": "2024-11-05T01:26:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    26,
                    35,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T01:26:35Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    26,
                    35,
                    1,
                    310,
                    0
                ],
                "title": "Game Plot Design with an LLM-powered Assistant: An Empirical Study with\n  Game Designers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game Plot Design with an LLM-powered Assistant: An Empirical Study with\n  Game Designers"
                },
                "summary": "We introduce GamePlot, an LLM-powered assistant that supports game designers\nin crafting immersive narratives for turn-based games, and allows them to test\nthese games through a collaborative game play and refine the plot throughout\nthe process. Our user study with 14 game designers shows high levels of both\nsatisfaction with the generated game plots and sense of ownership over the\nnarratives, but also reconfirms that LLM are limited in their ability to\ngenerate complex and truly innovative content. We also show that diverse user\npopulations have different expectations from AI assistants, and encourage\nresearchers to study how tailoring assistants to diverse user groups could\npotentially lead to increased job satisfaction and greater creativity and\ninnovation over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GamePlot, an LLM-powered assistant that supports game designers\nin crafting immersive narratives for turn-based games, and allows them to test\nthese games through a collaborative game play and refine the plot throughout\nthe process. Our user study with 14 game designers shows high levels of both\nsatisfaction with the generated game plots and sense of ownership over the\nnarratives, but also reconfirms that LLM are limited in their ability to\ngenerate complex and truly innovative content. We also show that diverse user\npopulations have different expectations from AI assistants, and encourage\nresearchers to study how tailoring assistants to diverse user groups could\npotentially lead to increased job satisfaction and greater creativity and\ninnovation over time."
                },
                "authors": [
                    {
                        "name": "Seyed Hossein Alavi"
                    },
                    {
                        "name": "Weijia Xu"
                    },
                    {
                        "name": "Nebojsa Jojic"
                    },
                    {
                        "name": "Daniel Kennett"
                    },
                    {
                        "name": "Raymond T. Ng"
                    },
                    {
                        "name": "Sudha Rao"
                    },
                    {
                        "name": "Haiyan Zhang"
                    },
                    {
                        "name": "Bill Dolan"
                    },
                    {
                        "name": "Vered Shwartz"
                    }
                ],
                "author_detail": {
                    "name": "Vered Shwartz"
                },
                "author": "Vered Shwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02712v1",
                "updated": "2024-11-05T01:24:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    24,
                    37,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T01:24:37Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    24,
                    37,
                    1,
                    310,
                    0
                ],
                "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via\n  Vision-Guided Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-DPO: Mitigating Hallucination in Large Vision Language Models via\n  Vision-Guided Direct Preference Optimization"
                },
                "summary": "Large vision-language models (LVLMs) suffer from hallucination, resulting in\nmisalignment between the output textual response and the input visual content.\nRecent research indicates that the over-reliance on the Large Language Model\n(LLM) backbone, as one cause of the LVLM hallucination, inherently introduces\nbias from language priors, leading to insufficient context attention to the\nvisual inputs.\n  We tackle this issue of hallucination by mitigating such over-reliance\nthrough preference learning. We propose Vision-guided Direct Preference\nOptimization (V-DPO) to enhance visual context learning at training time. To\ninterpret the effectiveness and generalizability of V-DPO on different types of\ntraining data, we construct a synthetic dataset containing both response- and\nimage-contrast preference pairs, compared against existing human-annotated\nhallucination samples. Our approach achieves significant improvements compared\nwith baseline methods across various hallucination benchmarks. Our analysis\nindicates that V-DPO excels in learning from image-contrast preference data,\ndemonstrating its superior ability to elicit and understand nuances of visual\ncontext. Our code is publicly available at https://github.com/YuxiXie/V-DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) suffer from hallucination, resulting in\nmisalignment between the output textual response and the input visual content.\nRecent research indicates that the over-reliance on the Large Language Model\n(LLM) backbone, as one cause of the LVLM hallucination, inherently introduces\nbias from language priors, leading to insufficient context attention to the\nvisual inputs.\n  We tackle this issue of hallucination by mitigating such over-reliance\nthrough preference learning. We propose Vision-guided Direct Preference\nOptimization (V-DPO) to enhance visual context learning at training time. To\ninterpret the effectiveness and generalizability of V-DPO on different types of\ntraining data, we construct a synthetic dataset containing both response- and\nimage-contrast preference pairs, compared against existing human-annotated\nhallucination samples. Our approach achieves significant improvements compared\nwith baseline methods across various hallucination benchmarks. Our analysis\nindicates that V-DPO excels in learning from image-contrast preference data,\ndemonstrating its superior ability to elicit and understand nuances of visual\ncontext. Our code is publicly available at https://github.com/YuxiXie/V-DPO."
                },
                "authors": [
                    {
                        "name": "Yuxi Xie"
                    },
                    {
                        "name": "Guanzhen Li"
                    },
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "EMNLP 2024 Findings; 9 pages, 6 figures, 5 tables (16 pages, 8\n  figures, 8 tables including references and appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24190v2",
                "updated": "2024-11-05T01:14:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    1,
                    14,
                    48,
                    1,
                    310,
                    0
                ],
                "published": "2024-10-31T17:51:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"
                },
                "summary": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions."
                },
                "authors": [
                    {
                        "name": "Yujin Potter"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02688v1",
                "updated": "2024-11-05T00:16:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    0,
                    16,
                    1,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T00:16:01Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    0,
                    16,
                    1,
                    1,
                    310,
                    0
                ],
                "title": "On the loss of context-awareness in general instruction fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the loss of context-awareness in general instruction fine-tuning"
                },
                "summary": "Pretrained Large Language Models (LLMs) require post-training methods such as\nsupervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pretraining. In this paper, we investigate the loss\nof context awareness after SFT, defined as the capability to extract and\nunderstand information from the user-provided context and respond accordingly.\nWe are the first to identify and show that the loss of context-awareness\nappears on instruction-finetuned LLMs when the chat template is applied to the\ninput prompts. We identify the performance decline is partially caused by the\nbias embedded into the chat template to focus less on the user-provided\ncontext. Based on these observations, we propose two methods to mitigate the\nloss of context awareness in instruct models: post-hoc attention steering on\nuser prompts and conditional instruction fine-tuning with a context-dependency\nindicator. Empirical experiments on 4 context-dependent downstream tasks and 3\npretrained LLMs of different sizes show that our methods effectively mitigates\nthe loss of context awareness without compromising the general ability to\nfollow instructions. Our findings also strongly advocate the necessity to\ncarefully benchmark context awareness after instruction fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained Large Language Models (LLMs) require post-training methods such as\nsupervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pretraining. In this paper, we investigate the loss\nof context awareness after SFT, defined as the capability to extract and\nunderstand information from the user-provided context and respond accordingly.\nWe are the first to identify and show that the loss of context-awareness\nappears on instruction-finetuned LLMs when the chat template is applied to the\ninput prompts. We identify the performance decline is partially caused by the\nbias embedded into the chat template to focus less on the user-provided\ncontext. Based on these observations, we propose two methods to mitigate the\nloss of context awareness in instruct models: post-hoc attention steering on\nuser prompts and conditional instruction fine-tuning with a context-dependency\nindicator. Empirical experiments on 4 context-dependent downstream tasks and 3\npretrained LLMs of different sizes show that our methods effectively mitigates\nthe loss of context awareness without compromising the general ability to\nfollow instructions. Our findings also strongly advocate the necessity to\ncarefully benchmark context awareness after instruction fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]