[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian HÃ¼ger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "SÃ¸ren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "SÃ¸ren Stobbe"
                },
                "author": "SÃ¸ren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v1",
                "updated": "2025-06-25T07:26:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin HoÃfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "UroÅ¡ Seljak"
                    }
                ],
                "author_detail": {
                    "name": "UroÅ¡ Seljak"
                },
                "author": "UroÅ¡ Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. ThÃ©venet"
                    }
                ],
                "author_detail": {
                    "name": "M. ThÃ©venet"
                },
                "author": "M. ThÃ©venet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "ÃÃ±igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21593v1",
                "updated": "2025-06-18T07:54:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T07:54:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications"
                },
                "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems."
                },
                "authors": [
                    {
                        "name": "Abu Hanif Muhammad Syarubany"
                    },
                    {
                        "name": "Chang Dong Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang Dong Yoo"
                },
                "author": "Chang Dong Yoo",
                "arxiv_comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v1",
                "updated": "2025-06-18T05:07:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "RÃºben AdÃ£o"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "JoÃ£o Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.22432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22432v1",
                "updated": "2025-06-27T17:59:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    59,
                    1,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:59:01Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    59,
                    1,
                    4,
                    178,
                    0
                ],
                "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy"
                },
                "summary": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/"
                },
                "authors": [
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Rynson W. H. Lau"
                },
                "author": "Rynson W. H. Lau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22419v1",
                "updated": "2025-06-27T17:44:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    44,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:44:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    44,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements"
                },
                "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent."
                },
                "authors": [
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Despoina Magka"
                    },
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Tatiana Shavrina"
                    },
                    {
                        "name": "Jean-Christophe Gagnon-Audet"
                    },
                    {
                        "name": "Kelvin Niu"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Michael Shvartsman"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Edan Toledo"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Lucia Cipolina-Kun"
                    },
                    {
                        "name": "Abhishek Charnalia"
                    },
                    {
                        "name": "Derek Dunfield"
                    },
                    {
                        "name": "Alexander H. Miller"
                    },
                    {
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02003v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02003v6",
                "updated": "2025-06-27T17:28:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    28,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2023-10-02T16:55:19Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    16,
                    55,
                    19,
                    0,
                    275,
                    0
                ],
                "title": "L2MAC: Large Language Model Automatic Computer for Extensive Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L2MAC: Large Language Model Automatic Computer for Extensive Code\n  Generation"
                },
                "summary": "Transformer-based large language models (LLMs) are constrained by the fixed\ncontext window of the underlying transformer architecture, hindering their\nability to produce long and coherent outputs. Memory-augmented LLMs are a\npromising solution, but current approaches cannot handle long output generation\ntasks since they (1) only focus on reading memory and reduce its evolution to\nthe concatenation of new memories or (2) use very specialized memories that\ncannot adapt to other domains. This paper presents L2MAC, the first practical\nLLM-based general-purpose stored-program automatic computer (von Neumann\narchitecture) framework, an LLM-based multi-agent system, for long and\nconsistent output generation. Its memory has two components: the instruction\nregistry, which is populated with a prompt program to solve the user-given\ntask, and a file store, which will contain the final and intermediate outputs.\nEach instruction in turn is executed by a separate LLM agent, whose context is\nmanaged by a control unit capable of precise memory reading and writing to\nensure effective interaction with the file store. These components enable L2MAC\nto generate extensive outputs, bypassing the constraints of the finite context\nwindow while producing outputs that fulfill a complex user-specified task. We\nempirically demonstrate that L2MAC achieves state-of-the-art performance in\ngenerating large codebases for system design tasks, significantly outperforming\nother coding methods in implementing the detailed user-specified task; we show\nthat L2MAC works for general-purpose extensive text-based tasks, such as\nwriting an entire book; and we provide valuable insights into L2MAC's\nperformance improvement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) are constrained by the fixed\ncontext window of the underlying transformer architecture, hindering their\nability to produce long and coherent outputs. Memory-augmented LLMs are a\npromising solution, but current approaches cannot handle long output generation\ntasks since they (1) only focus on reading memory and reduce its evolution to\nthe concatenation of new memories or (2) use very specialized memories that\ncannot adapt to other domains. This paper presents L2MAC, the first practical\nLLM-based general-purpose stored-program automatic computer (von Neumann\narchitecture) framework, an LLM-based multi-agent system, for long and\nconsistent output generation. Its memory has two components: the instruction\nregistry, which is populated with a prompt program to solve the user-given\ntask, and a file store, which will contain the final and intermediate outputs.\nEach instruction in turn is executed by a separate LLM agent, whose context is\nmanaged by a control unit capable of precise memory reading and writing to\nensure effective interaction with the file store. These components enable L2MAC\nto generate extensive outputs, bypassing the constraints of the finite context\nwindow while producing outputs that fulfill a complex user-specified task. We\nempirically demonstrate that L2MAC achieves state-of-the-art performance in\ngenerating large codebases for system design tasks, significantly outperforming\nother coding methods in implementing the detailed user-specified task; we show\nthat L2MAC works for general-purpose extensive text-based tasks, such as\nwriting an entire book; and we provide valuable insights into L2MAC's\nperformance improvement over existing methods."
                },
                "authors": [
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Max Ruiz Luyten"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Published in The Twelfth International Conference on Learning\n  Representations (ICLR), 2024. Copyright 2023 by the author(s)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02003v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02003v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.5; D.2.2; D.2.3; D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22402v1",
                "updated": "2025-06-27T17:21:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    21,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:21:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    21,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Czech GEC: Insights from a Multi-Experiment Approach"
                },
                "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec."
                },
                "authors": [
                    {
                        "name": "Petr Pechman"
                    },
                    {
                        "name": "Milan Straka"
                    },
                    {
                        "name": "Jana StrakovÃ¡"
                    },
                    {
                        "name": "Jakub NÃ¡plava"
                    }
                ],
                "author_detail": {
                    "name": "Jakub NÃ¡plava"
                },
                "author": "Jakub NÃ¡plava",
                "arxiv_comment": "Accepted to TSD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01956v3",
                "updated": "2025-06-27T17:15:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    15,
                    9,
                    4,
                    178,
                    0
                ],
                "published": "2025-01-03T18:59:23Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    23,
                    4,
                    3,
                    0
                ],
                "title": "Metadata Conditioning Accelerates Language Model Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata Conditioning Accelerates Language Model Pre-training"
                },
                "summary": "The vast diversity of styles, domains, and quality levels present in language\nmodel pre-training corpora is essential in developing general model\ncapabilities, but efficiently learning and deploying the correct behaviors\nexemplified in each of these heterogeneous data sources is challenging. To\naddress this, we propose a new method, termed Metadata Conditioning then\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\nMeCo first provides metadata (e.g., URLs like www$.$wikipedia$.$org) alongside\nthe text during training and later uses a cooldown phase with only the standard\ntext, thereby enabling the model to function normally even without metadata.\nMeCo significantly accelerates pre-training across different model scales (600M\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\ninstance, a 1.6B language model trained with MeCo matches the downstream task\nperformance of standard pre-training while using 33% less data. Additionally,\nMeCo enables us to steer language models by conditioning the inference prompt\non either real or fabricated metadata that encodes the desired properties of\nthe output: for example, prepending wikipedia$.$org to reduce harmful\ngenerations or factquizmaster$.$com (fabricated) to improve common knowledge\ntask performance. We also demonstrate that MeCo is compatible with different\ntypes of metadata, such as model-generated topics. MeCo is remarkably simple,\nadds no computational overhead, and demonstrates promise in producing more\ncapable and steerable language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast diversity of styles, domains, and quality levels present in language\nmodel pre-training corpora is essential in developing general model\ncapabilities, but efficiently learning and deploying the correct behaviors\nexemplified in each of these heterogeneous data sources is challenging. To\naddress this, we propose a new method, termed Metadata Conditioning then\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\nMeCo first provides metadata (e.g., URLs like www$.$wikipedia$.$org) alongside\nthe text during training and later uses a cooldown phase with only the standard\ntext, thereby enabling the model to function normally even without metadata.\nMeCo significantly accelerates pre-training across different model scales (600M\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\ninstance, a 1.6B language model trained with MeCo matches the downstream task\nperformance of standard pre-training while using 33% less data. Additionally,\nMeCo enables us to steer language models by conditioning the inference prompt\non either real or fabricated metadata that encodes the desired properties of\nthe output: for example, prepending wikipedia$.$org to reduce harmful\ngenerations or factquizmaster$.$com (fabricated) to improve common knowledge\ntask performance. We also demonstrate that MeCo is compatible with different\ntypes of metadata, such as model-generated topics. MeCo is remarkably simple,\nadds no computational overhead, and demonstrates promise in producing more\ncapable and steerable language models."
                },
                "authors": [
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Luxi He"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "Accepted to ICML 2025. Code available at\n  https://github.com/princeton-pli/MeCo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22395v1",
                "updated": "2025-06-27T17:09:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    9,
                    44,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:09:44Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    9,
                    44,
                    4,
                    178,
                    0
                ],
                "title": "Test-Time Consistency in Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Consistency in Vision Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have achieved impressive performance across a\nwide range of multimodal tasks, yet they often exhibit inconsistent behavior\nwhen faced with semantically equivalent inputs, undermining their reliability\nand robustness. Recent benchmarks, such as MM-R3, highlight that even\nstate-of-the-art VLMs can produce divergent predictions across semantically\nequivalent inputs, despite maintaining high average accuracy. Prior work\naddresses this issue by modifying model architectures or conducting large-scale\nfine-tuning on curated datasets. In contrast, we propose a simple and effective\ntest-time consistency framework that enhances semantic consistency without\nsupervised re-training. Our method is entirely post-hoc, model-agnostic, and\napplicable to any VLM with access to its weights. Given a single test point, we\nenforce consistent predictions via two complementary objectives: (i) a\nCross-Entropy Agreement Loss that aligns predictive distributions across\nsemantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that\ndraws outputs toward a self-averaged consensus. Our method is plug-and-play and\nleverages information from a single test input itself to improve consistency.\nExperiments on the MM-R3 benchmark show that our framework yields substantial\ngains in consistency across state-of-the-art models, establishing a new\ndirection for inference-time adaptation in multimodal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have achieved impressive performance across a\nwide range of multimodal tasks, yet they often exhibit inconsistent behavior\nwhen faced with semantically equivalent inputs, undermining their reliability\nand robustness. Recent benchmarks, such as MM-R3, highlight that even\nstate-of-the-art VLMs can produce divergent predictions across semantically\nequivalent inputs, despite maintaining high average accuracy. Prior work\naddresses this issue by modifying model architectures or conducting large-scale\nfine-tuning on curated datasets. In contrast, we propose a simple and effective\ntest-time consistency framework that enhances semantic consistency without\nsupervised re-training. Our method is entirely post-hoc, model-agnostic, and\napplicable to any VLM with access to its weights. Given a single test point, we\nenforce consistent predictions via two complementary objectives: (i) a\nCross-Entropy Agreement Loss that aligns predictive distributions across\nsemantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that\ndraws outputs toward a self-averaged consensus. Our method is plug-and-play and\nleverages information from a single test input itself to improve consistency.\nExperiments on the MM-R3 benchmark show that our framework yields substantial\ngains in consistency across state-of-the-art models, establishing a new\ndirection for inference-time adaptation in multimodal learning."
                },
                "authors": [
                    {
                        "name": "Shih-Han Chou"
                    },
                    {
                        "name": "Shivam Chandhok"
                    },
                    {
                        "name": "James J. Little"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04778v2",
                "updated": "2025-06-27T17:08:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    8,
                    56,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-07T06:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    36,
                    55,
                    0,
                    281,
                    0
                ],
                "title": "MM-R$^3$: On (In-)Consistency of Vision-Language Models (VLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-R$^3$: On (In-)Consistency of Vision-Language Models (VLMs)"
                },
                "summary": "With the advent of LLMs and variants, a flurry of research has emerged,\nanalyzing the performance of such models across an array of tasks. While most\nstudies focus on evaluating the capabilities of state-of-the-art (SoTA) Vision\nLanguage Models (VLMs) through task accuracy (e.g., visual question answering,\ngrounding), our work explores the related but complementary aspect of\nconsistency - the ability of a VLM to produce semantically similar or identical\nresponses to semantically similar queries. We note that consistency is a\nfundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in VLMs. Armed with this perspective, we propose the MM-R3\nbenchmark, which allows us to analyze performance, in terms of consistency and\naccuracy, of SoTA VLMs on three tasks: Question Rephrasing, Image Restyling,\nand Context Reasoning. Our analysis reveals that consistency does not always\nalign with accuracy, indicating that models with higher accuracy are not\nnecessarily more consistent, and vice versa. Furthermore, we propose a simple\nyet effective mitigation strategy in the form of an adapter module trained to\nminimize inconsistency across prompts. With our proposed strategy, we are able\nto achieve absolute improvements of 5.7% and 12.5%, on average on widely used\nVLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their existing\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of LLMs and variants, a flurry of research has emerged,\nanalyzing the performance of such models across an array of tasks. While most\nstudies focus on evaluating the capabilities of state-of-the-art (SoTA) Vision\nLanguage Models (VLMs) through task accuracy (e.g., visual question answering,\ngrounding), our work explores the related but complementary aspect of\nconsistency - the ability of a VLM to produce semantically similar or identical\nresponses to semantically similar queries. We note that consistency is a\nfundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in VLMs. Armed with this perspective, we propose the MM-R3\nbenchmark, which allows us to analyze performance, in terms of consistency and\naccuracy, of SoTA VLMs on three tasks: Question Rephrasing, Image Restyling,\nand Context Reasoning. Our analysis reveals that consistency does not always\nalign with accuracy, indicating that models with higher accuracy are not\nnecessarily more consistent, and vice versa. Furthermore, we propose a simple\nyet effective mitigation strategy in the form of an adapter module trained to\nminimize inconsistency across prompts. With our proposed strategy, we are able\nto achieve absolute improvements of 5.7% and 12.5%, on average on widely used\nVLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their existing\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Shih-Han Chou"
                    },
                    {
                        "name": "Shivam Chandhok"
                    },
                    {
                        "name": "James J. Little"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22390v1",
                "updated": "2025-06-27T17:00:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    0,
                    48,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:00:48Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    0,
                    48,
                    4,
                    178,
                    0
                ],
                "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical\n  Study of Developer-ChatGPT Conversations in GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical\n  Study of Developer-ChatGPT Conversations in GitHub"
                },
                "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22385v1",
                "updated": "2025-06-27T16:51:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    51,
                    15,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:51:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    51,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A\n  Study on Defeasible Video Entailment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A\n  Study on Defeasible Video Entailment"
                },
                "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Jilei Sun"
                    },
                    {
                        "name": "Yunhui Guo"
                    },
                    {
                        "name": "Vibhav Gogate"
                    }
                ],
                "author_detail": {
                    "name": "Vibhav Gogate"
                },
                "author": "Vibhav Gogate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.10774v2",
                "updated": "2025-06-27T16:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    44,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2022-09-22T04:21:22Z",
                "published_parsed": [
                    2022,
                    9,
                    22,
                    4,
                    21,
                    22,
                    3,
                    265,
                    0
                ],
                "title": "PC Adjusted Testing for Low Dimensional Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PC Adjusted Testing for Low Dimensional Parameters"
                },
                "summary": "In this paper, we investigate the impact of high-dimensional Principal\nComponent (PC) adjustments on inferring the effects of variables on outcomes,\nwith a focus on applications in genetic association studies where PC adjustment\nis commonly used to account for population stratification. We consider\nhigh-dimensional linear regression in the regime where the number of covariates\ngrows proportionally to the number of samples. In this setting, we provide an\nasymptotically precise understanding of when PC adjustments yield valid tests\nwith controlled Type I error rates. Our results demonstrate that, under both\nfixed and diverging signal strengths, PC regression often fails to control the\nType I error at the desired nominal level. Furthermore, we establish necessary\nand sufficient conditions for Type I error inflation based on covariate\ndistributions. These theoretical findings are further supported by a series of\nnumerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the impact of high-dimensional Principal\nComponent (PC) adjustments on inferring the effects of variables on outcomes,\nwith a focus on applications in genetic association studies where PC adjustment\nis commonly used to account for population stratification. We consider\nhigh-dimensional linear regression in the regime where the number of covariates\ngrows proportionally to the number of samples. In this setting, we provide an\nasymptotically precise understanding of when PC adjustments yield valid tests\nwith controlled Type I error rates. Our results demonstrate that, under both\nfixed and diverging signal strengths, PC regression often fails to control the\nType I error at the desired nominal level. Furthermore, we establish necessary\nand sufficient conditions for Type I error inflation based on covariate\ndistributions. These theoretical findings are further supported by a series of\nnumerical experiments."
                },
                "authors": [
                    {
                        "name": "Sohom Bhattacharya"
                    },
                    {
                        "name": "Rounak Dey"
                    },
                    {
                        "name": "Rajarshi Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Mukherjee"
                },
                "author": "Rajarshi Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G10, 62G20, 62C20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22376v1",
                "updated": "2025-06-27T16:44:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    44,
                    11,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:44:11Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    44,
                    11,
                    4,
                    178,
                    0
                ],
                "title": "Probabilistic Optimality for Inference-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Optimality for Inference-time Scaling"
                },
                "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning."
                },
                "authors": [
                    {
                        "name": "Youkang Wang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Rubing Chen"
                    },
                    {
                        "name": "Xiao-Yong Wei"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22372v1",
                "updated": "2025-06-27T16:39:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    39,
                    12,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:39:12Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    39,
                    12,
                    4,
                    178,
                    0
                ],
                "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement"
                },
                "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems."
                },
                "authors": [
                    {
                        "name": "Maryam Mousavian"
                    },
                    {
                        "name": "Zahra Abbasiantaeb"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Fabio Crestani"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Crestani"
                },
                "author": "Fabio Crestani",
                "arxiv_doi": "10.1145/3731120.3744620",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731120.3744620",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.22372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22370v1",
                "updated": "2025-06-27T16:34:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    13,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:34:13Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    13,
                    4,
                    178,
                    0
                ],
                "title": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny"
                },
                "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution."
                },
                "authors": [
                    {
                        "name": "Carolina Carreira"
                    },
                    {
                        "name": "Ãlvaro Silva"
                    },
                    {
                        "name": "Alexandre Abreu"
                    },
                    {
                        "name": "Alexandra Mendes"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Mendes"
                },
                "author": "Alexandra Mendes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13868v2",
                "updated": "2025-06-27T16:34:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    8,
                    4,
                    178,
                    0
                ],
                "published": "2024-11-21T06:06:04Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    6,
                    4,
                    3,
                    326,
                    0
                ],
                "title": "Robust Detection of Watermarks for Large Language Models Under Human\n  Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Detection of Watermarks for Large Language Models Under Human\n  Edits"
                },
                "summary": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04466v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04466v4",
                "updated": "2025-06-27T16:34:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    0,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-06T12:34:23Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    34,
                    23,
                    6,
                    96,
                    0
                ],
                "title": "LoopGen: Training-Free Loopable Music Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopGen: Training-Free Loopable Music Generation"
                },
                "summary": "Loops--short audio segments designed for seamless repetition--are central to\nmany music genres, particularly those rooted in dance and electronic styles.\nHowever, current generative music models struggle to produce truly loopable\naudio, as generating a short waveform alone does not guarantee a smooth\ntransition from its endpoint back to its start, often resulting in audible\ndiscontinuities. We address this gap by modifying a non-autoregressive model\n(MAGNeT) to generate tokens in a circular pattern, letting the model attend to\nthe beginning of the audio when creating its ending. This inference-only\napproach results in generations that are aware of future context and loop\nnaturally, without the need for any additional training or data. We evaluate\nthe consistency of loop transitions by computing token perplexity around the\nseam of the loop, observing a 55% improvement. Blind listening tests further\nconfirm significant perceptual gains over baseline methods, improving mean\nratings by 70%. Taken together, these results highlight the effectiveness of\ninference-only approaches in improving generative models and underscore the\nadvantages of non-autoregressive methods for context-aware music generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loops--short audio segments designed for seamless repetition--are central to\nmany music genres, particularly those rooted in dance and electronic styles.\nHowever, current generative music models struggle to produce truly loopable\naudio, as generating a short waveform alone does not guarantee a smooth\ntransition from its endpoint back to its start, often resulting in audible\ndiscontinuities. We address this gap by modifying a non-autoregressive model\n(MAGNeT) to generate tokens in a circular pattern, letting the model attend to\nthe beginning of the audio when creating its ending. This inference-only\napproach results in generations that are aware of future context and loop\nnaturally, without the need for any additional training or data. We evaluate\nthe consistency of loop transitions by computing token perplexity around the\nseam of the loop, observing a 55% improvement. Blind listening tests further\nconfirm significant perceptual gains over baseline methods, improving mean\nratings by 70%. Taken together, these results highlight the effectiveness of\ninference-only approaches in improving generative models and underscore the\nadvantages of non-autoregressive methods for context-aware music generation."
                },
                "authors": [
                    {
                        "name": "Davide Marincione"
                    },
                    {
                        "name": "Giorgio Strano"
                    },
                    {
                        "name": "Donato Crisostomi"
                    },
                    {
                        "name": "Roberto Ribuoli"
                    },
                    {
                        "name": "Emanuele RodolÃ "
                    }
                ],
                "author_detail": {
                    "name": "Emanuele RodolÃ "
                },
                "author": "Emanuele RodolÃ ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04466v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04466v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22367v1",
                "updated": "2025-06-27T16:29:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    29,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:29:25Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    29,
                    25,
                    4,
                    178,
                    0
                ],
                "title": "Constraining the Stellar-to-Halo Mass Relation with Galaxy Clustering\n  and Weak Lensing from DES Year 3 Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the Stellar-to-Halo Mass Relation with Galaxy Clustering\n  and Weak Lensing from DES Year 3 Data"
                },
                "summary": "We develop a framework to study the relation between the stellar mass of a\ngalaxy and the total mass of its host dark matter halo using galaxy clustering\nand galaxy-galaxy lensing measurements. We model a wide range of scales,\nroughly from $\\sim 100 \\; {\\rm kpc}$ to $\\sim 100 \\; {\\rm Mpc}$, using a\ntheoretical framework based on the Halo Occupation Distribution and data from\nYear 3 of the Dark Energy Survey (DES) dataset. The new advances of this work\ninclude: 1) the generation and validation of a new stellar mass-selected galaxy\nsample in the range of $\\log M_\\star/M_\\odot \\sim 9.6$ to $\\sim 11.5$; 2) the\njoint-modeling framework of galaxy clustering and galaxy-galaxy lensing that is\nable to describe our stellar mass-selected sample deep into the 1-halo regime;\nand 3) stellar-to-halo mass relation (SHMR) constraints from this dataset. In\ngeneral, our SHMR constraints agree well with existing literature with various\nweak lensing measurements. We constrain the free parameters in the SHMR\nfunctional form $\\log M_\\star (M_h) = \\log(\\epsilon M_1) + f\\left[ \\log\\left(\nM_h / M_1 \\right) \\right] - f(0)$, with $f(x) \\equiv -\\log(10^{\\alpha x}+1) +\n\\delta [\\log(1+\\exp(x))]^\\gamma / [1+\\exp(10^{-x})]$, to be $\\log M_1 =\n11.559^{+0.334}_{-0.415}$, $\\log \\epsilon = -1.689^{+0.333}_{-0.220}$, $\\alpha\n= -1.637^{+0.107}_{-0.096}$, $\\gamma = 0.588^{+0.265}_{-0.220}$ and $\\delta =\n4.227^{+2.223}_{-1.776}$. The inferred average satellite fraction is within\n$\\sim 5-35\\%$ for our fiducial results and we do not see any clear trends with\nredshift or stellar mass. Furthermore, we find that the inferred average galaxy\nbias values follow the generally expected trends with stellar mass and\nredshift. Our study is the first SHMR in DES in this mass range, and we expect\nthe stellar mass sample to be of general interest for other science cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework to study the relation between the stellar mass of a\ngalaxy and the total mass of its host dark matter halo using galaxy clustering\nand galaxy-galaxy lensing measurements. We model a wide range of scales,\nroughly from $\\sim 100 \\; {\\rm kpc}$ to $\\sim 100 \\; {\\rm Mpc}$, using a\ntheoretical framework based on the Halo Occupation Distribution and data from\nYear 3 of the Dark Energy Survey (DES) dataset. The new advances of this work\ninclude: 1) the generation and validation of a new stellar mass-selected galaxy\nsample in the range of $\\log M_\\star/M_\\odot \\sim 9.6$ to $\\sim 11.5$; 2) the\njoint-modeling framework of galaxy clustering and galaxy-galaxy lensing that is\nable to describe our stellar mass-selected sample deep into the 1-halo regime;\nand 3) stellar-to-halo mass relation (SHMR) constraints from this dataset. In\ngeneral, our SHMR constraints agree well with existing literature with various\nweak lensing measurements. We constrain the free parameters in the SHMR\nfunctional form $\\log M_\\star (M_h) = \\log(\\epsilon M_1) + f\\left[ \\log\\left(\nM_h / M_1 \\right) \\right] - f(0)$, with $f(x) \\equiv -\\log(10^{\\alpha x}+1) +\n\\delta [\\log(1+\\exp(x))]^\\gamma / [1+\\exp(10^{-x})]$, to be $\\log M_1 =\n11.559^{+0.334}_{-0.415}$, $\\log \\epsilon = -1.689^{+0.333}_{-0.220}$, $\\alpha\n= -1.637^{+0.107}_{-0.096}$, $\\gamma = 0.588^{+0.265}_{-0.220}$ and $\\delta =\n4.227^{+2.223}_{-1.776}$. The inferred average satellite fraction is within\n$\\sim 5-35\\%$ for our fiducial results and we do not see any clear trends with\nredshift or stellar mass. Furthermore, we find that the inferred average galaxy\nbias values follow the generally expected trends with stellar mass and\nredshift. Our study is the first SHMR in DES in this mass range, and we expect\nthe stellar mass sample to be of general interest for other science cases."
                },
                "authors": [
                    {
                        "name": "G. Zacharegkas"
                    },
                    {
                        "name": "C. Chang"
                    },
                    {
                        "name": "J. Prat"
                    },
                    {
                        "name": "W. Hartley"
                    },
                    {
                        "name": "S. Mucesh"
                    },
                    {
                        "name": "A. Alarcon"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "A. Amon"
                    },
                    {
                        "name": "K. Bechtol"
                    },
                    {
                        "name": "M. R. Becker"
                    },
                    {
                        "name": "G. Bernstein"
                    },
                    {
                        "name": "J. Blazek"
                    },
                    {
                        "name": "A. Campos"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "M. Carrasco Kind"
                    },
                    {
                        "name": "R. Cawthon"
                    },
                    {
                        "name": "R. Chen"
                    },
                    {
                        "name": "A. Choi"
                    },
                    {
                        "name": "J. Cordero"
                    },
                    {
                        "name": "C. Davis"
                    },
                    {
                        "name": "J. Derose"
                    },
                    {
                        "name": "H. Diehl"
                    },
                    {
                        "name": "S. Dodelson"
                    },
                    {
                        "name": "C. Doux"
                    },
                    {
                        "name": "A. Drlica-Wagner"
                    },
                    {
                        "name": "K. Eckert"
                    },
                    {
                        "name": "T. F. Eifler"
                    },
                    {
                        "name": "J. Elvin-Poole"
                    },
                    {
                        "name": "S. Everett"
                    },
                    {
                        "name": "X. Fang"
                    },
                    {
                        "name": "A. Ferte"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "G. Giannini"
                    },
                    {
                        "name": "D. Gruen"
                    },
                    {
                        "name": "R. A. Gruendl"
                    },
                    {
                        "name": "I. Harrison"
                    },
                    {
                        "name": "H. Huang"
                    },
                    {
                        "name": "E. M. Huff"
                    },
                    {
                        "name": "M. Jarvis"
                    },
                    {
                        "name": "E. Krause"
                    },
                    {
                        "name": "N. Kuropatkin"
                    },
                    {
                        "name": "P. F. Leget"
                    },
                    {
                        "name": "N. Maccrann"
                    },
                    {
                        "name": "J. McCullough"
                    },
                    {
                        "name": "J. Myles"
                    },
                    {
                        "name": "A. N. Alsina"
                    },
                    {
                        "name": "S. Pandey"
                    },
                    {
                        "name": "M. Raveri"
                    },
                    {
                        "name": "R. P. Rollins"
                    },
                    {
                        "name": "A. Roodman"
                    },
                    {
                        "name": "A. J. Ross"
                    },
                    {
                        "name": "E. S. Rykoff"
                    },
                    {
                        "name": "C. Sanchez"
                    },
                    {
                        "name": "L. F. Secco"
                    },
                    {
                        "name": "I. Sevilla-Noarbe"
                    },
                    {
                        "name": "E. Sheldon"
                    },
                    {
                        "name": "T. Shin"
                    },
                    {
                        "name": "M. A. Troxel"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "B. Yanny"
                    },
                    {
                        "name": "B. Yin"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "J. Zuntz"
                    }
                ],
                "author_detail": {
                    "name": "J. Zuntz"
                },
                "author": "J. Zuntz",
                "arxiv_comment": "34 pages, main text ends at page 24, 6 appendices, 19 figures, 4\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22362v1",
                "updated": "2025-06-27T16:23:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    23,
                    7,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    23,
                    7,
                    4,
                    178,
                    0
                ],
                "title": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding"
                },
                "summary": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yunpeng Li"
                    },
                    {
                        "name": "George Sung"
                    },
                    {
                        "name": "Shao-Fu Shih"
                    },
                    {
                        "name": "Craig Dooley"
                    },
                    {
                        "name": "Alessio Centazzo"
                    },
                    {
                        "name": "Ramanan Rajeswaran"
                    }
                ],
                "author_detail": {
                    "name": "Ramanan Rajeswaran"
                },
                "author": "Ramanan Rajeswaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22359v1",
                "updated": "2025-06-27T16:20:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    20,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:20:18Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    20,
                    18,
                    4,
                    178,
                    0
                ],
                "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Level AI for Telecom: Moving Beyond Large Language Models"
                },
                "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management."
                },
                "authors": [
                    {
                        "name": "Viswanath Kumarskandpriya"
                    },
                    {
                        "name": "Abdulhalim Dandoush"
                    },
                    {
                        "name": "Abbas Bradai"
                    },
                    {
                        "name": "Ali Belgacem"
                    }
                ],
                "author_detail": {
                    "name": "Ali Belgacem"
                },
                "author": "Ali Belgacem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03768v2",
                "updated": "2025-06-27T16:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    1,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2024-12-04T23:14:00Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    23,
                    14,
                    0,
                    2,
                    339,
                    0
                ],
                "title": "Learning Networks from Wide-Sense Stationary Stochastic Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Networks from Wide-Sense Stationary Stochastic Processes"
                },
                "summary": "Complex networked systems driven by latent inputs are common in fields like\nneuroscience, finance, and engineering. A key inference problem here is to\nlearn edge connectivity from node outputs (potentials). We focus on systems\ngoverned by steady-state linear conservation laws: $X_t = {L^{\\ast}}Y_{t}$,\nwhere $X_t, Y_t \\in \\mathbb{R}^p$ denote inputs and potentials, respectively,\nand the sparsity pattern of the $p \\times p$ Laplacian $L^{\\ast}$ encodes the\nedge structure. Assuming $X_t$ to be a wide-sense stationary stochastic process\nwith a known spectral density matrix, we learn the support of $L^{\\ast}$ from\ntemporally correlated samples of $Y_t$ via an $\\ell_1$-regularized Whittle's\nmaximum likelihood estimator (MLE). The regularization is particularly useful\nfor learning large-scale networks in the high-dimensional setting where the\nnetwork size $p$ significantly exceeds the number of samples $n$.\n  We show that the MLE problem is strictly convex, admitting a unique solution.\nUnder a novel mutual incoherence condition and certain sufficient conditions on\n$(n, p, d)$, we show that the ML estimate recovers the sparsity pattern of\n$L^\\ast$ with high probability, where $d$ is the maximum degree of the graph\nunderlying $L^{\\ast}$. We provide recovery guarantees for $L^\\ast$ in\nelement-wise maximum, Frobenius, and operator norms. Finally, we complement our\ntheoretical results with several simulation studies on synthetic and benchmark\ndatasets, including engineered systems (power and water networks), and\nreal-world datasets from neural systems (such as the human brain).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex networked systems driven by latent inputs are common in fields like\nneuroscience, finance, and engineering. A key inference problem here is to\nlearn edge connectivity from node outputs (potentials). We focus on systems\ngoverned by steady-state linear conservation laws: $X_t = {L^{\\ast}}Y_{t}$,\nwhere $X_t, Y_t \\in \\mathbb{R}^p$ denote inputs and potentials, respectively,\nand the sparsity pattern of the $p \\times p$ Laplacian $L^{\\ast}$ encodes the\nedge structure. Assuming $X_t$ to be a wide-sense stationary stochastic process\nwith a known spectral density matrix, we learn the support of $L^{\\ast}$ from\ntemporally correlated samples of $Y_t$ via an $\\ell_1$-regularized Whittle's\nmaximum likelihood estimator (MLE). The regularization is particularly useful\nfor learning large-scale networks in the high-dimensional setting where the\nnetwork size $p$ significantly exceeds the number of samples $n$.\n  We show that the MLE problem is strictly convex, admitting a unique solution.\nUnder a novel mutual incoherence condition and certain sufficient conditions on\n$(n, p, d)$, we show that the ML estimate recovers the sparsity pattern of\n$L^\\ast$ with high probability, where $d$ is the maximum degree of the graph\nunderlying $L^{\\ast}$. We provide recovery guarantees for $L^\\ast$ in\nelement-wise maximum, Frobenius, and operator norms. Finally, we complement our\ntheoretical results with several simulation studies on synthetic and benchmark\ndatasets, including engineered systems (power and water networks), and\nreal-world datasets from neural systems (such as the human brain)."
                },
                "authors": [
                    {
                        "name": "Anirudh Rayas"
                    },
                    {
                        "name": "Jiajun Cheng"
                    },
                    {
                        "name": "Rajasekhar Anguluri"
                    },
                    {
                        "name": "Deepjyoti Deka"
                    },
                    {
                        "name": "Gautam Dasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Dasarathy"
                },
                "author": "Gautam Dasarathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03583v2",
                "updated": "2025-06-27T15:58:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    58,
                    19,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-04T16:47:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    47,
                    30,
                    4,
                    94,
                    0
                ],
                "title": "Scalable Hypergraph Structure Learning with Diverse Smoothness Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Hypergraph Structure Learning with Diverse Smoothness Priors"
                },
                "summary": "In graph signal processing, learning the weighted connections between nodes\nfrom a set of sample signals is a fundamental task when the underlying\nrelationships are not known a priori. This task is typically addressed by\nfinding a graph Laplacian on which the observed signals are smooth. With the\nextension of graphs to hypergraphs - where edges can connect more than two\nnodes - graph learning methods have similarly been generalized to hypergraphs.\nHowever, the absence of a unified framework for calculating total variation has\nled to divergent definitions of smoothness and, consequently, differing\napproaches to hyperedge recovery. We confront this challenge through\ngeneralization of several previously proposed hypergraph total variations,\nsubsequently allowing ease of substitution into a vector based optimization. To\nthis end, we propose a novel hypergraph learning method that recovers a\nhypergraph topology from time-series signals based on a smoothness prior. Our\napproach, designated as Hypergraph Structure Learning with Smoothness (HSLS),\naddresses key limitations in prior works, such as hyperedge selection and\nconvergence issues, by formulating the problem as a convex optimization solved\nvia a forward-backward-forward algorithm, ensuring guaranteed convergence.\nAdditionally, we introduce a process that simultaneously limits the span of the\nhyperedge search and maintains a valid hyperedge selection set. In doing so,\nour method becomes scalable in increasingly complex network structures. The\nexperimental results demonstrate improved performance, in terms of accuracy,\nover other state-of-the-art hypergraph inference methods; furthermore, we\nempirically show our method to be robust to total variation terms, biased\ntowards global smoothness, and scalable to larger hypergraphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In graph signal processing, learning the weighted connections between nodes\nfrom a set of sample signals is a fundamental task when the underlying\nrelationships are not known a priori. This task is typically addressed by\nfinding a graph Laplacian on which the observed signals are smooth. With the\nextension of graphs to hypergraphs - where edges can connect more than two\nnodes - graph learning methods have similarly been generalized to hypergraphs.\nHowever, the absence of a unified framework for calculating total variation has\nled to divergent definitions of smoothness and, consequently, differing\napproaches to hyperedge recovery. We confront this challenge through\ngeneralization of several previously proposed hypergraph total variations,\nsubsequently allowing ease of substitution into a vector based optimization. To\nthis end, we propose a novel hypergraph learning method that recovers a\nhypergraph topology from time-series signals based on a smoothness prior. Our\napproach, designated as Hypergraph Structure Learning with Smoothness (HSLS),\naddresses key limitations in prior works, such as hyperedge selection and\nconvergence issues, by formulating the problem as a convex optimization solved\nvia a forward-backward-forward algorithm, ensuring guaranteed convergence.\nAdditionally, we introduce a process that simultaneously limits the span of the\nhyperedge search and maintains a valid hyperedge selection set. In doing so,\nour method becomes scalable in increasingly complex network structures. The\nexperimental results demonstrate improved performance, in terms of accuracy,\nover other state-of-the-art hypergraph inference methods; furthermore, we\nempirically show our method to be robust to total variation terms, biased\ntowards global smoothness, and scalable to larger hypergraphs."
                },
                "authors": [
                    {
                        "name": "Benjamin T. Brown"
                    },
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Daniel L. Lau"
                    },
                    {
                        "name": "Gonzalo R. Arce"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo R. Arce"
                },
                "author": "Gonzalo R. Arce",
                "arxiv_comment": "15 pages, 7 figures, submitted to IEEE for possible publication;\n  Section I includes more applications, comparisons, and enumerated list of\n  novel contributions; removed numerical analysis of TV terms in Section II,\n  added more general discussion; updated Algorithm 1 and corresponding text;\n  third experiment of Section V-C replaced with new experiment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07197v3",
                "updated": "2025-06-27T15:56:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    56,
                    21,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-09T18:25:49Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    18,
                    25,
                    49,
                    2,
                    99,
                    0
                ],
                "title": "Rapid inference and comparison of gravitational-wave population models\n  with neural variational posteriors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid inference and comparison of gravitational-wave population models\n  with neural variational posteriors"
                },
                "summary": "The LIGO-Virgo-KAGRA catalog has been analyzed with an abundance of different\npopulation models due to theoretical uncertainty in the formation of\ngravitational-wave sources. To expedite model exploration, we introduce an\nefficient and accurate variational Bayesian approach that learns the population\nposterior with a normalizing flow and serves as a drop-in replacement for\nexisting samplers. With hardware acceleration, inference takes just seconds for\nthe current set of black-hole mergers and readily scales to larger catalogs.\nThe trained posteriors provide an arbitrary number of independent samples with\nexact probability densities, unlike established stochastic sampling algorithms,\nwhile requiring up to three orders of magnitude fewer likelihood evaluations\nand as few as $\\mathcal{O}(10^3)$. Provided the posterior support is covered,\ndiscrepancies can be addressed with smoothed importance sampling, which\nquantifies a goodness-of-fit metric for the variational approximation while\nalso estimating the evidence for Bayesian model selection. Neural variational\ninference thus enables interactive development, analysis, and comparison of\npopulation models, making it a useful tool for astrophysical interpretation of\ncurrent and future gravitational-wave observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LIGO-Virgo-KAGRA catalog has been analyzed with an abundance of different\npopulation models due to theoretical uncertainty in the formation of\ngravitational-wave sources. To expedite model exploration, we introduce an\nefficient and accurate variational Bayesian approach that learns the population\nposterior with a normalizing flow and serves as a drop-in replacement for\nexisting samplers. With hardware acceleration, inference takes just seconds for\nthe current set of black-hole mergers and readily scales to larger catalogs.\nThe trained posteriors provide an arbitrary number of independent samples with\nexact probability densities, unlike established stochastic sampling algorithms,\nwhile requiring up to three orders of magnitude fewer likelihood evaluations\nand as few as $\\mathcal{O}(10^3)$. Provided the posterior support is covered,\ndiscrepancies can be addressed with smoothed importance sampling, which\nquantifies a goodness-of-fit metric for the variational approximation while\nalso estimating the evidence for Bayesian model selection. Neural variational\ninference thus enables interactive development, analysis, and comparison of\npopulation models, making it a useful tool for astrophysical interpretation of\ncurrent and future gravitational-wave observations."
                },
                "authors": [
                    {
                        "name": "Matthew Mould"
                    },
                    {
                        "name": "Noah E. Wolfe"
                    },
                    {
                        "name": "Salvatore Vitale"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Vitale"
                },
                "author": "Salvatore Vitale",
                "arxiv_doi": "10.1103/xk1z-fxnm",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/xk1z-fxnm",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. D 111, 123049 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22343v1",
                "updated": "2025-06-27T15:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    53,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    53,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts"
                },
                "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Garrett Wen"
                    },
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Jiayuan Wu"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22334v1",
                "updated": "2025-06-27T15:40:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    40,
                    42,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:40:42Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    40,
                    42,
                    4,
                    178,
                    0
                ],
                "title": "Linking climate and dengue in the Philippines using a two-stage Bayesian\n  spatio-temporal model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linking climate and dengue in the Philippines using a two-stage Bayesian\n  spatio-temporal model"
                },
                "summary": "Dengue is an infectious disease which poses significant socioeconomic and\ndisease burden in many tropical and subtropical regions of the world. This work\naims to provide additional insight into the association between dengue and\nclimate in the Philippines. We employ a two-stage modelling framework: the\nfirst stage fits climate models, while the second stage fits a health model\nthat uses the climate predictions from the first stage as inputs. We postulate\na Bayesian spatio-temporal model and use the integrated nested Laplace\napproximation (INLA) approach for inference. To account for the uncertainty in\nthe climate models, we perform posterior sampling and then perform Bayesian\nmodel averaging to compute the final posterior estimates of second-stage model\nparameters. The results indicate that temperature is positively associated with\ndengue, although extremely hot conditions tend to have a negative effect.\nMoreover, the relationship between rainfall and dengue varies in space. In\nareas with uniform amounts of rainfall all year round, rainfall is negatively\nassociated with dengue. In contrast, in regions with pronounced dry and wet\nseason, rainfall shows a positive association with dengue. Finally, there\nremains unexplained structured variation in space and time after accounting for\nthe impact of climate variables and other covariates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dengue is an infectious disease which poses significant socioeconomic and\ndisease burden in many tropical and subtropical regions of the world. This work\naims to provide additional insight into the association between dengue and\nclimate in the Philippines. We employ a two-stage modelling framework: the\nfirst stage fits climate models, while the second stage fits a health model\nthat uses the climate predictions from the first stage as inputs. We postulate\na Bayesian spatio-temporal model and use the integrated nested Laplace\napproximation (INLA) approach for inference. To account for the uncertainty in\nthe climate models, we perform posterior sampling and then perform Bayesian\nmodel averaging to compute the final posterior estimates of second-stage model\nparameters. The results indicate that temperature is positively associated with\ndengue, although extremely hot conditions tend to have a negative effect.\nMoreover, the relationship between rainfall and dengue varies in space. In\nareas with uniform amounts of rainfall all year round, rainfall is negatively\nassociated with dengue. In contrast, in regions with pronounced dry and wet\nseason, rainfall shows a positive association with dengue. Finally, there\nremains unexplained structured variation in space and time after accounting for\nthe impact of climate variables and other covariates."
                },
                "authors": [
                    {
                        "name": "Stephen Jun Villejo"
                    },
                    {
                        "name": "Sara Martino"
                    },
                    {
                        "name": "Janine Illian"
                    }
                ],
                "author_detail": {
                    "name": "Janine Illian"
                },
                "author": "Janine Illian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22321v1",
                "updated": "2025-06-27T15:35:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    35,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:35:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    35,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist\n  Sampling with Bandwidth Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist\n  Sampling with Bandwidth Extension"
                },
                "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB."
                },
                "authors": [
                    {
                        "name": "Tarikul Islam Tamiti"
                    },
                    {
                        "name": "Anomadarshi Barua"
                    }
                ],
                "author_detail": {
                    "name": "Anomadarshi Barua"
                },
                "author": "Anomadarshi Barua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22316v1",
                "updated": "2025-06-27T15:25:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    25,
                    23,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:25:23Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    25,
                    23,
                    4,
                    178,
                    0
                ],
                "title": "Evaluating Scoring Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Scoring Bias in LLM-as-a-Judge"
                },
                "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection."
                },
                "authors": [
                    {
                        "name": "Qingquan Li"
                    },
                    {
                        "name": "Shaoyu Dou"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Haixiang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haixiang Hu"
                },
                "author": "Haixiang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22313v1",
                "updated": "2025-06-27T15:23:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    23,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:23:18Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    23,
                    18,
                    4,
                    178,
                    0
                ],
                "title": "Manifold-Constrained Gaussian Processes for Inference of Mixed-effects\n  Ordinary Differential Equations with Application to Pharmacokinetics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manifold-Constrained Gaussian Processes for Inference of Mixed-effects\n  Ordinary Differential Equations with Application to Pharmacokinetics"
                },
                "summary": "Pharmacokinetic modeling using ordinary differential equations (ODEs) has an\nimportant role in dose optimization studies, where dosing must balance\nsustained therapeutic efficacy with the risk of adverse side effects. Such ODE\nmodels characterize drug plasma concentration over time and allow\npharmacokinetic parameters to be inferred, such as drug absorption and\nelimination rates. For time-course studies involving treatment groups with\nmultiple subjects, mixed-effects ODE models are commonly used. However,\nexisting methods tend to lack uncertainty quantification on a subject-level,\nfor key measures such as peak or trough concentration and for making\npredictions of drug concentration. To address such limitations, we propose an\nextension of manifold-constrained Gaussian processes for inference of general\nmixed-effects ODE models within a Bayesian statistical framework. We evaluate\nour method on simulated examples, demonstrating its ability to provide fast and\naccurate inference for parameters and trajectories using nested optimization.\nTo illustrate the practical efficacy of the proposed method, we provide a real\ndata analysis of a pharmacokinetic model used for an HIV combination therapy\nstudy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pharmacokinetic modeling using ordinary differential equations (ODEs) has an\nimportant role in dose optimization studies, where dosing must balance\nsustained therapeutic efficacy with the risk of adverse side effects. Such ODE\nmodels characterize drug plasma concentration over time and allow\npharmacokinetic parameters to be inferred, such as drug absorption and\nelimination rates. For time-course studies involving treatment groups with\nmultiple subjects, mixed-effects ODE models are commonly used. However,\nexisting methods tend to lack uncertainty quantification on a subject-level,\nfor key measures such as peak or trough concentration and for making\npredictions of drug concentration. To address such limitations, we propose an\nextension of manifold-constrained Gaussian processes for inference of general\nmixed-effects ODE models within a Bayesian statistical framework. We evaluate\nour method on simulated examples, demonstrating its ability to provide fast and\naccurate inference for parameters and trajectories using nested optimization.\nTo illustrate the practical efficacy of the proposed method, we provide a real\ndata analysis of a pharmacokinetic model used for an HIV combination therapy\nstudy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhao"
                    },
                    {
                        "name": "Samuel W. K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Samuel W. K. Wong"
                },
                "author": "Samuel W. K. Wong",
                "arxiv_comment": "34 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00521v2",
                "updated": "2025-06-27T15:19:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    19,
                    29,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-01T08:13:29Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    13,
                    29,
                    1,
                    91,
                    0
                ],
                "title": "Automated detection of atomicity violations in large-scale systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of atomicity violations in large-scale systems"
                },
                "summary": "Atomicity violations in interrupt-driven programs pose a significant threat\nto software safety in critical systems. These violations occur when the\nexecution sequence of operations on shared resources is disrupted by\nasynchronous interrupts. Detecting atomicity violations is challenging due to\nthe vast program state space, application-level code dependencies, and complex\ndomain-specific knowledge. We propose Clover, a hybrid framework that\nintegrates static analysis with large language model (LLM) agents to detect\natomicity violations in real-world programs. Clover first performs static\nanalysis to extract critical code snippets and operation information. It then\ninitiates a multi-agent process, where the expert agent leverages\ndomain-specific knowledge to detect atomicity violations, which are\nsubsequently validated by the judge agent. Evaluations on RaceBench 2.1,\nSV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of\n92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomicity violations in interrupt-driven programs pose a significant threat\nto software safety in critical systems. These violations occur when the\nexecution sequence of operations on shared resources is disrupted by\nasynchronous interrupts. Detecting atomicity violations is challenging due to\nthe vast program state space, application-level code dependencies, and complex\ndomain-specific knowledge. We propose Clover, a hybrid framework that\nintegrates static analysis with large language model (LLM) agents to detect\natomicity violations in real-world programs. Clover first performs static\nanalysis to extract critical code snippets and operation information. It then\ninitiates a multi-agent process, where the expert agent leverages\ndomain-specific knowledge to detect atomicity violations, which are\nsubsequently validated by the judge agent. Evaluations on RaceBench 2.1,\nSV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of\n92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score."
                },
                "authors": [
                    {
                        "name": "Hang He"
                    },
                    {
                        "name": "Yixing Luo"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Ting Su"
                    },
                    {
                        "name": "Haiying Sun"
                    },
                    {
                        "name": "Geguang Pu"
                    }
                ],
                "author_detail": {
                    "name": "Geguang Pu"
                },
                "author": "Geguang Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22305v1",
                "updated": "2025-06-27T15:16:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    16,
                    43,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:16:43Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    16,
                    43,
                    4,
                    178,
                    0
                ],
                "title": "Detection of Personal Data in Structured Datasets Using a Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Personal Data in Structured Datasets Using a Large Language\n  Model"
                },
                "summary": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information."
                },
                "authors": [
                    {
                        "name": "Albert Agisha Ntwali"
                    },
                    {
                        "name": "Luca RÃ¼ck"
                    },
                    {
                        "name": "Martin Heckmann"
                    }
                ],
                "author_detail": {
                    "name": "Martin Heckmann"
                },
                "author": "Martin Heckmann",
                "arxiv_comment": "10 pages",
                "arxiv_journal_ref": "LLM-DPM '2025, Next Gen Data and Process Management: Large\n  Language Models and Beyond, June 22, 2025, Berlin, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4; I.2.7; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22292v1",
                "updated": "2025-06-27T15:04:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    4,
                    9,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:04:09Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    4,
                    9,
                    4,
                    178,
                    0
                ],
                "title": "Scalable inference of large-scale random kronecker graphs via tensor\n  decomposition and Einstein summation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable inference of large-scale random kronecker graphs via tensor\n  decomposition and Einstein summation"
                },
                "summary": "In this paper, we extend the analysis of random Kronecker graphs to\nmulti-dimensional networks represented as tensors, enabling a more detailed and\nnuanced understanding of complex network structures. We decompose the adjacency\ntensor of such networks into two components: a low-rank signal tensor that\ncaptures the essential network structure and a zero-mean noise tensor that\naccounts for random variations. Building on recent advancements in tensor\ndecomposition and random tensor theory, we introduce a generalized\ndenoise-and-solve framework that leverages the Einstein summation convention\nfor efficient tensor operations. This approach significantly reduces\ncomputational complexity while demonstrating strong performance in network\ninference tasks, providing a scalable and efficient solution for analyzing\nlarge-scale, multi-dimensional networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we extend the analysis of random Kronecker graphs to\nmulti-dimensional networks represented as tensors, enabling a more detailed and\nnuanced understanding of complex network structures. We decompose the adjacency\ntensor of such networks into two components: a low-rank signal tensor that\ncaptures the essential network structure and a zero-mean noise tensor that\naccounts for random variations. Building on recent advancements in tensor\ndecomposition and random tensor theory, we introduce a generalized\ndenoise-and-solve framework that leverages the Einstein summation convention\nfor efficient tensor operations. This approach significantly reduces\ncomputational complexity while demonstrating strong performance in network\ninference tasks, providing a scalable and efficient solution for analyzing\nlarge-scale, multi-dimensional networks."
                },
                "authors": [
                    {
                        "name": "Sanaa Khobizy"
                    }
                ],
                "author_detail": {
                    "name": "Sanaa Khobizy"
                },
                "author": "Sanaa Khobizy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22283v1",
                "updated": "2025-06-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    55,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:55:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    55,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal\n  Misalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Token Reduction in LVLMs under Cross-modal\n  Misalignment"
                },
                "summary": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10872v2",
                "updated": "2025-06-27T14:42:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    42,
                    1,
                    4,
                    178,
                    0
                ],
                "published": "2022-12-21T09:35:19Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    9,
                    35,
                    19,
                    2,
                    355,
                    0
                ],
                "title": "Is it easier to count communities than find them?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is it easier to count communities than find them?"
                },
                "summary": "Random graph models with community structure have been studied extensively in\nthe literature. For both the problems of detecting and recovering community\nstructure, an interesting landscape of statistical and computational phase\ntransitions has emerged. A natural unanswered question is: might it be possible\nto infer properties of the community structure (for instance, the number and\nsizes of communities) even in situations where actually finding those\ncommunities is believed to be computationally hard? We show the answer is no.\nIn particular, we consider certain hypothesis testing problems between models\nwith different community structures, and we show (in the low-degree polynomial\nframework) that testing between two options is as hard as finding the\ncommunities.\n  Our methods give the first computational lower bounds for testing between two\ndifferent ``planted'' distributions, whereas previous results have considered\ntesting between a planted distribution and an i.i.d. ``null'' distribution. We\nalso show a formal relationship between the low--degree frameworks for recovery\nin a planted model and for testing two planted models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random graph models with community structure have been studied extensively in\nthe literature. For both the problems of detecting and recovering community\nstructure, an interesting landscape of statistical and computational phase\ntransitions has emerged. A natural unanswered question is: might it be possible\nto infer properties of the community structure (for instance, the number and\nsizes of communities) even in situations where actually finding those\ncommunities is believed to be computationally hard? We show the answer is no.\nIn particular, we consider certain hypothesis testing problems between models\nwith different community structures, and we show (in the low-degree polynomial\nframework) that testing between two options is as hard as finding the\ncommunities.\n  Our methods give the first computational lower bounds for testing between two\ndifferent ``planted'' distributions, whereas previous results have considered\ntesting between a planted distribution and an i.i.d. ``null'' distribution. We\nalso show a formal relationship between the low--degree frameworks for recovery\nin a planted model and for testing two planted models."
                },
                "authors": [
                    {
                        "name": "Cynthia Rush"
                    },
                    {
                        "name": "Fiona Skerman"
                    },
                    {
                        "name": "Alexander S. Wein"
                    },
                    {
                        "name": "Dana Yang"
                    }
                ],
                "author_detail": {
                    "name": "Dana Yang"
                },
                "author": "Dana Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C80, 62F03, 68Q25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2; G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22270v1",
                "updated": "2025-06-27T14:39:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    39,
                    38,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:39:38Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    39,
                    38,
                    4,
                    178,
                    0
                ],
                "title": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values"
                },
                "summary": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content."
                },
                "authors": [
                    {
                        "name": "Ahmad Mel"
                    },
                    {
                        "name": "Sebastien Noir"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Noir"
                },
                "author": "Sebastien Noir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22267v1",
                "updated": "2025-06-27T14:36:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    36,
                    39,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:36:39Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    36,
                    39,
                    4,
                    178,
                    0
                ],
                "title": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph\n  is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph\n  is All You Need"
                },
                "summary": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users."
                },
                "authors": [
                    {
                        "name": "Junaid Ahmed Khan"
                    },
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Andrea Proia"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22255v1",
                "updated": "2025-06-27T14:24:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    24,
                    1,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:24:01Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    24,
                    1,
                    4,
                    178,
                    0
                ],
                "title": "Projected Compression: Trainable Projection for Efficient Transformer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Projected Compression: Trainable Projection for Efficient Transformer\n  Compression"
                },
                "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens."
                },
                "authors": [
                    {
                        "name": "Maciej Stefaniak"
                    },
                    {
                        "name": "MichaÅ Krutul"
                    },
                    {
                        "name": "Jan MaÅaÅnicki"
                    },
                    {
                        "name": "Maciej PiÃ³ro"
                    },
                    {
                        "name": "Jakub Krajewski"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Marek Cygan"
                    },
                    {
                        "name": "Kamil Adamczewski"
                    },
                    {
                        "name": "Jan Ludziejewski"
                    }
                ],
                "author_detail": {
                    "name": "Jan Ludziejewski"
                },
                "author": "Jan Ludziejewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22243v1",
                "updated": "2025-06-27T14:10:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    10,
                    10,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:10:10Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    10,
                    10,
                    4,
                    178,
                    0
                ],
                "title": "Simulation-based digital twinning of activation and repolarisation\n  sequences from the ECG across healthy and diseased hearts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based digital twinning of activation and repolarisation\n  sequences from the ECG across healthy and diseased hearts"
                },
                "summary": "Abnormal patterns of ventricular repolarisation contribute to lethal\narrhythmias in various cardiac conditions, including inherited and acquired\nchannelopathies, cardiomyopathies, and ischaemic heart disease. However,\nmethods to detect these repolarisation abnormalities are limited.\n  In this study, we introduce and assess a novel simulation-based method to\ninfer ventricular activation and repolarisation times from the 12-lead\nelectrocardiogram (ECG) and magnetic resonance-derived ventricular anatomical\nreconstruction, applicable for the first time to both healthy controls and\ncases with abnormal repolarisation.\n  First, ventricular activation times were reconstructed through iterative\nrefinement of early activation sites and conduction velocities, until the model\nand target QRS complexes matched. Then, ventricular repolarisation times were\nreconstructed through iterative refinement of ventricular action potential\ndurations and an action potential shape parameter until the model and target T\nwaves matched, including regularisation. Repolarisation inference was evaluated\nagainst 18 benchmark simulations with known repolarisation times, including\nboth control and hypertrophic cardiomyopathy (HCM) cases with abnormal\nrepolarisation.\n  Inferred repolarisation times showed good agreement with the ground truth in\ncontrol and HCM (Spearman r=0.63+/-0.11 and 0.65+/-0.19, respectively), with\ninferred model T waves closely matching the target T waves (r=0.81+/-0.05 and\n0.78+/-0.08, respectively). The method further demonstrated flexibility in\nreconstructing the macroscopic patterns of delayed repolarisation across a\nrange of abnormal ventricular repolarisation sequences, demonstrating\napplicability to a range of pathological cases.\n  Simulation-based inference can accurately reconstruct repolarisation times\nfrom the 12-lead ECG in cases with both normal and abnormal repolarisation\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abnormal patterns of ventricular repolarisation contribute to lethal\narrhythmias in various cardiac conditions, including inherited and acquired\nchannelopathies, cardiomyopathies, and ischaemic heart disease. However,\nmethods to detect these repolarisation abnormalities are limited.\n  In this study, we introduce and assess a novel simulation-based method to\ninfer ventricular activation and repolarisation times from the 12-lead\nelectrocardiogram (ECG) and magnetic resonance-derived ventricular anatomical\nreconstruction, applicable for the first time to both healthy controls and\ncases with abnormal repolarisation.\n  First, ventricular activation times were reconstructed through iterative\nrefinement of early activation sites and conduction velocities, until the model\nand target QRS complexes matched. Then, ventricular repolarisation times were\nreconstructed through iterative refinement of ventricular action potential\ndurations and an action potential shape parameter until the model and target T\nwaves matched, including regularisation. Repolarisation inference was evaluated\nagainst 18 benchmark simulations with known repolarisation times, including\nboth control and hypertrophic cardiomyopathy (HCM) cases with abnormal\nrepolarisation.\n  Inferred repolarisation times showed good agreement with the ground truth in\ncontrol and HCM (Spearman r=0.63+/-0.11 and 0.65+/-0.19, respectively), with\ninferred model T waves closely matching the target T waves (r=0.81+/-0.05 and\n0.78+/-0.08, respectively). The method further demonstrated flexibility in\nreconstructing the macroscopic patterns of delayed repolarisation across a\nrange of abnormal ventricular repolarisation sequences, demonstrating\napplicability to a range of pathological cases.\n  Simulation-based inference can accurately reconstruct repolarisation times\nfrom the 12-lead ECG in cases with both normal and abnormal repolarisation\npatterns."
                },
                "authors": [
                    {
                        "name": "James A Coleman"
                    },
                    {
                        "name": "Julia Camps"
                    },
                    {
                        "name": "Abdallah I Hasaballa"
                    },
                    {
                        "name": "Alfonso Bueno-Orovio"
                    }
                ],
                "author_detail": {
                    "name": "Alfonso Bueno-Orovio"
                },
                "author": "Alfonso Bueno-Orovio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08134v3",
                "updated": "2025-06-27T14:00:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    0,
                    6,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-09T18:37:14Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    37,
                    14,
                    0,
                    160,
                    0
                ],
                "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning"
                },
                "summary": "Peer review, the bedrock of scientific advancement in machine learning (ML),\nis strained by a crisis of scale. Exponential growth in manuscript submissions\nto premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite\ncapacity of qualified reviewers, leading to concerns about review quality,\nconsistency, and reviewer fatigue. This position paper argues that AI-assisted\npeer review must become an urgent research and infrastructure priority. We\nadvocate for a comprehensive AI-augmented ecosystem, leveraging Large Language\nModels (LLMs) not as replacements for human judgment, but as sophisticated\ncollaborators for authors, reviewers, and Area Chairs (ACs). We propose\nspecific roles for AI in enhancing factual verification, guiding reviewer\nperformance, assisting authors in quality improvement, and supporting ACs in\ndecision-making. Crucially, we contend that the development of such systems\nhinges on access to more granular, structured, and ethically-sourced peer\nreview process data. We outline a research agenda, including illustrative\nexperiments, to develop and validate these AI assistants, and discuss\nsignificant technical and ethical challenges. We call upon the ML community to\nproactively build this AI-assisted future, ensuring the continued integrity and\nscalability of scientific validation, while maintaining high standards of peer\nreview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review, the bedrock of scientific advancement in machine learning (ML),\nis strained by a crisis of scale. Exponential growth in manuscript submissions\nto premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite\ncapacity of qualified reviewers, leading to concerns about review quality,\nconsistency, and reviewer fatigue. This position paper argues that AI-assisted\npeer review must become an urgent research and infrastructure priority. We\nadvocate for a comprehensive AI-augmented ecosystem, leveraging Large Language\nModels (LLMs) not as replacements for human judgment, but as sophisticated\ncollaborators for authors, reviewers, and Area Chairs (ACs). We propose\nspecific roles for AI in enhancing factual verification, guiding reviewer\nperformance, assisting authors in quality improvement, and supporting ACs in\ndecision-making. Crucially, we contend that the development of such systems\nhinges on access to more granular, structured, and ethically-sourced peer\nreview process data. We outline a research agenda, including illustrative\nexperiments, to develop and validate these AI assistants, and discuss\nsignificant technical and ethical challenges. We call upon the ML community to\nproactively build this AI-assisted future, ensuring the continued integrity and\nscalability of scientific validation, while maintaining high standards of peer\nreview."
                },
                "authors": [
                    {
                        "name": "Qiyao Wei"
                    },
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "18 pages, 3 figures. Position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.5.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10814v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10814v3",
                "updated": "2025-06-27T13:58:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    58,
                    15,
                    4,
                    178,
                    0
                ],
                "published": "2025-01-18T16:23:09Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    16,
                    23,
                    9,
                    5,
                    18,
                    0
                ],
                "title": "No More Sliding Window: Efficient 3D Medical Image Segmentation with\n  Differentiable Top-k Patch Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No More Sliding Window: Efficient 3D Medical Image Segmentation with\n  Differentiable Top-k Patch Sampling"
                },
                "summary": "3D models surpass 2D models in CT/MRI segmentation by effectively capturing\ninter-slice relationships. However, the added depth dimension substantially\nincreases memory consumption. While patch-based training alleviates memory\nconstraints, it significantly slows down the inference speed due to the sliding\nwindow (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel\nend-to-end trainable framework that enhances the efficiency of generic 3D\nsegmentation backbone during an inference step by eliminating the need for SW.\nNMSW employs a differentiable Top-k module to selectively sample only the most\nrelevant patches, thereby minimizing redundant computations. When patch-level\npredictions are insufficient, the framework intelligently leverages coarse\nglobal predictions to refine results. Evaluated across 3 tasks using 3\nsegmentation backbones, NMSW achieves competitive accuracy compared to SW\ninference while significantly reducing computational complexity by 91% (88.0 to\n8.00 TMACs). Moreover, it delivers a 9.1x faster inference on the H100 GPU\n(99.0 to 8.3 sec) and a 11.1x faster inference on the Xeon Gold CPU (2110 to\n189 sec). NMSW is model-agnostic, further boosting efficiency when integrated\nwith any existing efficient segmentation backbones. The code is avaialble:\nhttps://github.com/Youngseok0001/open_nmsw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D models surpass 2D models in CT/MRI segmentation by effectively capturing\ninter-slice relationships. However, the added depth dimension substantially\nincreases memory consumption. While patch-based training alleviates memory\nconstraints, it significantly slows down the inference speed due to the sliding\nwindow (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel\nend-to-end trainable framework that enhances the efficiency of generic 3D\nsegmentation backbone during an inference step by eliminating the need for SW.\nNMSW employs a differentiable Top-k module to selectively sample only the most\nrelevant patches, thereby minimizing redundant computations. When patch-level\npredictions are insufficient, the framework intelligently leverages coarse\nglobal predictions to refine results. Evaluated across 3 tasks using 3\nsegmentation backbones, NMSW achieves competitive accuracy compared to SW\ninference while significantly reducing computational complexity by 91% (88.0 to\n8.00 TMACs). Moreover, it delivers a 9.1x faster inference on the H100 GPU\n(99.0 to 8.3 sec) and a 11.1x faster inference on the Xeon Gold CPU (2110 to\n189 sec). NMSW is model-agnostic, further boosting efficiency when integrated\nwith any existing efficient segmentation backbones. The code is avaialble:\nhttps://github.com/Youngseok0001/open_nmsw."
                },
                "authors": [
                    {
                        "name": "Young Seok Jeon"
                    },
                    {
                        "name": "Hongfei Yang"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10814v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10814v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22232v1",
                "updated": "2025-06-27T13:49:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    37,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:49:37Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    37,
                    4,
                    178,
                    0
                ],
                "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging In-Context Learning for Political Bias Testing of LLMs"
                },
                "summary": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    },
                    {
                        "name": "Lena A. JÃ¤ger"
                    }
                ],
                "author_detail": {
                    "name": "Lena A. JÃ¤ger"
                },
                "author": "Lena A. JÃ¤ger",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22231v1",
                "updated": "2025-06-27T13:49:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:49:02Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    2,
                    4,
                    178,
                    0
                ],
                "title": "Adapting University Policies for Generative AI: Opportunities,\n  Challenges, and Policy Solutions in Higher Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting University Policies for Generative AI: Opportunities,\n  Challenges, and Policy Solutions in Higher Education"
                },
                "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity."
                },
                "authors": [
                    {
                        "name": "Russell Beale"
                    }
                ],
                "author_detail": {
                    "name": "Russell Beale"
                },
                "author": "Russell Beale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1; K.3.2; K.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22228v1",
                "updated": "2025-06-27T13:45:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    45,
                    55,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:45:55Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    45,
                    55,
                    4,
                    178,
                    0
                ],
                "title": "Uncovering smooth structures in single-cell data with PCS-guided\n  neighbor embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering smooth structures in single-cell data with PCS-guided\n  neighbor embeddings"
                },
                "summary": "Single-cell sequencing is revolutionizing biology by enabling detailed\ninvestigations of cell-state transitions. Many biological processes unfold\nalong continuous trajectories, yet it remains challenging to extract smooth,\nlow-dimensional representations from inherently noisy, high-dimensional\nsingle-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,\nare widely used to embed high-dimensional single-cell data into low dimensions.\nBut they often introduce undesirable distortions, resulting in misleading\ninterpretations. Existing evaluation methods for NE algorithms primarily focus\non separating discrete cell types rather than capturing continuous cell-state\ntransitions, while dynamic modeling approaches rely on strong assumptions about\ncellular processes and specialized data. To address these challenges, we build\non the Predictability-Computability-Stability (PCS) framework for reliable and\nreproducible data-driven discoveries. First, we systematically evaluate popular\nNE algorithms through empirical analysis, simulation, and theory, and reveal\ntheir key shortcomings, such as artifacts and instability. We then introduce\nNESS, a principled and interpretable machine learning approach to improve NE\nrepresentations by leveraging algorithmic stability and to enable robust\ninference of smooth biological structures. NESS offers useful concepts,\nquantitative stability metrics, and efficient computational workflows to\nuncover developmental trajectories and cell-state transitions in single-cell\ndata. Finally, we apply NESS to six single-cell datasets, spanning pluripotent\nstem cell differentiation, organoid development, and multiple tissue-specific\nlineage trajectories. Across these diverse contexts, NESS consistently yields\nuseful biological insights, such as identification of transitional and stable\ncell states and quantification of transcriptional dynamics during development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell sequencing is revolutionizing biology by enabling detailed\ninvestigations of cell-state transitions. Many biological processes unfold\nalong continuous trajectories, yet it remains challenging to extract smooth,\nlow-dimensional representations from inherently noisy, high-dimensional\nsingle-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,\nare widely used to embed high-dimensional single-cell data into low dimensions.\nBut they often introduce undesirable distortions, resulting in misleading\ninterpretations. Existing evaluation methods for NE algorithms primarily focus\non separating discrete cell types rather than capturing continuous cell-state\ntransitions, while dynamic modeling approaches rely on strong assumptions about\ncellular processes and specialized data. To address these challenges, we build\non the Predictability-Computability-Stability (PCS) framework for reliable and\nreproducible data-driven discoveries. First, we systematically evaluate popular\nNE algorithms through empirical analysis, simulation, and theory, and reveal\ntheir key shortcomings, such as artifacts and instability. We then introduce\nNESS, a principled and interpretable machine learning approach to improve NE\nrepresentations by leveraging algorithmic stability and to enable robust\ninference of smooth biological structures. NESS offers useful concepts,\nquantitative stability metrics, and efficient computational workflows to\nuncover developmental trajectories and cell-state transitions in single-cell\ndata. Finally, we apply NESS to six single-cell datasets, spanning pluripotent\nstem cell differentiation, organoid development, and multiple tissue-specific\nlineage trajectories. Across these diverse contexts, NESS consistently yields\nuseful biological insights, such as identification of transitional and stable\ncell states and quantification of transcriptional dynamics during development."
                },
                "authors": [
                    {
                        "name": "Rong Ma"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Jingyuan Hu"
                    },
                    {
                        "name": "Bin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yu"
                },
                "author": "Bin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04396v2",
                "updated": "2025-06-27T13:42:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    42,
                    7,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-06T12:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    50,
                    14,
                    3,
                    65,
                    0
                ],
                "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models"
                },
                "summary": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Zejian Yuan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "Accepted by ACL 2025 main conference, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11733v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11733v3",
                "updated": "2025-06-27T13:38:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    38,
                    47,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-17T12:20:39Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    20,
                    39,
                    0,
                    48,
                    0
                ],
                "title": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking\n  Incremental Learning of Situation and Language Model using a Text-Simulated\n  Situated Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking\n  Incremental Learning of Situation and Language Model using a Text-Simulated\n  Situated Environment"
                },
                "summary": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments."
                },
                "authors": [
                    {
                        "name": "Jonathan Jordan"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "Accepted at The 28th International Conference of Text, Speech and\n  Dialogue (TSD2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11733v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11733v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22216v1",
                "updated": "2025-06-27T13:35:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    35,
                    34,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:35:34Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    35,
                    34,
                    4,
                    178,
                    0
                ],
                "title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep\n  Reinforcement Learning"
                },
                "summary": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement."
                },
                "authors": [
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Pingping Liu"
                    },
                    {
                        "name": "Tongshun Zhang"
                    },
                    {
                        "name": "Zhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Zhang"
                },
                "author": "Zhe Zhang",
                "arxiv_comment": "6 pages, 8 figures, accepted by ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17944v2",
                "updated": "2025-06-27T13:30:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    30,
                    9,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-22T08:40:56Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    8,
                    40,
                    56,
                    6,
                    173,
                    0
                ],
                "title": "SegChange-R1: LLM-Augmented Remote Sensing Change Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegChange-R1: LLM-Augmented Remote Sensing Change Detection"
                },
                "summary": "Remote sensing change detection is used in urban planning, terrain analysis,\nand environmental monitoring by analyzing feature changes in the same area over\ntime. In this paper, we propose a large language model (LLM) augmented\ninference approach (SegChange-R1), which enhances the detection capability by\nintegrating textual descriptive information and guides the model to focus on\nrelevant change regions, accelerating convergence. We designed a linear\nattention-based spatial transformation module (BEV) to address modal\nmisalignment by unifying features from different times into a BEV space.\nFurthermore, we introduce DVCD, a novel dataset for building change detection\nfrom UAV viewpoints. Experiments on four widely-used datasets demonstrate\nsignificant improvements over existing method The code and pre-trained models\nare available in {https://github.com/Yu-Zhouz/SegChange-R1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing change detection is used in urban planning, terrain analysis,\nand environmental monitoring by analyzing feature changes in the same area over\ntime. In this paper, we propose a large language model (LLM) augmented\ninference approach (SegChange-R1), which enhances the detection capability by\nintegrating textual descriptive information and guides the model to focus on\nrelevant change regions, accelerating convergence. We designed a linear\nattention-based spatial transformation module (BEV) to address modal\nmisalignment by unifying features from different times into a BEV space.\nFurthermore, we introduce DVCD, a novel dataset for building change detection\nfrom UAV viewpoints. Experiments on four widely-used datasets demonstrate\nsignificant improvements over existing method The code and pre-trained models\nare available in {https://github.com/Yu-Zhouz/SegChange-R1}."
                },
                "authors": [
                    {
                        "name": "Fei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Fei Zhou"
                },
                "author": "Fei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22210v1",
                "updated": "2025-06-27T13:29:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    29,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:29:25Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    29,
                    25,
                    4,
                    178,
                    0
                ],
                "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses"
                },
                "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality."
                },
                "authors": [
                    {
                        "name": "Weronika Åajewska"
                    },
                    {
                        "name": "Ivica Kostric"
                    },
                    {
                        "name": "Gabriel Iturra-Bocaz"
                    },
                    {
                        "name": "Mariam Arustashvili"
                    },
                    {
                        "name": "Krisztian Balog"
                    }
                ],
                "author_detail": {
                    "name": "Krisztian Balog"
                },
                "author": "Krisztian Balog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v1",
                "updated": "2025-06-27T13:09:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhi Zhang"
                },
                "author": "Yuzhi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22189v1",
                "updated": "2025-06-27T12:57:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    57,
                    0,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:57:00Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    57,
                    0,
                    4,
                    178,
                    0
                ],
                "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Modularity of Agentic Systems for Drug Discovery"
                },
                "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems."
                },
                "authors": [
                    {
                        "name": "Laura van Weesep"
                    },
                    {
                        "name": "Samuel Genheden"
                    },
                    {
                        "name": "Ola Engkvist"
                    },
                    {
                        "name": "Jens SjÃ¶lund"
                    }
                ],
                "author_detail": {
                    "name": "Jens SjÃ¶lund"
                },
                "author": "Jens SjÃ¶lund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03313v2",
                "updated": "2025-06-27T12:53:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    53,
                    42,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-05T09:45:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models"
                },
                "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."
                },
                "authors": [
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22182v1",
                "updated": "2025-06-27T12:45:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    45,
                    44,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:45:44Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    45,
                    44,
                    4,
                    178,
                    0
                ],
                "title": "Average-case complexity in statistical inference: A puzzle-driven\n  research seminar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Average-case complexity in statistical inference: A puzzle-driven\n  research seminar"
                },
                "summary": "These notes describe our experience with running a student seminar on\naverage-case complexity in statistical inference using the jigsaw learning\nformat at ETH Zurich in Fall of 2024. The jigsaw learning technique is an\nactive learning technique where students work in groups on independent parts of\nthe task and then reassemble the groups to combine all the parts together. We\nimplemented this technique for the proofs of various recent research\ndevelopments, combined with a presentation by one of the students in the\nbeginning of the session. We describe our experience and thoughts on such a\nformat applied in a student research seminar: including, but not limited to,\nhigher engagement, more accessible talks by the students, and increased student\nparticipation in discussions. In the Appendix, we include all the exercises\nsheets for the topic, which may be of independent interest for courses on\nstatistical-to-computational gaps and average-case complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "These notes describe our experience with running a student seminar on\naverage-case complexity in statistical inference using the jigsaw learning\nformat at ETH Zurich in Fall of 2024. The jigsaw learning technique is an\nactive learning technique where students work in groups on independent parts of\nthe task and then reassemble the groups to combine all the parts together. We\nimplemented this technique for the proofs of various recent research\ndevelopments, combined with a presentation by one of the students in the\nbeginning of the session. We describe our experience and thoughts on such a\nformat applied in a student research seminar: including, but not limited to,\nhigher engagement, more accessible talks by the students, and increased student\nparticipation in discussions. In the Appendix, we include all the exercises\nsheets for the topic, which may be of independent interest for courses on\nstatistical-to-computational gaps and average-case complexity."
                },
                "authors": [
                    {
                        "name": "Anastasia Kireeva"
                    },
                    {
                        "name": "Afonso S. Bandeira"
                    }
                ],
                "author_detail": {
                    "name": "Afonso S. Bandeira"
                },
                "author": "Afonso S. Bandeira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97D40, 97K50, 97K80, 97P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12380v2",
                "updated": "2025-06-27T12:45:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    45,
                    33,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-18T11:53:01Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    11,
                    53,
                    1,
                    6,
                    138,
                    0
                ],
                "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL\n  via Graph Matching and Stepwise Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL\n  via Graph Matching and Stepwise Reward"
                },
                "summary": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models."
                },
                "authors": [
                    {
                        "name": "Han Weng"
                    },
                    {
                        "name": "Puzhen Wu"
                    },
                    {
                        "name": "Cui Longjie"
                    },
                    {
                        "name": "Yi Zhan"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Dun Zeng"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Xiaoming Yin"
                    },
                    {
                        "name": "Yang Sun"
                    },
                    {
                        "name": "Xing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xing Chen"
                },
                "author": "Xing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13488v2",
                "updated": "2025-06-27T12:34:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    34,
                    59,
                    4,
                    178,
                    0
                ],
                "published": "2024-12-18T04:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    4,
                    14,
                    35,
                    2,
                    353,
                    0
                ],
                "title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language\n  Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT, while our open-source framework\nestablishes a reproducible benchmark for future research, which is available at\n[https://github.com/0-ml/speft].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT, while our open-source framework\nestablishes a reproducible benchmark for future research, which is available at\n[https://github.com/0-ml/speft]."
                },
                "authors": [
                    {
                        "name": "Xinxin Liu"
                    },
                    {
                        "name": "Aaron Thomas"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18193v2",
                "updated": "2025-06-27T12:26:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    26,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-23T18:00:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    18,
                    0,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "Characterising the z $\\sim$ 7.66 Type-II AGN candidate SMACS S06355\n  using BEAGLE-AGN and JWST NIRSpec/NIRCam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising the z $\\sim$ 7.66 Type-II AGN candidate SMACS S06355\n  using BEAGLE-AGN and JWST NIRSpec/NIRCam"
                },
                "summary": "The presence of Active Galactic Nuclei (AGN) in low mass (Mstar $\\lesssim$\n$10^{9}$ Msun) galaxies at high redshift has been established, and it is\nimportant to characterise these objects and the impact of their feedback on the\nhost galaxies. In this paper we apply the Spectral Energy Distribution (SED)\nfitting code BEAGLE-AGN to SMACS S06355, a z $\\sim$ 7.66 Type-II AGN candidate\nfrom the JWST NIRSpec Early Release Observations. This object's spectrum\nincludes a detection of the [NeIV]2426 line, indicating an obscured AGN due to\nits high ionization potential energy ($\\sim$ 63eV). We use BEAGLE-AGN to\nsimultaneously model the Narrow Line Region (NLR) AGN and star-forming galaxy\ncontributions to the observed line fluxes and photometry. Having a\nhigh-ionization emission line allows the contribution of the NLR to the\nremaining lines to be probabilistically disentangled. The HII region\nmetallicity is derived to be 12+log(O/H)$^{\\mathrm{HII}}$ =\n$7.82^{+0.18}_{-0.19}$. Assuming that the Neon-to-Oxygen abundance is similar\nto solar we derive a high NLR metallicity of 12+log(O/H)$^\\mathrm{{NLR}}$ =\n$8.86^{+0.14}_{-0.16}$, with the 2$\\sigma$ lower-limit extending to\n12+log(O/H)$^{\\mathrm{NLR}}$ $\\sim$ 8.54, showing the derivation is uncertain.\nWe discuss this result with respect to non-solar Neon abundances that might\nboost the inferred NLR metallicity. The NLR metallicity places SMACS S06355 in\na comparable region of the mass-metallicity plane to intermediate (1.5\n$\\lesssim$ z $\\lesssim$ 3.0) redshift obscured AGN. Our derived accretion disc\nluminosity, log($L_{acc}$ / erg $s^{-1}$) = $45.19^{+0.12}_{-0.11}$, is\nmoderately high yet still uncertain. We highlight that deviations between\nbolometric luminosity calibrations and model grid tracks become enhanced at low\nmetallicities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of Active Galactic Nuclei (AGN) in low mass (Mstar $\\lesssim$\n$10^{9}$ Msun) galaxies at high redshift has been established, and it is\nimportant to characterise these objects and the impact of their feedback on the\nhost galaxies. In this paper we apply the Spectral Energy Distribution (SED)\nfitting code BEAGLE-AGN to SMACS S06355, a z $\\sim$ 7.66 Type-II AGN candidate\nfrom the JWST NIRSpec Early Release Observations. This object's spectrum\nincludes a detection of the [NeIV]2426 line, indicating an obscured AGN due to\nits high ionization potential energy ($\\sim$ 63eV). We use BEAGLE-AGN to\nsimultaneously model the Narrow Line Region (NLR) AGN and star-forming galaxy\ncontributions to the observed line fluxes and photometry. Having a\nhigh-ionization emission line allows the contribution of the NLR to the\nremaining lines to be probabilistically disentangled. The HII region\nmetallicity is derived to be 12+log(O/H)$^{\\mathrm{HII}}$ =\n$7.82^{+0.18}_{-0.19}$. Assuming that the Neon-to-Oxygen abundance is similar\nto solar we derive a high NLR metallicity of 12+log(O/H)$^\\mathrm{{NLR}}$ =\n$8.86^{+0.14}_{-0.16}$, with the 2$\\sigma$ lower-limit extending to\n12+log(O/H)$^{\\mathrm{NLR}}$ $\\sim$ 8.54, showing the derivation is uncertain.\nWe discuss this result with respect to non-solar Neon abundances that might\nboost the inferred NLR metallicity. The NLR metallicity places SMACS S06355 in\na comparable region of the mass-metallicity plane to intermediate (1.5\n$\\lesssim$ z $\\lesssim$ 3.0) redshift obscured AGN. Our derived accretion disc\nluminosity, log($L_{acc}$ / erg $s^{-1}$) = $45.19^{+0.12}_{-0.11}$, is\nmoderately high yet still uncertain. We highlight that deviations between\nbolometric luminosity calibrations and model grid tracks become enhanced at low\nmetallicities."
                },
                "authors": [
                    {
                        "name": "M. S. Silcock"
                    },
                    {
                        "name": "E. Curtis-Lake"
                    },
                    {
                        "name": "D. J. B. Smith"
                    },
                    {
                        "name": "I. E. B. Wallace"
                    },
                    {
                        "name": "A. Vidal-GarcÃ­a"
                    },
                    {
                        "name": "A. Plat"
                    },
                    {
                        "name": "M. Hirschmann"
                    },
                    {
                        "name": "A. Feltre"
                    },
                    {
                        "name": "J. Chevallard"
                    },
                    {
                        "name": "S. Charlot"
                    },
                    {
                        "name": "S. Carniani"
                    },
                    {
                        "name": "A. J. Bunker"
                    }
                ],
                "author_detail": {
                    "name": "A. J. Bunker"
                },
                "author": "A. J. Bunker",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04412v3",
                "updated": "2025-06-27T12:18:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    18,
                    30,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-06T13:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search"
                },
                "summary": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel\ninference-time framework that generalizes repeated sampling with principled\nmulti-turn exploration and exploitation. At each node in the search tree,\nAB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate\nresponses or \"go deeper\" by revisiting existing ones based on external feedback\nsignals. We evaluate our method on complex coding and engineering tasks using\nfrontier models. Empirical results show that AB-MCTS consistently outperforms\nboth repeated sampling and standard MCTS, underscoring the importance of\ncombining the response diversity of LLMs with multi-turn solution refinement\nfor effective inference-time scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel\ninference-time framework that generalizes repeated sampling with principled\nmulti-turn exploration and exploitation. At each node in the search tree,\nAB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate\nresponses or \"go deeper\" by revisiting existing ones based on external feedback\nsignals. We evaluate our method on complex coding and engineering tasks using\nfrontier models. Empirical results show that AB-MCTS consistently outperforms\nboth repeated sampling and standard MCTS, underscoring the importance of\ncombining the response diversity of LLMs with multi-turn solution refinement\nfor effective inference-time scaling."
                },
                "authors": [
                    {
                        "name": "Yuichi Inoue"
                    },
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "So Kuroki"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "Presented at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22157v1",
                "updated": "2025-06-27T12:10:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    10,
                    57,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:10:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    10,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "Training Language Model to Critique for Better Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Model to Critique for Better Refinement"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops."
                },
                "authors": [
                    {
                        "name": "Tianshu Yu"
                    },
                    {
                        "name": "Chao Xiang"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xinyu Mu"
                    },
                    {
                        "name": "Chuxiong Sun"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22147v1",
                "updated": "2025-06-27T11:45:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    45,
                    47,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T11:45:47Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    45,
                    47,
                    4,
                    178,
                    0
                ],
                "title": "JADES reveals a large population of low mass black holes at high\n  redshift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES reveals a large population of low mass black holes at high\n  redshift"
                },
                "summary": "JWST has revealed a large population of active galactic nuclei (AGN) in the\ndistant universe, which are challenging our understanding of early massive\nblack hole seeding and growth. We expand the exploration of this population to\nlower luminosities by stacking $\\sim 600$ NIRSpec grating spectra from the JWST\nAdvanced Deep Extragalactic Survey (JADES) at $3<z<7$, in bins of redshift,\n[OIII]5007 luminosity and equivalent width, UV luminosity and stellar mass. In\nvarious stacks, we detect a broad component of H$\\alpha$ without a counterpart\nin [OIII], implying that it is not due to outflows but is tracing the Broad\nLine Region (BLR) of a large population of low-luminosity AGN not detected in\nindividual spectra. We also consider the possible contribution from Supernovae\n(SNe) and Very Massive Stars and conclude that while this is very unlikely, we\ncannot exclude some potential contribution by SNe to some of the stacks. The\ndetection, in some stacks, of high [OIII]4363/H$\\gamma$, typical of AGN,\nfurther confirms that such stacks reveal a large population of AGN. We infer\nthat the stacks probe black holes with masses of a few times $10^6~M_\\odot$\naccreting at rates $L/L_{Edd}\\sim 0.02-0.1$, i.e. a low mass and dormant\nparameter space poorly explored by previous studies on individual targets. We\nidentify populations of black holes that fall within the scatter of the local\n$M_{BH}-M_{*}$ scaling relation, indicating that there is a population of\nhigh-z BHs that are not overmassive relative to their host galaxies and which\nhave been mostly missed in previous JWST observations. Yet, on average, the\nstacks are still overmassive relative the local relation, with some of them 1-2\ndex above it. We infer that the BH mass function (BHMF) at $3<z<5$ rises\nsteeply at low masses. The BHMF is consistent with models in which BHs evolve\nthrough short bursts of super-Eddington accretion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST has revealed a large population of active galactic nuclei (AGN) in the\ndistant universe, which are challenging our understanding of early massive\nblack hole seeding and growth. We expand the exploration of this population to\nlower luminosities by stacking $\\sim 600$ NIRSpec grating spectra from the JWST\nAdvanced Deep Extragalactic Survey (JADES) at $3<z<7$, in bins of redshift,\n[OIII]5007 luminosity and equivalent width, UV luminosity and stellar mass. In\nvarious stacks, we detect a broad component of H$\\alpha$ without a counterpart\nin [OIII], implying that it is not due to outflows but is tracing the Broad\nLine Region (BLR) of a large population of low-luminosity AGN not detected in\nindividual spectra. We also consider the possible contribution from Supernovae\n(SNe) and Very Massive Stars and conclude that while this is very unlikely, we\ncannot exclude some potential contribution by SNe to some of the stacks. The\ndetection, in some stacks, of high [OIII]4363/H$\\gamma$, typical of AGN,\nfurther confirms that such stacks reveal a large population of AGN. We infer\nthat the stacks probe black holes with masses of a few times $10^6~M_\\odot$\naccreting at rates $L/L_{Edd}\\sim 0.02-0.1$, i.e. a low mass and dormant\nparameter space poorly explored by previous studies on individual targets. We\nidentify populations of black holes that fall within the scatter of the local\n$M_{BH}-M_{*}$ scaling relation, indicating that there is a population of\nhigh-z BHs that are not overmassive relative to their host galaxies and which\nhave been mostly missed in previous JWST observations. Yet, on average, the\nstacks are still overmassive relative the local relation, with some of them 1-2\ndex above it. We infer that the BH mass function (BHMF) at $3<z<5$ rises\nsteeply at low masses. The BHMF is consistent with models in which BHs evolve\nthrough short bursts of super-Eddington accretion."
                },
                "authors": [
                    {
                        "name": "Sophia Geris"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Yuki Isobe"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Ignas Juodzbalis"
                    },
                    {
                        "name": "Charlotte Simmonds"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Alessandro Trinca"
                    },
                    {
                        "name": "Raffaella Schneider"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stephane Charlot"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Pierluigi Rinaldi"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Hannah Uebler"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_comment": "Submitted, 33 pages, 25 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22146v1",
                "updated": "2025-06-27T11:44:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    44,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T11:44:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    44,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem\n  in VLMs"
                },
                "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Izadi"
                    },
                    {
                        "name": "Mohammad Ali Banayeeanzade"
                    },
                    {
                        "name": "Fatemeh Askari"
                    },
                    {
                        "name": "Ali Rahimiakbar"
                    },
                    {
                        "name": "Mohammad Mahdi Vahedi"
                    },
                    {
                        "name": "Hosein Hasani"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    }
                ],
                "author_detail": {
                    "name": "Mahdieh Soleymani Baghshah"
                },
                "author": "Mahdieh Soleymani Baghshah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24616v3",
                "updated": "2025-06-27T11:43:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    43,
                    3,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-30T14:08:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX"
                },
                "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."
                },
                "authors": [
                    {
                        "name": "Nikita Martynov"
                    },
                    {
                        "name": "Anastasia Mordasheva"
                    },
                    {
                        "name": "Dmitriy Gorbetskiy"
                    },
                    {
                        "name": "Danil Astafurov"
                    },
                    {
                        "name": "Ulyana Isaeva"
                    },
                    {
                        "name": "Elina Basyrova"
                    },
                    {
                        "name": "Sergey Skachkov"
                    },
                    {
                        "name": "Victoria Berestova"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Valeriia Zanina"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "arxiv_comment": "178 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22139v1",
                "updated": "2025-06-27T11:30:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    30,
                    51,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T11:30:51Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    30,
                    51,
                    4,
                    178,
                    0
                ],
                "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Shaojie Zhang"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Jianqin Yin"
                    },
                    {
                        "name": "Zhenbo Luo"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20362v2",
                "updated": "2025-06-27T11:25:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    25,
                    29,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-26T09:39:58Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    39,
                    58,
                    2,
                    85,
                    0
                ],
                "title": "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video\n  Understanding"
                },
                "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget."
                },
                "authors": [
                    {
                        "name": "Joao Pereira"
                    },
                    {
                        "name": "Vasco Lopes"
                    },
                    {
                        "name": "David Semedo"
                    },
                    {
                        "name": "Joao Neves"
                    }
                ],
                "author_detail": {
                    "name": "Joao Neves"
                },
                "author": "Joao Neves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22135v1",
                "updated": "2025-06-27T11:23:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    23,
                    52,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T11:23:52Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    23,
                    52,
                    4,
                    178,
                    0
                ],
                "title": "Brownian motion, bridges and Bayesian inference in phylogenetic tree\n  space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brownian motion, bridges and Bayesian inference in phylogenetic tree\n  space"
                },
                "summary": "Billera-Holmes-Vogtmann (BHV) tree space is a geodesic metric space of\nedge-weighted phylogenetic trees with a fixed leaf set. Constructing parametric\ndistributions on this space is challenging due to its non-Euclidean geometry\nand the intractability of normalizing constants. We address this by fitting\nBrownian motion transition kernels to tree-valued data via a non-Euclidean\nbridge construction. Each kernel is determined by a source tree $x_0$ (the\nBrownian motion's starting point) and a dispersion parameter $t_0$ (its\nduration). Observed trees are modelled as independent draws from the transition\nkernel defined by $(x_0, t_0)$, analogous to a Gaussian model in Euclidean\nspace. Brownian motion is approximated by an $m$-step random walk, with the\nparameter space augmented to include full sample paths. We develop a bridge\nalgorithm to sample paths conditional on their endpoints, and introduce methods\nfor sampling a Bayesian posterior for $(x_0, t_0)$ and for marginal likelihood\nevaluation. This enables hypothesis testing for alternative source trees. The\napproach is validated on simulated data and applied to an experimental data set\nof yeast gene trees. These methods provide a foundation for future development\nof a wider class of probabilistic models of tree-valued data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Billera-Holmes-Vogtmann (BHV) tree space is a geodesic metric space of\nedge-weighted phylogenetic trees with a fixed leaf set. Constructing parametric\ndistributions on this space is challenging due to its non-Euclidean geometry\nand the intractability of normalizing constants. We address this by fitting\nBrownian motion transition kernels to tree-valued data via a non-Euclidean\nbridge construction. Each kernel is determined by a source tree $x_0$ (the\nBrownian motion's starting point) and a dispersion parameter $t_0$ (its\nduration). Observed trees are modelled as independent draws from the transition\nkernel defined by $(x_0, t_0)$, analogous to a Gaussian model in Euclidean\nspace. Brownian motion is approximated by an $m$-step random walk, with the\nparameter space augmented to include full sample paths. We develop a bridge\nalgorithm to sample paths conditional on their endpoints, and introduce methods\nfor sampling a Bayesian posterior for $(x_0, t_0)$ and for marginal likelihood\nevaluation. This enables hypothesis testing for alternative source trees. The\napproach is validated on simulated data and applied to an experimental data set\nof yeast gene trees. These methods provide a foundation for future development\nof a wider class of probabilistic models of tree-valued data."
                },
                "authors": [
                    {
                        "name": "William M. Woodman"
                    },
                    {
                        "name": "Tom M. W. Nye"
                    }
                ],
                "author_detail": {
                    "name": "Tom M. W. Nye"
                },
                "author": "Tom M. W. Nye",
                "arxiv_comment": "21 pages plus appendix of 22 pages. 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60J65 (Primary) 92D15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09338v2",
                "updated": "2025-06-27T11:15:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    15,
                    26,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-14T12:33:05Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    33,
                    5,
                    2,
                    134,
                    0
                ],
                "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs"
                },
                "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem."
                },
                "authors": [
                    {
                        "name": "Jingcheng Niu"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Hamidreza Saghir"
                    },
                    {
                        "name": "Amir H. Abdi"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Abdi"
                },
                "author": "Amir H. Abdi",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08837v3",
                "updated": "2025-06-27T11:13:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    13,
                    27,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-10T14:23:55Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    23,
                    55,
                    1,
                    161,
                    0
                ],
                "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Patterns for Securing LLM Agents against Prompt Injections"
                },
                "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies."
                },
                "authors": [
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Beat Buesser"
                    },
                    {
                        "name": "Ana-Maria CreÅ£u"
                    },
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Daniel Dobos"
                    },
                    {
                        "name": "Daniel Fabian"
                    },
                    {
                        "name": "Marc Fischer"
                    },
                    {
                        "name": "David Froelicher"
                    },
                    {
                        "name": "Kathrin Grosse"
                    },
                    {
                        "name": "Daniel Naeff"
                    },
                    {
                        "name": "Ezinwanne Ozoani"
                    },
                    {
                        "name": "Andrew Paverd"
                    },
                    {
                        "name": "Florian TramÃ¨r"
                    },
                    {
                        "name": "VÃ¡clav Volhejn"
                    }
                ],
                "author_detail": {
                    "name": "VÃ¡clav Volhejn"
                },
                "author": "VÃ¡clav Volhejn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15022v2",
                "updated": "2025-06-27T11:12:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    12,
                    53,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-17T23:18:10Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    23,
                    18,
                    10,
                    1,
                    168,
                    0
                ],
                "title": "The Orbit of WASP-4 b is in Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Orbit of WASP-4 b is in Decay"
                },
                "summary": "WASP-4 b is a hot Jupiter exhibiting a decreasing orbital period, prompting\ninvestigations into potential mechanisms driving its evolution. We analyzed 173\ntransit light curves, including 37 new observations, and derived mid-transit\ntimings with EXOFAST, forming the most extensive TTV dataset for this system.\nAdding 58 literature timings and removing unreliable data, we constructed a TTV\ndiagram with 216 points. Our analysis considered linear, quadratic, and apsidal\nmotion models, with the quadratic model proving to be significantly superior in\nall model comparison statistics. We found no significant periodic signals in\nthe data. The quadratic model allows us to infer a tidal quality factor of Q' ~\n80,000 from the orbital decay rate if this is due to stellar tides. Theoretical\nconsiderations indicate that such efficient dissipation is possible due to\ninternal gravity waves in the radiative core of WASP-4, but only in our models\nwith a more evolved host star, possibly near the end of its main-sequence\nlifetime, and with a larger radius than the observed one. Our main-sequence\nmodels produce only about a third of the required dissipation (Q' ~ 200,000 -\n500,000). Therefore, the observed orbital decay can only be explained by a\nslightly larger or more evolved host, resembling the case for WASP-12. Our\nfindings highlight the need for further stellar modeling and improvement in our\ncurrent understanding of tidal dissipation mechanisms driving orbital decay in\nclose-in exoplanetary systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WASP-4 b is a hot Jupiter exhibiting a decreasing orbital period, prompting\ninvestigations into potential mechanisms driving its evolution. We analyzed 173\ntransit light curves, including 37 new observations, and derived mid-transit\ntimings with EXOFAST, forming the most extensive TTV dataset for this system.\nAdding 58 literature timings and removing unreliable data, we constructed a TTV\ndiagram with 216 points. Our analysis considered linear, quadratic, and apsidal\nmotion models, with the quadratic model proving to be significantly superior in\nall model comparison statistics. We found no significant periodic signals in\nthe data. The quadratic model allows us to infer a tidal quality factor of Q' ~\n80,000 from the orbital decay rate if this is due to stellar tides. Theoretical\nconsiderations indicate that such efficient dissipation is possible due to\ninternal gravity waves in the radiative core of WASP-4, but only in our models\nwith a more evolved host star, possibly near the end of its main-sequence\nlifetime, and with a larger radius than the observed one. Our main-sequence\nmodels produce only about a third of the required dissipation (Q' ~ 200,000 -\n500,000). Therefore, the observed orbital decay can only be explained by a\nslightly larger or more evolved host, resembling the case for WASP-12. Our\nfindings highlight the need for further stellar modeling and improvement in our\ncurrent understanding of tidal dissipation mechanisms driving orbital decay in\nclose-in exoplanetary systems."
                },
                "authors": [
                    {
                        "name": "Ã. BaÅtÃ¼rk"
                    },
                    {
                        "name": "A. C. Kutluay"
                    },
                    {
                        "name": "A. Barker"
                    },
                    {
                        "name": "S. YalÃ§Ä±nkaya"
                    },
                    {
                        "name": "J. Southworth"
                    },
                    {
                        "name": "K. Barkaoui"
                    },
                    {
                        "name": "A. WÃ¼nsche"
                    },
                    {
                        "name": "M. J. Burgdorf"
                    },
                    {
                        "name": "M. Timmermans"
                    },
                    {
                        "name": "E. Jehin"
                    },
                    {
                        "name": "J. Tregloan-Reed"
                    },
                    {
                        "name": "R. Figuera Jaimes"
                    },
                    {
                        "name": "T. C. Hinse"
                    },
                    {
                        "name": "B. Duru"
                    },
                    {
                        "name": "J. Hitchcock"
                    },
                    {
                        "name": "P. Longa-PeÃ±a"
                    },
                    {
                        "name": "S. Rahvar"
                    },
                    {
                        "name": "S. Sajadian"
                    },
                    {
                        "name": "M. Bretton"
                    },
                    {
                        "name": "S. O. Selam"
                    },
                    {
                        "name": "M. Gillon"
                    },
                    {
                        "name": "M. Bonavita"
                    },
                    {
                        "name": "G. D'Ago"
                    },
                    {
                        "name": "M. Dominik"
                    },
                    {
                        "name": "U. G. JÃ¸rgensen"
                    },
                    {
                        "name": "C. Snodgrass"
                    },
                    {
                        "name": "P. Spyratos"
                    },
                    {
                        "name": "L. Mancini"
                    }
                ],
                "author_detail": {
                    "name": "L. Mancini"
                },
                "author": "L. Mancini",
                "arxiv_comment": "Accepted for publication by the Monthly Notices of the Royal\n  Astronomical Society on June 13, 2025. Contains 18 pages and 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03592v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03592v3",
                "updated": "2025-06-27T10:34:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    34,
                    56,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-05T15:26:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance"
                },
                "summary": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance."
                },
                "authors": [
                    {
                        "name": "Karl Audun Borgersen"
                    },
                    {
                        "name": "Morten Goodwin"
                    }
                ],
                "author_detail": {
                    "name": "Morten Goodwin"
                },
                "author": "Morten Goodwin",
                "arxiv_comment": "8 pages, 6 figures, v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03592v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03592v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07495v2",
                "updated": "2025-06-27T10:33:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    33,
                    27,
                    4,
                    178,
                    0
                ],
                "published": "2024-07-10T09:27:23Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    9,
                    27,
                    23,
                    2,
                    192,
                    0
                ],
                "title": "Beyond Fixed Length: Bucket Pre-training is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Fixed Length: Bucket Pre-training is All You Need"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious tasks, with pre-training stage serving as the cornerstone of their\ncapabilities. However, the conventional fixed-length data composition strategy\nfor pre-training presents several practical challenges. When using shorter\nsequences, documents are often truncated, potentially leading to information\nloss and affecting the model's ability to capture long-range dependencies.\nConversely, longer sequences require concatenation of multiple documents, which\ncan introduce noise and affect the natural document boundaries and semantic\ncoherence as well as require substantial computational overhead. To address\nthese challenges, we first establish three quantitative metrics for evaluating\ndata composition quality: padding ratio, truncation ratio, and concatenation\nratio. Building upon these metrics, we propose a novel multi-bucket data\ncomposition method that transcends the fixed-length paradigm. Our approach\nadaptively organizes training data to achieve optimal composition quality as\nmeasured by the proposed metrics, offering a more flexible and efficient\napproach for pre-training. We conduct extensive experiments and the results\ndemonstrate that our proposed method significantly enhances both the efficiency\nand effectiveness of LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious tasks, with pre-training stage serving as the cornerstone of their\ncapabilities. However, the conventional fixed-length data composition strategy\nfor pre-training presents several practical challenges. When using shorter\nsequences, documents are often truncated, potentially leading to information\nloss and affecting the model's ability to capture long-range dependencies.\nConversely, longer sequences require concatenation of multiple documents, which\ncan introduce noise and affect the natural document boundaries and semantic\ncoherence as well as require substantial computational overhead. To address\nthese challenges, we first establish three quantitative metrics for evaluating\ndata composition quality: padding ratio, truncation ratio, and concatenation\nratio. Building upon these metrics, we propose a novel multi-bucket data\ncomposition method that transcends the fixed-length paradigm. Our approach\nadaptively organizes training data to achieve optimal composition quality as\nmeasured by the proposed metrics, offering a more flexible and efficient\napproach for pre-training. We conduct extensive experiments and the results\ndemonstrate that our proposed method significantly enhances both the efficiency\nand effectiveness of LLM pre-training."
                },
                "authors": [
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Hongtao Liu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "8 pages, 5 figures, 3 tables. Accetped by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16383v2",
                "updated": "2025-06-27T10:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    25,
                    12,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-19T15:12:58Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    15,
                    12,
                    58,
                    3,
                    170,
                    0
                ],
                "title": "Large Language Models in Argument Mining: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Argument Mining: A Survey"
                },
                "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Yizheng Sun"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "Work draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11108v2",
                "updated": "2025-06-27T10:17:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    17,
                    13,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-15T11:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    55,
                    24,
                    1,
                    105,
                    0
                ],
                "title": "Benchmarking Vision Language Models on German Factual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vision Language Models on German Factual Data"
                },
                "summary": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages."
                },
                "authors": [
                    {
                        "name": "RenÃ© Peinl"
                    },
                    {
                        "name": "Vincent Tischler"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Tischler"
                },
                "author": "Vincent Tischler",
                "arxiv_comment": "Peinl, Ren\\'e; Tischler, Vincent (2025): Benchmarking Vision Language\n  Models on German Factual Data. 21st International Conference on Artificial\n  Intelligence Applications and Innovations, 26-29 June, 2025, Limassol, Cyprus\n  (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45 (Primary), 68T07 (Secondary), 68T10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05958v3",
                "updated": "2025-06-27T10:11:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    11,
                    0,
                    4,
                    178,
                    0
                ],
                "published": "2024-12-08T14:34:30Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    14,
                    34,
                    30,
                    6,
                    343,
                    0
                ],
                "title": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension"
                },
                "summary": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub."
                },
                "authors": [
                    {
                        "name": "Adem Ait"
                    },
                    {
                        "name": "Javier Luis CÃ¡novas Izquierdo"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "Accepted in the Euromicro Conference Series on Software Engineering\n  and Advanced Applications (SEAA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19367v3",
                "updated": "2025-06-27T10:09:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    9,
                    46,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-25T05:48:31Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    5,
                    48,
                    31,
                    1,
                    84,
                    0
                ],
                "title": "VGAT: A Cancer Survival Analysis Framework Transitioning from Generative\n  Visual Question Answering to Genomic Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGAT: A Cancer Survival Analysis Framework Transitioning from Generative\n  Visual Question Answering to Genomic Reconstruction"
                },
                "summary": "Multimodal learning combining pathology images and genomic sequences enhances\ncancer survival analysis but faces clinical implementation barriers due to\nlimited access to genomic sequencing in under-resourced regions. To enable\nsurvival prediction using only whole-slide images (WSI), we propose the\nVisual-Genomic Answering-Guided Transformer (VGAT), a framework integrating\nVisual Question Answering (VQA) techniques for genomic modality reconstruction.\nBy adapting VQA's text feature extraction approach, we derive stable genomic\nrepresentations that circumvent dimensionality challenges in raw genomic data.\nSimultaneously, a cluster-based visual prompt module selectively enhances\ndiscriminative WSI patches, addressing noise from unfiltered image regions.\nEvaluated across five TCGA datasets, VGAT outperforms existing WSI-only\nmethods, demonstrating the viability of genomic-informed inference without\nsequencing. This approach bridges multimodal research and clinical feasibility\nin resource-constrained settings. The code link is\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VGAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal learning combining pathology images and genomic sequences enhances\ncancer survival analysis but faces clinical implementation barriers due to\nlimited access to genomic sequencing in under-resourced regions. To enable\nsurvival prediction using only whole-slide images (WSI), we propose the\nVisual-Genomic Answering-Guided Transformer (VGAT), a framework integrating\nVisual Question Answering (VQA) techniques for genomic modality reconstruction.\nBy adapting VQA's text feature extraction approach, we derive stable genomic\nrepresentations that circumvent dimensionality challenges in raw genomic data.\nSimultaneously, a cluster-based visual prompt module selectively enhances\ndiscriminative WSI patches, addressing noise from unfiltered image regions.\nEvaluated across five TCGA datasets, VGAT outperforms existing WSI-only\nmethods, demonstrating the viability of genomic-informed inference without\nsequencing. This approach bridges multimodal research and clinical feasibility\nin resource-constrained settings. The code link is\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VGAT."
                },
                "authors": [
                    {
                        "name": "Zizhi Chen"
                    },
                    {
                        "name": "Minghao Han"
                    },
                    {
                        "name": "Xukun Zhang"
                    },
                    {
                        "name": "Shuwei Ma"
                    },
                    {
                        "name": "Tao Liu"
                    },
                    {
                        "name": "Xing Wei"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17066v2",
                "updated": "2025-06-27T10:04:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    4,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-18T16:13:07Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    16,
                    13,
                    7,
                    6,
                    138,
                    0
                ],
                "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration"
                },
                "summary": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community."
                },
                "authors": [
                    {
                        "name": "Tatia Tsmindashvili"
                    },
                    {
                        "name": "Ana Kolkhidashvili"
                    },
                    {
                        "name": "Dachi Kurtskhalia"
                    },
                    {
                        "name": "Nino Maghlakelidze"
                    },
                    {
                        "name": "Elene Mekvabishvili"
                    },
                    {
                        "name": "Guram Dentoshvili"
                    },
                    {
                        "name": "Orkhan Shamilov"
                    },
                    {
                        "name": "Zaal Gachechiladze"
                    },
                    {
                        "name": "Steven Saporta"
                    },
                    {
                        "name": "David Dachi Choladze"
                    }
                ],
                "author_detail": {
                    "name": "David Dachi Choladze"
                },
                "author": "David Dachi Choladze",
                "arxiv_comment": "Under review at IEEE Access. Supplementary material is included in\n  the main PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22075v1",
                "updated": "2025-06-27T10:03:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    3,
                    5,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T10:03:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    3,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "Reasoning in machine vision: learning to think fast and slow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning in machine vision: learning to think fast and slow"
                },
                "summary": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning."
                },
                "authors": [
                    {
                        "name": "Shaheer U. Saeed"
                    },
                    {
                        "name": "Yipei Wang"
                    },
                    {
                        "name": "Veeru Kasivisvanathan"
                    },
                    {
                        "name": "Brian R. Davidson"
                    },
                    {
                        "name": "Matthew J. Clarkson"
                    },
                    {
                        "name": "Yipeng Hu"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. Alexander"
                },
                "author": "Daniel C. Alexander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22058v1",
                "updated": "2025-06-27T09:53:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    53,
                    57,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:53:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    53,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "Lost at the Beginning of Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost at the Beginning of Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Sara Rajaee"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Anders SÃ¸gaard"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "9 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14883v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14883v3",
                "updated": "2025-06-27T09:50:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    50,
                    30,
                    4,
                    178,
                    0
                ],
                "published": "2024-04-23T10:09:46Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    10,
                    9,
                    46,
                    1,
                    114,
                    0
                ],
                "title": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable\n  Semantic Reference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable\n  Semantic Reference"
                },
                "summary": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference."
                },
                "authors": [
                    {
                        "name": "Vittoria Dentella"
                    },
                    {
                        "name": "Fritz Guenther"
                    },
                    {
                        "name": "Evelina Leivada"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Leivada"
                },
                "author": "Evelina Leivada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14883v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14883v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22050v1",
                "updated": "2025-06-27T09:45:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    37,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:45:37Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    37,
                    4,
                    178,
                    0
                ],
                "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs"
                },
                "summary": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts."
                },
                "authors": [
                    {
                        "name": "Delu Kong"
                    },
                    {
                        "name": "Lieve Macken"
                    }
                ],
                "author_detail": {
                    "name": "Lieve Macken"
                },
                "author": "Lieve Macken",
                "arxiv_comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22049v1",
                "updated": "2025-06-27T09:45:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    15,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:45:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling"
                },
                "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings."
                },
                "authors": [
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zijing Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Ajay Kumar Jaiswal"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Yin Lu"
                    },
                    {
                        "name": "Can Yang"
                    }
                ],
                "author_detail": {
                    "name": "Can Yang"
                },
                "author": "Can Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22041v1",
                "updated": "2025-06-27T09:39:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    39,
                    26,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:39:26Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    39,
                    26,
                    4,
                    178,
                    0
                ],
                "title": "Towards Scalable and Robust White Matter Lesion Localization via\n  Multimodal Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable and Robust White Matter Lesion Localization via\n  Multimodal Deep Learning"
                },
                "summary": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions."
                },
                "authors": [
                    {
                        "name": "Julia Machnio"
                    },
                    {
                        "name": "Sebastian NÃ¸rgaard Llambias"
                    },
                    {
                        "name": "Mads Nielsen"
                    },
                    {
                        "name": "Mostafa Mehdipour Ghazi"
                    }
                ],
                "author_detail": {
                    "name": "Mostafa Mehdipour Ghazi"
                },
                "author": "Mostafa Mehdipour Ghazi",
                "arxiv_comment": "2nd Sorbonne-Heidelberg Workshop on AI in medicine: Machine Learning\n  for multi-modal data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19897v2",
                "updated": "2025-06-27T09:38:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    38,
                    3,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-26T12:27:27Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    27,
                    27,
                    0,
                    146,
                    0
                ],
                "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows"
                },
                "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."
                },
                "authors": [
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Zhoumianze Liu"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Kanzhi Cheng"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Qintong Li"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ben Kao"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22038v1",
                "updated": "2025-06-27T09:34:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    34,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:34:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    34,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in\n  Children's Literature Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in\n  Children's Literature Translation"
                },
                "summary": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT."
                },
                "authors": [
                    {
                        "name": "Delu Kong"
                    },
                    {
                        "name": "Lieve Macken"
                    }
                ],
                "author_detail": {
                    "name": "Lieve Macken"
                },
                "author": "Lieve Macken",
                "arxiv_comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03492v2",
                "updated": "2025-06-27T09:33:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    33,
                    10,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-04T15:04:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    4,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores"
                },
                "summary": "Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Robert E. Blackwell"
                    },
                    {
                        "name": "Jon Barry"
                    },
                    {
                        "name": "Anthony G. Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Anthony G. Cohn"
                },
                "author": "Anthony G. Cohn",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06582v3",
                "updated": "2025-06-27T09:16:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    16,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-01-11T16:37:49Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    16,
                    37,
                    49,
                    5,
                    11,
                    0
                ],
                "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting"
                },
                "summary": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community."
                },
                "authors": [
                    {
                        "name": "Steven H. Wang"
                    },
                    {
                        "name": "Maksim Zubkov"
                    },
                    {
                        "name": "Kexin Fan"
                    },
                    {
                        "name": "Sarah Harrell"
                    },
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Andreas Plesner"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Accepted to ACL 2025. See the project page at\n  https://www.atticusprojectai.org/acord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22028v1",
                "updated": "2025-06-27T09:14:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:14:14Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    14,
                    4,
                    178,
                    0
                ],
                "title": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with\n  code generating LLMs and reusable Pythonic policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with\n  code generating LLMs and reusable Pythonic policies"
                },
                "summary": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC)."
                },
                "authors": [
                    {
                        "name": "Ossi Parikka"
                    },
                    {
                        "name": "Roel Pieters"
                    }
                ],
                "author_detail": {
                    "name": "Roel Pieters"
                },
                "author": "Roel Pieters",
                "arxiv_comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). For further information, videos and\n  code, see https://github.com/ozzyuni/LMPVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22026v1",
                "updated": "2025-06-27T08:47:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    47,
                    28,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:47:28Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    47,
                    28,
                    4,
                    178,
                    0
                ],
                "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature-Grounded Novelty Assessment of Scientific Ideas"
                },
                "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation."
                },
                "authors": [
                    {
                        "name": "Simra Shahid"
                    },
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Daniel S. Weld"
                    },
                    {
                        "name": "Tom Hope"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hope"
                },
                "author": "Tom Hope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09493v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09493v5",
                "updated": "2025-06-27T08:45:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    45,
                    28,
                    4,
                    178,
                    0
                ],
                "published": "2024-05-15T16:38:28Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    16,
                    38,
                    28,
                    2,
                    136,
                    0
                ],
                "title": "C-Learner: Constrained Learning for Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-Learner: Constrained Learning for Causal Inference"
                },
                "summary": "Popular debiased estimation methods for causal inference -- such as augmented\ninverse propensity weighting and targeted maximum likelihood estimation --\nenjoy desirable asymptotic properties like statistical efficiency and double\nrobustness but they can produce unstable estimates when there is limited\noverlap between treatment and control, requiring additional assumptions or ad\nhoc adjustments in practice (e.g., truncating propensity scores). In contrast,\nsimple plug-in estimators are stable but lack desirable asymptotic properties.\nWe propose a novel debiasing approach that achieves the best of both worlds,\nproducing stable plug-in estimates with desirable asymptotic properties. Our\nconstrained learning framework solves for the best plug-in estimator under the\nconstraint that the first-order error with respect to the plugged-in quantity\nis zero, and can leverage flexible model classes including neural networks and\ntree ensembles. In several experimental settings, including ones in which we\nhandle text-based covariates by fine-tuning language models, our constrained\nlearning-based estimator outperforms basic versions of one-step estimation and\ntargeting in challenging settings with limited overlap between treatment and\ncontrol, and performs similarly otherwise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popular debiased estimation methods for causal inference -- such as augmented\ninverse propensity weighting and targeted maximum likelihood estimation --\nenjoy desirable asymptotic properties like statistical efficiency and double\nrobustness but they can produce unstable estimates when there is limited\noverlap between treatment and control, requiring additional assumptions or ad\nhoc adjustments in practice (e.g., truncating propensity scores). In contrast,\nsimple plug-in estimators are stable but lack desirable asymptotic properties.\nWe propose a novel debiasing approach that achieves the best of both worlds,\nproducing stable plug-in estimates with desirable asymptotic properties. Our\nconstrained learning framework solves for the best plug-in estimator under the\nconstraint that the first-order error with respect to the plugged-in quantity\nis zero, and can leverage flexible model classes including neural networks and\ntree ensembles. In several experimental settings, including ones in which we\nhandle text-based covariates by fine-tuning language models, our constrained\nlearning-based estimator outperforms basic versions of one-step estimation and\ntargeting in challenging settings with limited overlap between treatment and\ncontrol, and performs similarly otherwise."
                },
                "authors": [
                    {
                        "name": "Tiffany Tianhui Cai"
                    },
                    {
                        "name": "Yuri Fonseca"
                    },
                    {
                        "name": "Kaiwen Hou"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09493v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09493v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22023v1",
                "updated": "2025-06-27T08:45:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    45,
                    21,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:45:21Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    45,
                    21,
                    4,
                    178,
                    0
                ],
                "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic\n  Chunk-wise Prediction Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic\n  Chunk-wise Prediction Policy"
                },
                "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems."
                },
                "authors": [
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Zhihan Li"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Hanglei Zhang"
                    },
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "17 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20967v2",
                "updated": "2025-06-27T08:42:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    42,
                    17,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-26T03:10:13Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    10,
                    13,
                    3,
                    177,
                    0
                ],
                "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing"
                },
                "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in\nvideo generation. However, directly applying existing video editing methods to\nVideo DiTs often incurs substantial computational overhead, due to\nresource-intensive attention modification or finetuning. To alleviate this\nproblem, we present DFVEdit, an efficient zero-shot video editing method\ntailored for Video DiTs. DFVEdit eliminates the need for both attention\nmodification and fine-tuning by directly operating on clean latents via flow\ntransformation. To be more specific, we observe that editing and sampling can\nbe unified under the continuous flow perspective. Building upon this\nfoundation, we propose the Conditional Delta Flow Vector (CDFV) -- a\ntheoretically unbiased estimation of DFV -- and integrate Implicit Cross\nAttention (ICA) guidance as well as Embedding Reinforcement (ER) to further\nenhance editing quality. DFVEdit excels in practical efficiency, offering at\nleast 20x inference speed-up and 85% memory reduction on Video DiTs compared to\nattention-engineering-based editing methods. Extensive quantitative and\nqualitative experiments demonstrate that DFVEdit can be seamlessly applied to\npopular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art\nperformance on structural fidelity, spatial-temporal consistency, and editing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in\nvideo generation. However, directly applying existing video editing methods to\nVideo DiTs often incurs substantial computational overhead, due to\nresource-intensive attention modification or finetuning. To alleviate this\nproblem, we present DFVEdit, an efficient zero-shot video editing method\ntailored for Video DiTs. DFVEdit eliminates the need for both attention\nmodification and fine-tuning by directly operating on clean latents via flow\ntransformation. To be more specific, we observe that editing and sampling can\nbe unified under the continuous flow perspective. Building upon this\nfoundation, we propose the Conditional Delta Flow Vector (CDFV) -- a\ntheoretically unbiased estimation of DFV -- and integrate Implicit Cross\nAttention (ICA) guidance as well as Embedding Reinforcement (ER) to further\nenhance editing quality. DFVEdit excels in practical efficiency, offering at\nleast 20x inference speed-up and 85% memory reduction on Video DiTs compared to\nattention-engineering-based editing methods. Extensive quantitative and\nqualitative experiments demonstrate that DFVEdit can be seamlessly applied to\npopular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art\nperformance on structural fidelity, spatial-temporal consistency, and editing\nquality."
                },
                "authors": [
                    {
                        "name": "Lingling Cai"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "arxiv_comment": "Zero-shot video editing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18508v2",
                "updated": "2025-06-27T08:33:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    33,
                    46,
                    4,
                    178,
                    0
                ],
                "published": "2024-09-27T07:45:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    45,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Adaptive inference with random ellipsoids through Conformal Conditional\n  Linear Expectation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive inference with random ellipsoids through Conformal Conditional\n  Linear Expectation"
                },
                "summary": "We propose two new conformity scores for conformal prediction, in a general\nmultivariate regression framework. The underlying score functions are based on\na covariance analysis of the residuals and the input points. We give\ntheoretical guarantees on the prediction sets, which consist in explicit\nellipsoids. We study the asymptotic properties of the ellipsoids, and show that\ntheir volume is reduced compared to that of classic balls, under ellipticity\nassumptions. Finally, we illustrate the effectiveness of all our results on an\nin-depth numerical study, including heavy-tailed as well as non-elliptical\ndistributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two new conformity scores for conformal prediction, in a general\nmultivariate regression framework. The underlying score functions are based on\na covariance analysis of the residuals and the input points. We give\ntheoretical guarantees on the prediction sets, which consist in explicit\nellipsoids. We study the asymptotic properties of the ellipsoids, and show that\ntheir volume is reduced compared to that of classic balls, under ellipticity\nassumptions. Finally, we illustrate the effectiveness of all our results on an\nin-depth numerical study, including heavy-tailed as well as non-elliptical\ndistributions."
                },
                "authors": [
                    {
                        "name": "Iain Henderson"
                    },
                    {
                        "name": "Adrien Mazoyer"
                    },
                    {
                        "name": "Fabrice Gamboa"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Gamboa"
                },
                "arxiv_affiliation": "IMT",
                "author": "Fabrice Gamboa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02862v3",
                "updated": "2025-06-27T08:31:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    31,
                    28,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-03T05:28:11Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    5,
                    28,
                    11,
                    5,
                    123,
                    0
                ],
                "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs"
                },
                "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."
                },
                "authors": [
                    {
                        "name": "Haoming Yang"
                    },
                    {
                        "name": "Ke Ma"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yingfei Sun"
                    },
                    {
                        "name": "Qianqian Xu"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14496v2",
                "updated": "2025-06-27T08:30:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    30,
                    22,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-20T12:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    26,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization"
                },
                "summary": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yi R Fung"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "28 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22015v1",
                "updated": "2025-06-27T08:28:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    28,
                    21,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:28:21Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    28,
                    21,
                    4,
                    178,
                    0
                ],
                "title": "Towards Universal & Efficient Model Compression via Exponential Torque\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Universal & Efficient Model Compression via Exponential Torque\n  Pruning"
                },
                "summary": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop."
                },
                "authors": [
                    {
                        "name": "Sarthak Ketanbhai Modi"
                    },
                    {
                        "name": "Lim Zi Pong"
                    },
                    {
                        "name": "Shourya Kuchhal"
                    },
                    {
                        "name": "Yoshi Cao"
                    },
                    {
                        "name": "Yupeng Cheng"
                    },
                    {
                        "name": "Teo Yon Shin"
                    },
                    {
                        "name": "Lin Shang-Wei"
                    },
                    {
                        "name": "Zhiming Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Li"
                },
                "author": "Zhiming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22005v1",
                "updated": "2025-06-27T08:17:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    17,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:17:18Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    17,
                    18,
                    4,
                    178,
                    0
                ],
                "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for\n  Theorem Proving"
                },
                "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults."
                },
                "authors": [
                    {
                        "name": "Naoto Onda"
                    },
                    {
                        "name": "Kazumi Kasaura"
                    },
                    {
                        "name": "Yuta Oriike"
                    },
                    {
                        "name": "Masaya Taniguchi"
                    },
                    {
                        "name": "Akiyoshi Sannai"
                    },
                    {
                        "name": "Sho Sonoda"
                    }
                ],
                "author_detail": {
                    "name": "Sho Sonoda"
                },
                "author": "Sho Sonoda",
                "arxiv_comment": "15 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22004v1",
                "updated": "2025-06-27T08:17:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    17,
                    7,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:17:07Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    17,
                    7,
                    4,
                    178,
                    0
                ],
                "title": "GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep\n  Learning"
                },
                "summary": "Inference tasks with time series over graphs are of importance in\napplications such as urban water networks, economics, and networked\nneuroscience. Addressing these tasks typically relies on identifying a\ncomputationally affordable model that jointly captures the graph-temporal\npatterns of the data. In this work, we propose a graph-aware state space model\nfor graph time series, where both the latent state and the observation equation\nare parametric graph-induced models with a limited number of parameters that\nneed to be learned. More specifically, we consider the state equation to follow\na stochastic partial differential equation driven by noise over the graphs\nedges accounting not only for potential edge uncertainties but also for\nincreasing the degrees of freedom in the latter in a tractable manner. The\ngraph structure conditioning of the noise dispersion allows the state variable\nto deviate from the stochastic process in certain neighborhoods. The\nobservation model is a sampled and graph-filtered version of the state\ncapturing multi-hop neighboring influence. The goal is to learn the parameters\nin both state and observation models from the partially observed data for\ndownstream tasks such as prediction and imputation. The model is inferred first\nthrough a maximum likelihood approach that provides theoretical tractability\nbut is limited in expressivity and scalability. To improve on the latter, we\nuse the state-space formulation to build a principled deep learning\narchitecture that jointly learns the parameters and tracks the state in an\nend-to-end manner in the spirit of Kalman neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference tasks with time series over graphs are of importance in\napplications such as urban water networks, economics, and networked\nneuroscience. Addressing these tasks typically relies on identifying a\ncomputationally affordable model that jointly captures the graph-temporal\npatterns of the data. In this work, we propose a graph-aware state space model\nfor graph time series, where both the latent state and the observation equation\nare parametric graph-induced models with a limited number of parameters that\nneed to be learned. More specifically, we consider the state equation to follow\na stochastic partial differential equation driven by noise over the graphs\nedges accounting not only for potential edge uncertainties but also for\nincreasing the degrees of freedom in the latter in a tractable manner. The\ngraph structure conditioning of the noise dispersion allows the state variable\nto deviate from the stochastic process in certain neighborhoods. The\nobservation model is a sampled and graph-filtered version of the state\ncapturing multi-hop neighboring influence. The goal is to learn the parameters\nin both state and observation models from the partially observed data for\ndownstream tasks such as prediction and imputation. The model is inferred first\nthrough a maximum likelihood approach that provides theoretical tractability\nbut is limited in expressivity and scalability. To improve on the latter, we\nuse the state-space formulation to build a principled deep learning\narchitecture that jointly learns the parameters and tracks the state in an\nend-to-end manner in the spirit of Kalman neural networks."
                },
                "authors": [
                    {
                        "name": "Mohammad Sabbaqi"
                    },
                    {
                        "name": "Riccardo Taormina"
                    },
                    {
                        "name": "Elvin Isufi"
                    }
                ],
                "author_detail": {
                    "name": "Elvin Isufi"
                },
                "author": "Elvin Isufi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19466v2",
                "updated": "2025-06-27T08:11:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    11,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-24T09:48:01Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    48,
                    1,
                    1,
                    175,
                    0
                ],
                "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models"
                },
                "summary": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Jiexiong Liu"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Qihang Zhou"
                    },
                    {
                        "name": "KunLun Meta"
                    }
                ],
                "author_detail": {
                    "name": "KunLun Meta"
                },
                "author": "KunLun Meta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21998v1",
                "updated": "2025-06-27T08:09:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    9,
                    49,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:09:49Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    9,
                    49,
                    4,
                    178,
                    0
                ],
                "title": "INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User\n  Privacy at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User\n  Privacy at the Edge"
                },
                "summary": "Data streams produced by mobile devices, such as smartphones, offer highly\nvaluable sources of information to build ubiquitous services. Such data streams\nare generally uploaded and centralized to be processed by third parties,\npotentially exposing sensitive personal information. In this context, existing\nprotection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),\nhave been investigated. Alas, none of them have actually been implemented, nor\ndeployed in real-life, in mobile devices to enforce user privacy at the edge.\nMoreover, the diversity of embedded sensors and the resulting data deluge makes\nit impractical to provision such services directly on mobiles, due to their\nconstrained storage capacity, communication bandwidth and processing power.\nThis article reports on the FLI technique, which leverages a piece-wise linear\napproximation technique to capture compact representations of data streams in\nmobile devices. Beyond the FLI storage layer, we introduce Divide \\& Stay, a\nnew privacy preservation technique to execute Points of Interest (POIs)\ninference. Finally, we deploy both of them on Android and iOS as the INTACT\nframework, making a concrete step towards enforcing privacy and trust in\nubiquitous computing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data streams produced by mobile devices, such as smartphones, offer highly\nvaluable sources of information to build ubiquitous services. Such data streams\nare generally uploaded and centralized to be processed by third parties,\npotentially exposing sensitive personal information. In this context, existing\nprotection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),\nhave been investigated. Alas, none of them have actually been implemented, nor\ndeployed in real-life, in mobile devices to enforce user privacy at the edge.\nMoreover, the diversity of embedded sensors and the resulting data deluge makes\nit impractical to provision such services directly on mobiles, due to their\nconstrained storage capacity, communication bandwidth and processing power.\nThis article reports on the FLI technique, which leverages a piece-wise linear\napproximation technique to capture compact representations of data streams in\nmobile devices. Beyond the FLI storage layer, we introduce Divide \\& Stay, a\nnew privacy preservation technique to execute Points of Interest (POIs)\ninference. Finally, we deploy both of them on Android and iOS as the INTACT\nframework, making a concrete step towards enforcing privacy and trust in\nubiquitous computing systems."
                },
                "authors": [
                    {
                        "name": "RÃ©my Raes"
                    },
                    {
                        "name": "Olivier Ruas"
                    },
                    {
                        "name": "Adrien Luxey-Bitri"
                    },
                    {
                        "name": "Romain Rouvoy"
                    }
                ],
                "author_detail": {
                    "name": "Romain Rouvoy"
                },
                "arxiv_affiliation": "SPIRALS",
                "author": "Romain Rouvoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18348v2",
                "updated": "2025-06-27T08:05:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    5,
                    9,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-23T07:12:08Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    12,
                    8,
                    0,
                    174,
                    0
                ],
                "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team"
                },
                "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research."
                },
                "authors": [
                    {
                        "name": "Weilun Yu"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Yonggui Huang"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Honggang Qi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Xiaoli Diao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18023v2",
                "updated": "2025-06-27T08:05:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    5,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-25T09:32:08Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "title": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference"
                },
                "summary": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Geng"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "ACL25 May ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10926v2",
                "updated": "2025-06-27T08:03:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    3,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-14T15:05:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    5,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "Federated Data-Efficient Instruction Tuning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Data-Efficient Instruction Tuning for Large Language Models"
                },
                "summary": "Instruction tuning is a crucial step in improving the responsiveness of\npretrained large language models (LLMs) to human instructions. Federated\nlearning (FL) helps to exploit the use of vast private instruction data from\nclients, becoming popular for LLM tuning by improving data diversity. Existing\nfederated tuning simply consumes all local data, causing excessive\ncomputational overhead and overfitting to local data, while centralized\ndata-efficient solutions are not suitable for FL due to privacy concerns. This\nwork presents FedHDS, a federated data-efficient instruction tuning approach,\nwhich tunes LLMs with a representative subset of edge-side data. It reduces the\ndata redundancy at both intra- and inter-client levels without sharing raw\ndata. Experiments with various LLMs, datasets and partitions show that FedHDS\nimproves Rouge-L on unseen tasks by an average of 10.72% over the SOTA\nfull-data federated instruction tuning methods, while using less than 1.5% of\nthe data samples, improving training efficiency by up to tens of times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a crucial step in improving the responsiveness of\npretrained large language models (LLMs) to human instructions. Federated\nlearning (FL) helps to exploit the use of vast private instruction data from\nclients, becoming popular for LLM tuning by improving data diversity. Existing\nfederated tuning simply consumes all local data, causing excessive\ncomputational overhead and overfitting to local data, while centralized\ndata-efficient solutions are not suitable for FL due to privacy concerns. This\nwork presents FedHDS, a federated data-efficient instruction tuning approach,\nwhich tunes LLMs with a representative subset of edge-side data. It reduces the\ndata redundancy at both intra- and inter-client levels without sharing raw\ndata. Experiments with various LLMs, datasets and partitions show that FedHDS\nimproves Rouge-L on unseen tasks by an average of 10.72% over the SOTA\nfull-data federated instruction tuning methods, while using less than 1.5% of\nthe data samples, improving training efficiency by up to tens of times."
                },
                "authors": [
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20888v2",
                "updated": "2025-06-27T07:59:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    59,
                    43,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-27T08:32:51Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    8,
                    32,
                    51,
                    1,
                    147,
                    0
                ],
                "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge\n  Distillation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge\n  Distillation of Large Language Models"
                },
                "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community."
                },
                "authors": [
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Junbing Yan"
                    },
                    {
                        "name": "Wenrui Cai"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.22423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22423v1",
                "updated": "2025-06-27T17:46:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    46,
                    33,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:46:33Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    46,
                    33,
                    4,
                    178,
                    0
                ],
                "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under\n  Physical Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under\n  Physical Attacks"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training."
                },
                "authors": [
                    {
                        "name": "Pritam Dash"
                    },
                    {
                        "name": "Ethan Chan"
                    },
                    {
                        "name": "Nathan P. Lawrence"
                    },
                    {
                        "name": "Karthik Pattabiraman"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Pattabiraman"
                },
                "author": "Karthik Pattabiraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22419v1",
                "updated": "2025-06-27T17:44:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    44,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:44:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    44,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements"
                },
                "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent."
                },
                "authors": [
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Despoina Magka"
                    },
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Tatiana Shavrina"
                    },
                    {
                        "name": "Jean-Christophe Gagnon-Audet"
                    },
                    {
                        "name": "Kelvin Niu"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Michael Shvartsman"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Edan Toledo"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Lucia Cipolina-Kun"
                    },
                    {
                        "name": "Abhishek Charnalia"
                    },
                    {
                        "name": "Derek Dunfield"
                    },
                    {
                        "name": "Alexander H. Miller"
                    },
                    {
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02003v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02003v6",
                "updated": "2025-06-27T17:28:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    28,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2023-10-02T16:55:19Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    16,
                    55,
                    19,
                    0,
                    275,
                    0
                ],
                "title": "L2MAC: Large Language Model Automatic Computer for Extensive Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L2MAC: Large Language Model Automatic Computer for Extensive Code\n  Generation"
                },
                "summary": "Transformer-based large language models (LLMs) are constrained by the fixed\ncontext window of the underlying transformer architecture, hindering their\nability to produce long and coherent outputs. Memory-augmented LLMs are a\npromising solution, but current approaches cannot handle long output generation\ntasks since they (1) only focus on reading memory and reduce its evolution to\nthe concatenation of new memories or (2) use very specialized memories that\ncannot adapt to other domains. This paper presents L2MAC, the first practical\nLLM-based general-purpose stored-program automatic computer (von Neumann\narchitecture) framework, an LLM-based multi-agent system, for long and\nconsistent output generation. Its memory has two components: the instruction\nregistry, which is populated with a prompt program to solve the user-given\ntask, and a file store, which will contain the final and intermediate outputs.\nEach instruction in turn is executed by a separate LLM agent, whose context is\nmanaged by a control unit capable of precise memory reading and writing to\nensure effective interaction with the file store. These components enable L2MAC\nto generate extensive outputs, bypassing the constraints of the finite context\nwindow while producing outputs that fulfill a complex user-specified task. We\nempirically demonstrate that L2MAC achieves state-of-the-art performance in\ngenerating large codebases for system design tasks, significantly outperforming\nother coding methods in implementing the detailed user-specified task; we show\nthat L2MAC works for general-purpose extensive text-based tasks, such as\nwriting an entire book; and we provide valuable insights into L2MAC's\nperformance improvement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) are constrained by the fixed\ncontext window of the underlying transformer architecture, hindering their\nability to produce long and coherent outputs. Memory-augmented LLMs are a\npromising solution, but current approaches cannot handle long output generation\ntasks since they (1) only focus on reading memory and reduce its evolution to\nthe concatenation of new memories or (2) use very specialized memories that\ncannot adapt to other domains. This paper presents L2MAC, the first practical\nLLM-based general-purpose stored-program automatic computer (von Neumann\narchitecture) framework, an LLM-based multi-agent system, for long and\nconsistent output generation. Its memory has two components: the instruction\nregistry, which is populated with a prompt program to solve the user-given\ntask, and a file store, which will contain the final and intermediate outputs.\nEach instruction in turn is executed by a separate LLM agent, whose context is\nmanaged by a control unit capable of precise memory reading and writing to\nensure effective interaction with the file store. These components enable L2MAC\nto generate extensive outputs, bypassing the constraints of the finite context\nwindow while producing outputs that fulfill a complex user-specified task. We\nempirically demonstrate that L2MAC achieves state-of-the-art performance in\ngenerating large codebases for system design tasks, significantly outperforming\nother coding methods in implementing the detailed user-specified task; we show\nthat L2MAC works for general-purpose extensive text-based tasks, such as\nwriting an entire book; and we provide valuable insights into L2MAC's\nperformance improvement over existing methods."
                },
                "authors": [
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Max Ruiz Luyten"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Published in The Twelfth International Conference on Learning\n  Representations (ICLR), 2024. Copyright 2023 by the author(s)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02003v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02003v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.5; D.2.2; D.2.3; D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22402v1",
                "updated": "2025-06-27T17:21:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    21,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:21:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    21,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Czech GEC: Insights from a Multi-Experiment Approach"
                },
                "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec."
                },
                "authors": [
                    {
                        "name": "Petr Pechman"
                    },
                    {
                        "name": "Milan Straka"
                    },
                    {
                        "name": "Jana StrakovÃ¡"
                    },
                    {
                        "name": "Jakub NÃ¡plava"
                    }
                ],
                "author_detail": {
                    "name": "Jakub NÃ¡plava"
                },
                "author": "Jakub NÃ¡plava",
                "arxiv_comment": "Accepted to TSD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04778v2",
                "updated": "2025-06-27T17:08:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    8,
                    56,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-07T06:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    36,
                    55,
                    0,
                    281,
                    0
                ],
                "title": "MM-R$^3$: On (In-)Consistency of Vision-Language Models (VLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-R$^3$: On (In-)Consistency of Vision-Language Models (VLMs)"
                },
                "summary": "With the advent of LLMs and variants, a flurry of research has emerged,\nanalyzing the performance of such models across an array of tasks. While most\nstudies focus on evaluating the capabilities of state-of-the-art (SoTA) Vision\nLanguage Models (VLMs) through task accuracy (e.g., visual question answering,\ngrounding), our work explores the related but complementary aspect of\nconsistency - the ability of a VLM to produce semantically similar or identical\nresponses to semantically similar queries. We note that consistency is a\nfundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in VLMs. Armed with this perspective, we propose the MM-R3\nbenchmark, which allows us to analyze performance, in terms of consistency and\naccuracy, of SoTA VLMs on three tasks: Question Rephrasing, Image Restyling,\nand Context Reasoning. Our analysis reveals that consistency does not always\nalign with accuracy, indicating that models with higher accuracy are not\nnecessarily more consistent, and vice versa. Furthermore, we propose a simple\nyet effective mitigation strategy in the form of an adapter module trained to\nminimize inconsistency across prompts. With our proposed strategy, we are able\nto achieve absolute improvements of 5.7% and 12.5%, on average on widely used\nVLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their existing\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of LLMs and variants, a flurry of research has emerged,\nanalyzing the performance of such models across an array of tasks. While most\nstudies focus on evaluating the capabilities of state-of-the-art (SoTA) Vision\nLanguage Models (VLMs) through task accuracy (e.g., visual question answering,\ngrounding), our work explores the related but complementary aspect of\nconsistency - the ability of a VLM to produce semantically similar or identical\nresponses to semantically similar queries. We note that consistency is a\nfundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in VLMs. Armed with this perspective, we propose the MM-R3\nbenchmark, which allows us to analyze performance, in terms of consistency and\naccuracy, of SoTA VLMs on three tasks: Question Rephrasing, Image Restyling,\nand Context Reasoning. Our analysis reveals that consistency does not always\nalign with accuracy, indicating that models with higher accuracy are not\nnecessarily more consistent, and vice versa. Furthermore, we propose a simple\nyet effective mitigation strategy in the form of an adapter module trained to\nminimize inconsistency across prompts. With our proposed strategy, we are able\nto achieve absolute improvements of 5.7% and 12.5%, on average on widely used\nVLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their existing\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Shih-Han Chou"
                    },
                    {
                        "name": "Shivam Chandhok"
                    },
                    {
                        "name": "James J. Little"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22390v1",
                "updated": "2025-06-27T17:00:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    0,
                    48,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:00:48Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    0,
                    48,
                    4,
                    178,
                    0
                ],
                "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical\n  Study of Developer-ChatGPT Conversations in GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical\n  Study of Developer-ChatGPT Conversations in GitHub"
                },
                "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22385v1",
                "updated": "2025-06-27T16:51:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    51,
                    15,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:51:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    51,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A\n  Study on Defeasible Video Entailment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A\n  Study on Defeasible Video Entailment"
                },
                "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Jilei Sun"
                    },
                    {
                        "name": "Yunhui Guo"
                    },
                    {
                        "name": "Vibhav Gogate"
                    }
                ],
                "author_detail": {
                    "name": "Vibhav Gogate"
                },
                "author": "Vibhav Gogate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14968v2",
                "updated": "2025-06-27T16:48:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    48,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-17T20:30:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    20,
                    30,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild\n  Personalization"
                },
                "summary": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast"
                },
                "authors": [
                    {
                        "name": "Rajat Kumar Jenamani"
                    },
                    {
                        "name": "Tom Silver"
                    },
                    {
                        "name": "Ben Dodson"
                    },
                    {
                        "name": "Shiqin Tong"
                    },
                    {
                        "name": "Anthony Song"
                    },
                    {
                        "name": "Yuting Yang"
                    },
                    {
                        "name": "Ziang Liu"
                    },
                    {
                        "name": "Benjamin Howe"
                    },
                    {
                        "name": "Aimee Whitneck"
                    },
                    {
                        "name": "Tapomayukh Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Tapomayukh Bhattacharjee"
                },
                "author": "Tapomayukh Bhattacharjee",
                "arxiv_comment": "RSS 2025 - Best Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22376v1",
                "updated": "2025-06-27T16:44:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    44,
                    11,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:44:11Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    44,
                    11,
                    4,
                    178,
                    0
                ],
                "title": "Probabilistic Optimality for Inference-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Optimality for Inference-time Scaling"
                },
                "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning."
                },
                "authors": [
                    {
                        "name": "Youkang Wang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Rubing Chen"
                    },
                    {
                        "name": "Xiao-Yong Wei"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22372v1",
                "updated": "2025-06-27T16:39:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    39,
                    12,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:39:12Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    39,
                    12,
                    4,
                    178,
                    0
                ],
                "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement"
                },
                "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems."
                },
                "authors": [
                    {
                        "name": "Maryam Mousavian"
                    },
                    {
                        "name": "Zahra Abbasiantaeb"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Fabio Crestani"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Crestani"
                },
                "author": "Fabio Crestani",
                "arxiv_doi": "10.1145/3731120.3744620",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731120.3744620",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.22372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22370v1",
                "updated": "2025-06-27T16:34:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    13,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:34:13Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    13,
                    4,
                    178,
                    0
                ],
                "title": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny"
                },
                "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution."
                },
                "authors": [
                    {
                        "name": "Carolina Carreira"
                    },
                    {
                        "name": "Ãlvaro Silva"
                    },
                    {
                        "name": "Alexandre Abreu"
                    },
                    {
                        "name": "Alexandra Mendes"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Mendes"
                },
                "author": "Alexandra Mendes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13868v2",
                "updated": "2025-06-27T16:34:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    34,
                    8,
                    4,
                    178,
                    0
                ],
                "published": "2024-11-21T06:06:04Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    6,
                    4,
                    3,
                    326,
                    0
                ],
                "title": "Robust Detection of Watermarks for Large Language Models Under Human\n  Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Detection of Watermarks for Large Language Models Under Human\n  Edits"
                },
                "summary": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22359v1",
                "updated": "2025-06-27T16:20:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    20,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:20:18Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    20,
                    18,
                    4,
                    178,
                    0
                ],
                "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Level AI for Telecom: Moving Beyond Large Language Models"
                },
                "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management."
                },
                "authors": [
                    {
                        "name": "Viswanath Kumarskandpriya"
                    },
                    {
                        "name": "Abdulhalim Dandoush"
                    },
                    {
                        "name": "Abbas Bradai"
                    },
                    {
                        "name": "Ali Belgacem"
                    }
                ],
                "author_detail": {
                    "name": "Ali Belgacem"
                },
                "author": "Ali Belgacem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22358v1",
                "updated": "2025-06-27T16:16:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    16,
                    15,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T16:16:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    16,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "AI Model Passport: Data and System Traceability Framework for\n  Transparent AI in Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Model Passport: Data and System Traceability Framework for\n  Transparent AI in Health"
                },
                "summary": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains."
                },
                "authors": [
                    {
                        "name": "Varvara Kalokyri"
                    },
                    {
                        "name": "Nikolaos S. Tachos"
                    },
                    {
                        "name": "Charalampos N. Kalantzopoulos"
                    },
                    {
                        "name": "Stelios Sfakianakis"
                    },
                    {
                        "name": "Haridimos Kondylakis"
                    },
                    {
                        "name": "Dimitrios I. Zaridis"
                    },
                    {
                        "name": "Sara Colantonio"
                    },
                    {
                        "name": "Daniele Regge"
                    },
                    {
                        "name": "Nikolaos Papanikolaou"
                    },
                    {
                        "name": "The ProCAncer-I consortium"
                    },
                    {
                        "name": "Konstantinos Marias"
                    },
                    {
                        "name": "Dimitrios I. Fotiadis"
                    },
                    {
                        "name": "Manolis Tsiknakis"
                    }
                ],
                "author_detail": {
                    "name": "Manolis Tsiknakis"
                },
                "author": "Manolis Tsiknakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22343v1",
                "updated": "2025-06-27T15:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    53,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    53,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts"
                },
                "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Garrett Wen"
                    },
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Jiayuan Wu"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22338v1",
                "updated": "2025-06-27T15:49:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    49,
                    58,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:49:58Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    49,
                    58,
                    4,
                    178,
                    0
                ],
                "title": "A Deep Learning framework for building damage assessment using VHR SAR\n  and geospatial data: demonstration on the 2023 Turkiye Earthquake",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Learning framework for building damage assessment using VHR SAR\n  and geospatial data: demonstration on the 2023 Turkiye Earthquake"
                },
                "summary": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper."
                },
                "authors": [
                    {
                        "name": "Luigi Russo"
                    },
                    {
                        "name": "Deodato Tapete"
                    },
                    {
                        "name": "Silvia Liberata Ullo"
                    },
                    {
                        "name": "Paolo Gamba"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Gamba"
                },
                "author": "Paolo Gamba",
                "arxiv_comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted\n  to IEEE Journal of Selected Topics in Applied Earth Observations and Remote\n  Sensing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22323v1",
                "updated": "2025-06-27T15:36:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    36,
                    10,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:36:10Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    36,
                    10,
                    4,
                    178,
                    0
                ],
                "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin\n  America",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin\n  America"
                },
                "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses."
                },
                "authors": [
                    {
                        "name": "Alessio Di Santo"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Di Santo"
                },
                "author": "Alessio Di Santo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22316v1",
                "updated": "2025-06-27T15:25:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    25,
                    23,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:25:23Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    25,
                    23,
                    4,
                    178,
                    0
                ],
                "title": "Evaluating Scoring Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Scoring Bias in LLM-as-a-Judge"
                },
                "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection."
                },
                "authors": [
                    {
                        "name": "Qingquan Li"
                    },
                    {
                        "name": "Shaoyu Dou"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Haixiang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haixiang Hu"
                },
                "author": "Haixiang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00521v2",
                "updated": "2025-06-27T15:19:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    19,
                    29,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-01T08:13:29Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    13,
                    29,
                    1,
                    91,
                    0
                ],
                "title": "Automated detection of atomicity violations in large-scale systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of atomicity violations in large-scale systems"
                },
                "summary": "Atomicity violations in interrupt-driven programs pose a significant threat\nto software safety in critical systems. These violations occur when the\nexecution sequence of operations on shared resources is disrupted by\nasynchronous interrupts. Detecting atomicity violations is challenging due to\nthe vast program state space, application-level code dependencies, and complex\ndomain-specific knowledge. We propose Clover, a hybrid framework that\nintegrates static analysis with large language model (LLM) agents to detect\natomicity violations in real-world programs. Clover first performs static\nanalysis to extract critical code snippets and operation information. It then\ninitiates a multi-agent process, where the expert agent leverages\ndomain-specific knowledge to detect atomicity violations, which are\nsubsequently validated by the judge agent. Evaluations on RaceBench 2.1,\nSV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of\n92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomicity violations in interrupt-driven programs pose a significant threat\nto software safety in critical systems. These violations occur when the\nexecution sequence of operations on shared resources is disrupted by\nasynchronous interrupts. Detecting atomicity violations is challenging due to\nthe vast program state space, application-level code dependencies, and complex\ndomain-specific knowledge. We propose Clover, a hybrid framework that\nintegrates static analysis with large language model (LLM) agents to detect\natomicity violations in real-world programs. Clover first performs static\nanalysis to extract critical code snippets and operation information. It then\ninitiates a multi-agent process, where the expert agent leverages\ndomain-specific knowledge to detect atomicity violations, which are\nsubsequently validated by the judge agent. Evaluations on RaceBench 2.1,\nSV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of\n92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score."
                },
                "authors": [
                    {
                        "name": "Hang He"
                    },
                    {
                        "name": "Yixing Luo"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Ting Su"
                    },
                    {
                        "name": "Haiying Sun"
                    },
                    {
                        "name": "Geguang Pu"
                    }
                ],
                "author_detail": {
                    "name": "Geguang Pu"
                },
                "author": "Geguang Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22305v1",
                "updated": "2025-06-27T15:16:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    16,
                    43,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T15:16:43Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    15,
                    16,
                    43,
                    4,
                    178,
                    0
                ],
                "title": "Detection of Personal Data in Structured Datasets Using a Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Personal Data in Structured Datasets Using a Large Language\n  Model"
                },
                "summary": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information."
                },
                "authors": [
                    {
                        "name": "Albert Agisha Ntwali"
                    },
                    {
                        "name": "Luca RÃ¼ck"
                    },
                    {
                        "name": "Martin Heckmann"
                    }
                ],
                "author_detail": {
                    "name": "Martin Heckmann"
                },
                "author": "Martin Heckmann",
                "arxiv_comment": "10 pages",
                "arxiv_journal_ref": "LLM-DPM '2025, Next Gen Data and Process Management: Large\n  Language Models and Beyond, June 22, 2025, Berlin, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4; I.2.7; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22283v1",
                "updated": "2025-06-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    55,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:55:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    55,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal\n  Misalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Token Reduction in LVLMs under Cross-modal\n  Misalignment"
                },
                "summary": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22270v1",
                "updated": "2025-06-27T14:39:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    39,
                    38,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:39:38Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    39,
                    38,
                    4,
                    178,
                    0
                ],
                "title": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values"
                },
                "summary": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content."
                },
                "authors": [
                    {
                        "name": "Ahmad Mel"
                    },
                    {
                        "name": "Sebastien Noir"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Noir"
                },
                "author": "Sebastien Noir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22267v1",
                "updated": "2025-06-27T14:36:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    36,
                    39,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T14:36:39Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    36,
                    39,
                    4,
                    178,
                    0
                ],
                "title": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph\n  is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph\n  is All You Need"
                },
                "summary": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users."
                },
                "authors": [
                    {
                        "name": "Junaid Ahmed Khan"
                    },
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Andrea Proia"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08134v3",
                "updated": "2025-06-27T14:00:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    14,
                    0,
                    6,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-09T18:37:14Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    37,
                    14,
                    0,
                    160,
                    0
                ],
                "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning"
                },
                "summary": "Peer review, the bedrock of scientific advancement in machine learning (ML),\nis strained by a crisis of scale. Exponential growth in manuscript submissions\nto premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite\ncapacity of qualified reviewers, leading to concerns about review quality,\nconsistency, and reviewer fatigue. This position paper argues that AI-assisted\npeer review must become an urgent research and infrastructure priority. We\nadvocate for a comprehensive AI-augmented ecosystem, leveraging Large Language\nModels (LLMs) not as replacements for human judgment, but as sophisticated\ncollaborators for authors, reviewers, and Area Chairs (ACs). We propose\nspecific roles for AI in enhancing factual verification, guiding reviewer\nperformance, assisting authors in quality improvement, and supporting ACs in\ndecision-making. Crucially, we contend that the development of such systems\nhinges on access to more granular, structured, and ethically-sourced peer\nreview process data. We outline a research agenda, including illustrative\nexperiments, to develop and validate these AI assistants, and discuss\nsignificant technical and ethical challenges. We call upon the ML community to\nproactively build this AI-assisted future, ensuring the continued integrity and\nscalability of scientific validation, while maintaining high standards of peer\nreview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review, the bedrock of scientific advancement in machine learning (ML),\nis strained by a crisis of scale. Exponential growth in manuscript submissions\nto premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite\ncapacity of qualified reviewers, leading to concerns about review quality,\nconsistency, and reviewer fatigue. This position paper argues that AI-assisted\npeer review must become an urgent research and infrastructure priority. We\nadvocate for a comprehensive AI-augmented ecosystem, leveraging Large Language\nModels (LLMs) not as replacements for human judgment, but as sophisticated\ncollaborators for authors, reviewers, and Area Chairs (ACs). We propose\nspecific roles for AI in enhancing factual verification, guiding reviewer\nperformance, assisting authors in quality improvement, and supporting ACs in\ndecision-making. Crucially, we contend that the development of such systems\nhinges on access to more granular, structured, and ethically-sourced peer\nreview process data. We outline a research agenda, including illustrative\nexperiments, to develop and validate these AI assistants, and discuss\nsignificant technical and ethical challenges. We call upon the ML community to\nproactively build this AI-assisted future, ensuring the continued integrity and\nscalability of scientific validation, while maintaining high standards of peer\nreview."
                },
                "authors": [
                    {
                        "name": "Qiyao Wei"
                    },
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "18 pages, 3 figures. Position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.5.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22232v1",
                "updated": "2025-06-27T13:49:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    37,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:49:37Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    37,
                    4,
                    178,
                    0
                ],
                "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging In-Context Learning for Political Bias Testing of LLMs"
                },
                "summary": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    },
                    {
                        "name": "Lena A. JÃ¤ger"
                    }
                ],
                "author_detail": {
                    "name": "Lena A. JÃ¤ger"
                },
                "author": "Lena A. JÃ¤ger",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22231v1",
                "updated": "2025-06-27T13:49:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:49:02Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    49,
                    2,
                    4,
                    178,
                    0
                ],
                "title": "Adapting University Policies for Generative AI: Opportunities,\n  Challenges, and Policy Solutions in Higher Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting University Policies for Generative AI: Opportunities,\n  Challenges, and Policy Solutions in Higher Education"
                },
                "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity."
                },
                "authors": [
                    {
                        "name": "Russell Beale"
                    }
                ],
                "author_detail": {
                    "name": "Russell Beale"
                },
                "author": "Russell Beale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1; K.3.2; K.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04396v2",
                "updated": "2025-06-27T13:42:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    42,
                    7,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-06T12:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    50,
                    14,
                    3,
                    65,
                    0
                ],
                "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models"
                },
                "summary": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Zejian Yuan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "Accepted by ACL 2025 main conference, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22223v1",
                "updated": "2025-06-27T13:38:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    38,
                    50,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:38:50Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    38,
                    50,
                    4,
                    178,
                    0
                ],
                "title": "V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles"
                },
                "summary": "This paper introduces a novel intention-sharing mechanism for Electrically\nPower-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing\nthe ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete\npredicted trajectory points with a compact elliptical geographical area\nrepresentation derived via quadratic polynomial fitting and Least Squares\nMethod (LSM). This approach encodes trajectory predictions with fixed-size data\npayloads, independent of the number of forecasted points, enabling\nhigher-frequency transmissions and improved network reliability. Simulation\nresults demonstrate superior inter-packet gap (IPG) performance compared to\nstandard ETSI VAMs, particularly under constrained communication conditions. A\nphysical experiment validates the feasibility of real-time deployment on\nembedded systems. The method supports scalable, low-latency intention sharing,\ncontributing to cooperative perception and enhanced safety for vulnerable road\nusers in connected and automated mobility ecosystems. Finally, we discuss the\nviability of LSM and open the door to other methods for prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel intention-sharing mechanism for Electrically\nPower-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing\nthe ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete\npredicted trajectory points with a compact elliptical geographical area\nrepresentation derived via quadratic polynomial fitting and Least Squares\nMethod (LSM). This approach encodes trajectory predictions with fixed-size data\npayloads, independent of the number of forecasted points, enabling\nhigher-frequency transmissions and improved network reliability. Simulation\nresults demonstrate superior inter-packet gap (IPG) performance compared to\nstandard ETSI VAMs, particularly under constrained communication conditions. A\nphysical experiment validates the feasibility of real-time deployment on\nembedded systems. The method supports scalable, low-latency intention sharing,\ncontributing to cooperative perception and enhanced safety for vulnerable road\nusers in connected and automated mobility ecosystems. Finally, we discuss the\nviability of LSM and open the door to other methods for prediction."
                },
                "authors": [
                    {
                        "name": "Felipe Valle Quiroz"
                    },
                    {
                        "name": "Johan Elfing"
                    },
                    {
                        "name": "Joel PÃ¥lsson"
                    },
                    {
                        "name": "Elena Haller"
                    },
                    {
                        "name": "Oscar Amador Molina"
                    }
                ],
                "author_detail": {
                    "name": "Oscar Amador Molina"
                },
                "author": "Oscar Amador Molina",
                "arxiv_comment": "Accepted into FAST-zero'25: 8th International Symposium on Future\n  Active Safety Technology toward zero traffic accidents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11733v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11733v3",
                "updated": "2025-06-27T13:38:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    38,
                    47,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-17T12:20:39Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    20,
                    39,
                    0,
                    48,
                    0
                ],
                "title": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking\n  Incremental Learning of Situation and Language Model using a Text-Simulated\n  Situated Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking\n  Incremental Learning of Situation and Language Model using a Text-Simulated\n  Situated Environment"
                },
                "summary": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments."
                },
                "authors": [
                    {
                        "name": "Jonathan Jordan"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "Accepted at The 28th International Conference of Text, Speech and\n  Dialogue (TSD2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11733v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11733v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17944v2",
                "updated": "2025-06-27T13:30:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    30,
                    9,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-22T08:40:56Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    8,
                    40,
                    56,
                    6,
                    173,
                    0
                ],
                "title": "SegChange-R1: LLM-Augmented Remote Sensing Change Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegChange-R1: LLM-Augmented Remote Sensing Change Detection"
                },
                "summary": "Remote sensing change detection is used in urban planning, terrain analysis,\nand environmental monitoring by analyzing feature changes in the same area over\ntime. In this paper, we propose a large language model (LLM) augmented\ninference approach (SegChange-R1), which enhances the detection capability by\nintegrating textual descriptive information and guides the model to focus on\nrelevant change regions, accelerating convergence. We designed a linear\nattention-based spatial transformation module (BEV) to address modal\nmisalignment by unifying features from different times into a BEV space.\nFurthermore, we introduce DVCD, a novel dataset for building change detection\nfrom UAV viewpoints. Experiments on four widely-used datasets demonstrate\nsignificant improvements over existing method The code and pre-trained models\nare available in {https://github.com/Yu-Zhouz/SegChange-R1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing change detection is used in urban planning, terrain analysis,\nand environmental monitoring by analyzing feature changes in the same area over\ntime. In this paper, we propose a large language model (LLM) augmented\ninference approach (SegChange-R1), which enhances the detection capability by\nintegrating textual descriptive information and guides the model to focus on\nrelevant change regions, accelerating convergence. We designed a linear\nattention-based spatial transformation module (BEV) to address modal\nmisalignment by unifying features from different times into a BEV space.\nFurthermore, we introduce DVCD, a novel dataset for building change detection\nfrom UAV viewpoints. Experiments on four widely-used datasets demonstrate\nsignificant improvements over existing method The code and pre-trained models\nare available in {https://github.com/Yu-Zhouz/SegChange-R1}."
                },
                "authors": [
                    {
                        "name": "Fei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Fei Zhou"
                },
                "author": "Fei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22210v1",
                "updated": "2025-06-27T13:29:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    29,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:29:25Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    29,
                    25,
                    4,
                    178,
                    0
                ],
                "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses"
                },
                "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality."
                },
                "authors": [
                    {
                        "name": "Weronika Åajewska"
                    },
                    {
                        "name": "Ivica Kostric"
                    },
                    {
                        "name": "Gabriel Iturra-Bocaz"
                    },
                    {
                        "name": "Mariam Arustashvili"
                    },
                    {
                        "name": "Krisztian Balog"
                    }
                ],
                "author_detail": {
                    "name": "Krisztian Balog"
                },
                "author": "Krisztian Balog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v1",
                "updated": "2025-06-27T13:09:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhi Zhang"
                },
                "author": "Yuzhi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22189v1",
                "updated": "2025-06-27T12:57:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    57,
                    0,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:57:00Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    57,
                    0,
                    4,
                    178,
                    0
                ],
                "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Modularity of Agentic Systems for Drug Discovery"
                },
                "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems."
                },
                "authors": [
                    {
                        "name": "Laura van Weesep"
                    },
                    {
                        "name": "Samuel Genheden"
                    },
                    {
                        "name": "Ola Engkvist"
                    },
                    {
                        "name": "Jens SjÃ¶lund"
                    }
                ],
                "author_detail": {
                    "name": "Jens SjÃ¶lund"
                },
                "author": "Jens SjÃ¶lund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03313v2",
                "updated": "2025-06-27T12:53:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    53,
                    42,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-05T09:45:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models"
                },
                "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."
                },
                "authors": [
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22185v1",
                "updated": "2025-06-27T12:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    46,
                    12,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    46,
                    12,
                    4,
                    178,
                    0
                ],
                "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration"
                },
                "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Alexander Bakhtin"
                    },
                    {
                        "name": "Noman Ahmad"
                    },
                    {
                        "name": "Mikel Robredo"
                    },
                    {
                        "name": "Ruoyu Su"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    },
                    {
                        "name": "Davide Taibi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Taibi"
                },
                "author": "Davide Taibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12380v2",
                "updated": "2025-06-27T12:45:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    45,
                    33,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-18T11:53:01Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    11,
                    53,
                    1,
                    6,
                    138,
                    0
                ],
                "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL\n  via Graph Matching and Stepwise Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL\n  via Graph Matching and Stepwise Reward"
                },
                "summary": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models."
                },
                "authors": [
                    {
                        "name": "Han Weng"
                    },
                    {
                        "name": "Puzhen Wu"
                    },
                    {
                        "name": "Cui Longjie"
                    },
                    {
                        "name": "Yi Zhan"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Dun Zeng"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Xiaoming Yin"
                    },
                    {
                        "name": "Yang Sun"
                    },
                    {
                        "name": "Xing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xing Chen"
                },
                "author": "Xing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13488v2",
                "updated": "2025-06-27T12:34:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    34,
                    59,
                    4,
                    178,
                    0
                ],
                "published": "2024-12-18T04:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    4,
                    14,
                    35,
                    2,
                    353,
                    0
                ],
                "title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language\n  Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT, while our open-source framework\nestablishes a reproducible benchmark for future research, which is available at\n[https://github.com/0-ml/speft].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT, while our open-source framework\nestablishes a reproducible benchmark for future research, which is available at\n[https://github.com/0-ml/speft]."
                },
                "authors": [
                    {
                        "name": "Xinxin Liu"
                    },
                    {
                        "name": "Aaron Thomas"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22169v1",
                "updated": "2025-06-27T12:31:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    31,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:31:24Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    31,
                    24,
                    4,
                    178,
                    0
                ],
                "title": "MCFuser: High-Performance and Rapid Fusion of Memory-Bound\n  Compute-Intensive Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCFuser: High-Performance and Rapid Fusion of Memory-Bound\n  Compute-Intensive Operators"
                },
                "summary": "Operator fusion, a key technique to improve data locality and alleviate GPU\nmemory bandwidth pressure, often fails to extend to the fusion of multiple\ncompute-intensive operators due to saturated computation throughput. However,\nthe dynamicity of tensor dimension sizes could potentially lead to these\noperators becoming memory-bound, necessitating the generation of fused kernels,\na task hindered by limited search spaces for fusion strategies, redundant\nmemory access, and prolonged tuning time, leading to sub-optimal performance\nand inefficient deployment.\n  We introduce MCFuser, a pioneering framework designed to overcome these\nobstacles by generating high-performance fused kernels for what we define as\nmemory-bound compute-intensive (MBCI) operator chains. Leveraging high-level\ntiling expressions to delineate a comprehensive search space, coupled with\nDirected Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,\nMCFuser streamlines kernel optimization. By implementing guidelines to prune\nthe search space and incorporating an analytical performance model with a\nheuristic search, MCFuser not only significantly accelerates the tuning process\nbut also demonstrates superior performance. Benchmarked against leading\ncompilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a\n5.9x speedup in kernel performance and outpaces other baselines while reducing\ntuning time by over 70-fold, showcasing its agility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operator fusion, a key technique to improve data locality and alleviate GPU\nmemory bandwidth pressure, often fails to extend to the fusion of multiple\ncompute-intensive operators due to saturated computation throughput. However,\nthe dynamicity of tensor dimension sizes could potentially lead to these\noperators becoming memory-bound, necessitating the generation of fused kernels,\na task hindered by limited search spaces for fusion strategies, redundant\nmemory access, and prolonged tuning time, leading to sub-optimal performance\nand inefficient deployment.\n  We introduce MCFuser, a pioneering framework designed to overcome these\nobstacles by generating high-performance fused kernels for what we define as\nmemory-bound compute-intensive (MBCI) operator chains. Leveraging high-level\ntiling expressions to delineate a comprehensive search space, coupled with\nDirected Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,\nMCFuser streamlines kernel optimization. By implementing guidelines to prune\nthe search space and incorporating an analytical performance model with a\nheuristic search, MCFuser not only significantly accelerates the tuning process\nbut also demonstrates superior performance. Benchmarked against leading\ncompilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a\n5.9x speedup in kernel performance and outpaces other baselines while reducing\ntuning time by over 70-fold, showcasing its agility."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Donglin Yang"
                    },
                    {
                        "name": "Xiaobo Zhou"
                    },
                    {
                        "name": "Dazhao Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dazhao Cheng"
                },
                "author": "Dazhao Cheng",
                "arxiv_doi": "10.1109/SC41406.2024.00040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SC41406.2024.00040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.22169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, accepted at SC 2024",
                "arxiv_journal_ref": "SC24: International Conference for High Performance Computing,\n  Networking, Storage and Analysis. IEEE, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04412v3",
                "updated": "2025-06-27T12:18:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    18,
                    30,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-06T13:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search"
                },
                "summary": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel\ninference-time framework that generalizes repeated sampling with principled\nmulti-turn exploration and exploitation. At each node in the search tree,\nAB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate\nresponses or \"go deeper\" by revisiting existing ones based on external feedback\nsignals. We evaluate our method on complex coding and engineering tasks using\nfrontier models. Empirical results show that AB-MCTS consistently outperforms\nboth repeated sampling and standard MCTS, underscoring the importance of\ncombining the response diversity of LLMs with multi-turn solution refinement\nfor effective inference-time scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel\ninference-time framework that generalizes repeated sampling with principled\nmulti-turn exploration and exploitation. At each node in the search tree,\nAB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate\nresponses or \"go deeper\" by revisiting existing ones based on external feedback\nsignals. We evaluate our method on complex coding and engineering tasks using\nfrontier models. Empirical results show that AB-MCTS consistently outperforms\nboth repeated sampling and standard MCTS, underscoring the importance of\ncombining the response diversity of LLMs with multi-turn solution refinement\nfor effective inference-time scaling."
                },
                "authors": [
                    {
                        "name": "Yuichi Inoue"
                    },
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "So Kuroki"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "Presented at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22157v1",
                "updated": "2025-06-27T12:10:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    10,
                    57,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T12:10:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    12,
                    10,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "Training Language Model to Critique for Better Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Model to Critique for Better Refinement"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops."
                },
                "authors": [
                    {
                        "name": "Tianshu Yu"
                    },
                    {
                        "name": "Chao Xiang"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xinyu Mu"
                    },
                    {
                        "name": "Chuxiong Sun"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24616v3",
                "updated": "2025-06-27T11:43:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    43,
                    3,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-30T14:08:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX"
                },
                "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."
                },
                "authors": [
                    {
                        "name": "Nikita Martynov"
                    },
                    {
                        "name": "Anastasia Mordasheva"
                    },
                    {
                        "name": "Dmitriy Gorbetskiy"
                    },
                    {
                        "name": "Danil Astafurov"
                    },
                    {
                        "name": "Ulyana Isaeva"
                    },
                    {
                        "name": "Elina Basyrova"
                    },
                    {
                        "name": "Sergey Skachkov"
                    },
                    {
                        "name": "Victoria Berestova"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Valeriia Zanina"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "arxiv_comment": "178 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22139v1",
                "updated": "2025-06-27T11:30:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    30,
                    51,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T11:30:51Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    30,
                    51,
                    4,
                    178,
                    0
                ],
                "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Shaojie Zhang"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Jianqin Yin"
                    },
                    {
                        "name": "Zhenbo Luo"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09338v2",
                "updated": "2025-06-27T11:15:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    15,
                    26,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-14T12:33:05Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    33,
                    5,
                    2,
                    134,
                    0
                ],
                "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs"
                },
                "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem."
                },
                "authors": [
                    {
                        "name": "Jingcheng Niu"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Hamidreza Saghir"
                    },
                    {
                        "name": "Amir H. Abdi"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Abdi"
                },
                "author": "Amir H. Abdi",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08837v3",
                "updated": "2025-06-27T11:13:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    11,
                    13,
                    27,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-10T14:23:55Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    23,
                    55,
                    1,
                    161,
                    0
                ],
                "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Patterns for Securing LLM Agents against Prompt Injections"
                },
                "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies."
                },
                "authors": [
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Beat Buesser"
                    },
                    {
                        "name": "Ana-Maria CreÅ£u"
                    },
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Daniel Dobos"
                    },
                    {
                        "name": "Daniel Fabian"
                    },
                    {
                        "name": "Marc Fischer"
                    },
                    {
                        "name": "David Froelicher"
                    },
                    {
                        "name": "Kathrin Grosse"
                    },
                    {
                        "name": "Daniel Naeff"
                    },
                    {
                        "name": "Ezinwanne Ozoani"
                    },
                    {
                        "name": "Andrew Paverd"
                    },
                    {
                        "name": "Florian TramÃ¨r"
                    },
                    {
                        "name": "VÃ¡clav Volhejn"
                    }
                ],
                "author_detail": {
                    "name": "VÃ¡clav Volhejn"
                },
                "author": "VÃ¡clav Volhejn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03592v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03592v3",
                "updated": "2025-06-27T10:34:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    34,
                    56,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-05T15:26:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance"
                },
                "summary": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance."
                },
                "authors": [
                    {
                        "name": "Karl Audun Borgersen"
                    },
                    {
                        "name": "Morten Goodwin"
                    }
                ],
                "author_detail": {
                    "name": "Morten Goodwin"
                },
                "author": "Morten Goodwin",
                "arxiv_comment": "8 pages, 6 figures, v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03592v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03592v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07495v2",
                "updated": "2025-06-27T10:33:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    33,
                    27,
                    4,
                    178,
                    0
                ],
                "published": "2024-07-10T09:27:23Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    9,
                    27,
                    23,
                    2,
                    192,
                    0
                ],
                "title": "Beyond Fixed Length: Bucket Pre-training is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Fixed Length: Bucket Pre-training is All You Need"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious tasks, with pre-training stage serving as the cornerstone of their\ncapabilities. However, the conventional fixed-length data composition strategy\nfor pre-training presents several practical challenges. When using shorter\nsequences, documents are often truncated, potentially leading to information\nloss and affecting the model's ability to capture long-range dependencies.\nConversely, longer sequences require concatenation of multiple documents, which\ncan introduce noise and affect the natural document boundaries and semantic\ncoherence as well as require substantial computational overhead. To address\nthese challenges, we first establish three quantitative metrics for evaluating\ndata composition quality: padding ratio, truncation ratio, and concatenation\nratio. Building upon these metrics, we propose a novel multi-bucket data\ncomposition method that transcends the fixed-length paradigm. Our approach\nadaptively organizes training data to achieve optimal composition quality as\nmeasured by the proposed metrics, offering a more flexible and efficient\napproach for pre-training. We conduct extensive experiments and the results\ndemonstrate that our proposed method significantly enhances both the efficiency\nand effectiveness of LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious tasks, with pre-training stage serving as the cornerstone of their\ncapabilities. However, the conventional fixed-length data composition strategy\nfor pre-training presents several practical challenges. When using shorter\nsequences, documents are often truncated, potentially leading to information\nloss and affecting the model's ability to capture long-range dependencies.\nConversely, longer sequences require concatenation of multiple documents, which\ncan introduce noise and affect the natural document boundaries and semantic\ncoherence as well as require substantial computational overhead. To address\nthese challenges, we first establish three quantitative metrics for evaluating\ndata composition quality: padding ratio, truncation ratio, and concatenation\nratio. Building upon these metrics, we propose a novel multi-bucket data\ncomposition method that transcends the fixed-length paradigm. Our approach\nadaptively organizes training data to achieve optimal composition quality as\nmeasured by the proposed metrics, offering a more flexible and efficient\napproach for pre-training. We conduct extensive experiments and the results\ndemonstrate that our proposed method significantly enhances both the efficiency\nand effectiveness of LLM pre-training."
                },
                "authors": [
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Hongtao Liu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "8 pages, 5 figures, 3 tables. Accetped by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16383v2",
                "updated": "2025-06-27T10:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    25,
                    12,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-19T15:12:58Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    15,
                    12,
                    58,
                    3,
                    170,
                    0
                ],
                "title": "Large Language Models in Argument Mining: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Argument Mining: A Survey"
                },
                "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Yizheng Sun"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "Work draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11108v2",
                "updated": "2025-06-27T10:17:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    17,
                    13,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-15T11:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    55,
                    24,
                    1,
                    105,
                    0
                ],
                "title": "Benchmarking Vision Language Models on German Factual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Vision Language Models on German Factual Data"
                },
                "summary": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages."
                },
                "authors": [
                    {
                        "name": "RenÃ© Peinl"
                    },
                    {
                        "name": "Vincent Tischler"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Tischler"
                },
                "author": "Vincent Tischler",
                "arxiv_comment": "Peinl, Ren\\'e; Tischler, Vincent (2025): Benchmarking Vision Language\n  Models on German Factual Data. 21st International Conference on Artificial\n  Intelligence Applications and Innovations, 26-29 June, 2025, Limassol, Cyprus\n  (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45 (Primary), 68T07 (Secondary), 68T10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05958v3",
                "updated": "2025-06-27T10:11:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    11,
                    0,
                    4,
                    178,
                    0
                ],
                "published": "2024-12-08T14:34:30Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    14,
                    34,
                    30,
                    6,
                    343,
                    0
                ],
                "title": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension"
                },
                "summary": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have facilitated the definition of autonomous\nintelligent agents. Such agents have already demonstrated their potential in\nsolving complex tasks in different domains. And they can further increase their\nperformance when collaborating with other agents in a multi-agent system.\nHowever, the orchestration and coordination of these agents is still\nchallenging, especially when they need to interact with humans as part of\nhuman-agentic collaborative workflows. These kinds of workflows need to be\nprecisely specified so that it is clear whose responsible for each task, what\nstrategies agents can follow to complete individual tasks or how decisions will\nbe taken when different alternatives are proposed, among others. Current\nbusiness process modeling languages fall short when it comes to specifying\nthese new mixed collaborative scenarios. In this exploratory paper, we extend a\nwell-known process modeling language (i.e., BPMN) to enable the definition of\nthis new type of workflow. Our extension covers both the formalization of the\nnew metamodeling concepts required and the proposal of a BPMN-like graphical\nnotation to facilitate the definition of these workflows. Our extension has\nbeen implemented and is available as an open-source human-agentic workflow\nmodeling editor on GitHub."
                },
                "authors": [
                    {
                        "name": "Adem Ait"
                    },
                    {
                        "name": "Javier Luis CÃ¡novas Izquierdo"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "Accepted in the Euromicro Conference Series on Software Engineering\n  and Advanced Applications (SEAA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17066v2",
                "updated": "2025-06-27T10:04:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    10,
                    4,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-18T16:13:07Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    16,
                    13,
                    7,
                    6,
                    138,
                    0
                ],
                "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration"
                },
                "summary": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community."
                },
                "authors": [
                    {
                        "name": "Tatia Tsmindashvili"
                    },
                    {
                        "name": "Ana Kolkhidashvili"
                    },
                    {
                        "name": "Dachi Kurtskhalia"
                    },
                    {
                        "name": "Nino Maghlakelidze"
                    },
                    {
                        "name": "Elene Mekvabishvili"
                    },
                    {
                        "name": "Guram Dentoshvili"
                    },
                    {
                        "name": "Orkhan Shamilov"
                    },
                    {
                        "name": "Zaal Gachechiladze"
                    },
                    {
                        "name": "Steven Saporta"
                    },
                    {
                        "name": "David Dachi Choladze"
                    }
                ],
                "author_detail": {
                    "name": "David Dachi Choladze"
                },
                "author": "David Dachi Choladze",
                "arxiv_comment": "Under review at IEEE Access. Supplementary material is included in\n  the main PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22058v1",
                "updated": "2025-06-27T09:53:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    53,
                    57,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:53:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    53,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "Lost at the Beginning of Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost at the Beginning of Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Sara Rajaee"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Anders SÃ¸gaard"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "9 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14883v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14883v3",
                "updated": "2025-06-27T09:50:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    50,
                    30,
                    4,
                    178,
                    0
                ],
                "published": "2024-04-23T10:09:46Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    10,
                    9,
                    46,
                    1,
                    114,
                    0
                ],
                "title": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable\n  Semantic Reference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable\n  Semantic Reference"
                },
                "summary": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference."
                },
                "authors": [
                    {
                        "name": "Vittoria Dentella"
                    },
                    {
                        "name": "Fritz Guenther"
                    },
                    {
                        "name": "Evelina Leivada"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Leivada"
                },
                "author": "Evelina Leivada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14883v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14883v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22050v1",
                "updated": "2025-06-27T09:45:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    37,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:45:37Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    37,
                    4,
                    178,
                    0
                ],
                "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs"
                },
                "summary": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts."
                },
                "authors": [
                    {
                        "name": "Delu Kong"
                    },
                    {
                        "name": "Lieve Macken"
                    }
                ],
                "author_detail": {
                    "name": "Lieve Macken"
                },
                "author": "Lieve Macken",
                "arxiv_comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22049v1",
                "updated": "2025-06-27T09:45:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    15,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:45:15Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    45,
                    15,
                    4,
                    178,
                    0
                ],
                "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling"
                },
                "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings."
                },
                "authors": [
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zijing Liu"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Ajay Kumar Jaiswal"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Yin Lu"
                    },
                    {
                        "name": "Can Yang"
                    }
                ],
                "author_detail": {
                    "name": "Can Yang"
                },
                "author": "Can Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19897v2",
                "updated": "2025-06-27T09:38:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    38,
                    3,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-26T12:27:27Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    27,
                    27,
                    0,
                    146,
                    0
                ],
                "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows"
                },
                "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."
                },
                "authors": [
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Zhoumianze Liu"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Kanzhi Cheng"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Qintong Li"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ben Kao"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22038v1",
                "updated": "2025-06-27T09:34:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    34,
                    40,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:34:40Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    34,
                    40,
                    4,
                    178,
                    0
                ],
                "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in\n  Children's Literature Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in\n  Children's Literature Translation"
                },
                "summary": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT."
                },
                "authors": [
                    {
                        "name": "Delu Kong"
                    },
                    {
                        "name": "Lieve Macken"
                    }
                ],
                "author_detail": {
                    "name": "Lieve Macken"
                },
                "author": "Lieve Macken",
                "arxiv_comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03492v2",
                "updated": "2025-06-27T09:33:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    33,
                    10,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-04T15:04:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    4,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores"
                },
                "summary": "Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Robert E. Blackwell"
                    },
                    {
                        "name": "Jon Barry"
                    },
                    {
                        "name": "Anthony G. Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Anthony G. Cohn"
                },
                "author": "Anthony G. Cohn",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06582v3",
                "updated": "2025-06-27T09:16:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    16,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-01-11T16:37:49Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    16,
                    37,
                    49,
                    5,
                    11,
                    0
                ],
                "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting"
                },
                "summary": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community."
                },
                "authors": [
                    {
                        "name": "Steven H. Wang"
                    },
                    {
                        "name": "Maksim Zubkov"
                    },
                    {
                        "name": "Kexin Fan"
                    },
                    {
                        "name": "Sarah Harrell"
                    },
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Andreas Plesner"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Accepted to ACL 2025. See the project page at\n  https://www.atticusprojectai.org/acord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22028v1",
                "updated": "2025-06-27T09:14:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:14:14Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    14,
                    4,
                    178,
                    0
                ],
                "title": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with\n  code generating LLMs and reusable Pythonic policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with\n  code generating LLMs and reusable Pythonic policies"
                },
                "summary": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC)."
                },
                "authors": [
                    {
                        "name": "Ossi Parikka"
                    },
                    {
                        "name": "Roel Pieters"
                    }
                ],
                "author_detail": {
                    "name": "Roel Pieters"
                },
                "author": "Roel Pieters",
                "arxiv_comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). For further information, videos and\n  code, see https://github.com/ozzyuni/LMPVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22026v1",
                "updated": "2025-06-27T08:47:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    47,
                    28,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:47:28Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    47,
                    28,
                    4,
                    178,
                    0
                ],
                "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature-Grounded Novelty Assessment of Scientific Ideas"
                },
                "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation."
                },
                "authors": [
                    {
                        "name": "Simra Shahid"
                    },
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Daniel S. Weld"
                    },
                    {
                        "name": "Tom Hope"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hope"
                },
                "author": "Tom Hope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11212v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11212v4",
                "updated": "2025-06-27T08:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    38,
                    23,
                    4,
                    178,
                    0
                ],
                "published": "2024-01-20T11:37:44Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    11,
                    37,
                    44,
                    5,
                    20,
                    0
                ],
                "title": "Programming Distributed Collective Processes in the eXchange Calculus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Distributed Collective Processes in the eXchange Calculus"
                },
                "summary": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Audrito"
                    },
                    {
                        "name": "Roberto Casadei"
                    },
                    {
                        "name": "Ferruccio Damiani"
                    },
                    {
                        "name": "Gianluca Torta"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "arxiv_comment": "41 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11212v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11212v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; F.1.1; F.4.3; I.2.11; J.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02862v3",
                "updated": "2025-06-27T08:31:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    31,
                    28,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-03T05:28:11Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    5,
                    28,
                    11,
                    5,
                    123,
                    0
                ],
                "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs"
                },
                "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."
                },
                "authors": [
                    {
                        "name": "Haoming Yang"
                    },
                    {
                        "name": "Ke Ma"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yingfei Sun"
                    },
                    {
                        "name": "Qianqian Xu"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14496v2",
                "updated": "2025-06-27T08:30:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    30,
                    22,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-20T12:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    26,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for\n  Interactive Environment Generalization"
                },
                "summary": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yi R Fung"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "28 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22005v1",
                "updated": "2025-06-27T08:17:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    17,
                    18,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T08:17:18Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    17,
                    18,
                    4,
                    178,
                    0
                ],
                "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for\n  Theorem Proving"
                },
                "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults."
                },
                "authors": [
                    {
                        "name": "Naoto Onda"
                    },
                    {
                        "name": "Kazumi Kasaura"
                    },
                    {
                        "name": "Yuta Oriike"
                    },
                    {
                        "name": "Masaya Taniguchi"
                    },
                    {
                        "name": "Akiyoshi Sannai"
                    },
                    {
                        "name": "Sho Sonoda"
                    }
                ],
                "author_detail": {
                    "name": "Sho Sonoda"
                },
                "author": "Sho Sonoda",
                "arxiv_comment": "15 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19466v2",
                "updated": "2025-06-27T08:11:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    11,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-24T09:48:01Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    48,
                    1,
                    1,
                    175,
                    0
                ],
                "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models"
                },
                "summary": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Jiexiong Liu"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Qihang Zhou"
                    },
                    {
                        "name": "KunLun Meta"
                    }
                ],
                "author_detail": {
                    "name": "KunLun Meta"
                },
                "author": "KunLun Meta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18348v2",
                "updated": "2025-06-27T08:05:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    5,
                    9,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-23T07:12:08Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    12,
                    8,
                    0,
                    174,
                    0
                ],
                "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team"
                },
                "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research."
                },
                "authors": [
                    {
                        "name": "Weilun Yu"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Yonggui Huang"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Honggang Qi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Xiaoli Diao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18023v2",
                "updated": "2025-06-27T08:05:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    5,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-25T09:32:08Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "title": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference"
                },
                "summary": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Geng"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "ACL25 May ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10926v2",
                "updated": "2025-06-27T08:03:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    8,
                    3,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-14T15:05:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    5,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "Federated Data-Efficient Instruction Tuning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Data-Efficient Instruction Tuning for Large Language Models"
                },
                "summary": "Instruction tuning is a crucial step in improving the responsiveness of\npretrained large language models (LLMs) to human instructions. Federated\nlearning (FL) helps to exploit the use of vast private instruction data from\nclients, becoming popular for LLM tuning by improving data diversity. Existing\nfederated tuning simply consumes all local data, causing excessive\ncomputational overhead and overfitting to local data, while centralized\ndata-efficient solutions are not suitable for FL due to privacy concerns. This\nwork presents FedHDS, a federated data-efficient instruction tuning approach,\nwhich tunes LLMs with a representative subset of edge-side data. It reduces the\ndata redundancy at both intra- and inter-client levels without sharing raw\ndata. Experiments with various LLMs, datasets and partitions show that FedHDS\nimproves Rouge-L on unseen tasks by an average of 10.72% over the SOTA\nfull-data federated instruction tuning methods, while using less than 1.5% of\nthe data samples, improving training efficiency by up to tens of times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a crucial step in improving the responsiveness of\npretrained large language models (LLMs) to human instructions. Federated\nlearning (FL) helps to exploit the use of vast private instruction data from\nclients, becoming popular for LLM tuning by improving data diversity. Existing\nfederated tuning simply consumes all local data, causing excessive\ncomputational overhead and overfitting to local data, while centralized\ndata-efficient solutions are not suitable for FL due to privacy concerns. This\nwork presents FedHDS, a federated data-efficient instruction tuning approach,\nwhich tunes LLMs with a representative subset of edge-side data. It reduces the\ndata redundancy at both intra- and inter-client levels without sharing raw\ndata. Experiments with various LLMs, datasets and partitions show that FedHDS\nimproves Rouge-L on unseen tasks by an average of 10.72% over the SOTA\nfull-data federated instruction tuning methods, while using less than 1.5% of\nthe data samples, improving training efficiency by up to tens of times."
                },
                "authors": [
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20888v2",
                "updated": "2025-06-27T07:59:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    59,
                    43,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-27T08:32:51Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    8,
                    32,
                    51,
                    1,
                    147,
                    0
                ],
                "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge\n  Distillation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge\n  Distillation of Large Language Models"
                },
                "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community."
                },
                "authors": [
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Junbing Yan"
                    },
                    {
                        "name": "Wenrui Cai"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15237v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15237v4",
                "updated": "2025-06-27T07:54:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    54,
                    57,
                    4,
                    178,
                    0
                ],
                "published": "2024-08-27T17:56:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
                },
                "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "NeurIPS 2024. v4 updates: mention concurrent work of speculative\n  decoding for SSM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15237v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15237v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10432v2",
                "updated": "2025-06-27T07:52:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    52,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-13T14:55:59Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    55,
                    59,
                    3,
                    72,
                    0
                ],
                "title": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models"
                },
                "summary": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability."
                },
                "authors": [
                    {
                        "name": "Can Zheng"
                    },
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Guofa Cai"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Chung G. Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chung G. Kang"
                },
                "author": "Chung G. Kang",
                "arxiv_comment": "6 pages, 7 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12016v2",
                "updated": "2025-06-27T07:42:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    42,
                    47,
                    4,
                    178,
                    0
                ],
                "published": "2025-03-15T06:52:10Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    6,
                    52,
                    10,
                    5,
                    74,
                    0
                ],
                "title": "A Survey on Federated Fine-tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Federated Fine-tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive success across\nvarious tasks. Integrating LLMs with Federated Learning (FL), a paradigm known\nas FedLLM, offers a promising avenue for collaborative model adaptation while\npreserving data privacy. This survey provides a systematic and comprehensive\nreview of FedLLM. We begin by tracing the historical development of both LLMs\nand FL, summarizing relevant prior research to set the context. Subsequently,\nwe delve into an in-depth analysis of the fundamental challenges inherent in\ndeploying FedLLM. Addressing these challenges often requires efficient\nadaptation strategies; therefore, we conduct an extensive examination of\nexisting Parameter-Efficient Fine-tuning (PEFT) methods and explore their\napplicability within the FL framework. To rigorously evaluate the performance\nof FedLLM, we undertake a thorough review of existing fine-tuning datasets and\nevaluation benchmarks. Furthermore, we discuss FedLLM's diverse real-world\napplications across multiple domains. Finally, we identify critical open\nchallenges and outline promising research directions to foster future\nadvancements in FedLLM. This survey aims to serve as a foundational resource\nfor researchers and practitioners, offering valuable insights into the rapidly\nevolving landscape of federated fine-tuning for LLMs. It also establishes a\nroadmap for future innovations in privacy-preserving AI. We actively maintain a\nGitHub repo\n\\href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}\nto track cutting-edge advancements in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive success across\nvarious tasks. Integrating LLMs with Federated Learning (FL), a paradigm known\nas FedLLM, offers a promising avenue for collaborative model adaptation while\npreserving data privacy. This survey provides a systematic and comprehensive\nreview of FedLLM. We begin by tracing the historical development of both LLMs\nand FL, summarizing relevant prior research to set the context. Subsequently,\nwe delve into an in-depth analysis of the fundamental challenges inherent in\ndeploying FedLLM. Addressing these challenges often requires efficient\nadaptation strategies; therefore, we conduct an extensive examination of\nexisting Parameter-Efficient Fine-tuning (PEFT) methods and explore their\napplicability within the FL framework. To rigorously evaluate the performance\nof FedLLM, we undertake a thorough review of existing fine-tuning datasets and\nevaluation benchmarks. Furthermore, we discuss FedLLM's diverse real-world\napplications across multiple domains. Finally, we identify critical open\nchallenges and outline promising research directions to foster future\nadvancements in FedLLM. This survey aims to serve as a foundational resource\nfor researchers and practitioners, offering valuable insights into the rapidly\nevolving landscape of federated fine-tuning for LLMs. It also establishes a\nroadmap for future innovations in privacy-preserving AI. We actively maintain a\nGitHub repo\n\\href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}\nto track cutting-edge advancements in this field."
                },
                "authors": [
                    {
                        "name": "Yebo Wu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Jingguang Li"
                    },
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Kahou Tam"
                    },
                    {
                        "name": "Zhanting Zhou"
                    },
                    {
                        "name": "Haicheng Liao"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12286v2",
                "updated": "2025-06-27T07:41:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    41,
                    49,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-14T00:25:26Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    25,
                    26,
                    5,
                    165,
                    0
                ],
                "title": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason"
                },
                "summary": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone, and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. A similar pattern is also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench-Verified than on other similar coding benchmarks.\nThese findings raise concerns about the validity of existing results and\nunderscore the need for more robust, contamination-resistant benchmarks to\nreliably evaluate LLMs' coding abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce two diagnostic\ntasks: file path identification from issue descriptions alone, and ground truth\nfunction reproduction with only the current file context and issue description\nto probe models' underlying knowledge. We present empirical evidence that\nperformance gains on SWE-Bench-Verified may be partially driven by memorization\nrather than genuine problem-solving. We show that state-of-the-art models\nachieve up to 76% accuracy in identifying buggy file paths using only issue\ndescriptions, without access to repository structure. This performance is\nmerely up to 53% on tasks from repositories not included in SWE-Bench, pointing\nto possible data contamination or memorization. A similar pattern is also\nobserved for the function reproduction task, where the verbatim similarity is\nmuch higher on SWE-Bench-Verified than on other similar coding benchmarks.\nThese findings raise concerns about the validity of existing results and\nunderscore the need for more robust, contamination-resistant benchmarks to\nreliably evaluate LLMs' coding abilities."
                },
                "authors": [
                    {
                        "name": "Shanchao Liang"
                    },
                    {
                        "name": "Spandan Garg"
                    },
                    {
                        "name": "Roshanak Zilouchian Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Roshanak Zilouchian Moghaddam"
                },
                "author": "Roshanak Zilouchian Moghaddam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21974v1",
                "updated": "2025-06-27T07:32:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    32,
                    16,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T07:32:16Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    32,
                    16,
                    4,
                    178,
                    0
                ],
                "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism"
                },
                "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation."
                },
                "authors": [
                    {
                        "name": "Simon MÃ¼nker"
                    },
                    {
                        "name": "Nils Schwager"
                    },
                    {
                        "name": "Achim Rettinger"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettinger"
                },
                "author": "Achim Rettinger",
                "arxiv_comment": "11 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02384v2",
                "updated": "2025-06-27T07:30:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    30,
                    35,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-04T15:02:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    2,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAIR: Improving Safety Alignment with Introspective Reasoning"
                },
                "summary": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Siyuan Zhang"
                    },
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Zeyu Xia"
                    },
                    {
                        "name": "Zhengwei Fang"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "22 pages, 8 figures, ICML2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21972v1",
                "updated": "2025-06-27T07:26:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    26,
                    33,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T07:26:33Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    26,
                    33,
                    4,
                    178,
                    0
                ],
                "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM\n  Vulnerabilities and Bypassing Modern Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM\n  Vulnerabilities and Bypassing Modern Defenses"
                },
                "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries."
                },
                "authors": [
                    {
                        "name": "Mohamed Ahmed"
                    },
                    {
                        "name": "Mohamed Abdelmouty"
                    },
                    {
                        "name": "Mingyu Kim"
                    },
                    {
                        "name": "Gunvanth Kandula"
                    },
                    {
                        "name": "Alex Park"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v6",
                "updated": "2025-06-27T07:21:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    21,
                    49,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Multilingual Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Multilingual Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nmultilingual Contrastive framework that aligns the internal forward process of\nother languages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nmultilingual Contrastive framework that aligns the internal forward process of\nother languages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research."
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24007v2",
                "updated": "2025-06-27T07:20:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    20,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-29T21:09:34Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    21,
                    9,
                    34,
                    3,
                    149,
                    0
                ],
                "title": "Preemptive Hallucination Reduction: An Input-Level Approach for\n  Multimodal Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preemptive Hallucination Reduction: An Input-Level Approach for\n  Multimodal Language Model"
                },
                "summary": "Visual hallucinations in Large Language Models (LLMs), where the model\ngenerates responses that are inconsistent with the visual input, pose a\nsignificant challenge to their reliability, particularly in contexts where\nprecise and trustworthy outputs are critical. Current research largely\nemphasizes post-hoc correction or model-specific fine-tuning strategies, with\nlimited exploration of preprocessing techniques to address hallucination issues\nat the input stage. This study presents a novel ensemble-based preprocessing\nframework that adaptively selects the most appropriate filtering approach --\nnoise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the\ntype of question posed, resulting into reduced hallucination without requiring\nany modifications to the underlying model architecture or training pipeline.\nEvaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal\nreasoning on visually complex inputs, our method achieves a 44.3% reduction in\nhallucination rates, as measured by Natural Language Inference (NLI) scores\nusing SelfCheckGPT. This demonstrates that intelligent input conditioning alone\ncan significantly enhance factual grounding in LLM responses. The findings\nhighlight the importance of adaptive preprocessing techniques in mitigating\nhallucinations, paving the way for more reliable multimodal systems capable of\naddressing real-world challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual hallucinations in Large Language Models (LLMs), where the model\ngenerates responses that are inconsistent with the visual input, pose a\nsignificant challenge to their reliability, particularly in contexts where\nprecise and trustworthy outputs are critical. Current research largely\nemphasizes post-hoc correction or model-specific fine-tuning strategies, with\nlimited exploration of preprocessing techniques to address hallucination issues\nat the input stage. This study presents a novel ensemble-based preprocessing\nframework that adaptively selects the most appropriate filtering approach --\nnoise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the\ntype of question posed, resulting into reduced hallucination without requiring\nany modifications to the underlying model architecture or training pipeline.\nEvaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal\nreasoning on visually complex inputs, our method achieves a 44.3% reduction in\nhallucination rates, as measured by Natural Language Inference (NLI) scores\nusing SelfCheckGPT. This demonstrates that intelligent input conditioning alone\ncan significantly enhance factual grounding in LLM responses. The findings\nhighlight the importance of adaptive preprocessing techniques in mitigating\nhallucinations, paving the way for more reliable multimodal systems capable of\naddressing real-world challenges."
                },
                "authors": [
                    {
                        "name": "Nokimul Hasan Arif"
                    },
                    {
                        "name": "Shadman Rabby"
                    },
                    {
                        "name": "Md Hefzul Hossain Papon"
                    },
                    {
                        "name": "Sabbir Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Sabbir Ahmed"
                },
                "author": "Sabbir Ahmed",
                "arxiv_comment": "Submitted for review in NCAA Springer, 21 pages, 4 figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05609v2",
                "updated": "2025-06-27T07:16:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    16,
                    41,
                    4,
                    178,
                    0
                ],
                "published": "2024-08-10T18:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    18,
                    23,
                    59,
                    5,
                    223,
                    0
                ],
                "title": "Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at\n  Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at\n  Scale"
                },
                "summary": "The sheer scale and diversity of transportation make it a formidable sector\nto decarbonize. Here, we consider an emerging opportunity to reduce carbon\nemissions: the growing adoption of semi-autonomous vehicles, which can be\nprogrammed to mitigate stop-and-go traffic through intelligent speed commands\nand, thus, reduce emissions. But would such dynamic eco-driving move the needle\non climate change? A comprehensive impact analysis has been out of reach due to\nthe vast array of traffic scenarios and the complexity of vehicle emissions. We\naddress this challenge with large-scale scenario modeling efforts and by using\nmulti-task deep reinforcement learning with a carefully designed network\ndecomposition strategy. We perform an in-depth prospective impact assessment of\ndynamic eco-driving at 6,011 signalized intersections across three major US\nmetropolitan cities, simulating a million traffic scenarios. Overall, we find\nthat vehicle trajectories optimized for emissions can cut city-wide\nintersection carbon emissions by 11-22%, without harming throughput or safety,\nand with reasonable assumptions, equivalent to the national emissions of Israel\nand Nigeria, respectively. We find that 10% eco-driving adoption yields 25%-50%\nof the total reduction, and nearly 70% of the benefits come from 20% of\nintersections, suggesting near-term implementation pathways. However, the\ncomposition of this high-impact subset of intersections varies considerably\nacross different adoption levels, with minimal overlap, calling for careful\nstrategic planning for eco-driving deployments. Moreover, the impact of\neco-driving, when considered jointly with projections of vehicle\nelectrification and hybrid vehicle adoption remains significant. More broadly,\nthis work paves the way for large-scale analysis of traffic externalities, such\nas time, safety, and air quality, and the potential impact of solution\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sheer scale and diversity of transportation make it a formidable sector\nto decarbonize. Here, we consider an emerging opportunity to reduce carbon\nemissions: the growing adoption of semi-autonomous vehicles, which can be\nprogrammed to mitigate stop-and-go traffic through intelligent speed commands\nand, thus, reduce emissions. But would such dynamic eco-driving move the needle\non climate change? A comprehensive impact analysis has been out of reach due to\nthe vast array of traffic scenarios and the complexity of vehicle emissions. We\naddress this challenge with large-scale scenario modeling efforts and by using\nmulti-task deep reinforcement learning with a carefully designed network\ndecomposition strategy. We perform an in-depth prospective impact assessment of\ndynamic eco-driving at 6,011 signalized intersections across three major US\nmetropolitan cities, simulating a million traffic scenarios. Overall, we find\nthat vehicle trajectories optimized for emissions can cut city-wide\nintersection carbon emissions by 11-22%, without harming throughput or safety,\nand with reasonable assumptions, equivalent to the national emissions of Israel\nand Nigeria, respectively. We find that 10% eco-driving adoption yields 25%-50%\nof the total reduction, and nearly 70% of the benefits come from 20% of\nintersections, suggesting near-term implementation pathways. However, the\ncomposition of this high-impact subset of intersections varies considerably\nacross different adoption levels, with minimal overlap, calling for careful\nstrategic planning for eco-driving deployments. Moreover, the impact of\neco-driving, when considered jointly with projections of vehicle\nelectrification and hybrid vehicle adoption remains significant. More broadly,\nthis work paves the way for large-scale analysis of traffic externalities, such\nas time, safety, and air quality, and the potential impact of solution\nstrategies."
                },
                "authors": [
                    {
                        "name": "Vindula Jayawardana"
                    },
                    {
                        "name": "Baptiste Freydt"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Cameron Hickert"
                    },
                    {
                        "name": "Edgar Sanchez"
                    },
                    {
                        "name": "Catherine Tang"
                    },
                    {
                        "name": "Mark Taylor"
                    },
                    {
                        "name": "Blaine Leonard"
                    },
                    {
                        "name": "Cathy Wu"
                    }
                ],
                "author_detail": {
                    "name": "Cathy Wu"
                },
                "author": "Cathy Wu",
                "arxiv_comment": "Accepted for publication at Transportation Research Part C: Emerging\n  Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21967v1",
                "updated": "2025-06-27T07:13:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    13,
                    29,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T07:13:29Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    13,
                    29,
                    4,
                    178,
                    0
                ],
                "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM\n  Agents"
                },
                "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Hanchao Liu"
                    },
                    {
                        "name": "Sai Zhou"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21964v1",
                "updated": "2025-06-27T07:11:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    11,
                    55,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T07:11:55Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    11,
                    55,
                    4,
                    178,
                    0
                ],
                "title": "Using Large Language Models to Suggest Informative Prior Distributions\n  in Bayesian Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Suggest Informative Prior Distributions\n  in Bayesian Statistics"
                },
                "summary": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence."
                },
                "authors": [
                    {
                        "name": "Michael A. Riegler"
                    },
                    {
                        "name": "Kristoffer Herland Hellton"
                    },
                    {
                        "name": "Vajira Thambawita"
                    },
                    {
                        "name": "Hugo L. Hammer"
                    }
                ],
                "author_detail": {
                    "name": "Hugo L. Hammer"
                },
                "author": "Hugo L. Hammer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21961v1",
                "updated": "2025-06-27T07:09:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    9,
                    11,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T07:09:11Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    9,
                    11,
                    4,
                    178,
                    0
                ],
                "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large\n  Language Models Based on ERG Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large\n  Language Models Based on ERG Theory"
                },
                "summary": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please."
                },
                "authors": [
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Yeon Su Park"
                    },
                    {
                        "name": "Sunwoo Kim"
                    },
                    {
                        "name": "Shin Yoo"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11095v3",
                "updated": "2025-06-27T06:52:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    52,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-16T12:18:40Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    18,
                    40,
                    6,
                    47,
                    0
                ],
                "title": "A Survey of Large Language Models in Psychotherapy: Current Landscape\n  and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models in Psychotherapy: Current Landscape\n  and Future Directions"
                },
                "summary": "Mental health is increasingly critical in contemporary healthcare, with\npsychotherapy demanding dynamic, context-sensitive interactions that\ntraditional NLP methods struggle to capture. Large Language Models (LLMs) offer\nsignificant potential for addressing this gap due to their ability to handle\nextensive context and multi-turn reasoning. This review introduces a conceptual\ntaxonomy dividing psychotherapy into interconnected stages--assessment,\ndiagnosis, and treatment--to systematically examine LLM advancements and\nchallenges. Our comprehensive analysis reveals imbalances in current research,\nsuch as a focus on common disorders, linguistic biases, fragmented methods, and\nlimited theoretical integration. We identify critical challenges including\ncapturing dynamic symptom fluctuations, overcoming linguistic and cultural\nbiases, and ensuring diagnostic reliability. Highlighting future directions, we\nadvocate for continuous multi-stage modeling, real-time adaptive systems\ngrounded in psychological theory, and diversified research covering broader\nmental disorders and therapeutic approaches, aiming toward more holistic and\nclinically integrated psychotherapy LLMs systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health is increasingly critical in contemporary healthcare, with\npsychotherapy demanding dynamic, context-sensitive interactions that\ntraditional NLP methods struggle to capture. Large Language Models (LLMs) offer\nsignificant potential for addressing this gap due to their ability to handle\nextensive context and multi-turn reasoning. This review introduces a conceptual\ntaxonomy dividing psychotherapy into interconnected stages--assessment,\ndiagnosis, and treatment--to systematically examine LLM advancements and\nchallenges. Our comprehensive analysis reveals imbalances in current research,\nsuch as a focus on common disorders, linguistic biases, fragmented methods, and\nlimited theoretical integration. We identify critical challenges including\ncapturing dynamic symptom fluctuations, overcoming linguistic and cultural\nbiases, and ensuring diagnostic reliability. Highlighting future directions, we\nadvocate for continuous multi-stage modeling, real-time adaptive systems\ngrounded in psychological theory, and diversified research covering broader\nmental disorders and therapeutic approaches, aiming toward more holistic and\nclinically integrated psychotherapy LLMs systems."
                },
                "authors": [
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Beibei Yu"
                    },
                    {
                        "name": "Lilin Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "John Torous"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16589v2",
                "updated": "2025-06-27T06:44:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    44,
                    48,
                    4,
                    178,
                    0
                ],
                "published": "2024-10-22T00:14:36Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    0,
                    14,
                    36,
                    1,
                    296,
                    0
                ],
                "title": "Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis\n  with Large Language Models"
                },
                "summary": "Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs."
                },
                "authors": [
                    {
                        "name": "Hongcheng Ding"
                    },
                    {
                        "name": "Fuzhen Hu"
                    },
                    {
                        "name": "Ruiting Deng"
                    },
                    {
                        "name": "Xuanze Zhao"
                    },
                    {
                        "name": "Shamsul Nahar Abdullah"
                    },
                    {
                        "name": "Deshinta Arrova Dewi"
                    }
                ],
                "author_detail": {
                    "name": "Deshinta Arrova Dewi"
                },
                "author": "Deshinta Arrova Dewi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15533v3",
                "updated": "2025-06-27T06:14:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    14,
                    36,
                    4,
                    178,
                    0
                ],
                "published": "2024-08-28T04:44:43Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    44,
                    43,
                    2,
                    241,
                    0
                ],
                "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Congqing He"
                    },
                    {
                        "name": "Xiaochen Xie"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11856v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11856v3",
                "updated": "2025-06-27T06:13:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    13,
                    14,
                    4,
                    178,
                    0
                ],
                "published": "2024-08-15T19:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    13,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "Dynamic Adaptive Optimization for Effective Sentiment Analysis\n  Fine-Tuning on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptive Optimization for Effective Sentiment Analysis\n  Fine-Tuning on Large Language Models"
                },
                "summary": "Sentiment analysis plays a crucial role in various domains, such as business\nintelligence and financial forecasting. Large language models (LLMs) have\nbecome a popular paradigm for sentiment analysis, leveraging multi-task\nlearning to address specific tasks concurrently. However, LLMs with fine-tuning\nfor sentiment analysis often underperforms due to the inherent challenges in\nmanaging diverse task complexities. Moreover, constant-weight approaches in\nmulti-task learning struggle to adapt to variations in data characteristics,\nfurther complicating model effectiveness. To address these issues, we propose a\nnovel multi-task learning framework with a dynamic adaptive optimization (DAO)\nmodule. This module is designed as a plug-and-play component that can be\nseamlessly integrated into existing models, providing an effective and flexible\nsolution for multi-task learning. The key component of the DAO module is\ndynamic adaptive loss, which dynamically adjusts the weights assigned to\ndifferent tasks based on their relative importance and data characteristics\nduring training. Sentiment analyses on a standard and customized financial text\ndataset demonstrate that the proposed framework achieves superior performance.\nSpecifically, this work improves the Mean Squared Error (MSE) and Accuracy\n(ACC) by 15.58% and 1.24% respectively, compared with previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment analysis plays a crucial role in various domains, such as business\nintelligence and financial forecasting. Large language models (LLMs) have\nbecome a popular paradigm for sentiment analysis, leveraging multi-task\nlearning to address specific tasks concurrently. However, LLMs with fine-tuning\nfor sentiment analysis often underperforms due to the inherent challenges in\nmanaging diverse task complexities. Moreover, constant-weight approaches in\nmulti-task learning struggle to adapt to variations in data characteristics,\nfurther complicating model effectiveness. To address these issues, we propose a\nnovel multi-task learning framework with a dynamic adaptive optimization (DAO)\nmodule. This module is designed as a plug-and-play component that can be\nseamlessly integrated into existing models, providing an effective and flexible\nsolution for multi-task learning. The key component of the DAO module is\ndynamic adaptive loss, which dynamically adjusts the weights assigned to\ndifferent tasks based on their relative importance and data characteristics\nduring training. Sentiment analyses on a standard and customized financial text\ndataset demonstrate that the proposed framework achieves superior performance.\nSpecifically, this work improves the Mean Squared Error (MSE) and Accuracy\n(ACC) by 15.58% and 1.24% respectively, compared with previous work."
                },
                "authors": [
                    {
                        "name": "Hongcheng Ding"
                    },
                    {
                        "name": "Xuanze Zhao"
                    },
                    {
                        "name": "Ruiting Deng"
                    },
                    {
                        "name": "Shamsul Nahar Abdullah"
                    },
                    {
                        "name": "Deshinta Arrova Dewi"
                    },
                    {
                        "name": "Zixiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zixiao Jiang"
                },
                "author": "Zixiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11856v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11856v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18343v2",
                "updated": "2025-06-27T06:11:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    11,
                    26,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-23T06:52:38Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    52,
                    38,
                    0,
                    174,
                    0
                ],
                "title": "TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for\n  Exploration and Rescue Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for\n  Exploration and Rescue Operations"
                },
                "summary": "The increasing demand for underwater exploration and rescue operations\nenforces the development of advanced wireless or semi-wireless underwater\nvessels equipped with manipulator arms. This paper presents the implementation\nof a semi-wireless underwater vehicle, \"TritonZ\" equipped with a manipulator\narm, tailored for effective underwater exploration and rescue operations. The\nvehicle's compact design enables deployment in different submarine\nsurroundings, addressing the need for wireless systems capable of navigating\nchallenging underwater terrains. The manipulator arm can interact with the\nenvironment, allowing the robot to perform sophisticated tasks during\nexploration and rescue missions in emergency situations. TritonZ is equipped\nwith various sensors such as Pi-Camera, Humidity, and Temperature sensors to\nsend real-time environmental data. Our underwater vehicle controlled using a\ncustomized remote controller can navigate efficiently in the water where\nPi-Camera enables live streaming of the surroundings. Motion control and video\ncapture are performed simultaneously using this camera. The manipulator arm is\ndesigned to perform various tasks, similar to grasping, manipulating, and\ncollecting underwater objects. Experimental results shows the efficacy of the\nproposed remotely operated vehicle in performing a variety of underwater\nexploration and rescue tasks. Additionally, the results show that TritonZ can\nmaintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.\nFurthermore, the vehicle can sustain waves underwater by maintaining its\nposition as well as average velocity. The full project details and source code\ncan be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for underwater exploration and rescue operations\nenforces the development of advanced wireless or semi-wireless underwater\nvessels equipped with manipulator arms. This paper presents the implementation\nof a semi-wireless underwater vehicle, \"TritonZ\" equipped with a manipulator\narm, tailored for effective underwater exploration and rescue operations. The\nvehicle's compact design enables deployment in different submarine\nsurroundings, addressing the need for wireless systems capable of navigating\nchallenging underwater terrains. The manipulator arm can interact with the\nenvironment, allowing the robot to perform sophisticated tasks during\nexploration and rescue missions in emergency situations. TritonZ is equipped\nwith various sensors such as Pi-Camera, Humidity, and Temperature sensors to\nsend real-time environmental data. Our underwater vehicle controlled using a\ncustomized remote controller can navigate efficiently in the water where\nPi-Camera enables live streaming of the surroundings. Motion control and video\ncapture are performed simultaneously using this camera. The manipulator arm is\ndesigned to perform various tasks, similar to grasping, manipulating, and\ncollecting underwater objects. Experimental results shows the efficacy of the\nproposed remotely operated vehicle in performing a variety of underwater\nexploration and rescue tasks. Additionally, the results show that TritonZ can\nmaintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.\nFurthermore, the vehicle can sustain waves underwater by maintaining its\nposition as well as average velocity. The full project details and source code\ncan be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ"
                },
                "authors": [
                    {
                        "name": "Kawser Ahmed"
                    },
                    {
                        "name": "Mir Shahriar Fardin"
                    },
                    {
                        "name": "Md Arif Faysal Nayem"
                    },
                    {
                        "name": "Fahim Hafiz"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    }
                ],
                "author_detail": {
                    "name": "Swakkhar Shatabda"
                },
                "author": "Swakkhar Shatabda",
                "arxiv_doi": "10.1109/ICCIT64611.2024.11022145",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICCIT64611.2024.11022145",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.18343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 5 figures",
                "arxiv_journal_ref": "In: 2024 27th International Conference on Computer and Information\n  Technology (ICCIT), Cox's Bazar, Bangladesh, 20-22 December 2024, pp. 417-422",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21934v1",
                "updated": "2025-06-27T06:09:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    9,
                    56,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T06:09:56Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    9,
                    56,
                    4,
                    178,
                    0
                ],
                "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware\n  Layout Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware\n  Layout Design"
                },
                "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation."
                },
                "authors": [
                    {
                        "name": "Najmeh Forouzandehmehr"
                    },
                    {
                        "name": "Reza Yousefi Maragheh"
                    },
                    {
                        "name": "Sriram Kollipara"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Topojoy Biswas"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.3; I.2.11; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21931v1",
                "updated": "2025-06-27T05:45:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    5,
                    45,
                    59,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T05:45:59Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    5,
                    45,
                    59,
                    4,
                    178,
                    0
                ],
                "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization."
                },
                "authors": [
                    {
                        "name": "Reza Yousefi Maragheh"
                    },
                    {
                        "name": "Pratheek Vadla"
                    },
                    {
                        "name": "Priyank Gupta"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Aysenur Inan"
                    },
                    {
                        "name": "Kehui Yao"
                    },
                    {
                        "name": "Jianpeng Xu"
                    },
                    {
                        "name": "Praveen Kanumala"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sushant Kumar"
                },
                "author": "Sushant Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21924v1",
                "updated": "2025-06-27T05:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    5,
                    34,
                    57,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T05:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    5,
                    34,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D\n  Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D\n  Visual Grounding"
                },
                "summary": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy."
                },
                "authors": [
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21898v1",
                "updated": "2025-06-27T04:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    35,
                    52,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:35:52Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    35,
                    52,
                    4,
                    178,
                    0
                ],
                "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems."
                },
                "authors": [
                    {
                        "name": "Aimen Gaba"
                    },
                    {
                        "name": "Emily Wall"
                    },
                    {
                        "name": "Tejas Ramkumar Babu"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "Kyle Hall"
                    },
                    {
                        "name": "Cindy Xiong Bearfield"
                    }
                ],
                "author_detail": {
                    "name": "Cindy Xiong Bearfield"
                },
                "author": "Cindy Xiong Bearfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01463v2",
                "updated": "2025-06-27T04:34:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    34,
                    30,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-01T19:17:20Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    17,
                    20,
                    3,
                    121,
                    0
                ],
                "title": "Enhancing Cloud Security through Topic Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cloud Security through Topic Modelling"
                },
                "summary": "Protecting cloud applications is critical in an era where security threats\nare increasingly sophisticated and persistent. Continuous Integration and\nContinuous Deployment (CI/CD) pipelines are particularly vulnerable, making\ninnovative security approaches essential. This research explores the\napplication of Natural Language Processing (NLP) techniques, specifically Topic\nModelling, to analyse security-related text data and anticipate potential\nthreats. We focus on Latent Dirichlet Allocation (LDA) and Probabilistic Latent\nSemantic Analysis (PLSA) to extract meaningful patterns from data sources,\nincluding logs, reports, and deployment traces. Using the Gensim framework in\nPython, these methods categorise log entries into security-relevant topics\n(e.g., phishing, encryption failures). The identified topics are leveraged to\nhighlight patterns indicative of security issues across CI/CD's continuous\nstages (build, test, deploy). This approach introduces a semantic layer that\nsupports early vulnerability recognition and contextual understanding of\nruntime behaviours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting cloud applications is critical in an era where security threats\nare increasingly sophisticated and persistent. Continuous Integration and\nContinuous Deployment (CI/CD) pipelines are particularly vulnerable, making\ninnovative security approaches essential. This research explores the\napplication of Natural Language Processing (NLP) techniques, specifically Topic\nModelling, to analyse security-related text data and anticipate potential\nthreats. We focus on Latent Dirichlet Allocation (LDA) and Probabilistic Latent\nSemantic Analysis (PLSA) to extract meaningful patterns from data sources,\nincluding logs, reports, and deployment traces. Using the Gensim framework in\nPython, these methods categorise log entries into security-relevant topics\n(e.g., phishing, encryption failures). The identified topics are leveraged to\nhighlight patterns indicative of security issues across CI/CD's continuous\nstages (build, test, deploy). This approach introduces a semantic layer that\nsupports early vulnerability recognition and contextual understanding of\nruntime behaviours."
                },
                "authors": [
                    {
                        "name": "Sabbir M. Saleh"
                    },
                    {
                        "name": "Nazim Madhavji"
                    },
                    {
                        "name": "John Steinbacher"
                    }
                ],
                "author_detail": {
                    "name": "John Steinbacher"
                },
                "author": "John Steinbacher",
                "arxiv_comment": "7 pages, 5 figures, 28th ACIS International Winter Conference on\n  Software Engineering, Artificial Intelligence, Networking and\n  Parallel/Distributed Computing (SNPD 2024-Winter)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20118v4",
                "updated": "2025-06-27T04:01:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    1,
                    12,
                    4,
                    178,
                    0
                ],
                "published": "2025-04-28T08:04:44Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    4,
                    44,
                    0,
                    118,
                    0
                ],
                "title": "OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese\n  Medicine Knowledge Retrieval and Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese\n  Medicine Knowledge Retrieval and Diagnosis"
                },
                "summary": "Traditional Chinese Medicine (TCM) represents a rich repository of ancient\nmedical knowledge that continues to play an important role in modern\nhealthcare. Due to the complexity and breadth of the TCM literature, the\nintegration of AI technologies is critical for its modernization and broader\naccessibility. However, this integration poses considerable challenges,\nincluding the interpretation of obscure classical Chinese texts and the\nmodeling of intricate semantic relationships among TCM concepts. In this paper,\nwe develop OpenTCM, an LLM-based system that combines a domain-specific TCM\nknowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).\nFirst, we extract more than 3.73 million classical Chinese characters from 68\ngynecological books in the Chinese Medical Classics Database, with the help of\nTCM and gynecology experts. Second, we construct a comprehensive\nmulti-relational knowledge graph comprising more than 48,000 entities and\n152,000 interrelationships, using customized prompts and Chinese-oriented LLMs\nsuch as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,\nwe empower OpenTCM with GraphRAG, enabling high-fidelity ingredient knowledge\nretrieval and diagnostic question-answering without model fine-tuning.\nExperimental evaluations demonstrate that OpenTCM achieves mean expert scores\n(MES) of 4.378 in ingredient information retrieval and 4.045 in diagnostic\nquestion-answering tasks, outperforming state-of-the-art solutions in\nreal-world TCM use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Chinese Medicine (TCM) represents a rich repository of ancient\nmedical knowledge that continues to play an important role in modern\nhealthcare. Due to the complexity and breadth of the TCM literature, the\nintegration of AI technologies is critical for its modernization and broader\naccessibility. However, this integration poses considerable challenges,\nincluding the interpretation of obscure classical Chinese texts and the\nmodeling of intricate semantic relationships among TCM concepts. In this paper,\nwe develop OpenTCM, an LLM-based system that combines a domain-specific TCM\nknowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).\nFirst, we extract more than 3.73 million classical Chinese characters from 68\ngynecological books in the Chinese Medical Classics Database, with the help of\nTCM and gynecology experts. Second, we construct a comprehensive\nmulti-relational knowledge graph comprising more than 48,000 entities and\n152,000 interrelationships, using customized prompts and Chinese-oriented LLMs\nsuch as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,\nwe empower OpenTCM with GraphRAG, enabling high-fidelity ingredient knowledge\nretrieval and diagnostic question-answering without model fine-tuning.\nExperimental evaluations demonstrate that OpenTCM achieves mean expert scores\n(MES) of 4.378 in ingredient information retrieval and 4.045 in diagnostic\nquestion-answering tasks, outperforming state-of-the-art solutions in\nreal-world TCM use cases."
                },
                "authors": [
                    {
                        "name": "Jinglin He"
                    },
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Lai Kwan Lam"
                    },
                    {
                        "name": "Waikei Leung"
                    },
                    {
                        "name": "Lixing He"
                    },
                    {
                        "name": "Yuanan Jiang"
                    },
                    {
                        "name": "Chi Chiu Wang"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Hongkai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Chen"
                },
                "author": "Hongkai Chen",
                "arxiv_comment": "8 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18746v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18746v4",
                "updated": "2025-06-27T03:58:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    58,
                    25,
                    4,
                    178,
                    0
                ],
                "published": "2025-05-24T15:25:44Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    15,
                    25,
                    44,
                    5,
                    144,
                    0
                ],
                "title": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking"
                },
                "summary": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/TencentHunyuan/C3-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/TencentHunyuan/C3-Benchmark."
                },
                "authors": [
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18746v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18746v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20758v2",
                "updated": "2025-06-27T03:53:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    53,
                    26,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-28T06:20:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    6,
                    20,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "Collective Reasoning Among LLMs: A Framework for Answer Validation\n  Without Ground Truth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective Reasoning Among LLMs: A Framework for Answer Validation\n  Without Ground Truth"
                },
                "summary": "We introduce a new approach in which several advanced large language\nmodels-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct,\nClaude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer\nintricate, doctoral-level probability problems without relying on any single\n\"correct\" reference. Rather than depending on an established ground truth, our\ninvestigation focuses on how agreement among diverse models can signal the\nreliability of their outputs and, by extension, reflect the overall quality of\nthe generated questions. To measure this inter-model alignment, we apply a\nsuite of statistical evaluations, including chi-square tests, Fleiss' Kappa\ncoefficients, and confidence interval calculations, thereby capturing both\nprecision in answers and clarity in question phrasing. Our analysis reveals\nthat Claude and Gemini tend to frame questions more coherently and\nunambiguously, which is evidenced by their tighter confidence intervals and\ngreater concordance with responding agents. In contrast, LLAMA exhibits wider\nconfidence bands and a lower level of agreement, indicating more variability\nand reduced consistency in its question formulations. These observations\nsupport the notion that a multi-model collaborative strategy not only improves\nanswer dependability but also offers an effective, data-driven mechanism for\nevaluating and refining question quality when no definitive solution exists.\nUltimately, this work delivers actionable insights into enhancing AI-guided\nreasoning processes through coordinated interactions among heterogeneous\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new approach in which several advanced large language\nmodels-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct,\nClaude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer\nintricate, doctoral-level probability problems without relying on any single\n\"correct\" reference. Rather than depending on an established ground truth, our\ninvestigation focuses on how agreement among diverse models can signal the\nreliability of their outputs and, by extension, reflect the overall quality of\nthe generated questions. To measure this inter-model alignment, we apply a\nsuite of statistical evaluations, including chi-square tests, Fleiss' Kappa\ncoefficients, and confidence interval calculations, thereby capturing both\nprecision in answers and clarity in question phrasing. Our analysis reveals\nthat Claude and Gemini tend to frame questions more coherently and\nunambiguously, which is evidenced by their tighter confidence intervals and\ngreater concordance with responding agents. In contrast, LLAMA exhibits wider\nconfidence bands and a lower level of agreement, indicating more variability\nand reduced consistency in its question formulations. These observations\nsupport the notion that a multi-model collaborative strategy not only improves\nanswer dependability but also offers an effective, data-driven mechanism for\nevaluating and refining question quality when no definitive solution exists.\nUltimately, this work delivers actionable insights into enhancing AI-guided\nreasoning processes through coordinated interactions among heterogeneous\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Seyed Pouyan Mousavi Davoudi"
                    },
                    {
                        "name": "Amin Gholami Davodi"
                    },
                    {
                        "name": "Alireza Amiri-Margavi"
                    },
                    {
                        "name": "Mahdi Jafari"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Jafari"
                },
                "author": "Mahdi Jafari",
                "arxiv_comment": "7pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21885v1",
                "updated": "2025-06-27T03:43:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    48,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T03:43:48Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    48,
                    4,
                    178,
                    0
                ],
                "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for\n  Intelligent Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for\n  Intelligent Vehicles"
                },
                "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving."
                },
                "authors": [
                    {
                        "name": "Chuheng Wei"
                    },
                    {
                        "name": "Ziye Qin"
                    },
                    {
                        "name": "Ziyan Zhang"
                    },
                    {
                        "name": "Guoyuan Wu"
                    },
                    {
                        "name": "Matthew J. Barth"
                    }
                ],
                "author_detail": {
                    "name": "Matthew J. Barth"
                },
                "author": "Matthew J. Barth",
                "arxiv_comment": "Accepted by IEEE IV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]